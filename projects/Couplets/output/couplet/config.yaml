dataloader:
  test:
  - _target_: libai.data.build_nlp_test_loader
    dataset: {_target_: dataset.dataset.CoupletsDataset, is_train: false, maxlen: 64, path: /dataset/1564ee80/v3/couplets}
    num_workers: 4
  train:
    _target_: libai.data.build.build_nlp_train_loader
    dataset:
    - {_target_: dataset.dataset.CoupletsDataset, is_train: true, maxlen: 64, path: /dataset/1564ee80/v3/couplets}
    num_workers: 4
graph:
  auto_parallel: {enabled: false, mainstem_algo: true, prune_parallel_cast_ops: false, sbp_collector: false}
  debug: -1
  enabled: true
  eval_graph: {_target_: libai.models.utils.GraphBase, is_train: false}
  train_graph: {_target_: libai.models.utils.GraphBase, is_train: true}
model:
  _target_: modeling.model.Seq2Seq
  cfg: {apply_query_key_layer_scaling: true, attention_dropout_prob: 0.1, bias_dropout_fusion: false, bias_gelu_fusion: false, embedding_dropout_prob: 0.1, hidden_dropout_prob: 0.1, hidden_layers: 6, hidden_size: 512, initializer_range: 0.02, intermediate_size: 512, layernorm_epsilon: 1.0e-05, max_position_embeddings: 64, num_attention_heads: 8, scale_mask_softmax_fusion: false, vocab_size: 9027}
optim:
  _target_: oneflow.nn.optimizer.adamw.AdamW
  betas: [0.9, 0.999]
  do_bias_correction: true
  eps: 1.0e-08
  lr: 0.0001
  params: {_target_: libai.optim.get_default_optimizer_params, clip_grad_max_norm: 1.0, clip_grad_norm_type: 2.0, weight_decay_bias: 0.0, weight_decay_norm: 0.0}
  weight_decay: 0.01
train:
  activation_checkpoint: {enabled: false}
  amp: {enabled: true}
  checkpointer: {max_to_keep: 100, period: 5000}
  consumed_train_samples: 0
  consumed_valid_samples: 0
  dist: {data_parallel_size: 1, num_gpus_per_node: 1, num_nodes: 1, pipeline_num_layers: 10000, pipeline_parallel_size: 1, tensor_parallel_size: 1}
  eval_period: 100
  evaluation: {enabled: false}
  global_batch_size: 128
  input_placement_device: cuda
  load_weight: ''
  log_period: 10
  nccl_fusion_max_ops: 24
  nccl_fusion_threshold_mb: 16
  num_accumulation_steps: 1
  output_dir: output/couplet/
  rdma_enabled: true
  recompute_grad: {enabled: true}
  resume: false
  samples: 0
  scheduler: {_target_: libai.scheduler.WarmupCosineLR, alpha: 0.01, warmup_factor: 0.001, warmup_method: linear}
  seed: 1234
  start_iter: 0
  test_micro_batch_size: 32
  topk: [1]
  train_epoch: 20
  train_iter: 0
  train_micro_batch_size: 128
  train_samples: null
  warmup_ratio: 0.01
  zero_optimization: {enabled: false, stage: 1}
transformer_cfg: {apply_query_key_layer_scaling: true, attention_dropout_prob: 0.1, bias_dropout_fusion: false, bias_gelu_fusion: false, embedding_dropout_prob: 0.1, hidden_dropout_prob: 0.1, hidden_layers: 6, hidden_size: 512, initializer_range: 0.02, intermediate_size: 512, layernorm_epsilon: 1.0e-05, max_position_embeddings: 64, num_attention_heads: 8, scale_mask_softmax_fusion: false, vocab_size: 9027}
