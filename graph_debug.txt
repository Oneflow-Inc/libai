
RWKV_HEAD_QK_DIM 256

RWKV_HEAD_QK_DIM 256


[32m[08/24 02:43:17 libai]: [0mRank of current process: 0. World size: 2
[32m[08/24 02:43:17 libai]: [0mCommand line arguments: Namespace(config_file='projects/RWKV_v4/configs/config_test.py', eval_only=False, fast_dev_run=False, opts=[], resume=False)
[32m[08/24 02:43:17 libai]: [0mContents of args.config_file=projects/RWKV_v4/configs/config_test.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15momegaconf[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mOmegaConf[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mget_config[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mtokenizer[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mGPT2Tokenizer[39m
[38;5;242m# é…ç½® dataloader `build_image_train_loader` å’Œ `build_image_test_loader` æ˜¯ LiBai æä¾›çš„ç”¨äºŽåˆ›å»ºå›¾åƒæ•°æ®çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›† DataLoader çš„ä¸¤ä¸ªå‡½æ•°[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbuild[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mbuild_nlp_test_loader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mbuild_nlp_train_loader[39m
[38;5;197mimport[39m[38;5;15m [39m[38;5;15moneflow[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mflow[39m

[38;5;242m# é…ç½® model[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mprojects[39m[38;5;15m.[39m[38;5;15mRWKV_v4[39m[38;5;15m.[39m[38;5;15mmodeling[39m[38;5;15m.[39m[38;5;15mmodel[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mGPT[39m[38;5;15m [39m[38;5;15m,[39m[38;5;15mGPTConfig[39m
[38;5;242m# å¯¼å…¥è‡ªå®šä¹‰çš„ dataset[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mprojects[39m[38;5;15m.[39m[38;5;15mRWKV_v4[39m[38;5;15m.[39m[38;5;15mdataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mRWKVDataset[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mprojects[39m[38;5;15m.[39m[38;5;15mRWKV_v4[39m[38;5;15m.[39m[38;5;15mutils[39m[38;5;15m.[39m[38;5;15mconfig_optimizer[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mget_RWKV_v4_config_optim[39m


[38;5;15mtest[39m[38;5;197m=[39m[38;5;15mOmegaConf[39m[38;5;197m.[39m[38;5;15mcreate[39m[38;5;15m([39m[38;5;15m)[39m
[38;5;15mtest[39m[38;5;197m.[39m[38;5;15menable[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mweight_style[39m[38;5;197m=[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;186m"[39m[38;5;186mpytorch[39m[38;5;186m"[39m
[38;5;15m)[39m
[38;5;15mtest[39m[38;5;197m.[39m[38;5;15mpath[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186m/home/zhangxiaoyu/RWKV-LM/RWKV-v4/for_load.pth[39m[38;5;186m"[39m

[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/models/graph.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mgraph[39m

[38;5;15mgraph[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m
[38;5;15mgraph[39m[38;5;197m.[39m[38;5;15mdebug[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m2[39m

[38;5;242m# optim = get_config("common/optim.py").optim[39m
[38;5;15moptim[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mflow[39m[38;5;197m.[39m[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mAdam[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mparams[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mget_RWKV_v4_config_optim[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mlr[39m[38;5;197m=[39m[38;5;141m8e-4[39m[38;5;15m,[39m
[38;5;15m)[39m


[38;5;242m# é…ç½®model[39m
[38;5;15mmodel[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mGPT[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mvocab_size[39m[38;5;197m=[39m[38;5;141m6064[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mctx_len[39m[38;5;197m=[39m[38;5;141m1024[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mmodel_type[39m[38;5;197m=[39m[38;5;186m'[39m[38;5;186mRWKV[39m[38;5;186m'[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mn_layer[39m[38;5;197m=[39m[38;5;141m6[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mn_embd[39m[38;5;197m=[39m[38;5;141m1024[39m
[38;5;15m)[39m

[38;5;242m# è®­ç»ƒè¿‡ç¨‹[39m
[38;5;15mtrain[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mget_config[39m[38;5;15m([39m[38;5;186m"[39m[38;5;186mcommon/train.py[39m[38;5;186m"[39m[38;5;15m)[39m[38;5;197m.[39m[38;5;15mtrain[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15minput_placement_device[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mcpu[39m[38;5;186m"[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m6[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m4[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mscheduler[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mflow[39m[38;5;197m.[39m[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr_scheduler[39m[38;5;197m.[39m[38;5;15mStepLR[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mstep_size[39m[38;5;197m=[39m[38;5;141m1000[39m[38;5;15m,[39m[38;5;15m [39m
[38;5;15m        [39m[38;5;15mgamma[39m[38;5;197m=[39m[38;5;141m1.0[39m
[38;5;15m)[39m[38;5;15m [39m

[38;5;242m# false = fp32[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m

[38;5;15mdatafile[39m[38;5;197m=[39m[38;5;186m"[39m[38;5;186m/home/zhangxiaoyu/RWKV-LM/data/enwik8[39m[38;5;186m"[39m
[38;5;242m# èŽ·å¾—ä¸€ä¸ª DataLoader çš„é…ç½®å¯¹è±¡[39m
[38;5;15mdataloader[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mOmegaConf[39m[38;5;197m.[39m[38;5;15mcreate[39m[38;5;15m([39m[38;5;15m)[39m
[38;5;15mdataloader[39m[38;5;197m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mbuild_nlp_train_loader[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;15mdataset[39m[38;5;197m=[39m[38;5;15m[[39m
[38;5;15m        [39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mRWKVDataset[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m            [39m[38;5;15mdata_dir[39m[38;5;197m=[39m[38;5;15mdatafile[39m[38;5;15m,[39m
[38;5;15m            [39m[38;5;15mctx_len[39m[38;5;197m=[39m[38;5;141m1024[39m[38;5;15m,[39m
[38;5;15m            [39m[38;5;15mepoch_length_fixed[39m[38;5;197m=[39m[38;5;141m9996[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m][39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mnum_workers[39m[38;5;197m=[39m[38;5;141m4[39m[38;5;15m,[39m
[38;5;15m)[39m

[38;5;242m# train.train_iter=3[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_epoch[39m[38;5;197m=[39m[38;5;141m1[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15moutput_dir[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186moutput/rwkv_output_loss_compare[39m[38;5;186m"[39m
[38;5;242m# train.load_weight = "/home/zhangxiaoyu/RWKV-LM/libai/projects/RWKV_v4/model/output_model/" # é‡‡ç”¨åŒä¸€ä¸ªmodelè¿›è¡Œåˆå§‹åŒ–[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mrdma_enabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m

[38;5;242m# model.cfg.hidden_dropout_prob= 0.0 # å…³é—­æ‰€æœ‰çš„dropout[39m
[38;5;242m# model.cfg.attention_probs_dropout_prob= 0.0[39m
[38;5;242m# model.cfg.bias_dropout_fusion= False[39m

[38;5;242m# train.dist.pipeline_parallel_size=2[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mevaluation[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m

[38;5;242m# train.train_iter=5[39m
[38;5;242m# train.dist.tensor_parallel_size = 2  # å¹¶è¡Œåº¦ä¸º 4 çš„æ¨¡åž‹å¹¶è¡Œ[39m
[38;5;242m# train.dist.tensor_parallel_size = 4  # å¹¶è¡Œåº¦ä¸º 4 çš„æ¨¡åž‹å¹¶è¡Œ[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mactivation_checkpoint[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m [39m

[32m[08/24 02:43:17 libai]: [0mFull config saved to output/rwkv_output_loss_compare/config.yaml
[32m[08/24 02:43:17 lb.engine.default]: [0m> compiling dataset index builder ...
make: Entering directory '/home/zhangxiaoyu/libai/libai/data/data_utils'
make: Nothing to be done for 'default'.
make: Leaving directory '/home/zhangxiaoyu/libai/libai/data/data_utils'
[32m[08/24 02:43:17 lb.engine.default]: [0m>>> done with dataset index builder. Compilation time: 0.057 seconds
[32m[08/24 02:43:17 lb.engine.default]: [0m>>> done with compiling. Compilation time: 0.059 seconds
[32m[08/24 02:43:17 lb.engine.default]: [0mPrepare training, validating, testing set
building token list... building token list... data has 99621832 tokens, 6064 unique.
data has 99621832 tokens, 6064 unique.
[32m[08/24 02:43:19 lb.engine.default]: [0mAuto-scaling the config to train.train_iter=10000, train.warmup_iter=0
[32m[08/24 02:43:22 lb.engine.default]: [0mModel:
GPT(
  (emb): VocabEmbedding(num_embeddings=6064, embedding_dim=1024)
  (blocks): Sequential(
    (0): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (1): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (2): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (3): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (4): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
    (5): Block(
      (ln1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (ln2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      (att): RWKV_TimeMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (output): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)
      )
      (ffn): RWKV_ChannelMix(
        (time_shift): ZeroPad2d()
        (key): Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)
        (receptance): Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)
        (value): Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)
      )
    )
  )
  (ln_out): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (head): Linear1D(in_features=1024, out_features=6064, bias=False, parallel=row)
  (head_q): Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)
  (head_k): Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)
)
[32m[08/24 02:43:22 lb.engine.default]: [0mGraph debug mode on, automatically output debug info.
[32m[08/24 02:43:22 lb.engine.default]: [0mGraph debug mode on, automatically output debug info.
[32m[08/24 02:43:23 lb.engine.trainer]: [0mStarting training from iteration 0
(GRAPH:GraphBase_0:GraphBase) start building graph.
(GRAPH:GraphBase_0:GraphBase) start building graph builders of parameters and buffers.
(GRAPH:GraphBase_0:GraphBase) end building graph builders of parameters and buffers.
(GRAPH:GraphBase_0:GraphBase) start building graph inputs.
I20220824 02:43:23.489161 1392289 lazy_op_interpreter.cpp:403] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "_GraphBase_0_input.1.0_idx"
device_tag: "cpu"
scope_symbol_id: 131
input_conf {
  out: "out"
  blob_conf {
    shape {
      dim: 8
      dim: 1024
    }
    data_type: kInt64
    is_dynamic: false
    nd_sbp {
      sbp_parallel {
        split_parallel {
          axis: 0
        }
      }
    }
  }
}

I20220824 02:43:23.489503 1392289 lazy_op_interpreter.cpp:406] Lazy nn.Graph name GraphBase_0 add op : 
_GraphBase_0_input.1.0_idx
(INPUT:_GraphBase_0_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
I20220824 02:43:23.489766 1392289 lazy_op_interpreter.cpp:403] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "_GraphBase_0_input.1.1_targets"
device_tag: "cpu"
scope_symbol_id: 131
input_conf {
  out: "out"
  blob_conf {
    shape {
      dim: 8
      dim: 1024
    }
    data_type: kInt64
    is_dynamic: false
    nd_sbp {
      sbp_parallel {
        split_parallel {
          axis: 0
        }
      }
    }
  }
}

I20220824 02:43:23.489872 1392289 lazy_op_interpreter.cpp:406] Lazy nn.Graph name GraphBase_0 add op : 
_GraphBase_0_input.1.1_targets
(INPUT:_GraphBase_0_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
(GRAPH:GraphBase_0:GraphBase) end building graph inputs.
(GRAPH:GraphBase_0:GraphBase) start building graph modules.
[32m[08/24 02:43:23 lb.models.utils.graph_base]: [0mStart compling the train graph which may take some time. Please wait for a moment ...
(MODULE:model:GPT())
(INPUT:_model_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
       dtype=oneflow.int64))
(INPUT:_model_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
       dtype=oneflow.int64))
(BUFFER:model.copy_mask:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32))
I20220824 02:43:23.491492 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.emb.weight"
device_tag: "cuda"
scope_symbol_id: 139
variable_conf {
  out: "out"
  shape {
    dim: 6064
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.491787 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.emb.weight
(MODULE:model.emb:VocabEmbedding(num_embeddings=6064, embedding_dim=1024))
(INPUT:_model.emb_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
       dtype=oneflow.int64))
(PARAMETER:model.emb.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
       grad_fn=<accumulate_grad>))
I20220824 02:43:23.493257 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.emb-gather-0"
device_tag: "cuda"
scope_symbol_id: 143
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 309; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/embedding.py\': line 160; ... 7 more"
user_conf {
  op_type_name: "gather"
  input {
    key: "in"
    value {
      s: "model.emb.weight/out"
    }
  }
  input {
    key: "indices"
    value {
      s: "_GraphBase_0_input.1.0_idx/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  input_order: "indices"
  output_order: "out"
}

I20220824 02:43:23.493664 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.emb-gather-0
(OUTPUT:_model.emb_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(MODULE:model.blocks:Sequential())
(INPUT:_model.blocks_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(MODULE:model.blocks.0:Block())
(INPUT:_model.blocks.0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(MODULE:model.blocks.0.ln0:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.0.ln0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(PARAMETER:model.blocks.0.ln0.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ln0.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.496912 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln0.weight"
device_tag: "cuda"
scope_symbol_id: 150
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.497071 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0.weight
I20220824 02:43:23.497870 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln0.bias"
device_tag: "cuda"
scope_symbol_id: 154
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.498023 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0.bias
I20220824 02:43:23.498322 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-layer_norm-1"
device_tag: "cuda"
scope_symbol_id: 157
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 215; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.0.ln0.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln0.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.498848 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-layer_norm-1
(OUTPUT:_model.blocks.0.ln0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.0.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.0.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.0.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.500499 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 162
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.500640 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1.weight
I20220824 02:43:23.501394 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 166
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.501538 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1.bias
I20220824 02:43:23.501746 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-layer_norm-2"
device_tag: "cuda"
scope_symbol_id: 169
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.0.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.502251 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-layer_norm-2
(OUTPUT:_model.blocks.0.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.0.att:RWKV_TimeMix())
(INPUT:_model.blocks.0.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.0.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.0.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.0.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.504073 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.time_shift-pad-3"
device_tag: "cuda"
scope_symbol_id: 174
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.504331 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_shift-pad-3
(OUTPUT:_model.blocks.0.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.505056 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 178
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.505172 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_mix_k
I20220824 02:43:23.505338 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-4"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-4/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.505578 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-4
I20220824 02:43:23.505743 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-5"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-5/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.505975 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-5
I20220824 02:43:23.506096 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_add-6"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-5/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_add-6/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.506309 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_add-6
I20220824 02:43:23.506433 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-7"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-6/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-7/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.506651 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-7
I20220824 02:43:23.506774 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-8"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-4/z_0"
      s: "model.blocks.0.att-broadcast_mul-7/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-8/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.506994 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-8
I20220824 02:43:23.507611 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 197
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.507725 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_mix_v
I20220824 02:43:23.507864 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-9"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-9/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.508091 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-9
I20220824 02:43:23.508229 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-10"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-10/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.508428 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-10
I20220824 02:43:23.508531 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_add-11"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-10/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_add-11/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.508731 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_add-11
I20220824 02:43:23.508848 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-12"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-11/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-12/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.509055 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-12
I20220824 02:43:23.509172 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-13"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-9/z_0"
      s: "model.blocks.0.att-broadcast_mul-12/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-13/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.509374 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-13
I20220824 02:43:23.509984 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 216
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.510098 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_mix_r
I20220824 02:43:23.510246 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-14"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-14/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.510476 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-14
I20220824 02:43:23.510614 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-15"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-15/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.510838 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-15
I20220824 02:43:23.510941 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_add-16"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-15/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.511152 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_add-16
I20220824 02:43:23.511265 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-17"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-17/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.511492 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-17
I20220824 02:43:23.511610 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-18"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-14/z_0"
      s: "model.blocks.0.att-broadcast_mul-17/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-18/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.511816 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-18
(MODULE:model.blocks.0.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.512984 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 236
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.513104 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key.weight
I20220824 02:43:23.513526 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-hierarchical_parallel_cast-19"
device_tag: "cuda"
scope_symbol_id: 239
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-add_n-8/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.513744 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-hierarchical_parallel_cast-19
I20220824 02:43:23.513954 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-broadcast_matmul-20"
device_tag: "cuda"
scope_symbol_id: 239
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul-20/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.514232 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-broadcast_matmul-20
(OUTPUT:_model.blocks.0.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.0.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.515445 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 247
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.515563 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value.weight
I20220824 02:43:23.516011 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-hierarchical_parallel_cast-21"
device_tag: "cuda"
scope_symbol_id: 250
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-add_n-13/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.516225 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-hierarchical_parallel_cast-21
I20220824 02:43:23.516376 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-broadcast_matmul-22"
device_tag: "cuda"
scope_symbol_id: 250
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul-22/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.516640 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-broadcast_matmul-22
(OUTPUT:_model.blocks.0.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.0.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.517891 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 258
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.518003 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance.weight
I20220824 02:43:23.518391 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23"
device_tag: "cuda"
scope_symbol_id: 261
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-add_n-18/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.518607 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-hierarchical_parallel_cast-23
I20220824 02:43:23.518750 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-broadcast_matmul-24"
device_tag: "cuda"
scope_symbol_id: 261
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-24/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.519007 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-broadcast_matmul-24
(OUTPUT:_model.blocks.0.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.519296 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-sigmoid_v2-25"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-24/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-sigmoid_v2-25/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.519490 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-sigmoid_v2-25
I20220824 02:43:23.520112 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_decay"
device_tag: "cuda"
scope_symbol_id: 271
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.520223 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_decay
I20220824 02:43:23.520809 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.time_first"
device_tag: "cuda"
scope_symbol_id: 275
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.520922 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_first
I20220824 02:43:23.521116 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-wkv-26"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "wkv"
  input {
    key: "k"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul-20/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.0.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul-22/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.0.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-wkv-26/y_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  output_order: "y"
}

I20220824 02:43:23.521387 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-wkv-26
I20220824 02:43:23.521518 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-exp-27"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "exp"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-exp-27/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.521687 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-exp-27
I20220824 02:43:23.521791 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-28"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-exp-27/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-28/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: -1
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.522002 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-28
I20220824 02:43:23.522125 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-29"
device_tag: "cuda"
scope_symbol_id: 181
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-sigmoid_v2-25/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-wkv-26/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-29/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.522336 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-29
(MODULE:model.blocks.0.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.0.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
(PARAMETER:model.blocks.0.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.523485 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 292
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.523603 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output.weight
I20220824 02:43:23.524027 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-hierarchical_parallel_cast-30"
device_tag: "cuda"
scope_symbol_id: 295
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-29/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-hierarchical_parallel_cast-30/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.524238 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-hierarchical_parallel_cast-30
I20220824 02:43:23.524384 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-broadcast_matmul-31"
device_tag: "cuda"
scope_symbol_id: 295
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att.output-hierarchical_parallel_cast-30/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-31/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.524643 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-broadcast_matmul-31
(OUTPUT:_model.blocks.0.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.0.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.524947 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0-add_n-32"
device_tag: "cuda"
scope_symbol_id: 301
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
      s: "model.blocks.0.att.output-broadcast_matmul-31/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0-add_n-32/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.525183 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0-add_n-32
(MODULE:model.blocks.0.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.0.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.526382 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 306
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.526499 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2.weight
I20220824 02:43:23.527127 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 310
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.527242 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2.bias
I20220824 02:43:23.527408 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-layer_norm-33"
device_tag: "cuda"
scope_symbol_id: 313
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.0.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-32/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.527782 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-layer_norm-33
(OUTPUT:_model.blocks.0.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.0.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.0.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.0.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.0.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.0.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.0.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.529214 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.time_shift-pad-34"
device_tag: "cuda"
scope_symbol_id: 318
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-34/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.529459 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_shift-pad-34
(OUTPUT:_model.blocks.0.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.530146 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 322
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.530268 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_mix_k
I20220824 02:43:23.530403 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-35"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-35/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.530632 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-35
I20220824 02:43:23.530768 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-36"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-36/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.530993 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-36
I20220824 02:43:23.531100 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_add-37"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-36/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_add-37/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.531301 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_add-37
I20220824 02:43:23.531423 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-38"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-34/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-37/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-38/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.531653 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-38
I20220824 02:43:23.531780 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-39"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-35/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-38/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-39/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.531992 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-39
I20220824 02:43:23.532616 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 341
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.532739 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_mix_r
I20220824 02:43:23.532869 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-40"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-40/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.533088 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-40
I20220824 02:43:23.533224 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-41"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-41/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.533435 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-41
I20220824 02:43:23.533535 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_add-42"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-41/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_add-42/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.533749 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_add-42
I20220824 02:43:23.533869 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-43"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-34/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-42/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-43/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.534097 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-43
I20220824 02:43:23.534212 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-44"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-40/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-43/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-44/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.534417 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-44
(MODULE:model.blocks.0.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.0.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.535583 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 361
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.535701 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key.weight
I20220824 02:43:23.536088 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-hierarchical_parallel_cast-45"
device_tag: "cuda"
scope_symbol_id: 364
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-add_n-39/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-hierarchical_parallel_cast-45/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.536293 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-hierarchical_parallel_cast-45
I20220824 02:43:23.536439 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-broadcast_matmul-46"
device_tag: "cuda"
scope_symbol_id: 364
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn.key-hierarchical_parallel_cast-45/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-46/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.536705 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-broadcast_matmul-46
(OUTPUT:_model.blocks.0.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.536981 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-relu-47"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-46/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-relu-47/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.537175 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-relu-47
I20220824 02:43:23.537318 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-square-48"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-relu-47/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-square-48/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.537493 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-square-48
(MODULE:model.blocks.0.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.0.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.0.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.538638 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 378
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.538751 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value.weight
I20220824 02:43:23.539140 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-hierarchical_parallel_cast-49"
device_tag: "cuda"
scope_symbol_id: 381
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-square-48/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-hierarchical_parallel_cast-49/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.539358 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-hierarchical_parallel_cast-49
I20220824 02:43:23.539505 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-broadcast_matmul-50"
device_tag: "cuda"
scope_symbol_id: 381
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn.value-hierarchical_parallel_cast-49/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-50/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.539774 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-broadcast_matmul-50
(OUTPUT:_model.blocks.0.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.0.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.0.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.0.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.540982 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.0.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 389
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.541093 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance.weight
I20220824 02:43:23.541494 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-51"
device_tag: "cuda"
scope_symbol_id: 392
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-add_n-44/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-51/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.541700 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-hierarchical_parallel_cast-51
I20220824 02:43:23.541843 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-broadcast_matmul-52"
device_tag: "cuda"
scope_symbol_id: 392
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-51/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-52/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.542111 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-broadcast_matmul-52
(OUTPUT:_model.blocks.0.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.542343 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-sigmoid_v2-53"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-52/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2-53/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.542536 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-sigmoid_v2-53
I20220824 02:43:23.542649 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-54"
device_tag: "cuda"
scope_symbol_id: 325
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2-53/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-50/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-54/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.542860 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-54
(OUTPUT:_model.blocks.0.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 02:43:23.543087 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0-add_n-55"
device_tag: "cuda"
scope_symbol_id: 301
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0-add_n-32/out_0"
      s: "model.blocks.0.ffn-broadcast_mul-54/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0-add_n-55/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.543308 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0-add_n-55
(OUTPUT:_model.blocks.0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.1:Block())
(INPUT:_model.blocks.1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.1.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.1.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.545049 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 410
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.545166 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1.weight
I20220824 02:43:23.545758 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 414
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.545874 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1.bias
I20220824 02:43:23.546027 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-layer_norm-56"
device_tag: "cuda"
scope_symbol_id: 417
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.1.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-55/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.546401 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-layer_norm-56
(OUTPUT:_model.blocks.1.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.1.att:RWKV_TimeMix())
(INPUT:_model.blocks.1.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.1.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.1.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.1.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.547818 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.time_shift-pad-57"
device_tag: "cuda"
scope_symbol_id: 422
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.548055 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_shift-pad-57
(OUTPUT:_model.blocks.1.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.548774 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 426
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.548890 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_mix_k
I20220824 02:43:23.549023 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-58"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-58/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.549257 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-58
I20220824 02:43:23.549396 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-59"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-59/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.549615 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-59
I20220824 02:43:23.549722 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_add-60"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-59/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_add-60/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.549926 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_add-60
I20220824 02:43:23.550045 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-61"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-60/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-61/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.550269 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-61
I20220824 02:43:23.550384 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-62"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-58/z_0"
      s: "model.blocks.1.att-broadcast_mul-61/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-62/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.550594 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-62
I20220824 02:43:23.551184 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 445
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.551295 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_mix_v
I20220824 02:43:23.551429 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-63"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-63/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.551659 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-63
I20220824 02:43:23.553315 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-64"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-64/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.553651 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-64
I20220824 02:43:23.553771 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_add-65"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-64/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_add-65/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.553985 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_add-65
I20220824 02:43:23.554118 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-66"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-65/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-66/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.554353 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-66
I20220824 02:43:23.554477 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-67"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-63/z_0"
      s: "model.blocks.1.att-broadcast_mul-66/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-67/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.554689 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-67
I20220824 02:43:23.555470 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 464
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.555596 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_mix_r
I20220824 02:43:23.555750 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-68"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-68/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.555991 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-68
I20220824 02:43:23.556128 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-69"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-69/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.556355 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-69
I20220824 02:43:23.556463 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_add-70"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-69/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_add-70/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.556669 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_add-70
I20220824 02:43:23.556792 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-71"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-70/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-71/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.557021 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-71
I20220824 02:43:23.557139 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-72"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-68/z_0"
      s: "model.blocks.1.att-broadcast_mul-71/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-72/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.557349 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-72
(MODULE:model.blocks.1.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.558578 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 484
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.558699 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key.weight
I20220824 02:43:23.559118 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.key-hierarchical_parallel_cast-73"
device_tag: "cuda"
scope_symbol_id: 487
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-add_n-62/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.key-hierarchical_parallel_cast-73/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.559331 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key-hierarchical_parallel_cast-73
I20220824 02:43:23.559485 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.key-broadcast_matmul-74"
device_tag: "cuda"
scope_symbol_id: 487
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.key-hierarchical_parallel_cast-73/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-74/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.559757 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key-broadcast_matmul-74
(OUTPUT:_model.blocks.1.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.1.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.560904 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 495
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.561018 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value.weight
I20220824 02:43:23.561429 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.value-hierarchical_parallel_cast-75"
device_tag: "cuda"
scope_symbol_id: 498
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-add_n-67/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.value-hierarchical_parallel_cast-75/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.561647 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value-hierarchical_parallel_cast-75
I20220824 02:43:23.561796 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.value-broadcast_matmul-76"
device_tag: "cuda"
scope_symbol_id: 498
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.value-hierarchical_parallel_cast-75/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-76/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.562055 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value-broadcast_matmul-76
(OUTPUT:_model.blocks.1.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.1.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.563227 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 506
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.563349 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance.weight
I20220824 02:43:23.563751 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-hierarchical_parallel_cast-77"
device_tag: "cuda"
scope_symbol_id: 509
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-add_n-72/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-hierarchical_parallel_cast-77/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.563971 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-hierarchical_parallel_cast-77
I20220824 02:43:23.564116 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-broadcast_matmul-78"
device_tag: "cuda"
scope_symbol_id: 509
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.receptance-hierarchical_parallel_cast-77/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-78/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.564384 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-broadcast_matmul-78
(OUTPUT:_model.blocks.1.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.564630 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-sigmoid_v2-79"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-78/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-sigmoid_v2-79/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.564818 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-sigmoid_v2-79
I20220824 02:43:23.565408 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_decay"
device_tag: "cuda"
scope_symbol_id: 519
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.565521 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_decay
I20220824 02:43:23.566129 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.time_first"
device_tag: "cuda"
scope_symbol_id: 523
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.566242 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_first
I20220824 02:43:23.566385 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-wkv-80"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "wkv"
  input {
    key: "k"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-74/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.1.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-76/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.1.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-wkv-80/y_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  output_order: "y"
}

I20220824 02:43:23.566663 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-wkv-80
I20220824 02:43:23.566766 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-exp-81"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "exp"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-exp-81/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.566926 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-exp-81
I20220824 02:43:23.567025 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-82"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-exp-81/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-82/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: -1
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.567232 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-82
I20220824 02:43:23.567350 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-83"
device_tag: "cuda"
scope_symbol_id: 429
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-sigmoid_v2-79/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-wkv-80/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-83/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.567567 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-83
(MODULE:model.blocks.1.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.1.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
(PARAMETER:model.blocks.1.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.568712 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 540
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.568830 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output.weight
I20220824 02:43:23.569231 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-hierarchical_parallel_cast-84"
device_tag: "cuda"
scope_symbol_id: 543
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-83/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-hierarchical_parallel_cast-84/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.569437 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-hierarchical_parallel_cast-84
I20220824 02:43:23.569581 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-broadcast_matmul-85"
device_tag: "cuda"
scope_symbol_id: 543
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att.output-hierarchical_parallel_cast-84/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-85/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.569881 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-broadcast_matmul-85
(OUTPUT:_model.blocks.1.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.1.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.570194 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1-add_n-86"
device_tag: "cuda"
scope_symbol_id: 549
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0-add_n-55/out_0"
      s: "model.blocks.1.att.output-broadcast_matmul-85/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1-add_n-86/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.570420 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1-add_n-86
(MODULE:model.blocks.1.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.1.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.571578 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 554
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.571698 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2.weight
I20220824 02:43:23.572289 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 558
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.572414 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2.bias
I20220824 02:43:23.572568 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-layer_norm-87"
device_tag: "cuda"
scope_symbol_id: 561
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.1.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-86/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.572933 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-layer_norm-87
(OUTPUT:_model.blocks.1.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.1.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.1.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.1.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.1.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.1.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.1.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.574245 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.time_shift-pad-88"
device_tag: "cuda"
scope_symbol_id: 566
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-88/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.574489 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_shift-pad-88
(OUTPUT:_model.blocks.1.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.575212 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 570
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.575323 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_mix_k
I20220824 02:43:23.575460 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-89"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-89/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.575699 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-89
I20220824 02:43:23.575842 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-90"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-90/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.576087 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-90
I20220824 02:43:23.576193 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_add-91"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-90/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_add-91/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.576397 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_add-91
I20220824 02:43:23.576512 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-92"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-88/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-91/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-92/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.576741 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-92
I20220824 02:43:23.576858 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-93"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-89/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-92/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-93/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.577064 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-93
I20220824 02:43:23.577666 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 589
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.577777 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_mix_r
I20220824 02:43:23.577924 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-94"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-94/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.578159 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-94
I20220824 02:43:23.578294 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-95"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-95/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.578512 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-95
I20220824 02:43:23.578617 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_add-96"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-95/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_add-96/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.578824 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_add-96
I20220824 02:43:23.578941 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-97"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-88/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-96/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-97/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.579165 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-97
I20220824 02:43:23.579289 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-98"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-94/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-97/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-98/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.579514 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-98
(MODULE:model.blocks.1.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.1.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.580686 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 609
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.580802 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key.weight
I20220824 02:43:23.581235 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-hierarchical_parallel_cast-99"
device_tag: "cuda"
scope_symbol_id: 612
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-add_n-93/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-hierarchical_parallel_cast-99/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.581447 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-hierarchical_parallel_cast-99
I20220824 02:43:23.581593 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-broadcast_matmul-100"
device_tag: "cuda"
scope_symbol_id: 612
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn.key-hierarchical_parallel_cast-99/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-100/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.581857 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-broadcast_matmul-100
(OUTPUT:_model.blocks.1.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.582108 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-relu-101"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-100/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-relu-101/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.582296 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-relu-101
I20220824 02:43:23.582413 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-square-102"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-relu-101/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-square-102/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.582585 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-square-102
(MODULE:model.blocks.1.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.1.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.1.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.583664 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 626
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.583789 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value.weight
I20220824 02:43:23.584184 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-hierarchical_parallel_cast-103"
device_tag: "cuda"
scope_symbol_id: 629
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-square-102/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-hierarchical_parallel_cast-103/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.584414 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-hierarchical_parallel_cast-103
I20220824 02:43:23.584560 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-broadcast_matmul-104"
device_tag: "cuda"
scope_symbol_id: 629
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn.value-hierarchical_parallel_cast-103/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-104/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.584832 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-broadcast_matmul-104
(OUTPUT:_model.blocks.1.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.1.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.1.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.1.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.585990 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.1.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 637
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.586103 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance.weight
I20220824 02:43:23.586514 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-105"
device_tag: "cuda"
scope_symbol_id: 640
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-add_n-98/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-105/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.586726 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-hierarchical_parallel_cast-105
I20220824 02:43:23.586870 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-broadcast_matmul-106"
device_tag: "cuda"
scope_symbol_id: 640
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-105/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-106/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.587146 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-broadcast_matmul-106
(OUTPUT:_model.blocks.1.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.587385 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-sigmoid_v2-107"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-106/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2-107/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.587581 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-sigmoid_v2-107
I20220824 02:43:23.587707 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-108"
device_tag: "cuda"
scope_symbol_id: 573
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2-107/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-104/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-108/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.587921 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-108
(OUTPUT:_model.blocks.1.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 02:43:23.588155 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1-add_n-109"
device_tag: "cuda"
scope_symbol_id: 549
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1-add_n-86/out_0"
      s: "model.blocks.1.ffn-broadcast_mul-108/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1-add_n-109/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.588377 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1-add_n-109
(OUTPUT:_model.blocks.1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.2:Block())
(INPUT:_model.blocks.2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.2.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.2.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.590121 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 658
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.590235 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1.weight
I20220824 02:43:23.590850 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 662
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.590963 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1.bias
I20220824 02:43:23.591116 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-layer_norm-110"
device_tag: "cuda"
scope_symbol_id: 665
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.2.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-109/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.591490 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-layer_norm-110
(OUTPUT:_model.blocks.2.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.2.att:RWKV_TimeMix())
(INPUT:_model.blocks.2.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.2.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.2.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.2.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.592900 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.time_shift-pad-111"
device_tag: "cuda"
scope_symbol_id: 670
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.593142 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_shift-pad-111
(OUTPUT:_model.blocks.2.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.593819 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 674
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.593933 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_mix_k
I20220824 02:43:23.594070 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-112"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-112/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.594306 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-112
I20220824 02:43:23.594447 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-113"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-113/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.594669 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-113
I20220824 02:43:23.594777 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_add-114"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-113/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_add-114/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.594980 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_add-114
I20220824 02:43:23.595096 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-115"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-114/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-115/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.595314 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-115
I20220824 02:43:23.595428 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-116"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-112/z_0"
      s: "model.blocks.2.att-broadcast_mul-115/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-116/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.595635 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-116
I20220824 02:43:23.596246 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 693
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.596357 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_mix_v
I20220824 02:43:23.596498 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-117"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-117/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.596725 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-117
I20220824 02:43:23.596863 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-118"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-118/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.597070 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-118
I20220824 02:43:23.597173 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_add-119"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-118/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_add-119/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.597368 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_add-119
I20220824 02:43:23.597491 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-120"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-119/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-120/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.597697 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-120
I20220824 02:43:23.597818 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-121"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-117/z_0"
      s: "model.blocks.2.att-broadcast_mul-120/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-121/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.598021 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-121
I20220824 02:43:23.598610 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 712
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.598726 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_mix_r
I20220824 02:43:23.598870 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-122"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-122/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.599092 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-122
I20220824 02:43:23.599228 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-123"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-123/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.599453 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-123
I20220824 02:43:23.599558 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_add-124"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-123/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_add-124/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.599772 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_add-124
I20220824 02:43:23.599889 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-125"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-124/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-125/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.600116 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-125
I20220824 02:43:23.600235 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-126"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-122/z_0"
      s: "model.blocks.2.att-broadcast_mul-125/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-126/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.600436 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-126
(MODULE:model.blocks.2.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.610069 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 732
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.610196 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key.weight
I20220824 02:43:23.610597 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.key-hierarchical_parallel_cast-127"
device_tag: "cuda"
scope_symbol_id: 735
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-add_n-116/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.key-hierarchical_parallel_cast-127/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.610808 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key-hierarchical_parallel_cast-127
I20220824 02:43:23.610953 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.key-broadcast_matmul-128"
device_tag: "cuda"
scope_symbol_id: 735
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.key-hierarchical_parallel_cast-127/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-128/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.611225 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key-broadcast_matmul-128
(OUTPUT:_model.blocks.2.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.2.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.613855 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 743
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.614073 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value.weight
I20220824 02:43:23.614722 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.value-hierarchical_parallel_cast-129"
device_tag: "cuda"
scope_symbol_id: 746
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-add_n-121/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.value-hierarchical_parallel_cast-129/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.615144 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value-hierarchical_parallel_cast-129
I20220824 02:43:23.615435 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.value-broadcast_matmul-130"
device_tag: "cuda"
scope_symbol_id: 746
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.value-hierarchical_parallel_cast-129/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-130/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.615994 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value-broadcast_matmul-130
(OUTPUT:_model.blocks.2.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.2.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.618342 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 754
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.618558 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance.weight
I20220824 02:43:23.619171 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-hierarchical_parallel_cast-131"
device_tag: "cuda"
scope_symbol_id: 757
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-add_n-126/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-hierarchical_parallel_cast-131/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.619596 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-hierarchical_parallel_cast-131
I20220824 02:43:23.619902 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-broadcast_matmul-132"
device_tag: "cuda"
scope_symbol_id: 757
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.receptance-hierarchical_parallel_cast-131/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-132/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.620441 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-broadcast_matmul-132
(OUTPUT:_model.blocks.2.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.620921 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-sigmoid_v2-133"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-132/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-sigmoid_v2-133/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.621305 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-sigmoid_v2-133
I20220824 02:43:23.622474 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_decay"
device_tag: "cuda"
scope_symbol_id: 767
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.622592 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_decay
I20220824 02:43:23.623172 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.time_first"
device_tag: "cuda"
scope_symbol_id: 771
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.623283 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_first
I20220824 02:43:23.623428 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-wkv-134"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "wkv"
  input {
    key: "k"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-128/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.2.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-130/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.2.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-wkv-134/y_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  output_order: "y"
}

I20220824 02:43:23.623715 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-wkv-134
I20220824 02:43:23.623819 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-exp-135"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "exp"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-exp-135/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.623980 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-exp-135
I20220824 02:43:23.624078 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-136"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-exp-135/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-136/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: -1
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.624289 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-136
I20220824 02:43:23.624413 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-137"
device_tag: "cuda"
scope_symbol_id: 677
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-sigmoid_v2-133/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-wkv-134/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-137/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.624629 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-137
(MODULE:model.blocks.2.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.2.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
(PARAMETER:model.blocks.2.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.625727 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 788
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.625847 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output.weight
I20220824 02:43:23.626247 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-hierarchical_parallel_cast-138"
device_tag: "cuda"
scope_symbol_id: 791
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-137/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-hierarchical_parallel_cast-138/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.626459 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-hierarchical_parallel_cast-138
I20220824 02:43:23.626603 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-broadcast_matmul-139"
device_tag: "cuda"
scope_symbol_id: 791
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att.output-hierarchical_parallel_cast-138/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-139/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.626861 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-broadcast_matmul-139
(OUTPUT:_model.blocks.2.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.2.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.627172 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2-add_n-140"
device_tag: "cuda"
scope_symbol_id: 797
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1-add_n-109/out_0"
      s: "model.blocks.2.att.output-broadcast_matmul-139/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2-add_n-140/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.627398 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2-add_n-140
(MODULE:model.blocks.2.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.2.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.628552 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 802
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.628669 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2.weight
I20220824 02:43:23.629243 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 806
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.629364 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2.bias
I20220824 02:43:23.629518 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-layer_norm-141"
device_tag: "cuda"
scope_symbol_id: 809
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.2.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-140/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.629892 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-layer_norm-141
(OUTPUT:_model.blocks.2.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.2.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.2.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.2.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.2.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.2.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.2.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.631212 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.time_shift-pad-142"
device_tag: "cuda"
scope_symbol_id: 814
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-142/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.631453 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_shift-pad-142
(OUTPUT:_model.blocks.2.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.632134 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 818
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.632252 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_mix_k
I20220824 02:43:23.632392 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-143"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-143/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.632624 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-143
I20220824 02:43:23.632766 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-144"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-144/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.632992 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-144
I20220824 02:43:23.633097 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_add-145"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-144/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_add-145/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.633298 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_add-145
I20220824 02:43:23.633419 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-146"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-142/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-145/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-146/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.633649 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-146
I20220824 02:43:23.633769 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-147"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-143/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-146/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-147/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.633970 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-147
I20220824 02:43:23.635650 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 837
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.635766 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_mix_r
I20220824 02:43:23.635908 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-148"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-148/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.636134 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-148
I20220824 02:43:23.636273 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-149"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-149/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.636493 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-149
I20220824 02:43:23.636593 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_add-150"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-149/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_add-150/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.636801 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_add-150
I20220824 02:43:23.636915 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-151"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-142/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-150/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-151/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.637126 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-151
I20220824 02:43:23.637311 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-152"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-148/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-151/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-152/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.637506 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-152
(MODULE:model.blocks.2.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.2.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.642233 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 857
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.642355 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key.weight
I20220824 02:43:23.643201 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-hierarchical_parallel_cast-153"
device_tag: "cuda"
scope_symbol_id: 860
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-add_n-147/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-hierarchical_parallel_cast-153/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.643414 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-hierarchical_parallel_cast-153
I20220824 02:43:23.643559 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-broadcast_matmul-154"
device_tag: "cuda"
scope_symbol_id: 860
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn.key-hierarchical_parallel_cast-153/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-154/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.643836 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-broadcast_matmul-154
(OUTPUT:_model.blocks.2.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.644083 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-relu-155"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-154/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-relu-155/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.644273 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-relu-155
I20220824 02:43:23.644387 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-square-156"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-relu-155/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-square-156/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.644562 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-square-156
(MODULE:model.blocks.2.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.2.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.2.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.648162 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 874
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.648319 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value.weight
I20220824 02:43:23.648770 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-hierarchical_parallel_cast-157"
device_tag: "cuda"
scope_symbol_id: 877
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-square-156/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-hierarchical_parallel_cast-157/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.649052 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-hierarchical_parallel_cast-157
I20220824 02:43:23.649242 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-broadcast_matmul-158"
device_tag: "cuda"
scope_symbol_id: 877
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn.value-hierarchical_parallel_cast-157/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-158/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.649591 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-broadcast_matmul-158
(OUTPUT:_model.blocks.2.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.2.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.2.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.2.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.651067 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.2.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 885
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.651216 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance.weight
I20220824 02:43:23.651662 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-159"
device_tag: "cuda"
scope_symbol_id: 888
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-add_n-152/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-159/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.651949 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-hierarchical_parallel_cast-159
I20220824 02:43:23.652137 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-broadcast_matmul-160"
device_tag: "cuda"
scope_symbol_id: 888
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-159/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-160/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.652491 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-broadcast_matmul-160
(OUTPUT:_model.blocks.2.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.652813 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-sigmoid_v2-161"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-160/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2-161/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.653065 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-sigmoid_v2-161
I20220824 02:43:23.653219 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-162"
device_tag: "cuda"
scope_symbol_id: 821
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2-161/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-158/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-162/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.653501 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-162
(OUTPUT:_model.blocks.2.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 02:43:23.653795 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2-add_n-163"
device_tag: "cuda"
scope_symbol_id: 797
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2-add_n-140/out_0"
      s: "model.blocks.2.ffn-broadcast_mul-162/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2-add_n-163/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.654080 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2-add_n-163
(OUTPUT:_model.blocks.2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.3:Block())
(INPUT:_model.blocks.3_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.3.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.3.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.656196 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 906
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.656337 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1.weight
I20220824 02:43:23.656980 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 910
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.657115 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1.bias
I20220824 02:43:23.657297 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-layer_norm-164"
device_tag: "cuda"
scope_symbol_id: 913
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.3.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-163/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.657744 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-layer_norm-164
(OUTPUT:_model.blocks.3.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.3.att:RWKV_TimeMix())
(INPUT:_model.blocks.3.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.3.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.3.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.3.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.659351 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.time_shift-pad-165"
device_tag: "cuda"
scope_symbol_id: 918
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.659643 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_shift-pad-165
(OUTPUT:_model.blocks.3.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.660405 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 922
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.660535 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_mix_k
I20220824 02:43:23.660696 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-166"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-166/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.660976 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-166
I20220824 02:43:23.661135 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-167"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-167/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.661401 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-167
I20220824 02:43:23.661525 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_add-168"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-167/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_add-168/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.661767 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_add-168
I20220824 02:43:23.661903 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-169"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-168/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-169/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.662159 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-169
I20220824 02:43:23.662294 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-170"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-166/z_0"
      s: "model.blocks.3.att-broadcast_mul-169/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-170/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.662539 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-170
I20220824 02:43:23.663213 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 941
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.663352 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_mix_v
I20220824 02:43:23.663513 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-171"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-171/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.663785 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-171
I20220824 02:43:23.663949 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-172"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-172/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.664197 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-172
I20220824 02:43:23.664319 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_add-173"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-172/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_add-173/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.664552 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_add-173
I20220824 02:43:23.664695 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-174"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-173/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-174/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.664965 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-174
I20220824 02:43:23.665099 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-175"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-171/z_0"
      s: "model.blocks.3.att-broadcast_mul-174/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-175/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.665339 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-175
I20220824 02:43:23.666044 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 960
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.666175 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_mix_r
I20220824 02:43:23.666314 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-176"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-176/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.666541 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-176
I20220824 02:43:23.666680 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-177"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-177/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.666931 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-177
I20220824 02:43:23.667034 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_add-178"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-177/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_add-178/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.667248 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_add-178
I20220824 02:43:23.667366 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-179"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-178/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-179/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.667580 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-179
I20220824 02:43:23.667699 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-180"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-176/z_0"
      s: "model.blocks.3.att-broadcast_mul-179/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-180/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.667905 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-180
(MODULE:model.blocks.3.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.669101 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 980
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.669224 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key.weight
I20220824 02:43:23.669656 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.key-hierarchical_parallel_cast-181"
device_tag: "cuda"
scope_symbol_id: 983
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-add_n-170/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.key-hierarchical_parallel_cast-181/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.669867 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key-hierarchical_parallel_cast-181
I20220824 02:43:23.670022 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.key-broadcast_matmul-182"
device_tag: "cuda"
scope_symbol_id: 983
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.key-hierarchical_parallel_cast-181/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-182/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.670280 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key-broadcast_matmul-182
(OUTPUT:_model.blocks.3.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.3.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.671577 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 991
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.671700 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value.weight
I20220824 02:43:23.672123 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.value-hierarchical_parallel_cast-183"
device_tag: "cuda"
scope_symbol_id: 994
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-add_n-175/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.value-hierarchical_parallel_cast-183/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.672339 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value-hierarchical_parallel_cast-183
I20220824 02:43:23.672485 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.value-broadcast_matmul-184"
device_tag: "cuda"
scope_symbol_id: 994
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.value-hierarchical_parallel_cast-183/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-184/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.672749 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value-broadcast_matmul-184
(OUTPUT:_model.blocks.3.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.3.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.674015 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1002
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.674137 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance.weight
I20220824 02:43:23.674540 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-hierarchical_parallel_cast-185"
device_tag: "cuda"
scope_symbol_id: 1005
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-add_n-180/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-hierarchical_parallel_cast-185/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.674743 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-hierarchical_parallel_cast-185
I20220824 02:43:23.674896 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-broadcast_matmul-186"
device_tag: "cuda"
scope_symbol_id: 1005
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.receptance-hierarchical_parallel_cast-185/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-186/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.675166 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-broadcast_matmul-186
(OUTPUT:_model.blocks.3.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.675408 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-sigmoid_v2-187"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-186/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-sigmoid_v2-187/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.675602 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-sigmoid_v2-187
I20220824 02:43:23.676180 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_decay"
device_tag: "cuda"
scope_symbol_id: 1015
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.676298 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_decay
I20220824 02:43:23.676925 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.time_first"
device_tag: "cuda"
scope_symbol_id: 1019
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.677047 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_first
I20220824 02:43:23.677196 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-wkv-188"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "wkv"
  input {
    key: "k"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-182/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.3.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-184/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.3.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-wkv-188/y_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  output_order: "y"
}

I20220824 02:43:23.677479 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-wkv-188
I20220824 02:43:23.677584 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-exp-189"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "exp"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-exp-189/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.677748 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-exp-189
I20220824 02:43:23.677846 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-190"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-exp-189/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-190/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: -1
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.678054 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-190
I20220824 02:43:23.678179 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-191"
device_tag: "cuda"
scope_symbol_id: 925
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-sigmoid_v2-187/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-wkv-188/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-191/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.678400 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-191
(MODULE:model.blocks.3.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.3.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
(PARAMETER:model.blocks.3.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.679562 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 1036
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.679677 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output.weight
I20220824 02:43:23.680104 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-hierarchical_parallel_cast-192"
device_tag: "cuda"
scope_symbol_id: 1039
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-191/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-hierarchical_parallel_cast-192/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.680317 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-hierarchical_parallel_cast-192
I20220824 02:43:23.680464 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-broadcast_matmul-193"
device_tag: "cuda"
scope_symbol_id: 1039
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att.output-hierarchical_parallel_cast-192/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-193/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.680745 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-broadcast_matmul-193
(OUTPUT:_model.blocks.3.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.3.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.681051 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3-add_n-194"
device_tag: "cuda"
scope_symbol_id: 1045
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2-add_n-163/out_0"
      s: "model.blocks.3.att.output-broadcast_matmul-193/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3-add_n-194/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.681272 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3-add_n-194
(MODULE:model.blocks.3.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.3.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.682442 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 1050
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.682566 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2.weight
I20220824 02:43:23.683140 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 1054
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.683256 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2.bias
I20220824 02:43:23.683413 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-layer_norm-195"
device_tag: "cuda"
scope_symbol_id: 1057
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.3.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-194/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.683786 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-layer_norm-195
(OUTPUT:_model.blocks.3.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.3.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.3.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.3.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.3.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.3.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.3.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.685077 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.time_shift-pad-196"
device_tag: "cuda"
scope_symbol_id: 1062
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-196/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.685320 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_shift-pad-196
(OUTPUT:_model.blocks.3.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.685983 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1066
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.686105 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_mix_k
I20220824 02:43:23.686273 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-197"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-197/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.686499 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-197
I20220824 02:43:23.686640 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-198"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-198/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.686894 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-198
I20220824 02:43:23.687000 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_add-199"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-198/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_add-199/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.687196 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_add-199
I20220824 02:43:23.687315 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-200"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-196/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-199/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-200/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.687537 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-200
I20220824 02:43:23.687661 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-201"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-197/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-200/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-201/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.687883 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-201
I20220824 02:43:23.688467 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1085
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.688582 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_mix_r
I20220824 02:43:23.688731 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-202"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-202/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.688956 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-202
I20220824 02:43:23.689088 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-203"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-203/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.689306 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-203
I20220824 02:43:23.689411 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_add-204"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-203/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_add-204/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.689620 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_add-204
I20220824 02:43:23.689735 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-205"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-196/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-204/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-205/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.689931 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-205
I20220824 02:43:23.690050 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-206"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-202/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-205/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-206/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.690254 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-206
(MODULE:model.blocks.3.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.3.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.691437 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 1105
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.691555 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key.weight
I20220824 02:43:23.692054 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-hierarchical_parallel_cast-207"
device_tag: "cuda"
scope_symbol_id: 1108
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-add_n-201/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-hierarchical_parallel_cast-207/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.692276 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-hierarchical_parallel_cast-207
I20220824 02:43:23.692428 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-broadcast_matmul-208"
device_tag: "cuda"
scope_symbol_id: 1108
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn.key-hierarchical_parallel_cast-207/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-208/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.692690 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-broadcast_matmul-208
(OUTPUT:_model.blocks.3.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.692940 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-relu-209"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-208/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-relu-209/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.693130 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-relu-209
I20220824 02:43:23.693246 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-square-210"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-relu-209/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-square-210/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.693418 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-square-210
(MODULE:model.blocks.3.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.3.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.3.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.694571 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 1122
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.694689 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value.weight
I20220824 02:43:23.695086 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-hierarchical_parallel_cast-211"
device_tag: "cuda"
scope_symbol_id: 1125
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-square-210/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-hierarchical_parallel_cast-211/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.695304 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-hierarchical_parallel_cast-211
I20220824 02:43:23.695453 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-broadcast_matmul-212"
device_tag: "cuda"
scope_symbol_id: 1125
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn.value-hierarchical_parallel_cast-211/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-212/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.695731 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-broadcast_matmul-212
(OUTPUT:_model.blocks.3.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.3.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.3.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.3.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.696971 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.3.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1133
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.697090 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance.weight
I20220824 02:43:23.697489 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-213"
device_tag: "cuda"
scope_symbol_id: 1136
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-add_n-206/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-213/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.697700 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-hierarchical_parallel_cast-213
I20220824 02:43:23.697846 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-broadcast_matmul-214"
device_tag: "cuda"
scope_symbol_id: 1136
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-213/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-214/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.698122 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-broadcast_matmul-214
(OUTPUT:_model.blocks.3.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.698362 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-sigmoid_v2-215"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-214/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2-215/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.698558 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-sigmoid_v2-215
I20220824 02:43:23.698673 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-216"
device_tag: "cuda"
scope_symbol_id: 1069
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2-215/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-212/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-216/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.698890 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-216
(OUTPUT:_model.blocks.3.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 02:43:23.699118 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3-add_n-217"
device_tag: "cuda"
scope_symbol_id: 1045
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3-add_n-194/out_0"
      s: "model.blocks.3.ffn-broadcast_mul-216/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3-add_n-217/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.699337 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3-add_n-217
(OUTPUT:_model.blocks.3_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.4:Block())
(INPUT:_model.blocks.4_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.4.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.4.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.701259 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 1154
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.701376 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1.weight
I20220824 02:43:23.702047 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 1158
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.702167 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1.bias
I20220824 02:43:23.702314 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-layer_norm-218"
device_tag: "cuda"
scope_symbol_id: 1161
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.4.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-217/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.702689 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-layer_norm-218
(OUTPUT:_model.blocks.4.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.4.att:RWKV_TimeMix())
(INPUT:_model.blocks.4.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.4.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.4.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.4.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.704144 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.time_shift-pad-219"
device_tag: "cuda"
scope_symbol_id: 1166
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.704388 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_shift-pad-219
(OUTPUT:_model.blocks.4.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.705107 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1170
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.705216 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_mix_k
I20220824 02:43:23.705358 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-220"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-220/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.705623 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-220
I20220824 02:43:23.705777 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-221"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-221/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.706001 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-221
I20220824 02:43:23.706104 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_add-222"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-221/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_add-222/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.706310 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_add-222
I20220824 02:43:23.706425 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-223"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-222/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-223/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.706629 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-223
I20220824 02:43:23.706748 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-224"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-220/z_0"
      s: "model.blocks.4.att-broadcast_mul-223/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-224/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.706954 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-224
I20220824 02:43:23.707536 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 1189
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.707654 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_mix_v
I20220824 02:43:23.707806 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-225"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-225/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.708039 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-225
I20220824 02:43:23.708175 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-226"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-226/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.708395 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-226
I20220824 02:43:23.708503 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_add-227"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-226/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_add-227/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.708703 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_add-227
I20220824 02:43:23.708829 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-228"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-227/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-228/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.709044 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-228
I20220824 02:43:23.709158 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-229"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-225/z_0"
      s: "model.blocks.4.att-broadcast_mul-228/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-229/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.709370 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-229
I20220824 02:43:23.709945 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1208
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.710059 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_mix_r
I20220824 02:43:23.710199 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-230"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-230/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.710422 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-230
I20220824 02:43:23.710561 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-231"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-231/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.710786 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-231
I20220824 02:43:23.710891 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_add-232"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-231/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_add-232/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.711095 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_add-232
I20220824 02:43:23.711210 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-233"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-232/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-233/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.711421 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-233
I20220824 02:43:23.711539 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-234"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-230/z_0"
      s: "model.blocks.4.att-broadcast_mul-233/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-234/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.711747 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-234
(MODULE:model.blocks.4.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.712864 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 1228
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.712989 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key.weight
I20220824 02:43:23.713393 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.key-hierarchical_parallel_cast-235"
device_tag: "cuda"
scope_symbol_id: 1231
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-add_n-224/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.key-hierarchical_parallel_cast-235/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.713615 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key-hierarchical_parallel_cast-235
I20220824 02:43:23.713760 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.key-broadcast_matmul-236"
device_tag: "cuda"
scope_symbol_id: 1231
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.key-hierarchical_parallel_cast-235/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-236/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.714025 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key-broadcast_matmul-236
(OUTPUT:_model.blocks.4.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.4.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.715162 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 1239
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.715279 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value.weight
I20220824 02:43:23.715703 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.value-hierarchical_parallel_cast-237"
device_tag: "cuda"
scope_symbol_id: 1242
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-add_n-229/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.value-hierarchical_parallel_cast-237/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.715921 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value-hierarchical_parallel_cast-237
I20220824 02:43:23.716066 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.value-broadcast_matmul-238"
device_tag: "cuda"
scope_symbol_id: 1242
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.value-hierarchical_parallel_cast-237/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-238/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.716336 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value-broadcast_matmul-238
(OUTPUT:_model.blocks.4.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.4.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.717489 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1250
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.717602 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance.weight
I20220824 02:43:23.718001 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-hierarchical_parallel_cast-239"
device_tag: "cuda"
scope_symbol_id: 1253
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-add_n-234/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-hierarchical_parallel_cast-239/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.718216 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-hierarchical_parallel_cast-239
I20220824 02:43:23.718367 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-broadcast_matmul-240"
device_tag: "cuda"
scope_symbol_id: 1253
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.receptance-hierarchical_parallel_cast-239/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-240/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.718632 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-broadcast_matmul-240
(OUTPUT:_model.blocks.4.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.718878 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-sigmoid_v2-241"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-240/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-sigmoid_v2-241/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.719069 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-sigmoid_v2-241
I20220824 02:43:23.719650 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_decay"
device_tag: "cuda"
scope_symbol_id: 1263
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.719770 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_decay
I20220824 02:43:23.720341 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.time_first"
device_tag: "cuda"
scope_symbol_id: 1267
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.720458 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_first
I20220824 02:43:23.720597 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-wkv-242"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "wkv"
  input {
    key: "k"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-236/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.4.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-238/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.4.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-wkv-242/y_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  output_order: "y"
}

I20220824 02:43:23.720867 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-wkv-242
I20220824 02:43:23.720968 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-exp-243"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "exp"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-exp-243/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.721134 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-exp-243
I20220824 02:43:23.721237 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-244"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-exp-243/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-244/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: -1
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.721449 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-244
I20220824 02:43:23.721570 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-245"
device_tag: "cuda"
scope_symbol_id: 1173
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-sigmoid_v2-241/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-wkv-242/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-245/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.721792 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-245
(MODULE:model.blocks.4.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.4.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
(PARAMETER:model.blocks.4.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.722980 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 1284
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.723095 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output.weight
I20220824 02:43:23.723512 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-hierarchical_parallel_cast-246"
device_tag: "cuda"
scope_symbol_id: 1287
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-245/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-hierarchical_parallel_cast-246/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.723735 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-hierarchical_parallel_cast-246
I20220824 02:43:23.723883 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-broadcast_matmul-247"
device_tag: "cuda"
scope_symbol_id: 1287
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att.output-hierarchical_parallel_cast-246/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-247/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.724148 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-broadcast_matmul-247
(OUTPUT:_model.blocks.4.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.4.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.724453 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4-add_n-248"
device_tag: "cuda"
scope_symbol_id: 1293
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3-add_n-217/out_0"
      s: "model.blocks.4.att.output-broadcast_matmul-247/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4-add_n-248/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.724686 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4-add_n-248
(MODULE:model.blocks.4.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.4.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.725901 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 1298
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.726020 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2.weight
I20220824 02:43:23.726635 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 1302
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.726752 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2.bias
I20220824 02:43:23.726912 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-layer_norm-249"
device_tag: "cuda"
scope_symbol_id: 1305
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.4.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-248/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.727290 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-layer_norm-249
(OUTPUT:_model.blocks.4.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.4.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.4.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.4.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.4.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.4.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.4.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.728664 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.time_shift-pad-250"
device_tag: "cuda"
scope_symbol_id: 1310
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-250/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.728911 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_shift-pad-250
(OUTPUT:_model.blocks.4.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.729604 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1314
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.729720 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_mix_k
I20220824 02:43:23.729856 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-251"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-251/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.730087 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-251
I20220824 02:43:23.730227 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-252"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-252/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.730449 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-252
I20220824 02:43:23.730556 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_add-253"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-252/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_add-253/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.730757 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_add-253
I20220824 02:43:23.730877 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-254"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-250/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-253/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-254/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.731115 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-254
I20220824 02:43:23.731235 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-255"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-251/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-254/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-255/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.731444 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-255
I20220824 02:43:23.732048 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1333
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.732165 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_mix_r
I20220824 02:43:23.732311 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-256"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-256/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.732535 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-256
I20220824 02:43:23.732673 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-257"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-257/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.732897 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-257
I20220824 02:43:23.733004 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_add-258"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-257/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_add-258/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.733211 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_add-258
I20220824 02:43:23.733325 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-259"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-250/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-258/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-259/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.733557 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-259
I20220824 02:43:23.733672 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-260"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-256/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-259/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-260/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.733873 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-260
(MODULE:model.blocks.4.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.4.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.734997 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 1353
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.735114 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key.weight
I20220824 02:43:23.735523 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-hierarchical_parallel_cast-261"
device_tag: "cuda"
scope_symbol_id: 1356
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-add_n-255/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-hierarchical_parallel_cast-261/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.735749 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-hierarchical_parallel_cast-261
I20220824 02:43:23.735903 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-broadcast_matmul-262"
device_tag: "cuda"
scope_symbol_id: 1356
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn.key-hierarchical_parallel_cast-261/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-262/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.736168 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-broadcast_matmul-262
(OUTPUT:_model.blocks.4.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.736418 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-relu-263"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-262/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-relu-263/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.736608 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-relu-263
I20220824 02:43:23.736721 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-square-264"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-relu-263/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-square-264/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.736898 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-square-264
(MODULE:model.blocks.4.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.4.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.4.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.737979 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 1370
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.738101 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value.weight
I20220824 02:43:23.738482 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-hierarchical_parallel_cast-265"
device_tag: "cuda"
scope_symbol_id: 1373
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-square-264/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-hierarchical_parallel_cast-265/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.738703 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-hierarchical_parallel_cast-265
I20220824 02:43:23.738853 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-broadcast_matmul-266"
device_tag: "cuda"
scope_symbol_id: 1373
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn.value-hierarchical_parallel_cast-265/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-266/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.739125 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-broadcast_matmul-266
(OUTPUT:_model.blocks.4.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.4.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.4.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.4.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.740267 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.4.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1381
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.740387 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance.weight
I20220824 02:43:23.740780 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-267"
device_tag: "cuda"
scope_symbol_id: 1384
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-add_n-260/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-267/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.740995 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-hierarchical_parallel_cast-267
I20220824 02:43:23.741142 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-broadcast_matmul-268"
device_tag: "cuda"
scope_symbol_id: 1384
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-267/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-268/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.741416 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-broadcast_matmul-268
(OUTPUT:_model.blocks.4.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.741658 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-sigmoid_v2-269"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-268/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2-269/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.741851 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-sigmoid_v2-269
I20220824 02:43:23.741966 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-270"
device_tag: "cuda"
scope_symbol_id: 1317
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2-269/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-266/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-270/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.742182 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-270
(OUTPUT:_model.blocks.4.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 02:43:23.742413 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4-add_n-271"
device_tag: "cuda"
scope_symbol_id: 1293
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4-add_n-248/out_0"
      s: "model.blocks.4.ffn-broadcast_mul-270/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4-add_n-271/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.742635 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4-add_n-271
(OUTPUT:_model.blocks.4_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.5:Block())
(INPUT:_model.blocks.5_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.blocks.5.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.5.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.744338 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln1.weight"
device_tag: "cuda"
scope_symbol_id: 1402
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.744457 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1.weight
I20220824 02:43:23.745016 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln1.bias"
device_tag: "cuda"
scope_symbol_id: 1406
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.745136 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1.bias
I20220824 02:43:23.745290 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-layer_norm-272"
device_tag: "cuda"
scope_symbol_id: 1409
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.5.ln1.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln1.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-271/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.745677 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-layer_norm-272
(OUTPUT:_model.blocks.5.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.5.att:RWKV_TimeMix())
(INPUT:_model.blocks.5.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.5.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.5.att.time_shift:ZeroPad2d())
(INPUT:_model.blocks.5.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.747102 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.time_shift-pad-273"
device_tag: "cuda"
scope_symbol_id: 1414
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 76; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.747346 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_shift-pad-273
(OUTPUT:_model.blocks.5.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.748046 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1418
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.748165 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_mix_k
I20220824 02:43:23.748304 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-274"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-274/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.748543 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-274
I20220824 02:43:23.748687 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-275"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-275/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.748908 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-275
I20220824 02:43:23.749017 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_add-276"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-275/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_add-276/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.749220 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_add-276
I20220824 02:43:23.749342 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-277"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-276/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-277/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.749555 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-277
I20220824 02:43:23.749675 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-278"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-274/z_0"
      s: "model.blocks.5.att-broadcast_mul-277/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-278/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.749886 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-278
I20220824 02:43:23.750470 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_mix_v"
device_tag: "cuda"
scope_symbol_id: 1437
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.750587 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_mix_v
I20220824 02:43:23.750720 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-279"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-279/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.750944 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-279
I20220824 02:43:23.751081 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-280"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_mix_v/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-280/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.751298 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-280
I20220824 02:43:23.751399 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_add-281"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-280/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_add-281/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.751610 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_add-281
I20220824 02:43:23.751739 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-282"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-281/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-282/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.751955 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-282
I20220824 02:43:23.752077 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-283"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 78; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-279/z_0"
      s: "model.blocks.5.att-broadcast_mul-282/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-283/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.752280 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-283
I20220824 02:43:23.752863 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1456
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.752977 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_mix_r
I20220824 02:43:23.753116 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-284"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-284/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.753337 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-284
I20220824 02:43:23.753468 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-285"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-285/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.753687 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-285
I20220824 02:43:23.753791 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_add-286"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-285/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_add-286/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.753993 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_add-286
I20220824 02:43:23.754112 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-287"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-286/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-287/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.754335 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-287
I20220824 02:43:23.754451 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-288"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 79; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-284/z_0"
      s: "model.blocks.5.att-broadcast_mul-287/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-288/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.754659 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-288
(MODULE:model.blocks.5.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.756278 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.key.weight"
device_tag: "cuda"
scope_symbol_id: 1476
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.756404 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key.weight
I20220824 02:43:23.756798 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.key-hierarchical_parallel_cast-289"
device_tag: "cuda"
scope_symbol_id: 1479
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-add_n-278/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.key-hierarchical_parallel_cast-289/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.757012 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key-hierarchical_parallel_cast-289
I20220824 02:43:23.757155 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.key-broadcast_matmul-290"
device_tag: "cuda"
scope_symbol_id: 1479
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 82; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.key-hierarchical_parallel_cast-289/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-290/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.757419 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key-broadcast_matmul-290
(OUTPUT:_model.blocks.5.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.5.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.758569 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.value.weight"
device_tag: "cuda"
scope_symbol_id: 1487
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.758689 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value.weight
I20220824 02:43:23.759090 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.value-hierarchical_parallel_cast-291"
device_tag: "cuda"
scope_symbol_id: 1490
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-add_n-283/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.value-hierarchical_parallel_cast-291/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.759311 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value-hierarchical_parallel_cast-291
I20220824 02:43:23.759459 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.value-broadcast_matmul-292"
device_tag: "cuda"
scope_symbol_id: 1490
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 83; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.value-hierarchical_parallel_cast-291/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-292/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.759738 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value-broadcast_matmul-292
(OUTPUT:_model.blocks.5.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.5.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.760880 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1498
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.760999 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance.weight
I20220824 02:43:23.761474 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-hierarchical_parallel_cast-293"
device_tag: "cuda"
scope_symbol_id: 1501
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-add_n-288/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-hierarchical_parallel_cast-293/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.761695 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-hierarchical_parallel_cast-293
I20220824 02:43:23.761842 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-broadcast_matmul-294"
device_tag: "cuda"
scope_symbol_id: 1501
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 84; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.receptance-hierarchical_parallel_cast-293/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-294/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.762111 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-broadcast_matmul-294
(OUTPUT:_model.blocks.5.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.762354 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-sigmoid_v2-295"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-294/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-sigmoid_v2-295/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.762552 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-sigmoid_v2-295
I20220824 02:43:23.763125 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_decay"
device_tag: "cuda"
scope_symbol_id: 1511
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.763239 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_decay
I20220824 02:43:23.763805 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.time_first"
device_tag: "cuda"
scope_symbol_id: 1515
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.763926 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_first
I20220824 02:43:23.764070 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-wkv-296"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "wkv"
  input {
    key: "k"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-290/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.5.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-292/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.5.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-wkv-296/y_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  output_order: "y"
}

I20220824 02:43:23.764346 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-wkv-296
I20220824 02:43:23.764451 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-exp-297"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "exp"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.time_decay/out"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-exp-297/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.764612 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-exp-297
I20220824 02:43:23.764714 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-298"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-exp-297/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-298/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: -1
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.764930 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-298
I20220824 02:43:23.765049 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-299"
device_tag: "cuda"
scope_symbol_id: 1421
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 86; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-sigmoid_v2-295/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-wkv-296/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-299/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.765264 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-299
(MODULE:model.blocks.5.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.5.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
(PARAMETER:model.blocks.5.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.766384 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.att.output.weight"
device_tag: "cuda"
scope_symbol_id: 1532
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.766505 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output.weight
I20220824 02:43:23.766891 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-hierarchical_parallel_cast-300"
device_tag: "cuda"
scope_symbol_id: 1535
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-299/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-hierarchical_parallel_cast-300/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.767102 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-hierarchical_parallel_cast-300
I20220824 02:43:23.767247 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-broadcast_matmul-301"
device_tag: "cuda"
scope_symbol_id: 1535
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 122; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att.output-hierarchical_parallel_cast-300/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-301/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.767518 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-broadcast_matmul-301
(OUTPUT:_model.blocks.5.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(OUTPUT:_model.blocks.5.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.767835 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5-add_n-302"
device_tag: "cuda"
scope_symbol_id: 1541
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 217; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4-add_n-271/out_0"
      s: "model.blocks.5.att.output-broadcast_matmul-301/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5-add_n-302/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.768071 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5-add_n-302
(MODULE:model.blocks.5.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.blocks.5.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.769299 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln2.weight"
device_tag: "cuda"
scope_symbol_id: 1546
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.769419 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2.weight
I20220824 02:43:23.770066 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ln2.bias"
device_tag: "cuda"
scope_symbol_id: 1550
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.770181 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2.bias
I20220824 02:43:23.770340 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-layer_norm-303"
device_tag: "cuda"
scope_symbol_id: 1553
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 8 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.blocks.5.ln2.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln2.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-302/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.770720 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-layer_norm-303
(OUTPUT:_model.blocks.5.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.blocks.5.ffn:RWKV_ChannelMix())
(INPUT:_model.blocks.5.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.blocks.5.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.blocks.5.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
       requires_grad=True))
(MODULE:model.blocks.5.ffn.time_shift:ZeroPad2d())
(INPUT:_model.blocks.5.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
I20220824 02:43:23.772156 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.time_shift-pad-304"
device_tag: "cuda"
scope_symbol_id: 1558
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 162; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/oneflow/python/oneflow/nn/modules/padding.py\': line 553; ... 9 more"
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-304/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: 1
        val: -1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.772409 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_shift-pad-304
(OUTPUT:_model.blocks.5.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<pad_backward>))
I20220824 02:43:23.773116 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.time_mix_k"
device_tag: "cuda"
scope_symbol_id: 1562
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.773234 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_mix_k
I20220824 02:43:23.773373 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-305"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-305/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.773603 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-305
I20220824 02:43:23.773746 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-306"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-306/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.773975 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-306
I20220824 02:43:23.774078 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_add-307"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-306/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_add-307/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.774279 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_add-307
I20220824 02:43:23.774400 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-308"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-304/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-307/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-308/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.774610 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-308
I20220824 02:43:23.774734 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-309"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 163; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-305/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-308/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-309/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.774935 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-309
I20220824 02:43:23.775539 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.time_mix_r"
device_tag: "cuda"
scope_symbol_id: 1581
variable_conf {
  out: "out"
  shape {
    dim: 1
    dim: 1
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.775660 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_mix_r
I20220824 02:43:23.775817 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-310"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-310/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.776048 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-310
I20220824 02:43:23.776188 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-311"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-311/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.776412 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-311
I20220824 02:43:23.776518 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_add-312"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-311/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_add-312/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.776723 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_add-312
I20220824 02:43:23.776839 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-313"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-304/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-312/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-313/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.777046 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-313
I20220824 02:43:23.777166 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-314"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 164; ... 8 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-310/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-313/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-314/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.777365 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-314
(MODULE:model.blocks.5.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col))
(INPUT:_model.blocks.5.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.778542 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.key.weight"
device_tag: "cuda"
scope_symbol_id: 1601
variable_conf {
  out: "out"
  shape {
    dim: 4096
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.778663 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key.weight
I20220824 02:43:23.779064 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-hierarchical_parallel_cast-315"
device_tag: "cuda"
scope_symbol_id: 1604
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-add_n-309/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-hierarchical_parallel_cast-315/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.779284 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-hierarchical_parallel_cast-315
I20220824 02:43:23.779430 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-broadcast_matmul-316"
device_tag: "cuda"
scope_symbol_id: 1604
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 166; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn.key-hierarchical_parallel_cast-315/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-316/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.779714 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-broadcast_matmul-316
(OUTPUT:_model.blocks.5.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.779960 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-relu-317"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "relu"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-316/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-relu-317/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.780155 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-relu-317
I20220824 02:43:23.780274 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-square-318"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 167; ... 8 more"
user_conf {
  op_type_name: "square"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-relu-317/y_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-square-318/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.780452 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-square-318
(MODULE:model.blocks.5.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row))
(INPUT:_model.blocks.5.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
       dtype=oneflow.float32, grad_fn=<square_backward>))
(PARAMETER:model.blocks.5.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.781538 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.value.weight"
device_tag: "cuda"
scope_symbol_id: 1618
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 4096
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.781663 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value.weight
I20220824 02:43:23.782066 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-hierarchical_parallel_cast-319"
device_tag: "cuda"
scope_symbol_id: 1621
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-square-318/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-hierarchical_parallel_cast-319/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.782284 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-hierarchical_parallel_cast-319
I20220824 02:43:23.782436 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-broadcast_matmul-320"
device_tag: "cuda"
scope_symbol_id: 1621
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 168; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn.value-hierarchical_parallel_cast-319/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-320/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.782709 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-broadcast_matmul-320
(OUTPUT:_model.blocks.5.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.blocks.5.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col))
(INPUT:_model.blocks.5.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.blocks.5.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.783831 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.blocks.5.ffn.receptance.weight"
device_tag: "cuda"
scope_symbol_id: 1629
variable_conf {
  out: "out"
  shape {
    dim: 1024
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.783948 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance.weight
I20220824 02:43:23.784369 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-321"
device_tag: "cuda"
scope_symbol_id: 1632
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-add_n-314/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-321/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.784586 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-hierarchical_parallel_cast-321
I20220824 02:43:23.784734 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-broadcast_matmul-322"
device_tag: "cuda"
scope_symbol_id: 1632
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 9 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-321/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-322/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.785010 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-broadcast_matmul-322
(OUTPUT:_model.blocks.5.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.785254 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-sigmoid_v2-323"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "sigmoid_v2"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-322/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2-323/y_0"
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.785447 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-sigmoid_v2-323
I20220824 02:43:23.785573 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-324"
device_tag: "cuda"
scope_symbol_id: 1565
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 170; ... 8 more"
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2-323/y_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-320/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-324/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.785790 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-324
(OUTPUT:_model.blocks.5.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
I20220824 02:43:23.786018 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5-add_n-325"
device_tag: "cuda"
scope_symbol_id: 1541
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 310; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 218; ... 7 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5-add_n-302/out_0"
      s: "model.blocks.5.ffn-broadcast_mul-324/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5-add_n-325/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.786240 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5-add_n-325
(OUTPUT:_model.blocks.5_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(OUTPUT:_model.blocks_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(MODULE:model.ln_out:LayerNorm((1024,), eps=1e-05, elementwise_affine=True))
(INPUT:_model.ln_out_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<add_n_backward>))
(PARAMETER:model.ln_out.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
(PARAMETER:model.ln_out.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.787521 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.ln_out.weight"
device_tag: "cuda"
scope_symbol_id: 1649
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.787642 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out.weight
I20220824 02:43:23.788203 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.ln_out.bias"
device_tag: "cuda"
scope_symbol_id: 1653
variable_conf {
  out: "out"
  shape {
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.788331 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out.bias
I20220824 02:43:23.788480 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-layer_norm-326"
device_tag: "cuda"
scope_symbol_id: 1656
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 313; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/layer_norm.py\': line 77; ... 7 more"
user_conf {
  op_type_name: "layer_norm"
  input {
    key: "beta"
    value {
      s: "model.ln_out.bias/out"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.ln_out.weight/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-325/out_0"
    }
  }
  output {
    key: "inv_variance"
    value {
      s: "model.ln_out-layer_norm-326/inv_variance_0"
    }
  }
  output {
    key: "mean"
    value {
      s: "model.ln_out-layer_norm-326/mean_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.ln_out-layer_norm-326/y_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "center"
    value {
      at_bool: true
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  attr {
    key: "scale"
    value {
      at_bool: true
    }
  }
  input_order: "x"
  input_order: "gamma"
  input_order: "beta"
  output_order: "y"
  output_order: "mean"
  output_order: "inv_variance"
}

I20220824 02:43:23.788856 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-layer_norm-326
(OUTPUT:_model.ln_out_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(MODULE:model.head_q:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col))
(INPUT:_model.head_q_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.head_q.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.790228 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.head_q.weight"
device_tag: "cuda"
scope_symbol_id: 1661
variable_conf {
  out: "out"
  shape {
    dim: 256
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.790340 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q.weight
I20220824 02:43:23.790750 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-hierarchical_parallel_cast-327"
device_tag: "cuda"
scope_symbol_id: 1664
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.ln_out-layer_norm-326/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-hierarchical_parallel_cast-327/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.790946 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-hierarchical_parallel_cast-327
I20220824 02:43:23.791092 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-broadcast_matmul-328"
device_tag: "cuda"
scope_symbol_id: 1664
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 317; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 7 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.head_q-hierarchical_parallel_cast-327/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_q.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-broadcast_matmul-328/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.791361 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-broadcast_matmul-328
(OUTPUT:_model.head_q_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 256),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
(MODULE:model.head_k:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col))
(INPUT:_model.head_k_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.head_k.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.792639 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.head_k.weight"
device_tag: "cuda"
scope_symbol_id: 1672
variable_conf {
  out: "out"
  shape {
    dim: 256
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.792762 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k.weight
I20220824 02:43:23.793231 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-hierarchical_parallel_cast-329"
device_tag: "cuda"
scope_symbol_id: 1675
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.ln_out-layer_norm-326/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-hierarchical_parallel_cast-329/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.793452 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-hierarchical_parallel_cast-329
I20220824 02:43:23.793601 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-broadcast_matmul-330"
device_tag: "cuda"
scope_symbol_id: 1675
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 318; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 7 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.head_k-hierarchical_parallel_cast-329/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_k.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-broadcast_matmul-330/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.793866 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-broadcast_matmul-330
(OUTPUT:_model.head_k_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 256),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.794139 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-331"
device_tag: "cuda"
scope_symbol_id: 1681
loc: ""
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model.head_k-broadcast_matmul-330/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-331/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 2
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 02:43:23.794360 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-331
I20220824 02:43:23.794502 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-332"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 319; ... 6 more"
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model.head_q-broadcast_matmul-328/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-transpose-331/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-332/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.794768 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-332
I20220824 02:43:23.794915 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-scalar_mul-333"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 319; ... 6 more"
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model-batch_matmul-332/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-scalar_mul-333/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0.00390625
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.795138 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-scalar_mul-333
I20220824 02:43:23.795327 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-one_hot-334"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 322; ... 6 more"
user_conf {
  op_type_name: "one_hot"
  input {
    key: "indices"
    value {
      s: "_GraphBase_0_input.1.0_idx/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model-one_hot-334/out_0"
    }
  }
  attr {
    key: "depth"
    value {
      at_int64: 6064
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kInt64
    }
  }
  attr {
    key: "floating_off_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "floating_on_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_off_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "integer_on_value"
    value {
      at_int64: 1
    }
  }
  input_order: "indices"
  output_order: "out"
}

I20220824 02:43:23.795578 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-one_hot-334
I20220824 02:43:23.795677 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-cast-335"
device_tag: "cuda"
scope_symbol_id: 1681
loc: ""
user_conf {
  op_type_name: "cast"
  input {
    key: "in"
    value {
      s: "model-one_hot-334/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-cast-335/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "pin_memory"
    value {
      at_bool: false
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.795871 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-cast-335
I20220824 02:43:23.795986 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-336"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 322; ... 6 more"
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model-scalar_mul-333/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-cast-335/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-336/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.796245 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-336
(MODULE:model.head:Linear1D(in_features=1024, out_features=6064, bias=False, parallel=row))
(INPUT:_model.head_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
       dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
(PARAMETER:model.head.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
       requires_grad=True))
I20220824 02:43:23.797449 1392289 lazy_op_interpreter.cpp:463] Lazy nn.Graph name GraphBase_0 try to add op: 
: name: "model.head.weight"
device_tag: "cuda"
scope_symbol_id: 1701
variable_conf {
  out: "out"
  shape {
    dim: 6064
    dim: 1024
  }
  data_type: kFloat
  initializer {
    empty_conf {
    }
  }
  nd_sbp: "B"
}

I20220824 02:43:23.797569 1392289 lazy_op_interpreter.cpp:466] Lazy nn.Graph name GraphBase_0 add op : 
model.head.weight
I20220824 02:43:23.797966 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-hierarchical_parallel_cast-337"
device_tag: "cuda"
scope_symbol_id: 1704
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.ln_out-layer_norm-326/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-hierarchical_parallel_cast-337/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "manual"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.798177 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-hierarchical_parallel_cast-337
I20220824 02:43:23.798319 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-broadcast_matmul-338"
device_tag: "cuda"
scope_symbol_id: 1704
loc: "Python Stack[-2]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 324; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/libai/layers/linear.py\': line 127; ... 7 more"
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.head-hierarchical_parallel_cast-337/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-broadcast_matmul-338/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.798585 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-broadcast_matmul-338
(OUTPUT:_model.head_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 6064),
       dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
I20220824 02:43:23.798830 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-add_n-339"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 324; ... 6 more"
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.head-broadcast_matmul-338/out_0"
      s: "model-batch_matmul-336/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-add_n-339/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.799059 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-add_n-339
I20220824 02:43:23.799227 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-340"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-add_n-339/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-340/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.799444 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-340
I20220824 02:43:23.799799 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-341"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "_GraphBase_0_input.1.1_targets/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-341/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.800012 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-341
I20220824 02:43:23.800213 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-342"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model-reshape-340/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-342/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 02:43:23.800419 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-342
I20220824 02:43:23.800531 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-343"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-transpose-342/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-343/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.800747 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-343
I20220824 02:43:23.800848 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-log_softmax-344"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "log_softmax"
  input {
    key: "in"
    value {
      s: "model-reshape-343/out_0"
    }
  }
  output {
    key: "prob"
    value {
      s: "model-log_softmax-344/prob_0"
    }
  }
  input_order: "in"
  output_order: "prob"
}

I20220824 02:43:23.801023 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-log_softmax-344
I20220824 02:43:23.801164 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-nll-345"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "nll"
  input {
    key: "input"
    value {
      s: "model-log_softmax-344/prob_0"
    }
  }
  input {
    key: "target"
    value {
      s: "model-reshape-341/out_0"
    }
  }
  output {
    key: "out_weight"
    value {
      s: "model-nll-345/out_weight_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-nll-345/output_0"
    }
  }
  attr {
    key: "ignore_index"
    value {
      at_int64: -100
    }
  }
  input_order: "input"
  input_order: "target"
  output_order: "output"
  output_order: "out_weight"
}

I20220824 02:43:23.801427 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-nll-345
I20220824 02:43:23.801538 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-346"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-nll-345/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-346/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.801743 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-346
I20220824 02:43:23.801872 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reduce_sum-347"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model-reshape-346/out_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model-reduce_sum-347/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.802083 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reduce_sum-347
I20220824 02:43:23.802186 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reduce_sum-348"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model-nll-345/out_weight_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model-reduce_sum-348/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.802402 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reduce_sum-348
I20220824 02:43:23.802521 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_div-349"
device_tag: "cuda"
scope_symbol_id: 1681
loc: "Python Stack[-2]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 106; Python Stack[-1]: \'forward\' at \'/home/zhangxiaoyu/libai/projects/RWKV_v4/modeling/model.py\': line 329; ... 6 more"
user_conf {
  op_type_name: "broadcast_div"
  input {
    key: "x"
    value {
      s: "model-reduce_sum-347/output_tensor_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model-reduce_sum-348/output_tensor_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model-broadcast_div-349/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.802716 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_div-349
(OUTPUT:_model_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(),
       dtype=oneflow.float32, grad_fn=<broadcast_div_backward>))
I20220824 02:43:23.802969 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "scalar_add-350"
device_tag: "cuda"
scope_symbol_id: 1743
loc: "Python Stack[-2]: \'run_step\' at \'/home/zhangxiaoyu/libai/libai/engine/trainer.py\': line 348; Python Stack[-1]: \'build\' at \'/home/zhangxiaoyu/libai/libai/models/utils/graph_base.py\': line 107; ... 5 more"
user_conf {
  op_type_name: "scalar_add"
  input {
    key: "in"
    value {
      s: "model-broadcast_div-349/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "scalar_add-350/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.803179 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
scalar_add-350
I20220824 02:43:23.803396 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "ones_like-351"
device_tag: "cuda"
scope_symbol_id: 1747
loc: ""
user_conf {
  op_type_name: "ones_like"
  input {
    key: "like"
    value {
      s: "scalar_add-350/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "ones_like-351/out_0"
    }
  }
  input_order: "like"
  output_order: "out"
}

I20220824 02:43:23.803561 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
ones_like-351
I20220824 02:43:23.803887 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_div-352"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "broadcast_div"
  input {
    key: "x"
    value {
      s: "ones_like-351/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model-reduce_sum-348/output_tensor_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model-broadcast_div-352/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.804081 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_div-352
I20220824 02:43:23.804201 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_div_grad-353"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "broadcast_div_grad"
  input {
    key: "dz"
    value {
      s: "ones_like-351/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model-reduce_sum-348/output_tensor_0"
    }
  }
  input {
    key: "z"
    value {
      s: "model-broadcast_div-349/z_0"
    }
  }
  output {
    key: "dy"
    value {
      s: "model-broadcast_div_grad-353/dy_0"
    }
  }
  input_order: "dz"
  input_order: "z"
  input_order: "y"
  output_order: "dy"
}

I20220824 02:43:23.804430 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_div_grad-353
I20220824 02:43:23.804560 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_like-354"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "broadcast_like"
  input {
    key: "like"
    value {
      s: "model-reshape-346/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model-broadcast_div-352/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model-broadcast_like-354/y_0"
    }
  }
  attr {
    key: "broadcast_axes"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.804797 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_like-354
I20220824 02:43:23.804898 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-broadcast_like-355"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "broadcast_like"
  input {
    key: "like"
    value {
      s: "model-nll-345/out_weight_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model-broadcast_div_grad-353/dy_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model-broadcast_like-355/y_0"
    }
  }
  attr {
    key: "broadcast_axes"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.805124 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-broadcast_like-355
I20220824 02:43:23.805220 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-356"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-broadcast_like-354/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-356/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.805413 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-356
I20220824 02:43:23.805543 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-nll_grad-357"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "nll_grad"
  input {
    key: "input"
    value {
      s: "model-log_softmax-344/prob_0"
    }
  }
  input {
    key: "out_grad"
    value {
      s: "model-reshape-356/out_0"
    }
  }
  input {
    key: "target"
    value {
      s: "model-reshape-341/out_0"
    }
  }
  output {
    key: "in_grad"
    value {
      s: "model-nll_grad-357/in_grad_0"
    }
  }
  attr {
    key: "ignore_index"
    value {
      at_int64: -100
    }
  }
  input_order: "out_grad"
  input_order: "input"
  input_order: "target"
  output_order: "in_grad"
}

I20220824 02:43:23.806246 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-nll_grad-357
I20220824 02:43:23.806351 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-log_softmax_grad-358"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "log_softmax_grad"
  input {
    key: "dy"
    value {
      s: "model-nll_grad-357/in_grad_0"
    }
  }
  input {
    key: "prob"
    value {
      s: "model-log_softmax-344/prob_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model-log_softmax_grad-358/dx_0"
    }
  }
  input_order: "prob"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.806510 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-log_softmax_grad-358
I20220824 02:43:23.806608 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-359"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-log_softmax_grad-358/dx_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-359/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8192
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.806790 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-359
I20220824 02:43:23.806880 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-360"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model-reshape-359/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-360/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 02:43:23.807052 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-360
I20220824 02:43:23.807142 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-reshape-361"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "reshape"
  input {
    key: "in"
    value {
      s: "model-transpose-360/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-reshape-361/out_0"
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
        dim: 6064
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.807312 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-reshape-361
I20220824 02:43:23.807404 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-broadcast_matmul-362"
device_tag: "cuda"
scope_symbol_id: 1782
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model-reshape-361/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-broadcast_matmul-362/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.807621 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-broadcast_matmul-362
I20220824 02:43:23.807741 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head-broadcast_matmul_grad_b-363"
device_tag: "cuda"
scope_symbol_id: 1782
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model-reshape-361/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head-hierarchical_parallel_cast-337/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head-broadcast_matmul_grad_b-363/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.807945 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head-broadcast_matmul_grad_b-363
I20220824 02:43:23.808038 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-364"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model-reshape-361/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-cast-335/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-364/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.808261 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-364
I20220824 02:43:23.808579 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-scalar_mul-365"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model-batch_matmul-364/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-scalar_mul-365/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0.00390625
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: 0
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.808781 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-scalar_mul-365
I20220824 02:43:23.808876 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-366"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model-scalar_mul-365/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-transpose-331/output_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-366/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: true
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.809113 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-366
I20220824 02:43:23.809204 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-batch_matmul-367"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "batch_matmul"
  input {
    key: "a"
    value {
      s: "model.head_q-broadcast_matmul-328/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model-scalar_mul-365/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model-batch_matmul-367/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: true
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.809420 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-batch_matmul-367
I20220824 02:43:23.809513 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-broadcast_matmul-368"
device_tag: "cuda"
scope_symbol_id: 1803
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model-batch_matmul-366/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_q.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-broadcast_matmul-368/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.809723 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-broadcast_matmul-368
I20220824 02:43:23.809809 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-broadcast_matmul_grad_b-369"
device_tag: "cuda"
scope_symbol_id: 1803
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model-batch_matmul-366/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_q-hierarchical_parallel_cast-327/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-broadcast_matmul_grad_b-369/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.810009 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-broadcast_matmul_grad_b-369
I20220824 02:43:23.810101 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model-transpose-370"
device_tag: "cuda"
scope_symbol_id: 1751
loc: ""
user_conf {
  op_type_name: "transpose"
  input {
    key: "input"
    value {
      s: "model-batch_matmul-367/out_0"
    }
  }
  output {
    key: "output"
    value {
      s: "model-transpose-370/output_0"
    }
  }
  attr {
    key: "perm"
    value {
      at_list_int32 {
        val: 0
        val: 2
        val: 1
      }
    }
  }
  input_order: "input"
  output_order: "output"
}

I20220824 02:43:23.810281 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model-transpose-370
I20220824 02:43:23.810586 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-hierarchical_parallel_cast-371"
device_tag: "cuda"
scope_symbol_id: 1803
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.head_q-broadcast_matmul-368/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-hierarchical_parallel_cast-371/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "identity"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.810746 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-hierarchical_parallel_cast-371
I20220824 02:43:23.810835 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_q-add_n-372"
device_tag: "cuda"
scope_symbol_id: 1803
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.head_q-hierarchical_parallel_cast-371/out_0"
      s: "model.head-broadcast_matmul-362/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_q-add_n-372/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.811014 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_q-add_n-372
I20220824 02:43:23.811106 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-broadcast_matmul-373"
device_tag: "cuda"
scope_symbol_id: 1820
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model-transpose-370/output_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_k.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-broadcast_matmul-373/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.811324 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-broadcast_matmul-373
I20220824 02:43:23.811416 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-broadcast_matmul_grad_b-374"
device_tag: "cuda"
scope_symbol_id: 1820
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model-transpose-370/output_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.head_k-hierarchical_parallel_cast-329/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-broadcast_matmul_grad_b-374/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.811619 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-broadcast_matmul_grad_b-374
I20220824 02:43:23.811947 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-hierarchical_parallel_cast-375"
device_tag: "cuda"
scope_symbol_id: 1820
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.head_k-broadcast_matmul-373/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-hierarchical_parallel_cast-375/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "identity"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.812117 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-hierarchical_parallel_cast-375
I20220824 02:43:23.812206 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.head_k-add_n-376"
device_tag: "cuda"
scope_symbol_id: 1820
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.head_k-hierarchical_parallel_cast-375/out_0"
      s: "model.head_q-add_n-372/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.head_k-add_n-376/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.812377 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.head_k-add_n-376
I20220824 02:43:23.812700 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-constant-377"
device_tag: "cuda"
scope_symbol_id: 1835
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.ln_out-constant-377/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.812865 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-constant-377
I20220824 02:43:23.813163 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-constant-378"
device_tag: "cuda"
scope_symbol_id: 1835
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.ln_out-constant-378/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.813325 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-constant-378
I20220824 02:43:23.813450 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-layer_norm_param_grad-379"
device_tag: "cuda"
scope_symbol_id: 1835
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.head_k-add_n-376/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.ln_out-layer_norm-326/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.ln_out-layer_norm-326/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-325/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.ln_out-layer_norm_param_grad-379/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.ln_out-layer_norm_param_grad-379/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.813724 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-layer_norm_param_grad-379
I20220824 02:43:23.813851 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.ln_out-layer_norm_grad-380"
device_tag: "cuda"
scope_symbol_id: 1835
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.head_k-add_n-376/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.ln_out.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.ln_out-layer_norm-326/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.ln_out-layer_norm-326/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-325/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.ln_out-layer_norm_grad-380/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.814128 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.ln_out-layer_norm_grad-380
I20220824 02:43:23.814242 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-381"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.ln_out-layer_norm_grad-380/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-320/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-381/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.814442 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-381
I20220824 02:43:23.814527 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-382"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.ln_out-layer_norm_grad-380/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2-323/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-382/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.814728 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-382
I20220824 02:43:23.814847 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-sigmoid_v2_grad-383"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-381/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-322/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2_grad-383/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.815032 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-sigmoid_v2_grad-383
I20220824 02:43:23.815135 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-broadcast_matmul-384"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-382/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-384/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.815354 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-broadcast_matmul-384
I20220824 02:43:23.815445 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.value-broadcast_matmul_grad_b-385"
device_tag: "cuda"
scope_symbol_id: 1864
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-382/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.value-hierarchical_parallel_cast-319/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul_grad_b-385/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.815644 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.value-broadcast_matmul_grad_b-385
I20220824 02:43:23.815743 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-broadcast_matmul-386"
device_tag: "cuda"
scope_symbol_id: 1871
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2_grad-383/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.815959 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-broadcast_matmul-386
I20220824 02:43:23.816045 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387"
device_tag: "cuda"
scope_symbol_id: 1871
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-sigmoid_v2_grad-383/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.receptance-hierarchical_parallel_cast-321/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.816246 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387
I20220824 02:43:23.816781 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-square_grad-388"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn.value-broadcast_matmul-384/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-relu-317/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ffn-square_grad-388/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.816972 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-square_grad-388
I20220824 02:43:23.817088 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-relu_grad-389"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn-square_grad-388/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-relu-317/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ffn-relu_grad-389/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 02:43:23.817260 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-relu_grad-389
I20220824 02:43:23.817344 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-390"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-390/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.817528 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-390
I20220824 02:43:23.817607 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-391"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-391/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.817795 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-391
I20220824 02:43:23.817916 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-392"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-391/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-392/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.818142 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-392
I20220824 02:43:23.818235 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-393"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-312/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-393/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.818511 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-393
I20220824 02:43:23.818596 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-394"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-304/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-394/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.818789 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-394
I20220824 02:43:23.818877 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-395"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn-scalar_add-312/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-394/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-395/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.819085 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-395
I20220824 02:43:23.819193 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-broadcast_matmul-396"
device_tag: "cuda"
scope_symbol_id: 1906
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-relu_grad-389/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-396/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.819415 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-broadcast_matmul-396
I20220824 02:43:23.819504 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.key-broadcast_matmul_grad_b-397"
device_tag: "cuda"
scope_symbol_id: 1906
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ffn-relu_grad-389/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.ffn.key-hierarchical_parallel_cast-315/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul_grad_b-397/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.819715 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.key-broadcast_matmul_grad_b-397
I20220824 02:43:23.820101 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-398"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-395/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-398/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.820302 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-398
I20220824 02:43:23.820389 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-399"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-398/out_0"
      s: "model.blocks.5.ffn-reduce_sum_like-392/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-399/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.820559 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-399
I20220824 02:43:23.820655 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-400"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-396/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-400/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.820847 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-400
I20220824 02:43:23.820926 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-401"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-396/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-401/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.821110 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-401
I20220824 02:43:23.821197 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-402"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-401/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-402/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.821426 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-402
I20220824 02:43:23.821519 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-403"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-400/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-390/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-403/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.821708 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-403
I20220824 02:43:23.821795 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-404"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-396/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn-scalar_add-307/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-404/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.821977 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-404
I20220824 02:43:23.822057 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-broadcast_mul-405"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn.key-broadcast_matmul-396/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-304/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-405/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.822242 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-broadcast_mul-405
I20220824 02:43:23.822328 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-reduce_sum_like-406"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.ffn-scalar_add-307/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-405/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-406/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.822552 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-reduce_sum_like-406
I20220824 02:43:23.822638 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-407"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-broadcast_mul-404/z_0"
      s: "model.blocks.5.ffn-broadcast_mul-393/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-407/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.822821 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-407
I20220824 02:43:23.822933 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.time_shift-pad-408"
device_tag: "cuda"
scope_symbol_id: 1947
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ffn-add_n-407/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-408/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.823133 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_shift-pad-408
I20220824 02:43:23.823220 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn.time_shift-add_n-409"
device_tag: "cuda"
scope_symbol_id: 1947
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn.time_shift-pad-408/y_0"
      s: "model.blocks.5.ffn-add_n-403/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn.time_shift-add_n-409/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.823379 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn.time_shift-add_n-409
I20220824 02:43:23.823709 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-constant-410"
device_tag: "cuda"
scope_symbol_id: 1954
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln2-constant-410/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.823880 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-constant-410
I20220824 02:43:23.824189 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-constant-411"
device_tag: "cuda"
scope_symbol_id: 1954
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln2-constant-411/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.824348 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-constant-411
I20220824 02:43:23.824441 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-layer_norm_param_grad-412"
device_tag: "cuda"
scope_symbol_id: 1954
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn.time_shift-add_n-409/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-302/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.5.ln2-layer_norm_param_grad-412/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.5.ln2-layer_norm_param_grad-412/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.824723 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-layer_norm_param_grad-412
I20220824 02:43:23.824823 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-layer_norm_grad-413"
device_tag: "cuda"
scope_symbol_id: 1954
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.ffn.time_shift-add_n-409/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln2-layer_norm-303/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5-add_n-302/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ln2-layer_norm_grad-413/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.825096 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-layer_norm_grad-413
I20220824 02:43:23.825188 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln2-add_n-414"
device_tag: "cuda"
scope_symbol_id: 1954
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ln2-layer_norm_grad-413/dx_0"
      s: "model.ln_out-layer_norm_grad-380/dx_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln2-add_n-414/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.825371 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln2-add_n-414
I20220824 02:43:23.825465 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-scalar_mul-415"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-reduce_sum_like-406/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-scalar_mul-415/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.825672 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-scalar_mul-415
I20220824 02:43:23.825757 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ffn-add_n-416"
device_tag: "cuda"
scope_symbol_id: 1852
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ffn-scalar_mul-415/out_0"
      s: "model.blocks.5.ffn-reduce_sum_like-402/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ffn-add_n-416/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.825944 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ffn-add_n-416
I20220824 02:43:23.826048 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-broadcast_matmul-417"
device_tag: "cuda"
scope_symbol_id: 1979
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ln2-add_n-414/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-417/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.826265 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-broadcast_matmul-417
I20220824 02:43:23.826352 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.output-broadcast_matmul_grad_b-418"
device_tag: "cuda"
scope_symbol_id: 1979
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.ln2-add_n-414/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.output-hierarchical_parallel_cast-300/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul_grad_b-418/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.826550 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.output-broadcast_matmul_grad_b-418
I20220824 02:43:23.826853 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-419"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-417/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-wkv-296/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-419/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.827049 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-419
I20220824 02:43:23.827134 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-420"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.output-broadcast_matmul-417/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-sigmoid_v2-295/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-420/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.827316 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-420
I20220824 02:43:23.827410 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-sigmoid_v2_grad-421"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.att-broadcast_mul-419/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-294/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.att-sigmoid_v2_grad-421/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.827600 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-sigmoid_v2_grad-421
I20220824 02:43:23.827733 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-wkv_grad-422"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "wkv_grad"
  input {
    key: "gy"
    value {
      s: "model.blocks.5.att-broadcast_mul-420/z_0"
    }
  }
  input {
    key: "k"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-290/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.5.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-292/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.5.att-scalar_mul-298/out_0"
    }
  }
  output {
    key: "gk"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gk_0"
    }
  }
  output {
    key: "gu"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gu_0"
    }
  }
  output {
    key: "gv"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gv_0"
    }
  }
  output {
    key: "gw"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gw_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  input_order: "gy"
  output_order: "gw"
  output_order: "gu"
  output_order: "gk"
  output_order: "gv"
}

I20220824 02:43:23.828025 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-wkv_grad-422
I20220824 02:43:23.828127 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum-423"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gw_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.5.att-reduce_sum-423/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.828356 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum-423
I20220824 02:43:23.828445 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum-424"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gu_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.5.att-reduce_sum-424/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.828631 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum-424
I20220824 02:43:23.828732 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-broadcast_matmul-425"
device_tag: "cuda"
scope_symbol_id: 2010
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-sigmoid_v2_grad-421/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-425/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.828972 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-broadcast_matmul-425
I20220824 02:43:23.829059 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.receptance-broadcast_matmul_grad_b-426"
device_tag: "cuda"
scope_symbol_id: 2010
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-sigmoid_v2_grad-421/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.receptance-hierarchical_parallel_cast-293/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul_grad_b-426/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.829257 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.receptance-broadcast_matmul_grad_b-426
I20220824 02:43:23.829358 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.key-broadcast_matmul-427"
device_tag: "cuda"
scope_symbol_id: 2019
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-427/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.829576 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key-broadcast_matmul-427
I20220824 02:43:23.829661 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.key-broadcast_matmul_grad_b-428"
device_tag: "cuda"
scope_symbol_id: 2019
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.key-hierarchical_parallel_cast-289/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul_grad_b-428/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.829857 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.key-broadcast_matmul_grad_b-428
I20220824 02:43:23.829962 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.value-broadcast_matmul-429"
device_tag: "cuda"
scope_symbol_id: 2026
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-429/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.830181 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value-broadcast_matmul-429
I20220824 02:43:23.830273 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.value-broadcast_matmul_grad_b-430"
device_tag: "cuda"
scope_symbol_id: 2026
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.5.att-wkv_grad-422/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.5.att.value-hierarchical_parallel_cast-291/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul_grad_b-430/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.830464 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.value-broadcast_matmul_grad_b-430
I20220824 02:43:23.831187 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-431"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-425/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-431/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.831400 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-431
I20220824 02:43:23.831485 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-432"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-425/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-432/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.831676 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-432
I20220824 02:43:23.831768 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-433"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-432/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-433/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.831971 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-433
I20220824 02:43:23.832060 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-434"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-425/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-286/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-434/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.832259 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-434
I20220824 02:43:23.832345 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-435"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.receptance-broadcast_matmul-425/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-435/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.832532 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-435
I20220824 02:43:23.832618 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-436"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att-scalar_add-286/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-435/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-436/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.832841 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-436
I20220824 02:43:23.832933 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-437"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-427/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-437/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.833130 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-437
I20220824 02:43:23.833212 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-438"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-427/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-438/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.833396 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-438
I20220824 02:43:23.833483 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-439"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-438/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-439/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.833709 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-439
I20220824 02:43:23.833793 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-440"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-437/z_0"
      s: "model.blocks.5.att-broadcast_mul-431/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-440/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.833964 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-440
I20220824 02:43:23.834051 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-441"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-427/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-276/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-441/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.834717 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-441
I20220824 02:43:23.834829 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-442"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.key-broadcast_matmul-427/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-442/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.835023 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-442
I20220824 02:43:23.835112 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-443"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att-scalar_add-276/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-442/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-443/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.835330 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-443
I20220824 02:43:23.835417 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-444"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-441/z_0"
      s: "model.blocks.5.att-broadcast_mul-434/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-444/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.835603 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-444
I20220824 02:43:23.835690 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-445"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-429/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-445/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.835898 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-445
I20220824 02:43:23.835979 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-446"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-429/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-446/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.836161 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-446
I20220824 02:43:23.836247 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-447"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att.time_mix_v/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-446/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-447/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.836467 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-447
I20220824 02:43:23.836553 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-448"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-445/z_0"
      s: "model.blocks.5.att-add_n-440/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-448/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.836776 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-448
I20220824 02:43:23.836863 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-449"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-429/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att-scalar_add-281/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-449/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.837054 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-449
I20220824 02:43:23.837134 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-broadcast_mul-450"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att.value-broadcast_matmul-429/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-273/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.5.att-broadcast_mul-450/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.837325 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-broadcast_mul-450
I20220824 02:43:23.837410 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-reduce_sum_like-451"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.5.att-scalar_add-281/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-broadcast_mul-450/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att-reduce_sum_like-451/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.837616 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-reduce_sum_like-451
I20220824 02:43:23.837703 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-452"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-broadcast_mul-449/z_0"
      s: "model.blocks.5.att-add_n-444/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-452/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.837888 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-452
I20220824 02:43:23.837987 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.time_shift-pad-453"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.5.att-add_n-452/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.5.att.time_shift-pad-453/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.838181 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_shift-pad-453
I20220824 02:43:23.838268 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att.time_shift-add_n-454"
device_tag: "cuda"
scope_symbol_id: 2107
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att.time_shift-pad-453/y_0"
      s: "model.blocks.5.att-add_n-448/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att.time_shift-add_n-454/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.838434 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att.time_shift-add_n-454
I20220824 02:43:23.838524 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-455"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-reduce_sum_like-436/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-455/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.838712 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-455
I20220824 02:43:23.838805 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-456"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-455/out_0"
      s: "model.blocks.5.att-reduce_sum_like-433/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-456/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.838981 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-456
I20220824 02:43:23.839071 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-457"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-reduce_sum_like-443/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-457/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.839252 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-457
I20220824 02:43:23.839336 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-458"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-457/out_0"
      s: "model.blocks.5.att-reduce_sum_like-439/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-458/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.839507 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-458
I20220824 02:43:23.839826 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-constant-459"
device_tag: "cuda"
scope_symbol_id: 2126
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln1-constant-459/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.839993 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-constant-459
I20220824 02:43:23.840307 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-constant-460"
device_tag: "cuda"
scope_symbol_id: 2126
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln1-constant-460/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.840492 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-constant-460
I20220824 02:43:23.840584 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-layer_norm_param_grad-461"
device_tag: "cuda"
scope_symbol_id: 2126
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.att.time_shift-add_n-454/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-271/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.5.ln1-layer_norm_param_grad-461/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.5.ln1-layer_norm_param_grad-461/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.840853 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-layer_norm_param_grad-461
I20220824 02:43:23.840952 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-layer_norm_grad-462"
device_tag: "cuda"
scope_symbol_id: 2126
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.5.att.time_shift-add_n-454/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.5.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.5.ln1-layer_norm-272/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-271/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.5.ln1-layer_norm_grad-462/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.841231 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-layer_norm_grad-462
I20220824 02:43:23.841322 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.ln1-add_n-463"
device_tag: "cuda"
scope_symbol_id: 2126
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.ln1-layer_norm_grad-462/dx_0"
      s: "model.blocks.5.ln2-add_n-414/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.ln1-add_n-463/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.841506 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.ln1-add_n-463
I20220824 02:43:23.841600 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-scalar_mul-464"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-reduce_sum_like-451/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-scalar_mul-464/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.841799 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-scalar_mul-464
I20220824 02:43:23.841883 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.5.att-add_n-465"
device_tag: "cuda"
scope_symbol_id: 1990
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.5.att-scalar_mul-464/out_0"
      s: "model.blocks.5.att-reduce_sum_like-447/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.5.att-add_n-465/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.842063 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.5.att-add_n-465
I20220824 02:43:23.842187 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-466"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-add_n-463/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-266/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-466/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.842370 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-466
I20220824 02:43:23.842453 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-467"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.5.ln1-add_n-463/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2-269/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-467/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.842643 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-467
I20220824 02:43:23.842742 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-sigmoid_v2_grad-468"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-466/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-268/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2_grad-468/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.842916 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-sigmoid_v2_grad-468
I20220824 02:43:23.843019 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-broadcast_matmul-469"
device_tag: "cuda"
scope_symbol_id: 2169
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-467/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-469/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.843243 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-broadcast_matmul-469
I20220824 02:43:23.843336 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.value-broadcast_matmul_grad_b-470"
device_tag: "cuda"
scope_symbol_id: 2169
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-467/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.value-hierarchical_parallel_cast-265/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul_grad_b-470/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.843533 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.value-broadcast_matmul_grad_b-470
I20220824 02:43:23.843624 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-broadcast_matmul-471"
device_tag: "cuda"
scope_symbol_id: 2176
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2_grad-468/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.843842 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-broadcast_matmul-471
I20220824 02:43:23.843928 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472"
device_tag: "cuda"
scope_symbol_id: 2176
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-sigmoid_v2_grad-468/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.receptance-hierarchical_parallel_cast-267/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.844120 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472
I20220824 02:43:23.844612 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-square_grad-473"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn.value-broadcast_matmul-469/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-relu-263/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ffn-square_grad-473/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.844802 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-square_grad-473
I20220824 02:43:23.844903 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-relu_grad-474"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn-square_grad-473/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-relu-263/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ffn-relu_grad-474/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 02:43:23.845086 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-relu_grad-474
I20220824 02:43:23.845172 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-475"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-475/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.845369 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-475
I20220824 02:43:23.845450 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-476"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-476/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.845638 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-476
I20220824 02:43:23.845724 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-477"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-476/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-477/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.845933 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-477
I20220824 02:43:23.846024 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-478"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-258/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-478/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.846284 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-478
I20220824 02:43:23.846369 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-479"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-250/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-479/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.846552 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-479
I20220824 02:43:23.846647 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-480"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn-scalar_add-258/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-479/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-480/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.846853 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-480
I20220824 02:43:23.846961 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-broadcast_matmul-481"
device_tag: "cuda"
scope_symbol_id: 2211
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-relu_grad-474/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-481/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.847180 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-broadcast_matmul-481
I20220824 02:43:23.847270 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.key-broadcast_matmul_grad_b-482"
device_tag: "cuda"
scope_symbol_id: 2211
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ffn-relu_grad-474/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.ffn.key-hierarchical_parallel_cast-261/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul_grad_b-482/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.847465 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.key-broadcast_matmul_grad_b-482
I20220824 02:43:23.847765 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-483"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-480/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-483/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.847966 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-483
I20220824 02:43:23.848053 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-484"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-483/out_0"
      s: "model.blocks.4.ffn-reduce_sum_like-477/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-484/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.848233 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-484
I20220824 02:43:23.848335 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-485"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-481/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-485/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.848534 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-485
I20220824 02:43:23.848616 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-486"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-481/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-486/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.848803 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-486
I20220824 02:43:23.848891 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-487"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-486/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-487/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.849115 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-487
I20220824 02:43:23.849202 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-488"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-485/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-475/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-488/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.849387 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-488
I20220824 02:43:23.849474 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-489"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-481/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn-scalar_add-253/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-489/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.849660 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-489
I20220824 02:43:23.849740 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-broadcast_mul-490"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn.key-broadcast_matmul-481/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-250/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-490/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.849931 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-broadcast_mul-490
I20220824 02:43:23.850021 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-reduce_sum_like-491"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.ffn-scalar_add-253/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-490/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-491/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.850244 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-reduce_sum_like-491
I20220824 02:43:23.850332 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-492"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-broadcast_mul-489/z_0"
      s: "model.blocks.4.ffn-broadcast_mul-478/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-492/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.850517 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-492
I20220824 02:43:23.850621 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.time_shift-pad-493"
device_tag: "cuda"
scope_symbol_id: 2252
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ffn-add_n-492/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-493/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.850814 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_shift-pad-493
I20220824 02:43:23.850899 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn.time_shift-add_n-494"
device_tag: "cuda"
scope_symbol_id: 2252
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn.time_shift-pad-493/y_0"
      s: "model.blocks.4.ffn-add_n-488/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn.time_shift-add_n-494/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.851140 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn.time_shift-add_n-494
I20220824 02:43:23.851434 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-constant-495"
device_tag: "cuda"
scope_symbol_id: 2259
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln2-constant-495/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.851598 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-constant-495
I20220824 02:43:23.851923 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-constant-496"
device_tag: "cuda"
scope_symbol_id: 2259
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln2-constant-496/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.852082 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-constant-496
I20220824 02:43:23.852171 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-layer_norm_param_grad-497"
device_tag: "cuda"
scope_symbol_id: 2259
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn.time_shift-add_n-494/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-248/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.4.ln2-layer_norm_param_grad-497/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.4.ln2-layer_norm_param_grad-497/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.852442 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-layer_norm_param_grad-497
I20220824 02:43:23.852545 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-layer_norm_grad-498"
device_tag: "cuda"
scope_symbol_id: 2259
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.ffn.time_shift-add_n-494/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln2-layer_norm-249/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4-add_n-248/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ln2-layer_norm_grad-498/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.852833 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-layer_norm_grad-498
I20220824 02:43:23.852926 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln2-add_n-499"
device_tag: "cuda"
scope_symbol_id: 2259
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ln2-layer_norm_grad-498/dx_0"
      s: "model.blocks.5.ln1-add_n-463/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln2-add_n-499/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.853109 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln2-add_n-499
I20220824 02:43:23.853204 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-scalar_mul-500"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-reduce_sum_like-491/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-scalar_mul-500/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.853394 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-scalar_mul-500
I20220824 02:43:23.853478 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ffn-add_n-501"
device_tag: "cuda"
scope_symbol_id: 2156
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ffn-scalar_mul-500/out_0"
      s: "model.blocks.4.ffn-reduce_sum_like-487/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ffn-add_n-501/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.853657 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ffn-add_n-501
I20220824 02:43:23.853763 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-broadcast_matmul-502"
device_tag: "cuda"
scope_symbol_id: 2284
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ln2-add_n-499/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-502/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.853982 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-broadcast_matmul-502
I20220824 02:43:23.854069 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.output-broadcast_matmul_grad_b-503"
device_tag: "cuda"
scope_symbol_id: 2284
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.ln2-add_n-499/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.output-hierarchical_parallel_cast-246/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul_grad_b-503/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.854275 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.output-broadcast_matmul_grad_b-503
I20220824 02:43:23.854576 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-504"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-502/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-wkv-242/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-504/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.854775 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-504
I20220824 02:43:23.854861 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-505"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.output-broadcast_matmul-502/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-sigmoid_v2-241/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-505/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.855057 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-505
I20220824 02:43:23.855151 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-sigmoid_v2_grad-506"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.att-broadcast_mul-504/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-240/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.att-sigmoid_v2_grad-506/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.855326 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-sigmoid_v2_grad-506
I20220824 02:43:23.855424 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-wkv_grad-507"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "wkv_grad"
  input {
    key: "gy"
    value {
      s: "model.blocks.4.att-broadcast_mul-505/z_0"
    }
  }
  input {
    key: "k"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-236/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.4.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-238/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.4.att-scalar_mul-244/out_0"
    }
  }
  output {
    key: "gk"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gk_0"
    }
  }
  output {
    key: "gu"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gu_0"
    }
  }
  output {
    key: "gv"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gv_0"
    }
  }
  output {
    key: "gw"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gw_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  input_order: "gy"
  output_order: "gw"
  output_order: "gu"
  output_order: "gk"
  output_order: "gv"
}

I20220824 02:43:23.855746 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-wkv_grad-507
I20220824 02:43:23.855852 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum-508"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gw_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.4.att-reduce_sum-508/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.856047 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum-508
I20220824 02:43:23.856137 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum-509"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gu_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.4.att-reduce_sum-509/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.856324 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum-509
I20220824 02:43:23.856427 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-broadcast_matmul-510"
device_tag: "cuda"
scope_symbol_id: 2315
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-sigmoid_v2_grad-506/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-510/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.856652 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-broadcast_matmul-510
I20220824 02:43:23.856739 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.receptance-broadcast_matmul_grad_b-511"
device_tag: "cuda"
scope_symbol_id: 2315
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-sigmoid_v2_grad-506/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.receptance-hierarchical_parallel_cast-239/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul_grad_b-511/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.856936 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.receptance-broadcast_matmul_grad_b-511
I20220824 02:43:23.857115 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.key-broadcast_matmul-512"
device_tag: "cuda"
scope_symbol_id: 2324
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-512/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.857344 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key-broadcast_matmul-512
I20220824 02:43:23.857434 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.key-broadcast_matmul_grad_b-513"
device_tag: "cuda"
scope_symbol_id: 2324
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.key-hierarchical_parallel_cast-235/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul_grad_b-513/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.857630 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.key-broadcast_matmul_grad_b-513
I20220824 02:43:23.857738 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.value-broadcast_matmul-514"
device_tag: "cuda"
scope_symbol_id: 2331
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-514/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.857956 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value-broadcast_matmul-514
I20220824 02:43:23.858043 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.value-broadcast_matmul_grad_b-515"
device_tag: "cuda"
scope_symbol_id: 2331
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.4.att-wkv_grad-507/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.4.att.value-hierarchical_parallel_cast-237/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul_grad_b-515/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.858233 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.value-broadcast_matmul_grad_b-515
I20220824 02:43:23.858952 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-516"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-510/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-516/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.859177 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-516
I20220824 02:43:23.859262 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-517"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-510/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-517/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.859455 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-517
I20220824 02:43:23.859544 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-518"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-517/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-518/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.859750 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-518
I20220824 02:43:23.859843 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-519"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-510/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-232/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-519/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.860040 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-519
I20220824 02:43:23.860121 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-520"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.receptance-broadcast_matmul-510/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-520/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.860306 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-520
I20220824 02:43:23.860394 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-521"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att-scalar_add-232/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-520/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-521/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.860608 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-521
I20220824 02:43:23.860705 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-522"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-512/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-522/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.860905 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-522
I20220824 02:43:23.860987 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-523"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-512/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-523/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.861173 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-523
I20220824 02:43:23.861259 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-524"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-523/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-524/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.861482 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-524
I20220824 02:43:23.861568 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-525"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-522/z_0"
      s: "model.blocks.4.att-broadcast_mul-516/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-525/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.861747 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-525
I20220824 02:43:23.861836 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-526"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-512/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-222/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-526/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.862027 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-526
I20220824 02:43:23.862113 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-527"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.key-broadcast_matmul-512/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-527/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.862304 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-527
I20220824 02:43:23.862391 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-528"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att-scalar_add-222/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-527/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-528/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.862612 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-528
I20220824 02:43:23.862695 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-529"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-526/z_0"
      s: "model.blocks.4.att-broadcast_mul-519/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-529/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.863406 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-529
I20220824 02:43:23.863503 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-530"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-514/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-530/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.863688 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-530
I20220824 02:43:23.863777 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-531"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-514/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-531/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.863962 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-531
I20220824 02:43:23.864049 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-532"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att.time_mix_v/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-531/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-532/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.864276 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-532
I20220824 02:43:23.864362 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-533"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-530/z_0"
      s: "model.blocks.4.att-add_n-525/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-533/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.864550 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-533
I20220824 02:43:23.864639 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-534"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-514/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att-scalar_add-227/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-534/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.864830 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-534
I20220824 02:43:23.864912 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-broadcast_mul-535"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att.value-broadcast_matmul-514/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-219/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.4.att-broadcast_mul-535/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.865096 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-broadcast_mul-535
I20220824 02:43:23.865185 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-reduce_sum_like-536"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.4.att-scalar_add-227/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-broadcast_mul-535/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att-reduce_sum_like-536/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.865404 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-reduce_sum_like-536
I20220824 02:43:23.865490 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-537"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-broadcast_mul-534/z_0"
      s: "model.blocks.4.att-add_n-529/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-537/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.865681 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-537
I20220824 02:43:23.865784 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.time_shift-pad-538"
device_tag: "cuda"
scope_symbol_id: 2412
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.4.att-add_n-537/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.4.att.time_shift-pad-538/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.865988 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_shift-pad-538
I20220824 02:43:23.866073 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att.time_shift-add_n-539"
device_tag: "cuda"
scope_symbol_id: 2412
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att.time_shift-pad-538/y_0"
      s: "model.blocks.4.att-add_n-533/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att.time_shift-add_n-539/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.866242 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att.time_shift-add_n-539
I20220824 02:43:23.866333 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-540"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-reduce_sum_like-521/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-540/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.866516 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-540
I20220824 02:43:23.866600 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-541"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-540/out_0"
      s: "model.blocks.4.att-reduce_sum_like-518/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-541/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.866773 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-541
I20220824 02:43:23.866860 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-542"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-reduce_sum_like-528/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-542/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.867048 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-542
I20220824 02:43:23.867141 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-543"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-542/out_0"
      s: "model.blocks.4.att-reduce_sum_like-524/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-543/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.867316 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-543
I20220824 02:43:23.867616 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-constant-544"
device_tag: "cuda"
scope_symbol_id: 2431
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln1-constant-544/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.867789 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-constant-544
I20220824 02:43:23.868077 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-constant-545"
device_tag: "cuda"
scope_symbol_id: 2431
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln1-constant-545/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.868255 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-constant-545
I20220824 02:43:23.868346 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-layer_norm_param_grad-546"
device_tag: "cuda"
scope_symbol_id: 2431
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.att.time_shift-add_n-539/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-217/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.4.ln1-layer_norm_param_grad-546/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.4.ln1-layer_norm_param_grad-546/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.868623 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-layer_norm_param_grad-546
I20220824 02:43:23.868723 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-layer_norm_grad-547"
device_tag: "cuda"
scope_symbol_id: 2431
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.4.att.time_shift-add_n-539/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.4.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.4.ln1-layer_norm-218/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-217/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.4.ln1-layer_norm_grad-547/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.869004 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-layer_norm_grad-547
I20220824 02:43:23.869093 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.ln1-add_n-548"
device_tag: "cuda"
scope_symbol_id: 2431
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.ln1-layer_norm_grad-547/dx_0"
      s: "model.blocks.4.ln2-add_n-499/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.ln1-add_n-548/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.869272 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.ln1-add_n-548
I20220824 02:43:23.869366 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-scalar_mul-549"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-reduce_sum_like-536/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-scalar_mul-549/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.869570 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-scalar_mul-549
I20220824 02:43:23.869654 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.4.att-add_n-550"
device_tag: "cuda"
scope_symbol_id: 2295
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.4.att-scalar_mul-549/out_0"
      s: "model.blocks.4.att-reduce_sum_like-532/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.4.att-add_n-550/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.869825 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.4.att-add_n-550
I20220824 02:43:23.869949 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-551"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-add_n-548/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-212/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-551/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.870144 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-551
I20220824 02:43:23.870229 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-552"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.4.ln1-add_n-548/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2-215/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-552/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.870424 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-552
I20220824 02:43:23.870525 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-sigmoid_v2_grad-553"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-551/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-214/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2_grad-553/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.870699 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-sigmoid_v2_grad-553
I20220824 02:43:23.870805 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-broadcast_matmul-554"
device_tag: "cuda"
scope_symbol_id: 2474
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-552/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-554/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.871026 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-broadcast_matmul-554
I20220824 02:43:23.871114 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.value-broadcast_matmul_grad_b-555"
device_tag: "cuda"
scope_symbol_id: 2474
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-552/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.value-hierarchical_parallel_cast-211/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul_grad_b-555/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.871307 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.value-broadcast_matmul_grad_b-555
I20220824 02:43:23.871399 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-broadcast_matmul-556"
device_tag: "cuda"
scope_symbol_id: 2481
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2_grad-553/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.871619 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-broadcast_matmul-556
I20220824 02:43:23.871711 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557"
device_tag: "cuda"
scope_symbol_id: 2481
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-sigmoid_v2_grad-553/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.receptance-hierarchical_parallel_cast-213/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.871906 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557
I20220824 02:43:23.872398 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-square_grad-558"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn.value-broadcast_matmul-554/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-relu-209/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ffn-square_grad-558/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.872584 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-square_grad-558
I20220824 02:43:23.872681 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-relu_grad-559"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn-square_grad-558/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-relu-209/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ffn-relu_grad-559/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 02:43:23.872849 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-relu_grad-559
I20220824 02:43:23.872933 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-560"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-560/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.873127 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-560
I20220824 02:43:23.873207 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-561"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-561/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.873402 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-561
I20220824 02:43:23.873492 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-562"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-561/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-562/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.873704 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-562
I20220824 02:43:23.873795 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-563"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-204/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-563/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.873980 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-563
I20220824 02:43:23.874060 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-564"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-196/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-564/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.874248 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-564
I20220824 02:43:23.874336 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-565"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn-scalar_add-204/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-564/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-565/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.874557 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-565
I20220824 02:43:23.874667 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-broadcast_matmul-566"
device_tag: "cuda"
scope_symbol_id: 2516
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-relu_grad-559/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-566/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.874899 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-broadcast_matmul-566
I20220824 02:43:23.874987 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.key-broadcast_matmul_grad_b-567"
device_tag: "cuda"
scope_symbol_id: 2516
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ffn-relu_grad-559/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.ffn.key-hierarchical_parallel_cast-207/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul_grad_b-567/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.875182 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.key-broadcast_matmul_grad_b-567
I20220824 02:43:23.875471 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-568"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-565/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-568/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.875667 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-568
I20220824 02:43:23.875759 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-569"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-568/out_0"
      s: "model.blocks.3.ffn-reduce_sum_like-562/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-569/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.875933 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-569
I20220824 02:43:23.876045 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-570"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-566/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-570/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.876237 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-570
I20220824 02:43:23.876318 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-571"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-566/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-571/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.876515 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-571
I20220824 02:43:23.876605 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-572"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-571/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-572/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.876828 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-572
I20220824 02:43:23.876914 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-573"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-570/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-560/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-573/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.877101 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-573
I20220824 02:43:23.877187 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-574"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-566/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn-scalar_add-199/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-574/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.877364 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-574
I20220824 02:43:23.877445 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-broadcast_mul-575"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn.key-broadcast_matmul-566/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-196/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-575/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.877626 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-broadcast_mul-575
I20220824 02:43:23.877712 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-reduce_sum_like-576"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.ffn-scalar_add-199/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-575/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-576/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.877936 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-reduce_sum_like-576
I20220824 02:43:23.878027 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-577"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-broadcast_mul-574/z_0"
      s: "model.blocks.3.ffn-broadcast_mul-563/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-577/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.878211 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-577
I20220824 02:43:23.878314 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.time_shift-pad-578"
device_tag: "cuda"
scope_symbol_id: 2557
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ffn-add_n-577/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-578/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.878505 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_shift-pad-578
I20220824 02:43:23.878592 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn.time_shift-add_n-579"
device_tag: "cuda"
scope_symbol_id: 2557
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn.time_shift-pad-578/y_0"
      s: "model.blocks.3.ffn-add_n-573/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn.time_shift-add_n-579/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.878751 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn.time_shift-add_n-579
I20220824 02:43:23.879046 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-constant-580"
device_tag: "cuda"
scope_symbol_id: 2564
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln2-constant-580/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.879217 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-constant-580
I20220824 02:43:23.879501 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-constant-581"
device_tag: "cuda"
scope_symbol_id: 2564
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln2-constant-581/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.879671 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-constant-581
I20220824 02:43:23.879771 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-layer_norm_param_grad-582"
device_tag: "cuda"
scope_symbol_id: 2564
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn.time_shift-add_n-579/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-194/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.3.ln2-layer_norm_param_grad-582/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.3.ln2-layer_norm_param_grad-582/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.880046 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-layer_norm_param_grad-582
I20220824 02:43:23.880146 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-layer_norm_grad-583"
device_tag: "cuda"
scope_symbol_id: 2564
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.ffn.time_shift-add_n-579/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln2-layer_norm-195/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3-add_n-194/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ln2-layer_norm_grad-583/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.880422 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-layer_norm_grad-583
I20220824 02:43:23.880514 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln2-add_n-584"
device_tag: "cuda"
scope_symbol_id: 2564
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ln2-layer_norm_grad-583/dx_0"
      s: "model.blocks.4.ln1-add_n-548/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln2-add_n-584/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.880699 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln2-add_n-584
I20220824 02:43:23.880795 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-scalar_mul-585"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-reduce_sum_like-576/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-scalar_mul-585/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.881013 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-scalar_mul-585
I20220824 02:43:23.881099 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ffn-add_n-586"
device_tag: "cuda"
scope_symbol_id: 2461
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ffn-scalar_mul-585/out_0"
      s: "model.blocks.3.ffn-reduce_sum_like-572/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ffn-add_n-586/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.881280 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ffn-add_n-586
I20220824 02:43:23.881387 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-broadcast_matmul-587"
device_tag: "cuda"
scope_symbol_id: 2589
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ln2-add_n-584/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-587/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.881608 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-broadcast_matmul-587
I20220824 02:43:23.881698 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.output-broadcast_matmul_grad_b-588"
device_tag: "cuda"
scope_symbol_id: 2589
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.ln2-add_n-584/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.output-hierarchical_parallel_cast-192/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul_grad_b-588/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.881894 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.output-broadcast_matmul_grad_b-588
I20220824 02:43:23.882200 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-589"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-587/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-wkv-188/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-589/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.882396 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-589
I20220824 02:43:23.882486 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-590"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.output-broadcast_matmul-587/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-sigmoid_v2-187/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-590/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.882687 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-590
I20220824 02:43:23.882782 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-sigmoid_v2_grad-591"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.att-broadcast_mul-589/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-186/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.att-sigmoid_v2_grad-591/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.882961 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-sigmoid_v2_grad-591
I20220824 02:43:23.883057 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-wkv_grad-592"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "wkv_grad"
  input {
    key: "gy"
    value {
      s: "model.blocks.3.att-broadcast_mul-590/z_0"
    }
  }
  input {
    key: "k"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-182/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.3.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-184/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.3.att-scalar_mul-190/out_0"
    }
  }
  output {
    key: "gk"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gk_0"
    }
  }
  output {
    key: "gu"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gu_0"
    }
  }
  output {
    key: "gv"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gv_0"
    }
  }
  output {
    key: "gw"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gw_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  input_order: "gy"
  output_order: "gw"
  output_order: "gu"
  output_order: "gk"
  output_order: "gv"
}

I20220824 02:43:23.883348 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-wkv_grad-592
I20220824 02:43:23.883448 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum-593"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gw_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.3.att-reduce_sum-593/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.883638 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum-593
I20220824 02:43:23.883734 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum-594"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gu_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.3.att-reduce_sum-594/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.883934 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum-594
I20220824 02:43:23.884040 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-broadcast_matmul-595"
device_tag: "cuda"
scope_symbol_id: 2620
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-sigmoid_v2_grad-591/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-595/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.884269 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-broadcast_matmul-595
I20220824 02:43:23.884357 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.receptance-broadcast_matmul_grad_b-596"
device_tag: "cuda"
scope_symbol_id: 2620
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-sigmoid_v2_grad-591/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.receptance-hierarchical_parallel_cast-185/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul_grad_b-596/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.884558 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.receptance-broadcast_matmul_grad_b-596
I20220824 02:43:23.884661 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.key-broadcast_matmul-597"
device_tag: "cuda"
scope_symbol_id: 2629
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-597/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.884877 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key-broadcast_matmul-597
I20220824 02:43:23.884964 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.key-broadcast_matmul_grad_b-598"
device_tag: "cuda"
scope_symbol_id: 2629
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.key-hierarchical_parallel_cast-181/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul_grad_b-598/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.885164 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.key-broadcast_matmul_grad_b-598
I20220824 02:43:23.885275 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.value-broadcast_matmul-599"
device_tag: "cuda"
scope_symbol_id: 2636
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-599/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.885489 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value-broadcast_matmul-599
I20220824 02:43:23.885576 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.value-broadcast_matmul_grad_b-600"
device_tag: "cuda"
scope_symbol_id: 2636
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.3.att-wkv_grad-592/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.3.att.value-hierarchical_parallel_cast-183/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul_grad_b-600/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.885768 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.value-broadcast_matmul_grad_b-600
I20220824 02:43:23.886492 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-601"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-595/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-601/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.886690 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-601
I20220824 02:43:23.886775 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-602"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-595/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-602/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.886970 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-602
I20220824 02:43:23.887058 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-603"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-602/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-603/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.887271 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-603
I20220824 02:43:23.887367 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-604"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-595/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-178/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-604/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.887567 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-604
I20220824 02:43:23.887648 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-605"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.receptance-broadcast_matmul-595/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-605/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.887848 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-605
I20220824 02:43:23.887938 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-606"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att-scalar_add-178/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-605/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-606/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.888145 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-606
I20220824 02:43:23.888236 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-607"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-597/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-607/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.888432 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-607
I20220824 02:43:23.888514 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-608"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-597/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-608/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.888703 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-608
I20220824 02:43:23.888798 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-609"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-608/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-609/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.889027 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-609
I20220824 02:43:23.889114 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-610"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-607/z_0"
      s: "model.blocks.3.att-broadcast_mul-601/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-610/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.889295 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-610
I20220824 02:43:23.889384 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-611"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-597/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-168/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-611/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.889572 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-611
I20220824 02:43:23.889652 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-612"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.key-broadcast_matmul-597/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-612/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.889835 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-612
I20220824 02:43:23.889920 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-613"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att-scalar_add-168/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-612/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-613/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.890138 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-613
I20220824 02:43:23.890231 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-614"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-611/z_0"
      s: "model.blocks.3.att-broadcast_mul-604/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-614/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.890426 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-614
I20220824 02:43:23.890516 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-615"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-599/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-615/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.890712 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-615
I20220824 02:43:23.890794 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-616"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-599/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-616/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.890980 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-616
I20220824 02:43:23.891067 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-617"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att.time_mix_v/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-616/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-617/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.891289 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-617
I20220824 02:43:23.891376 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-618"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-615/z_0"
      s: "model.blocks.3.att-add_n-610/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-618/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.891559 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-618
I20220824 02:43:23.891645 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-619"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-599/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att-scalar_add-173/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-619/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.892330 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-619
I20220824 02:43:23.892432 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-broadcast_mul-620"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att.value-broadcast_matmul-599/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-165/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.3.att-broadcast_mul-620/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.892627 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-broadcast_mul-620
I20220824 02:43:23.892716 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-reduce_sum_like-621"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.3.att-scalar_add-173/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-broadcast_mul-620/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att-reduce_sum_like-621/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.892939 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-reduce_sum_like-621
I20220824 02:43:23.893026 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-622"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-broadcast_mul-619/z_0"
      s: "model.blocks.3.att-add_n-614/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-622/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.893211 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-622
I20220824 02:43:23.893313 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.time_shift-pad-623"
device_tag: "cuda"
scope_symbol_id: 2717
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.3.att-add_n-622/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.3.att.time_shift-pad-623/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.893509 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_shift-pad-623
I20220824 02:43:23.893595 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att.time_shift-add_n-624"
device_tag: "cuda"
scope_symbol_id: 2717
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att.time_shift-pad-623/y_0"
      s: "model.blocks.3.att-add_n-618/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att.time_shift-add_n-624/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.893779 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att.time_shift-add_n-624
I20220824 02:43:23.893872 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-625"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-reduce_sum_like-606/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-625/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.894060 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-625
I20220824 02:43:23.894145 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-626"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-625/out_0"
      s: "model.blocks.3.att-reduce_sum_like-603/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-626/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.894323 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-626
I20220824 02:43:23.894412 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-627"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-reduce_sum_like-613/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-627/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.894595 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-627
I20220824 02:43:23.894678 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-628"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-627/out_0"
      s: "model.blocks.3.att-reduce_sum_like-609/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-628/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.894853 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-628
I20220824 02:43:23.895169 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-constant-629"
device_tag: "cuda"
scope_symbol_id: 2736
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln1-constant-629/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.895344 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-constant-629
I20220824 02:43:23.895627 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-constant-630"
device_tag: "cuda"
scope_symbol_id: 2736
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln1-constant-630/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.895807 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-constant-630
I20220824 02:43:23.895901 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-layer_norm_param_grad-631"
device_tag: "cuda"
scope_symbol_id: 2736
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.att.time_shift-add_n-624/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-163/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.3.ln1-layer_norm_param_grad-631/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.3.ln1-layer_norm_param_grad-631/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.896178 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-layer_norm_param_grad-631
I20220824 02:43:23.896281 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-layer_norm_grad-632"
device_tag: "cuda"
scope_symbol_id: 2736
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.3.att.time_shift-add_n-624/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.3.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.3.ln1-layer_norm-164/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-163/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.3.ln1-layer_norm_grad-632/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.896564 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-layer_norm_grad-632
I20220824 02:43:23.896665 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.ln1-add_n-633"
device_tag: "cuda"
scope_symbol_id: 2736
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.ln1-layer_norm_grad-632/dx_0"
      s: "model.blocks.3.ln2-add_n-584/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.ln1-add_n-633/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.896855 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.ln1-add_n-633
I20220824 02:43:23.896953 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-scalar_mul-634"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-reduce_sum_like-621/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-scalar_mul-634/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.897164 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-scalar_mul-634
I20220824 02:43:23.897249 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.3.att-add_n-635"
device_tag: "cuda"
scope_symbol_id: 2600
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.3.att-scalar_mul-634/out_0"
      s: "model.blocks.3.att-reduce_sum_like-617/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.3.att-add_n-635/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.897431 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.3.att-add_n-635
I20220824 02:43:23.897557 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-636"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-add_n-633/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-158/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-636/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.897745 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-636
I20220824 02:43:23.897828 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-637"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.3.ln1-add_n-633/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2-161/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-637/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.898006 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-637
I20220824 02:43:23.898108 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-sigmoid_v2_grad-638"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-636/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-160/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2_grad-638/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.898289 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-sigmoid_v2_grad-638
I20220824 02:43:23.898399 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-broadcast_matmul-639"
device_tag: "cuda"
scope_symbol_id: 2779
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-637/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-639/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.898623 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-broadcast_matmul-639
I20220824 02:43:23.898713 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.value-broadcast_matmul_grad_b-640"
device_tag: "cuda"
scope_symbol_id: 2779
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-637/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.value-hierarchical_parallel_cast-157/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul_grad_b-640/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.898911 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.value-broadcast_matmul_grad_b-640
I20220824 02:43:23.899004 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-broadcast_matmul-641"
device_tag: "cuda"
scope_symbol_id: 2786
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2_grad-638/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.899216 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-broadcast_matmul-641
I20220824 02:43:23.899302 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642"
device_tag: "cuda"
scope_symbol_id: 2786
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-sigmoid_v2_grad-638/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.receptance-hierarchical_parallel_cast-159/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.899493 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642
I20220824 02:43:23.900388 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-square_grad-643"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn.value-broadcast_matmul-639/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-relu-155/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ffn-square_grad-643/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.901170 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-square_grad-643
I20220824 02:43:23.901538 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-relu_grad-644"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn-square_grad-643/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-relu-155/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ffn-relu_grad-644/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 02:43:23.902148 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-relu_grad-644
I20220824 02:43:23.902446 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-645"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-645/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.903134 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-645
I20220824 02:43:23.903415 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-646"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-646/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.904071 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-646
I20220824 02:43:23.904323 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-647"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-646/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-647/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.904909 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-647
I20220824 02:43:23.905154 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-648"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-150/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-648/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.905707 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-648
I20220824 02:43:23.905930 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-649"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-142/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-649/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.906435 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-649
I20220824 02:43:23.906672 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-650"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn-scalar_add-150/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-649/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-650/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.907281 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-650
I20220824 02:43:23.907546 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-broadcast_matmul-651"
device_tag: "cuda"
scope_symbol_id: 2821
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-relu_grad-644/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-651/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.908164 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-broadcast_matmul-651
I20220824 02:43:23.908404 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.key-broadcast_matmul_grad_b-652"
device_tag: "cuda"
scope_symbol_id: 2821
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ffn-relu_grad-644/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.ffn.key-hierarchical_parallel_cast-153/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul_grad_b-652/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.908934 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.key-broadcast_matmul_grad_b-652
I20220824 02:43:23.909467 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-653"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-650/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-653/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.910008 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-653
I20220824 02:43:23.910240 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-654"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-653/out_0"
      s: "model.blocks.2.ffn-reduce_sum_like-647/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-654/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.910708 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-654
I20220824 02:43:23.910971 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-655"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-651/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-655/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.911507 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-655
I20220824 02:43:23.911746 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-656"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-651/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-656/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.912267 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-656
I20220824 02:43:23.912504 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-657"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-656/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-657/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.913069 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-657
I20220824 02:43:23.913300 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-658"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-655/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-645/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-658/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.913827 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-658
I20220824 02:43:23.914060 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-659"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-651/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn-scalar_add-145/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-659/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.914556 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-659
I20220824 02:43:23.914772 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-broadcast_mul-660"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn.key-broadcast_matmul-651/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-142/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-660/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.915268 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-broadcast_mul-660
I20220824 02:43:23.915501 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-reduce_sum_like-661"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.ffn-scalar_add-145/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-660/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-661/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.916142 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-reduce_sum_like-661
I20220824 02:43:23.916262 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-662"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-broadcast_mul-659/z_0"
      s: "model.blocks.2.ffn-broadcast_mul-648/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-662/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.916446 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-662
I20220824 02:43:23.916553 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.time_shift-pad-663"
device_tag: "cuda"
scope_symbol_id: 2862
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ffn-add_n-662/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-663/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.916762 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_shift-pad-663
I20220824 02:43:23.916851 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn.time_shift-add_n-664"
device_tag: "cuda"
scope_symbol_id: 2862
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn.time_shift-pad-663/y_0"
      s: "model.blocks.2.ffn-add_n-658/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn.time_shift-add_n-664/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.917026 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn.time_shift-add_n-664
I20220824 02:43:23.917330 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-constant-665"
device_tag: "cuda"
scope_symbol_id: 2869
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln2-constant-665/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.917500 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-constant-665
I20220824 02:43:23.917799 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-constant-666"
device_tag: "cuda"
scope_symbol_id: 2869
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln2-constant-666/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.917958 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-constant-666
I20220824 02:43:23.918051 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-layer_norm_param_grad-667"
device_tag: "cuda"
scope_symbol_id: 2869
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn.time_shift-add_n-664/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-140/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.2.ln2-layer_norm_param_grad-667/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.2.ln2-layer_norm_param_grad-667/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.918330 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-layer_norm_param_grad-667
I20220824 02:43:23.918431 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-layer_norm_grad-668"
device_tag: "cuda"
scope_symbol_id: 2869
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.ffn.time_shift-add_n-664/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln2-layer_norm-141/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2-add_n-140/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ln2-layer_norm_grad-668/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.918709 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-layer_norm_grad-668
I20220824 02:43:23.918804 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln2-add_n-669"
device_tag: "cuda"
scope_symbol_id: 2869
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ln2-layer_norm_grad-668/dx_0"
      s: "model.blocks.3.ln1-add_n-633/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln2-add_n-669/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.918989 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln2-add_n-669
I20220824 02:43:23.919087 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-scalar_mul-670"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-reduce_sum_like-661/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-scalar_mul-670/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.919301 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-scalar_mul-670
I20220824 02:43:23.919385 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ffn-add_n-671"
device_tag: "cuda"
scope_symbol_id: 2766
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ffn-scalar_mul-670/out_0"
      s: "model.blocks.2.ffn-reduce_sum_like-657/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ffn-add_n-671/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.919566 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ffn-add_n-671
I20220824 02:43:23.919674 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-broadcast_matmul-672"
device_tag: "cuda"
scope_symbol_id: 2894
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ln2-add_n-669/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-672/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.919919 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-broadcast_matmul-672
I20220824 02:43:23.920008 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.output-broadcast_matmul_grad_b-673"
device_tag: "cuda"
scope_symbol_id: 2894
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.ln2-add_n-669/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.output-hierarchical_parallel_cast-138/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul_grad_b-673/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.920205 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.output-broadcast_matmul_grad_b-673
I20220824 02:43:23.920934 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-674"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-672/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-wkv-134/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-674/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.921129 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-674
I20220824 02:43:23.921212 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-675"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.output-broadcast_matmul-672/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-sigmoid_v2-133/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-675/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.921391 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-675
I20220824 02:43:23.921485 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-sigmoid_v2_grad-676"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.att-broadcast_mul-674/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-132/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.att-sigmoid_v2_grad-676/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.921661 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-sigmoid_v2_grad-676
I20220824 02:43:23.921756 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-wkv_grad-677"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "wkv_grad"
  input {
    key: "gy"
    value {
      s: "model.blocks.2.att-broadcast_mul-675/z_0"
    }
  }
  input {
    key: "k"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-128/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.2.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-130/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.2.att-scalar_mul-136/out_0"
    }
  }
  output {
    key: "gk"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gk_0"
    }
  }
  output {
    key: "gu"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gu_0"
    }
  }
  output {
    key: "gv"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gv_0"
    }
  }
  output {
    key: "gw"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gw_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  input_order: "gy"
  output_order: "gw"
  output_order: "gu"
  output_order: "gk"
  output_order: "gv"
}

I20220824 02:43:23.922053 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-wkv_grad-677
I20220824 02:43:23.922155 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum-678"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gw_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.2.att-reduce_sum-678/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.922345 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum-678
I20220824 02:43:23.922432 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum-679"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gu_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.2.att-reduce_sum-679/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.922617 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum-679
I20220824 02:43:23.922721 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-broadcast_matmul-680"
device_tag: "cuda"
scope_symbol_id: 2925
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-sigmoid_v2_grad-676/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-680/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.922952 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-broadcast_matmul-680
I20220824 02:43:23.923039 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.receptance-broadcast_matmul_grad_b-681"
device_tag: "cuda"
scope_symbol_id: 2925
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-sigmoid_v2_grad-676/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.receptance-hierarchical_parallel_cast-131/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul_grad_b-681/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.923240 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.receptance-broadcast_matmul_grad_b-681
I20220824 02:43:23.923346 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.key-broadcast_matmul-682"
device_tag: "cuda"
scope_symbol_id: 2934
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-682/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.923559 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key-broadcast_matmul-682
I20220824 02:43:23.923645 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.key-broadcast_matmul_grad_b-683"
device_tag: "cuda"
scope_symbol_id: 2934
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.key-hierarchical_parallel_cast-127/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul_grad_b-683/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.923844 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.key-broadcast_matmul_grad_b-683
I20220824 02:43:23.923952 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.value-broadcast_matmul-684"
device_tag: "cuda"
scope_symbol_id: 2941
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-684/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.924170 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value-broadcast_matmul-684
I20220824 02:43:23.924257 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.value-broadcast_matmul_grad_b-685"
device_tag: "cuda"
scope_symbol_id: 2941
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.2.att-wkv_grad-677/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.2.att.value-hierarchical_parallel_cast-129/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul_grad_b-685/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.924461 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.value-broadcast_matmul_grad_b-685
I20220824 02:43:23.925580 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-686"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-680/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-686/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.925796 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-686
I20220824 02:43:23.925880 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-687"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-680/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-687/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.926074 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-687
I20220824 02:43:23.926162 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-688"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-687/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-688/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.926362 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-688
I20220824 02:43:23.926451 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-689"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-680/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-124/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-689/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.926647 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-689
I20220824 02:43:23.926728 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-690"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.receptance-broadcast_matmul-680/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-690/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.926926 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-690
I20220824 02:43:23.927013 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-691"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att-scalar_add-124/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-690/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-691/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.927223 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-691
I20220824 02:43:23.927314 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-692"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-682/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-692/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.927512 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-692
I20220824 02:43:23.927593 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-693"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-682/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-693/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.927795 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-693
I20220824 02:43:23.927884 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-694"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-693/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-694/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.928108 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-694
I20220824 02:43:23.928194 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-695"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-692/z_0"
      s: "model.blocks.2.att-broadcast_mul-686/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-695/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.928381 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-695
I20220824 02:43:23.928469 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-696"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-682/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-114/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-696/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.928668 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-696
I20220824 02:43:23.928748 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-697"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.key-broadcast_matmul-682/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-697/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.928934 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-697
I20220824 02:43:23.929018 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-698"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att-scalar_add-114/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-697/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-698/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.929235 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-698
I20220824 02:43:23.929319 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-699"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-696/z_0"
      s: "model.blocks.2.att-broadcast_mul-689/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-699/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.929504 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-699
I20220824 02:43:23.929591 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-700"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-684/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-700/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.929781 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-700
I20220824 02:43:23.929862 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-701"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-684/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-701/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.930053 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-701
I20220824 02:43:23.930140 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-702"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att.time_mix_v/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-701/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-702/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.930361 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-702
I20220824 02:43:23.930447 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-703"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-700/z_0"
      s: "model.blocks.2.att-add_n-695/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-703/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.930636 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-703
I20220824 02:43:23.930725 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-704"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-684/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att-scalar_add-119/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-704/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.930922 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-704
I20220824 02:43:23.931001 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-broadcast_mul-705"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att.value-broadcast_matmul-684/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-111/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.2.att-broadcast_mul-705/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.931188 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-broadcast_mul-705
I20220824 02:43:23.931275 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-reduce_sum_like-706"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.2.att-scalar_add-119/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-broadcast_mul-705/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att-reduce_sum_like-706/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.931504 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-reduce_sum_like-706
I20220824 02:43:23.931591 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-707"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-broadcast_mul-704/z_0"
      s: "model.blocks.2.att-add_n-699/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-707/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.931782 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-707
I20220824 02:43:23.931886 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.time_shift-pad-708"
device_tag: "cuda"
scope_symbol_id: 3022
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.2.att-add_n-707/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.2.att.time_shift-pad-708/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.932545 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_shift-pad-708
I20220824 02:43:23.932647 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att.time_shift-add_n-709"
device_tag: "cuda"
scope_symbol_id: 3022
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att.time_shift-pad-708/y_0"
      s: "model.blocks.2.att-add_n-703/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att.time_shift-add_n-709/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.932829 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att.time_shift-add_n-709
I20220824 02:43:23.932924 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-710"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-reduce_sum_like-691/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-710/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.933109 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-710
I20220824 02:43:23.933194 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-711"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-710/out_0"
      s: "model.blocks.2.att-reduce_sum_like-688/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-711/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.933382 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-711
I20220824 02:43:23.933473 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-712"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-reduce_sum_like-698/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-712/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.933658 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-712
I20220824 02:43:23.933743 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-713"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-712/out_0"
      s: "model.blocks.2.att-reduce_sum_like-694/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-713/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.933917 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-713
I20220824 02:43:23.934223 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-constant-714"
device_tag: "cuda"
scope_symbol_id: 3041
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln1-constant-714/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.934391 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-constant-714
I20220824 02:43:23.934679 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-constant-715"
device_tag: "cuda"
scope_symbol_id: 3041
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln1-constant-715/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.934834 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-constant-715
I20220824 02:43:23.934931 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-layer_norm_param_grad-716"
device_tag: "cuda"
scope_symbol_id: 3041
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.att.time_shift-add_n-709/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-109/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.2.ln1-layer_norm_param_grad-716/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.2.ln1-layer_norm_param_grad-716/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.935209 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-layer_norm_param_grad-716
I20220824 02:43:23.935307 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-layer_norm_grad-717"
device_tag: "cuda"
scope_symbol_id: 3041
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.2.att.time_shift-add_n-709/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.2.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.2.ln1-layer_norm-110/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-109/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.2.ln1-layer_norm_grad-717/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.935586 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-layer_norm_grad-717
I20220824 02:43:23.935680 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.ln1-add_n-718"
device_tag: "cuda"
scope_symbol_id: 3041
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.ln1-layer_norm_grad-717/dx_0"
      s: "model.blocks.2.ln2-add_n-669/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.ln1-add_n-718/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.935874 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.ln1-add_n-718
I20220824 02:43:23.935971 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-scalar_mul-719"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-reduce_sum_like-706/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-scalar_mul-719/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.936183 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-scalar_mul-719
I20220824 02:43:23.936275 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.2.att-add_n-720"
device_tag: "cuda"
scope_symbol_id: 2905
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.2.att-scalar_mul-719/out_0"
      s: "model.blocks.2.att-reduce_sum_like-702/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.2.att-add_n-720/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.936462 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.2.att-add_n-720
I20220824 02:43:23.936589 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-721"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-add_n-718/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-104/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-721/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.936774 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-721
I20220824 02:43:23.936856 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-722"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.2.ln1-add_n-718/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2-107/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-722/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.937036 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-722
I20220824 02:43:23.937139 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-sigmoid_v2_grad-723"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-721/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-106/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2_grad-723/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.937310 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-sigmoid_v2_grad-723
I20220824 02:43:23.937412 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-broadcast_matmul-724"
device_tag: "cuda"
scope_symbol_id: 3084
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-722/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-724/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.937631 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-broadcast_matmul-724
I20220824 02:43:23.937719 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.value-broadcast_matmul_grad_b-725"
device_tag: "cuda"
scope_symbol_id: 3084
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-722/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.value-hierarchical_parallel_cast-103/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul_grad_b-725/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.937923 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.value-broadcast_matmul_grad_b-725
I20220824 02:43:23.938017 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-broadcast_matmul-726"
device_tag: "cuda"
scope_symbol_id: 3091
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2_grad-723/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.938234 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-broadcast_matmul-726
I20220824 02:43:23.938320 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727"
device_tag: "cuda"
scope_symbol_id: 3091
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-sigmoid_v2_grad-723/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.receptance-hierarchical_parallel_cast-105/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.938515 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727
I20220824 02:43:23.939002 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-square_grad-728"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn.value-broadcast_matmul-724/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-relu-101/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ffn-square_grad-728/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.939193 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-square_grad-728
I20220824 02:43:23.939294 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-relu_grad-729"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn-square_grad-728/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-relu-101/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ffn-relu_grad-729/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 02:43:23.939463 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-relu_grad-729
I20220824 02:43:23.939556 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-730"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-730/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.939770 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-730
I20220824 02:43:23.939855 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-731"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-731/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.940047 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-731
I20220824 02:43:23.940135 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-732"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-731/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-732/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.940344 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-732
I20220824 02:43:23.940438 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-733"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-96/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-733/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.940637 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-733
I20220824 02:43:23.940719 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-734"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-88/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-734/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.940905 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-734
I20220824 02:43:23.940994 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-735"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn-scalar_add-96/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-734/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-735/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.941227 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-735
I20220824 02:43:23.941336 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-broadcast_matmul-736"
device_tag: "cuda"
scope_symbol_id: 3126
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-relu_grad-729/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-736/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.941560 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-broadcast_matmul-736
I20220824 02:43:23.941646 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.key-broadcast_matmul_grad_b-737"
device_tag: "cuda"
scope_symbol_id: 3126
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ffn-relu_grad-729/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.ffn.key-hierarchical_parallel_cast-99/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul_grad_b-737/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.941840 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.key-broadcast_matmul_grad_b-737
I20220824 02:43:23.942147 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-738"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-735/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-738/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.942345 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-738
I20220824 02:43:23.942431 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-739"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-738/out_0"
      s: "model.blocks.1.ffn-reduce_sum_like-732/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-739/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.942603 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-739
I20220824 02:43:23.942723 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-740"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-736/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-740/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.942929 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-740
I20220824 02:43:23.943012 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-741"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-736/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-741/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.943202 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-741
I20220824 02:43:23.943291 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-742"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-741/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-742/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.943519 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-742
I20220824 02:43:23.943606 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-743"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-740/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-730/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-743/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.943802 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-743
I20220824 02:43:23.943892 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-744"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-736/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn-scalar_add-91/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-744/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.944074 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-744
I20220824 02:43:23.944155 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-broadcast_mul-745"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn.key-broadcast_matmul-736/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-88/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-745/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.944350 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-broadcast_mul-745
I20220824 02:43:23.944443 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-reduce_sum_like-746"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.ffn-scalar_add-91/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-745/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-746/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.944665 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-reduce_sum_like-746
I20220824 02:43:23.944751 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-747"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-broadcast_mul-744/z_0"
      s: "model.blocks.1.ffn-broadcast_mul-733/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-747/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.944934 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-747
I20220824 02:43:23.945037 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.time_shift-pad-748"
device_tag: "cuda"
scope_symbol_id: 3167
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ffn-add_n-747/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-748/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.945231 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_shift-pad-748
I20220824 02:43:23.945317 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn.time_shift-add_n-749"
device_tag: "cuda"
scope_symbol_id: 3167
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn.time_shift-pad-748/y_0"
      s: "model.blocks.1.ffn-add_n-743/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn.time_shift-add_n-749/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.945487 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn.time_shift-add_n-749
I20220824 02:43:23.945811 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-constant-750"
device_tag: "cuda"
scope_symbol_id: 3174
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln2-constant-750/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.945991 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-constant-750
I20220824 02:43:23.946302 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-constant-751"
device_tag: "cuda"
scope_symbol_id: 3174
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln2-constant-751/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.946460 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-constant-751
I20220824 02:43:23.946554 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-layer_norm_param_grad-752"
device_tag: "cuda"
scope_symbol_id: 3174
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn.time_shift-add_n-749/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-86/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.1.ln2-layer_norm_param_grad-752/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.1.ln2-layer_norm_param_grad-752/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.946832 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-layer_norm_param_grad-752
I20220824 02:43:23.946944 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-layer_norm_grad-753"
device_tag: "cuda"
scope_symbol_id: 3174
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.ffn.time_shift-add_n-749/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln2-layer_norm-87/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1-add_n-86/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ln2-layer_norm_grad-753/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.947232 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-layer_norm_grad-753
I20220824 02:43:23.947322 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln2-add_n-754"
device_tag: "cuda"
scope_symbol_id: 3174
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ln2-layer_norm_grad-753/dx_0"
      s: "model.blocks.2.ln1-add_n-718/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln2-add_n-754/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.947511 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln2-add_n-754
I20220824 02:43:23.947607 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-scalar_mul-755"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-reduce_sum_like-746/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-scalar_mul-755/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.947827 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-scalar_mul-755
I20220824 02:43:23.947914 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ffn-add_n-756"
device_tag: "cuda"
scope_symbol_id: 3071
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ffn-scalar_mul-755/out_0"
      s: "model.blocks.1.ffn-reduce_sum_like-742/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ffn-add_n-756/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.948094 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ffn-add_n-756
I20220824 02:43:23.948202 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-broadcast_matmul-757"
device_tag: "cuda"
scope_symbol_id: 3199
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ln2-add_n-754/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-757/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.948422 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-broadcast_matmul-757
I20220824 02:43:23.948509 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.output-broadcast_matmul_grad_b-758"
device_tag: "cuda"
scope_symbol_id: 3199
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.ln2-add_n-754/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.output-hierarchical_parallel_cast-84/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul_grad_b-758/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.948714 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.output-broadcast_matmul_grad_b-758
I20220824 02:43:23.949309 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-759"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-757/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-wkv-80/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-759/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.949512 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-759
I20220824 02:43:23.949596 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-760"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.output-broadcast_matmul-757/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-sigmoid_v2-79/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-760/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.949790 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-760
I20220824 02:43:23.949887 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-sigmoid_v2_grad-761"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.att-broadcast_mul-759/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-78/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.att-sigmoid_v2_grad-761/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.950069 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-sigmoid_v2_grad-761
I20220824 02:43:23.950166 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-wkv_grad-762"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "wkv_grad"
  input {
    key: "gy"
    value {
      s: "model.blocks.1.att-broadcast_mul-760/z_0"
    }
  }
  input {
    key: "k"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-74/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.1.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-76/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.1.att-scalar_mul-82/out_0"
    }
  }
  output {
    key: "gk"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gk_0"
    }
  }
  output {
    key: "gu"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gu_0"
    }
  }
  output {
    key: "gv"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gv_0"
    }
  }
  output {
    key: "gw"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gw_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  input_order: "gy"
  output_order: "gw"
  output_order: "gu"
  output_order: "gk"
  output_order: "gv"
}

I20220824 02:43:23.950459 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-wkv_grad-762
I20220824 02:43:23.950567 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum-763"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gw_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.1.att-reduce_sum-763/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.950765 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum-763
I20220824 02:43:23.950852 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum-764"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gu_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.1.att-reduce_sum-764/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.951038 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum-764
I20220824 02:43:23.951140 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-broadcast_matmul-765"
device_tag: "cuda"
scope_symbol_id: 3230
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-sigmoid_v2_grad-761/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-765/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.951362 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-broadcast_matmul-765
I20220824 02:43:23.951449 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.receptance-broadcast_matmul_grad_b-766"
device_tag: "cuda"
scope_symbol_id: 3230
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-sigmoid_v2_grad-761/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.receptance-hierarchical_parallel_cast-77/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul_grad_b-766/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.951645 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.receptance-broadcast_matmul_grad_b-766
I20220824 02:43:23.951754 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.key-broadcast_matmul-767"
device_tag: "cuda"
scope_symbol_id: 3239
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-767/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.951982 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key-broadcast_matmul-767
I20220824 02:43:23.952071 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.key-broadcast_matmul_grad_b-768"
device_tag: "cuda"
scope_symbol_id: 3239
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.key-hierarchical_parallel_cast-73/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul_grad_b-768/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.952267 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.key-broadcast_matmul_grad_b-768
I20220824 02:43:23.952373 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.value-broadcast_matmul-769"
device_tag: "cuda"
scope_symbol_id: 3246
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-769/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.952587 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value-broadcast_matmul-769
I20220824 02:43:23.952673 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.value-broadcast_matmul_grad_b-770"
device_tag: "cuda"
scope_symbol_id: 3246
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.1.att-wkv_grad-762/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.1.att.value-hierarchical_parallel_cast-75/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul_grad_b-770/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.952865 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.value-broadcast_matmul_grad_b-770
I20220824 02:43:23.953564 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-771"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-765/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-771/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.953768 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-771
I20220824 02:43:23.953852 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-772"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-765/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-772/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.954057 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-772
I20220824 02:43:23.954146 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-773"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-772/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-773/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.954349 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-773
I20220824 02:43:23.954439 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-774"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-765/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-70/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-774/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.954635 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-774
I20220824 02:43:23.954716 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-775"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.receptance-broadcast_matmul-765/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-775/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.954901 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-775
I20220824 02:43:23.954988 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-776"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att-scalar_add-70/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-775/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-776/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.955212 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-776
I20220824 02:43:23.955302 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-777"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-767/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-777/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.955511 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-777
I20220824 02:43:23.955595 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-778"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-767/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-778/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.955783 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-778
I20220824 02:43:23.955873 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-779"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-778/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-779/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.956099 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-779
I20220824 02:43:23.956187 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-780"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-777/z_0"
      s: "model.blocks.1.att-broadcast_mul-771/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-780/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.956367 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-780
I20220824 02:43:23.956461 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-781"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-767/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-60/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-781/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.956657 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-781
I20220824 02:43:23.956737 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-782"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.key-broadcast_matmul-767/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-782/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.956930 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-782
I20220824 02:43:23.957017 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-783"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att-scalar_add-60/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-782/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-783/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.957230 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-783
I20220824 02:43:23.957316 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-784"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-781/z_0"
      s: "model.blocks.1.att-broadcast_mul-774/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-784/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.957502 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-784
I20220824 02:43:23.957592 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-785"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-769/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-785/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.957783 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-785
I20220824 02:43:23.957863 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-786"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-769/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-786/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.958047 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-786
I20220824 02:43:23.958133 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-787"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att.time_mix_v/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-786/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-787/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.958353 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-787
I20220824 02:43:23.958438 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-788"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-785/z_0"
      s: "model.blocks.1.att-add_n-780/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-788/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.958635 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-788
I20220824 02:43:23.958725 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-789"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-769/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att-scalar_add-65/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-789/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.958923 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-789
I20220824 02:43:23.959004 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-broadcast_mul-790"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att.value-broadcast_matmul-769/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-57/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.1.att-broadcast_mul-790/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.959193 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-broadcast_mul-790
I20220824 02:43:23.959280 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-reduce_sum_like-791"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.1.att-scalar_add-65/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-broadcast_mul-790/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att-reduce_sum_like-791/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.959507 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-reduce_sum_like-791
I20220824 02:43:23.959592 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-792"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-broadcast_mul-789/z_0"
      s: "model.blocks.1.att-add_n-784/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-792/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.959790 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-792
I20220824 02:43:23.959895 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.time_shift-pad-793"
device_tag: "cuda"
scope_symbol_id: 3327
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.1.att-add_n-792/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.1.att.time_shift-pad-793/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.960101 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_shift-pad-793
I20220824 02:43:23.960188 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att.time_shift-add_n-794"
device_tag: "cuda"
scope_symbol_id: 3327
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att.time_shift-pad-793/y_0"
      s: "model.blocks.1.att-add_n-788/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att.time_shift-add_n-794/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.960359 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att.time_shift-add_n-794
I20220824 02:43:23.960453 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-795"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-reduce_sum_like-776/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-795/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.960639 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-795
I20220824 02:43:23.960723 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-796"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-795/out_0"
      s: "model.blocks.1.att-reduce_sum_like-773/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-796/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.960896 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-796
I20220824 02:43:23.960985 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-797"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-reduce_sum_like-783/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-797/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.961187 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-797
I20220824 02:43:23.961272 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-798"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-797/out_0"
      s: "model.blocks.1.att-reduce_sum_like-779/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-798/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.961920 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-798
I20220824 02:43:23.962244 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-constant-799"
device_tag: "cuda"
scope_symbol_id: 3346
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln1-constant-799/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.962419 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-constant-799
I20220824 02:43:23.962703 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-constant-800"
device_tag: "cuda"
scope_symbol_id: 3346
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln1-constant-800/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.962883 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-constant-800
I20220824 02:43:23.962975 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-layer_norm_param_grad-801"
device_tag: "cuda"
scope_symbol_id: 3346
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.att.time_shift-add_n-794/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-55/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.1.ln1-layer_norm_param_grad-801/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.1.ln1-layer_norm_param_grad-801/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.963251 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-layer_norm_param_grad-801
I20220824 02:43:23.963351 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-layer_norm_grad-802"
device_tag: "cuda"
scope_symbol_id: 3346
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.1.att.time_shift-add_n-794/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.1.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.1.ln1-layer_norm-56/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-55/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.1.ln1-layer_norm_grad-802/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.963637 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-layer_norm_grad-802
I20220824 02:43:23.963734 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.ln1-add_n-803"
device_tag: "cuda"
scope_symbol_id: 3346
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.ln1-layer_norm_grad-802/dx_0"
      s: "model.blocks.1.ln2-add_n-754/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.ln1-add_n-803/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.963922 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.ln1-add_n-803
I20220824 02:43:23.964020 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-scalar_mul-804"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-reduce_sum_like-791/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-scalar_mul-804/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.964229 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-scalar_mul-804
I20220824 02:43:23.964314 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.1.att-add_n-805"
device_tag: "cuda"
scope_symbol_id: 3210
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.1.att-scalar_mul-804/out_0"
      s: "model.blocks.1.att-reduce_sum_like-787/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.1.att-add_n-805/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.964494 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.1.att-add_n-805
I20220824 02:43:23.964644 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-806"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-add_n-803/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-50/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-806/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.964831 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-806
I20220824 02:43:23.964915 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-807"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.1.ln1-add_n-803/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2-53/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-807/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.965118 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-807
I20220824 02:43:23.965220 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-sigmoid_v2_grad-808"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-806/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-52/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2_grad-808/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.965394 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-sigmoid_v2_grad-808
I20220824 02:43:23.965499 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-broadcast_matmul-809"
device_tag: "cuda"
scope_symbol_id: 3389
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-807/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-809/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.965719 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-broadcast_matmul-809
I20220824 02:43:23.965807 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.value-broadcast_matmul_grad_b-810"
device_tag: "cuda"
scope_symbol_id: 3389
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-807/z_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.value-hierarchical_parallel_cast-49/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul_grad_b-810/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.966002 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.value-broadcast_matmul_grad_b-810
I20220824 02:43:23.966094 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-broadcast_matmul-811"
device_tag: "cuda"
scope_symbol_id: 3396
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2_grad-808/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.966312 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-broadcast_matmul-811
I20220824 02:43:23.966399 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812"
device_tag: "cuda"
scope_symbol_id: 3396
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-sigmoid_v2_grad-808/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.receptance-hierarchical_parallel_cast-51/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.966598 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812
I20220824 02:43:23.967083 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-square_grad-813"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "square_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn.value-broadcast_matmul-809/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-relu-47/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ffn-square_grad-813/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.967278 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-square_grad-813
I20220824 02:43:23.967382 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-relu_grad-814"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "relu_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn-square_grad-813/dx_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-relu-47/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ffn-relu_grad-814/dx_0"
    }
  }
  input_order: "dy"
  input_order: "y"
  output_order: "dx"
}

I20220824 02:43:23.967555 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-relu_grad-814
I20220824 02:43:23.967641 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-815"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-815/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.967835 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-815
I20220824 02:43:23.967918 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-816"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-816/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.968111 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-816
I20220824 02:43:23.968200 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-817"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-816/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-817/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.968422 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-817
I20220824 02:43:23.968513 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-818"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-42/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-818/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.968713 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-818
I20220824 02:43:23.968794 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-819"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-34/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-819/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.968979 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-819
I20220824 02:43:23.969066 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-820"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn-scalar_add-42/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-819/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-820/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.969269 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-820
I20220824 02:43:23.969375 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-broadcast_matmul-821"
device_tag: "cuda"
scope_symbol_id: 3431
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-relu_grad-814/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-821/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.969594 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-broadcast_matmul-821
I20220824 02:43:23.969686 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.key-broadcast_matmul_grad_b-822"
device_tag: "cuda"
scope_symbol_id: 3431
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ffn-relu_grad-814/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.ffn.key-hierarchical_parallel_cast-45/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul_grad_b-822/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.969893 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.key-broadcast_matmul_grad_b-822
I20220824 02:43:23.970191 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-823"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-820/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-823/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.970392 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-823
I20220824 02:43:23.970479 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-824"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-823/out_0"
      s: "model.blocks.0.ffn-reduce_sum_like-817/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-824/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.970652 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-824
I20220824 02:43:23.970757 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-825"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-821/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-825/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.970958 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-825
I20220824 02:43:23.971040 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-826"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-821/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-826/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.971230 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-826
I20220824 02:43:23.971318 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-827"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-826/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-827/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.971554 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-827
I20220824 02:43:23.971640 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-828"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-825/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-815/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-828/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.971834 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-828
I20220824 02:43:23.971923 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-829"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-821/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn-scalar_add-37/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-829/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.972105 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-829
I20220824 02:43:23.972187 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-broadcast_mul-830"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn.key-broadcast_matmul-821/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-34/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-830/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.972374 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-broadcast_mul-830
I20220824 02:43:23.972462 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-reduce_sum_like-831"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.ffn-scalar_add-37/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-830/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-831/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.972687 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-reduce_sum_like-831
I20220824 02:43:23.972774 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-832"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-broadcast_mul-829/z_0"
      s: "model.blocks.0.ffn-broadcast_mul-818/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-832/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.972967 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-832
I20220824 02:43:23.973078 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.time_shift-pad-833"
device_tag: "cuda"
scope_symbol_id: 3472
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.ffn-add_n-832/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-833/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.973273 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_shift-pad-833
I20220824 02:43:23.973357 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn.time_shift-add_n-834"
device_tag: "cuda"
scope_symbol_id: 3472
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn.time_shift-pad-833/y_0"
      s: "model.blocks.0.ffn-add_n-828/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn.time_shift-add_n-834/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.973531 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn.time_shift-add_n-834
I20220824 02:43:23.973826 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-constant-835"
device_tag: "cuda"
scope_symbol_id: 3479
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln2-constant-835/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.973994 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-constant-835
I20220824 02:43:23.974282 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-constant-836"
device_tag: "cuda"
scope_symbol_id: 3479
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln2-constant-836/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "B"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.974440 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-constant-836
I20220824 02:43:23.974540 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-layer_norm_param_grad-837"
device_tag: "cuda"
scope_symbol_id: 3479
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn.time_shift-add_n-834/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-32/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.0.ln2-layer_norm_param_grad-837/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.0.ln2-layer_norm_param_grad-837/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.974822 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-layer_norm_param_grad-837
I20220824 02:43:23.974926 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-layer_norm_grad-838"
device_tag: "cuda"
scope_symbol_id: 3479
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ffn.time_shift-add_n-834/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln2.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln2-layer_norm-33/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0-add_n-32/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ln2-layer_norm_grad-838/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.975208 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-layer_norm_grad-838
I20220824 02:43:23.975301 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln2-add_n-839"
device_tag: "cuda"
scope_symbol_id: 3479
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ln2-layer_norm_grad-838/dx_0"
      s: "model.blocks.1.ln1-add_n-803/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln2-add_n-839/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.975484 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln2-add_n-839
I20220824 02:43:23.975579 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-scalar_mul-840"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-reduce_sum_like-831/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-scalar_mul-840/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.975782 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-scalar_mul-840
I20220824 02:43:23.975870 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ffn-add_n-841"
device_tag: "cuda"
scope_symbol_id: 3376
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ffn-scalar_mul-840/out_0"
      s: "model.blocks.0.ffn-reduce_sum_like-827/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ffn-add_n-841/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.976055 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ffn-add_n-841
I20220824 02:43:23.976171 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-broadcast_matmul-842"
device_tag: "cuda"
scope_symbol_id: 3504
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ln2-add_n-839/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.output.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-842/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.976392 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-broadcast_matmul-842
I20220824 02:43:23.976480 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.output-broadcast_matmul_grad_b-843"
device_tag: "cuda"
scope_symbol_id: 3504
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.ln2-add_n-839/out_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.output-hierarchical_parallel_cast-30/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul_grad_b-843/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.976675 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.output-broadcast_matmul_grad_b-843
I20220824 02:43:23.976975 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-844"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-842/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-wkv-26/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-844/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.977176 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-844
I20220824 02:43:23.977260 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-845"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.output-broadcast_matmul-842/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-sigmoid_v2-25/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-845/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.977443 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-845
I20220824 02:43:23.977545 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-sigmoid_v2_grad-846"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "sigmoid_v2_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.att-broadcast_mul-844/z_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-24/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.att-sigmoid_v2_grad-846/dx_0"
    }
  }
  input_order: "x"
  input_order: "dy"
  output_order: "dx"
}

I20220824 02:43:23.977728 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-sigmoid_v2_grad-846
I20220824 02:43:23.977825 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-wkv_grad-847"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "wkv_grad"
  input {
    key: "gy"
    value {
      s: "model.blocks.0.att-broadcast_mul-845/z_0"
    }
  }
  input {
    key: "k"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul-20/out_0"
    }
  }
  input {
    key: "u"
    value {
      s: "model.blocks.0.att.time_first/out"
    }
  }
  input {
    key: "v"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul-22/out_0"
    }
  }
  input {
    key: "w"
    value {
      s: "model.blocks.0.att-scalar_mul-28/out_0"
    }
  }
  output {
    key: "gk"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gk_0"
    }
  }
  output {
    key: "gu"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gu_0"
    }
  }
  output {
    key: "gv"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gv_0"
    }
  }
  output {
    key: "gw"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gw_0"
    }
  }
  attr {
    key: "B"
    value {
      at_int64: 8
    }
  }
  attr {
    key: "C"
    value {
      at_int64: 1024
    }
  }
  attr {
    key: "T"
    value {
      at_int64: 1024
    }
  }
  input_order: "w"
  input_order: "u"
  input_order: "k"
  input_order: "v"
  input_order: "gy"
  output_order: "gw"
  output_order: "gu"
  output_order: "gk"
  output_order: "gv"
}

I20220824 02:43:23.978117 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-wkv_grad-847
I20220824 02:43:23.978217 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum-848"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gw_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.0.att-reduce_sum-848/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.978422 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum-848
I20220824 02:43:23.978513 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum-849"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum"
  input {
    key: "input_tensor"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gu_0"
    }
  }
  output {
    key: "output_tensor"
    value {
      s: "model.blocks.0.att-reduce_sum-849/output_tensor_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
      }
    }
  }
  attr {
    key: "keepdims"
    value {
      at_bool: false
    }
  }
  input_order: "input_tensor"
  output_order: "output_tensor"
}

I20220824 02:43:23.978703 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum-849
I20220824 02:43:23.978811 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-broadcast_matmul-850"
device_tag: "cuda"
scope_symbol_id: 3535
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-sigmoid_v2_grad-846/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.receptance.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-850/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.979041 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-broadcast_matmul-850
I20220824 02:43:23.979128 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-broadcast_matmul_grad_b-851"
device_tag: "cuda"
scope_symbol_id: 3535
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-sigmoid_v2_grad-846/dx_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul_grad_b-851/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.979328 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-broadcast_matmul_grad_b-851
I20220824 02:43:23.979444 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-broadcast_matmul-852"
device_tag: "cuda"
scope_symbol_id: 3544
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.key.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul-852/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.979660 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-broadcast_matmul-852
I20220824 02:43:23.979753 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-broadcast_matmul_grad_b-853"
device_tag: "cuda"
scope_symbol_id: 3544
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gk_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul_grad_b-853/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.979955 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-broadcast_matmul_grad_b-853
I20220824 02:43:23.980062 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-broadcast_matmul-854"
device_tag: "cuda"
scope_symbol_id: 3551
loc: ""
user_conf {
  op_type_name: "broadcast_matmul"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.value.weight/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul-854/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  attr {
    key: "transpose_a"
    value {
      at_bool: false
    }
  }
  attr {
    key: "transpose_b"
    value {
      at_bool: false
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.980285 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-broadcast_matmul-854
I20220824 02:43:23.980373 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-broadcast_matmul_grad_b-855"
device_tag: "cuda"
scope_symbol_id: 3551
loc: ""
user_conf {
  op_type_name: "broadcast_matmul_grad_b"
  input {
    key: "a"
    value {
      s: "model.blocks.0.att-wkv_grad-847/gv_0"
    }
  }
  input {
    key: "b"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul_grad_b-855/out_0"
    }
  }
  attr {
    key: "alpha"
    value {
      at_double: 1
    }
  }
  input_order: "a"
  input_order: "b"
  output_order: "out"
}

I20220824 02:43:23.980567 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-broadcast_matmul_grad_b-855
I20220824 02:43:23.980873 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.receptance-hierarchical_parallel_cast-856"
device_tag: "cuda"
scope_symbol_id: 3535
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.receptance-broadcast_matmul-850/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "identity"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.981048 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.receptance-hierarchical_parallel_cast-856
I20220824 02:43:23.981357 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.key-hierarchical_parallel_cast-857"
device_tag: "cuda"
scope_symbol_id: 3544
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.key-broadcast_matmul-852/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "identity"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.981524 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.key-hierarchical_parallel_cast-857
I20220824 02:43:23.981818 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.value-hierarchical_parallel_cast-858"
device_tag: "cuda"
scope_symbol_id: 3551
loc: ""
user_conf {
  op_type_name: "hierarchical_parallel_cast"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.value-broadcast_matmul-854/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0"
    }
  }
  attr {
    key: "grad_mode"
    value {
      at_string: "identity"
    }
  }
  attr {
    key: "grad_nd_sbp"
    value {
      at_list_string {
      }
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.981989 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.value-hierarchical_parallel_cast-858
I20220824 02:43:23.982115 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-859"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-859/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.982328 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-859
I20220824 02:43:23.982411 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-860"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-860/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.982610 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-860
I20220824 02:43:23.982699 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-861"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att.time_mix_r/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-860/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-861/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.982903 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-861
I20220824 02:43:23.983000 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-862"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-862/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.983201 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-862
I20220824 02:43:23.983283 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-863"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-863/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.983482 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-863
I20220824 02:43:23.983569 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-864"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att-scalar_add-16/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-863/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-864/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.983791 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-864
I20220824 02:43:23.983887 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-865"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_k/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-865/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.984086 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-865
I20220824 02:43:23.984165 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-866"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-866/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.984354 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-866
I20220824 02:43:23.984439 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-867"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att.time_mix_k/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-866/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-867/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.984663 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-867
I20220824 02:43:23.984748 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-868"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-865/z_0"
      s: "model.blocks.0.att-broadcast_mul-859/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-868/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.984930 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-868
I20220824 02:43:23.985026 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-869"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-6/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-869/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.985240 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-869
I20220824 02:43:23.985323 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-870"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-870/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.985515 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-870
I20220824 02:43:23.985603 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-871"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att-scalar_add-6/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-870/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-871/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.985831 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-871
I20220824 02:43:23.985918 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-872"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-869/z_0"
      s: "model.blocks.0.att-broadcast_mul-862/z_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-872/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.986109 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-872
I20220824 02:43:23.986196 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-873"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_mix_v/out"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-873/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.986392 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-873
I20220824 02:43:23.986472 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-874"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-874/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.986667 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-874
I20220824 02:43:23.986755 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-875"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att.time_mix_v/out"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-874/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-875/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.986980 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-875
I20220824 02:43:23.987066 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-876"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-873/z_0"
      s: "model.blocks.0.att-add_n-868/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-876/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.987254 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-876
I20220824 02:43:23.987340 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-877"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att-scalar_add-11/out_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-877/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.987531 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-877
I20220824 02:43:23.987612 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-broadcast_mul-878"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "broadcast_mul"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0"
    }
  }
  input {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-3/y_0"
    }
  }
  output {
    key: "z"
    value {
      s: "model.blocks.0.att-broadcast_mul-878/z_0"
    }
  }
  input_order: "x"
  input_order: "y"
  output_order: "z"
}

I20220824 02:43:23.987803 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-broadcast_mul-878
I20220824 02:43:23.987891 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-reduce_sum_like-879"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "reduce_sum_like"
  input {
    key: "like"
    value {
      s: "model.blocks.0.att-scalar_add-11/out_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-broadcast_mul-878/z_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att-reduce_sum_like-879/y_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_list_int32 {
        val: 0
        val: 1
      }
    }
  }
  input_order: "x"
  input_order: "like"
  output_order: "y"
}

I20220824 02:43:23.988122 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-reduce_sum_like-879
I20220824 02:43:23.988214 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-880"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-broadcast_mul-877/z_0"
      s: "model.blocks.0.att-add_n-872/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-880/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.988404 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-880
I20220824 02:43:23.988515 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.time_shift-pad-881"
device_tag: "cuda"
scope_symbol_id: 3641
loc: ""
user_conf {
  op_type_name: "pad"
  input {
    key: "x"
    value {
      s: "model.blocks.0.att-add_n-880/out_0"
    }
  }
  output {
    key: "y"
    value {
      s: "model.blocks.0.att.time_shift-pad-881/y_0"
    }
  }
  attr {
    key: "floating_constant_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integral_constant_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "padding"
    value {
      at_list_int64 {
        val: 0
        val: 0
        val: -1
        val: 1
      }
    }
  }
  attr {
    key: "padding_after"
    value {
      at_list_int64 {
        val: 0
        val: 1
        val: 0
      }
    }
  }
  attr {
    key: "padding_before"
    value {
      at_list_int64 {
        val: 0
        val: -1
        val: 0
      }
    }
  }
  input_order: "x"
  output_order: "y"
}

I20220824 02:43:23.988716 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_shift-pad-881
I20220824 02:43:23.988804 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att.time_shift-add_n-882"
device_tag: "cuda"
scope_symbol_id: 3641
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att.time_shift-pad-881/y_0"
      s: "model.blocks.0.att-add_n-876/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att.time_shift-add_n-882/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.988981 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att.time_shift-add_n-882
I20220824 02:43:23.989073 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-883"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-reduce_sum_like-864/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-883/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.989259 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-883
I20220824 02:43:23.989344 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-884"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-883/out_0"
      s: "model.blocks.0.att-reduce_sum_like-861/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-884/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.989529 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-884
I20220824 02:43:23.989624 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-885"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-reduce_sum_like-871/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-885/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.989806 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-885
I20220824 02:43:23.989890 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-886"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-885/out_0"
      s: "model.blocks.0.att-reduce_sum_like-867/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-886/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.990516 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-886
I20220824 02:43:23.990844 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-constant-887"
device_tag: "cuda"
scope_symbol_id: 3660
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln1-constant-887/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.991019 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-constant-887
I20220824 02:43:23.991305 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-constant-888"
device_tag: "cuda"
scope_symbol_id: 3660
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln1-constant-888/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.991467 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-constant-888
I20220824 02:43:23.991564 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-layer_norm_param_grad-889"
device_tag: "cuda"
scope_symbol_id: 3660
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.att.time_shift-add_n-882/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.0.ln1-layer_norm_param_grad-889/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.0.ln1-layer_norm_param_grad-889/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.991863 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-layer_norm_param_grad-889
I20220824 02:43:23.991969 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-layer_norm_grad-890"
device_tag: "cuda"
scope_symbol_id: 3660
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.att.time_shift-add_n-882/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln1.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln1-layer_norm-2/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/y_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ln1-layer_norm_grad-890/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.992255 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-layer_norm_grad-890
I20220824 02:43:23.992347 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln1-add_n-891"
device_tag: "cuda"
scope_symbol_id: 3660
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.ln1-layer_norm_grad-890/dx_0"
      s: "model.blocks.0.ln2-add_n-839/out_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln1-add_n-891/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.992534 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln1-add_n-891
I20220824 02:43:23.992630 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-scalar_mul-892"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "scalar_mul"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-reduce_sum_like-879/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-scalar_mul-892/out_0"
    }
  }
  attr {
    key: "float_operand"
    value {
      at_double: 0
    }
  }
  attr {
    key: "has_float_operand"
    value {
      at_bool: false
    }
  }
  attr {
    key: "has_int_operand"
    value {
      at_bool: true
    }
  }
  attr {
    key: "int_operand"
    value {
      at_int64: -1
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.992842 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-scalar_mul-892
I20220824 02:43:23.992928 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.att-add_n-893"
device_tag: "cuda"
scope_symbol_id: 3515
loc: ""
user_conf {
  op_type_name: "add_n"
  input {
    key: "in"
    value {
      s: "model.blocks.0.att-scalar_mul-892/out_0"
      s: "model.blocks.0.att-reduce_sum_like-875/y_0"
    }
  }
  output {
    key: "out"
    value {
      s: "model.blocks.0.att-add_n-893/out_0"
    }
  }
  input_order: "in"
  output_order: "out"
}

I20220824 02:43:23.993119 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.att-add_n-893
I20220824 02:43:23.993492 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-constant-894"
device_tag: "cuda"
scope_symbol_id: 3684
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln0-constant-894/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.993659 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-constant-894
I20220824 02:43:23.993952 1392289 lazy_op_interpreter.cpp:667] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-constant-895"
device_tag: "cuda"
scope_symbol_id: 3684
loc: ""
user_conf {
  op_type_name: "constant"
  output {
    key: "out"
    value {
      s: "model.blocks.0.ln0-constant-895/out_0"
    }
  }
  attr {
    key: "dtype"
    value {
      at_data_type: kFloat
    }
  }
  attr {
    key: "floating_value"
    value {
      at_double: 0
    }
  }
  attr {
    key: "integer_value"
    value {
      at_int64: 0
    }
  }
  attr {
    key: "is_floating_value"
    value {
      at_bool: true
    }
  }
  attr {
    key: "nd_sbp"
    value {
      at_list_string {
        val: "S(0)"
      }
    }
  }
  attr {
    key: "shape"
    value {
      at_shape {
        dim: 8
        dim: 1024
      }
    }
  }
  output_order: "out"
}

I20220824 02:43:23.994112 1392289 lazy_op_interpreter.cpp:670] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-constant-895
I20220824 02:43:23.994204 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-layer_norm_param_grad-896"
device_tag: "cuda"
scope_symbol_id: 3684
loc: ""
user_conf {
  op_type_name: "layer_norm_param_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ln1-add_n-891/out_0"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  output {
    key: "beta_diff"
    value {
      s: "model.blocks.0.ln0-layer_norm_param_grad-896/beta_diff_0"
    }
  }
  output {
    key: "gamma_diff"
    value {
      s: "model.blocks.0.ln0-layer_norm_param_grad-896/gamma_diff_0"
    }
  }
  attr {
    key: "begin_params_axis"
    value {
      at_int64: 2
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  output_order: "gamma_diff"
  output_order: "beta_diff"
}

I20220824 02:43:23.994482 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-layer_norm_param_grad-896
I20220824 02:43:23.994583 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.blocks.0.ln0-layer_norm_grad-897"
device_tag: "cuda"
scope_symbol_id: 3684
loc: ""
user_conf {
  op_type_name: "layer_norm_grad"
  input {
    key: "dy"
    value {
      s: "model.blocks.0.ln1-add_n-891/out_0"
    }
  }
  input {
    key: "gamma"
    value {
      s: "model.blocks.0.ln0.weight/out"
    }
  }
  input {
    key: "inv_variance"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/inv_variance_0"
    }
  }
  input {
    key: "mean"
    value {
      s: "model.blocks.0.ln0-layer_norm-1/mean_0"
    }
  }
  input {
    key: "x"
    value {
      s: "model.emb-gather-0/out_0"
    }
  }
  output {
    key: "dx"
    value {
      s: "model.blocks.0.ln0-layer_norm_grad-897/dx_0"
    }
  }
  attr {
    key: "begin_norm_axis"
    value {
      at_int64: 2
    }
  }
  attr {
    key: "epsilon"
    value {
      at_double: 1e-05
    }
  }
  input_order: "dy"
  input_order: "x"
  input_order: "mean"
  input_order: "inv_variance"
  input_order: "gamma"
  output_order: "dx"
}

I20220824 02:43:23.994874 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.blocks.0.ln0-layer_norm_grad-897
I20220824 02:43:23.995260 1392289 lazy_op_interpreter.cpp:839] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "model.emb-unsorted_segment_sum_like-898"
device_tag: "cuda"
scope_symbol_id: 3707
loc: ""
user_conf {
  op_type_name: "unsorted_segment_sum_like"
  input {
    key: "data"
    value {
      s: "model.blocks.0.ln0-layer_norm_grad-897/dx_0"
    }
  }
  input {
    key: "like"
    value {
      s: "model.emb.weight/out"
    }
  }
  input {
    key: "segment_ids"
    value {
      s: "_GraphBase_0_input.1.0_idx/out"
    }
  }
  output {
    key: "out"
    value {
      s: "model.emb-unsorted_segment_sum_like-898/out_0"
    }
  }
  attr {
    key: "axis"
    value {
      at_int64: 0
    }
  }
  input_order: "data"
  input_order: "segment_ids"
  input_order: "like"
  output_order: "out"
}

I20220824 02:43:23.995538 1392289 lazy_op_interpreter.cpp:842] Lazy nn.Graph name GraphBase_0 add op : 
model.emb-unsorted_segment_sum_like-898
(GRAPH:GraphBase_0:GraphBase) end building graph modules.
(GRAPH:GraphBase_0:GraphBase) start building graph outputs.
I20220824 02:43:23.996552 1392289 lazy_op_interpreter.cpp:535] Lazy nn.Graph name GraphBase_0 try to add op: 
name: "_GraphBase_0_output.0.0.0_loss"
device_tag: "cpu"
scope_symbol_id: 3715
output_conf {
  in: "model-broadcast_div-349/z_0"
  out: "out"
  blob_conf {
    shape {
    }
    data_type: kFloat
    is_dynamic: false
    nd_sbp {
      sbp_parallel {
        broadcast_parallel {
        }
      }
    }
  }
}

I20220824 02:43:23.996696 1392289 lazy_op_interpreter.cpp:538] Lazy nn.Graph name GraphBase_0 add op : 
_GraphBase_0_output.0.0.0_loss
(OUTPUT:_GraphBase_0_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0]),
       sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(),
       dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
(GRAPH:GraphBase_0:GraphBase) end building graph outputs.
(GRAPH:GraphBase_0:GraphBase) start building graph with compile passes.
I20220824 02:43:24.093247 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_0-InsertPinnedIdentityOpPass
I20220824 02:43:24.162907 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_0-InsertPinnedIdentityOpPass
I20220824 02:43:24.162946 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_1-EliminateDeadNodesPass
I20220824 02:43:24.249428 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_1-EliminateDeadNodesPass
I20220824 02:43:24.249469 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_2-NormalizationExponentialAverageAutoTickPass
I20220824 02:43:24.249490 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_2-NormalizationExponentialAverageAutoTickPass
I20220824 02:43:24.249493 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_3-AutoMixedPrecision
I20220824 02:43:24.322285 1392289 auto_mixed_precision.cpp:214] WhiteSet include: model.emb-gather-0,
model.blocks.0.att.receptance-hierarchical_parallel_cast-23,
model.blocks.0.ffn.value-hierarchical_parallel_cast-49,
model.blocks.2.att.output-hierarchical_parallel_cast-138,
model.blocks.3.att.output-hierarchical_parallel_cast-192,
model.blocks.4.ffn.value-hierarchical_parallel_cast-265,
model.blocks.5.att.output-hierarchical_parallel_cast-300,
model.head_k-broadcast_matmul_grad_b-374_out_0_pinned_identity,
model.head_k-broadcast_matmul_grad_b-374,
model.blocks.5.ffn.value-broadcast_matmul_grad_b-385,
model.blocks.5.ffn.key-broadcast_matmul_grad_b-397_out_0_pinned_identity,
model.blocks.5.ffn-add_n-416_out_0_pinned_identity,
model.blocks.5.ffn-scalar_mul-415,
model.blocks.5.ffn-reduce_sum_like-406,
model.blocks.5.att.output-broadcast_matmul_grad_b-418_out_0_pinned_identity,
model.blocks.5.att.value-broadcast_matmul_grad_b-430_out_0_pinned_identity,
model.blocks.5.att.value-broadcast_matmul_grad_b-430,
model.blocks.5.ffn.key-broadcast_matmul_grad_b-397,
model.blocks.4.ffn-reduce_sum_like-491,
model.blocks.4.ffn-broadcast_mul-490,
model.blocks.4.att.output-broadcast_matmul_grad_b-503_out_0_pinned_identity,
model.blocks.4.att.value-broadcast_matmul_grad_b-515_out_0_pinned_identity,
model.blocks.4.att.value-broadcast_matmul_grad_b-515,
model.blocks.4.att-add_n-550_out_0_pinned_identity,
model.blocks.4.att-add_n-550,
model.blocks.4.att-scalar_mul-549,
model.blocks.4.att-broadcast_mul-535,
model.blocks.3.ffn.value-broadcast_matmul_grad_b-555,
model.blocks.3.ffn.key-broadcast_matmul_grad_b-567,
model.blocks.3.ffn-scalar_mul-585,
model.blocks.3.ffn-broadcast_mul-575,
model.blocks.3.att.output-broadcast_matmul_grad_b-588_out_0_pinned_identity,
model.blocks.3.att.output-broadcast_matmul_grad_b-588,
model.blocks.3.att-scalar_mul-634,
model.blocks.3.att-reduce_sum_like-621,
model.blocks.2.ffn.value-broadcast_matmul_grad_b-640_out_0_pinned_identity,
model.blocks.2.ffn.key-broadcast_matmul_grad_b-652_out_0_pinned_identity,
model.blocks.2.ffn-scalar_mul-670,
model.blocks.2.att.value-broadcast_matmul_grad_b-685_out_0_pinned_identity,
model.blocks.2.att-add_n-720_out_0_pinned_identity,
model.blocks.2.att-scalar_mul-719,
model.blocks.2.att-reduce_sum_like-706,
model.blocks.1.ffn.value-broadcast_matmul_grad_b-725_out_0_pinned_identity,
model.blocks.1.ffn.value-broadcast_matmul_grad_b-725,
model.blocks.1.ffn.key-broadcast_matmul_grad_b-737_out_0_pinned_identity,
model.head-broadcast_matmul_grad_b-363_out_0_pinned_identity,
model.blocks.1.ffn.key-broadcast_matmul_grad_b-737,
model.blocks.1.ffn-add_n-756,
model.blocks.1.ffn-scalar_mul-755,
model.blocks.1.ffn-reduce_sum_like-746,
model.blocks.1.att.value-broadcast_matmul_grad_b-770_out_0_pinned_identity,
model.blocks.1.att.value-broadcast_matmul_grad_b-770,
model.blocks.1.att.key-broadcast_matmul_grad_b-768_out_0_pinned_identity,
model.blocks.1.att-add_n-798,
model.blocks.1.att-reduce_sum_like-783,
model.blocks.1.att-broadcast_mul-782,
model.blocks.0.ffn.value-broadcast_matmul_grad_b-810_out_0_pinned_identity,
model.blocks.0.ffn.value-broadcast_matmul_grad_b-810,
model.blocks.0.ffn.key-broadcast_matmul_grad_b-822,
model.blocks.0.ffn-add_n-841_out_0_pinned_identity,
model.blocks.0.ffn-scalar_mul-840,
model.blocks.2.att-add_n-720,
model.blocks.0.ffn-reduce_sum_like-831,
model.blocks.3.ffn-reduce_sum_like-576,
model.blocks.0.att.output-broadcast_matmul_grad_b-843,
model.blocks.0.att.value-broadcast_matmul_grad_b-855,
model.blocks.0.att.key-broadcast_matmul_grad_b-853_out_0_pinned_identity,
model.blocks.0.att-scalar_mul-885,
model.blocks.2.ffn-broadcast_mul-660,
model.blocks.0.att-broadcast_mul-870,
model.blocks.1.ffn-broadcast_mul-745,
model.emb-unsorted_segment_sum_like-898,
model.blocks.0.ln0-layer_norm_grad-897,
model.blocks.0.ln1-add_n-891,
model.blocks.0.att.time_shift-add_n-882,
model.blocks.0.att.time_shift-pad-881,
model.blocks.0.att-add_n-880,
model.blocks.0.att-broadcast_mul-869,
model.blocks.0.att-reduce_sum_like-867,
model.blocks.0.att-add_n-876,
model.blocks.0.att.key-hierarchical_parallel_cast-857,
model.blocks.4.ffn-scalar_mul-500,
model.blocks.0.att.key-broadcast_matmul-852,
model.blocks.0.att-broadcast_mul-878,
model.blocks.0.att-broadcast_mul-877,
model.blocks.0.att.value-hierarchical_parallel_cast-858,
model.blocks.0.att.value-broadcast_matmul-854,
model.blocks.0.att-add_n-884_out_0_pinned_identity,
model.blocks.0.att-add_n-884,
model.blocks.0.att-scalar_mul-883,
model.blocks.0.att-reduce_sum_like-864,
model.blocks.0.att-broadcast_mul-863,
model.blocks.4.ffn-add_n-501,
model.blocks.0.att-broadcast_mul-862,
model.blocks.0.att-reduce_sum_like-861,
model.blocks.0.att-broadcast_mul-860,
model.blocks.5.ffn.value-hierarchical_parallel_cast-319,
model.blocks.0.att-broadcast_mul-859,
model.blocks.0.att.output-broadcast_matmul-842,
model.blocks.0.ln2-add_n-839,
model.blocks.0.ln2-layer_norm_grad-838,
model.blocks.0.ffn.time_shift-pad-833,
model.blocks.0.ffn.key-broadcast_matmul-821,
model.blocks.0.ffn-relu_grad-814,
model.blocks.0.ffn-add_n-824_out_0_pinned_identity,
model.blocks.0.ffn-reduce_sum_like-820,
model.blocks.1.att-scalar_mul-797,
model.blocks.0.ffn-broadcast_mul-819,
model.blocks.0.ffn-broadcast_mul-818,
model.blocks.0.ffn-broadcast_mul-816,
model.blocks.5.att-add_n-465,
model.blocks.1.ln1-layer_norm_grad-802,
model.blocks.1.att.time_shift-add_n-794,
model.blocks.1.att-add_n-792,
model.blocks.1.att-add_n-784,
model.blocks.1.att-broadcast_mul-781,
model.blocks.1.att-reduce_sum_like-779,
model.head-broadcast_matmul_grad_b-363,
model.blocks.5.ln2-layer_norm_grad-413,
model.blocks.4.ffn.value-broadcast_matmul_grad_b-470,
model.blocks.0.ffn-broadcast_mul-825,
model.blocks.2.ffn.time_shift-add_n-664,
model.blocks.3.ffn-add_n-586,
model.blocks.5.ffn-add_n-403,
model.blocks.5.ffn-broadcast_mul-400,
model.blocks.4.att.output-broadcast_matmul_grad_b-503,
model.blocks.4.att.receptance-hierarchical_parallel_cast-239,
model.blocks.3.ffn.value-broadcast_matmul-554,
model.blocks.5.ffn.key-broadcast_matmul-396,
model.blocks.5.ffn.value-broadcast_matmul-384,
model.head_k-add_n-376,
model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387,
model.blocks.5.att-broadcast_mul-438,
model.blocks.5.ffn.receptance-broadcast_matmul-386,
model.blocks.2.att.key-broadcast_matmul-682,
model.head_q-broadcast_matmul_grad_b-369_out_0_pinned_identity,
model.head_q-add_n-372,
model-scalar_mul-365,
model.blocks.3.att-add_n-635_out_0_pinned_identity,
model.blocks.5.att-reduce_sum_like-443,
model-batch_matmul-364,
model.blocks.5.att.key-hierarchical_parallel_cast-289,
model-transpose-360,
model.blocks.4.ffn-add_n-484,
model.blocks.4.att-broadcast_mul-530,
model.blocks.3.att-add_n-614,
model-scalar_mul-333,
model-batch_matmul-332,
model.head_k-broadcast_matmul-330,
model.blocks.5.ffn-reduce_sum_like-402,
model.blocks.4.att-add_n-537,
model.blocks.3.att-broadcast_mul-601,
model.head_k-hierarchical_parallel_cast-329,
model.head-broadcast_matmul-338,
model.blocks.0.att.output-broadcast_matmul_grad_b-843_out_0_pinned_identity,
model.blocks.5.att-scalar_mul-457,
model.blocks.0.ffn.value-broadcast_matmul-50,
model.blocks.0.att.receptance-hierarchical_parallel_cast-856,
model.blocks.0.ffn-broadcast_mul-43,
model.blocks.5.ffn-broadcast_mul-324,
model.blocks.1.att-reduce_sum_like-787,
model.blocks.5.ffn-broadcast_mul-310,
model.blocks.1.ffn-broadcast_mul-731,
model.blocks.3.att.output-broadcast_matmul-587,
model.blocks.4.att-broadcast_mul-233,
model.blocks.0.att.key-hierarchical_parallel_cast-19,
model.blocks.4.ffn-add_n-492,
model.blocks.5.ffn.key-hierarchical_parallel_cast-315,
model.blocks.5.att-broadcast_mul-434,
model.blocks.1.ffn-reduce_sum_like-732,
model.blocks.1.ffn.time_shift-add_n-749,
model.blocks.5.ffn.time_shift-pad-304,
model.blocks.5.ffn-broadcast_mul-394,
model-reshape-343,
model.blocks.5.ffn-broadcast_mul-308,
model.blocks.2.ffn-broadcast_mul-645,
model.blocks.5.att-broadcast_mul-435,
model.blocks.5.att.output-broadcast_matmul-301,
model-transpose-342,
model.blocks.5.ffn.receptance-hierarchical_parallel_cast-321,
model.blocks.2.att-add_n-713,
model.blocks.5.att-scalar_mul-464,
model.blocks.1.ffn-add_n-739_out_0_pinned_identity,
model.head_q-hierarchical_parallel_cast-371,
model.blocks.5.att-add_n-283,
model.blocks.4.ln1-layer_norm-218,
model.blocks.2.ffn-broadcast_mul-656,
model.blocks.2.ffn-reduce_sum_like-661,
model.blocks.0.att-reduce_sum_like-871,
model.blocks.1.att.time_shift-pad-793,
model.blocks.5.att-broadcast_mul-279,
model.blocks.3.att.value-broadcast_matmul_grad_b-600,
model.blocks.1.ffn-add_n-98,
model.blocks.4.att.key-broadcast_matmul_grad_b-513,
model.blocks.1.att.receptance-broadcast_matmul_grad_b-766_out_0_pinned_identity,
model.blocks.2.att.value-broadcast_matmul-684,
model.blocks.4.att-add_n-543,
model.blocks.4-add_n-271,
model.blocks.0.att-add_n-893_out_0_pinned_identity,
model.blocks.4.ffn-broadcast_mul-475,
model.blocks.5.att.key-broadcast_matmul-290,
model.blocks.0.ffn-scalar_mul-823,
model.blocks.2.att.time_shift-pad-111,
model.blocks.4.ffn-broadcast_mul-270,
model.blocks.1.att.output-broadcast_matmul_grad_b-758,
model.blocks.0.ffn-broadcast_mul-826,
model.blocks.4.ffn-reduce_sum_like-480,
model.blocks.4.ffn.key-broadcast_matmul_grad_b-482_out_0_pinned_identity,
model.blocks.5.att-broadcast_mul-282,
model.blocks.4.ffn.value-broadcast_matmul-266,
model.blocks.4.ffn.key-broadcast_matmul-262,
model.blocks.4.att-add_n-525,
model.blocks.4.ffn-broadcast_mul-251,
model.blocks.4.ffn-broadcast_mul-254,
model.blocks.3.ffn-broadcast_mul-571,
model.blocks.4.ffn-add_n-255,
model.blocks.3.att-broadcast_mul-166,
model.blocks.4-add_n-248,
model.blocks.5.att.key-broadcast_matmul-427,
model.blocks.2.ffn-reduce_sum_like-647,
model.blocks.4.att-add_n-234,
model.blocks.3.att-scalar_mul-625,
model.blocks.3.att-add_n-180,
model.blocks.4.att-broadcast_mul-225,
model.blocks.1.att.time_shift-pad-57,
model.blocks.4.att.key-broadcast_matmul-236,
model.blocks.5.ffn-add_n-407,
model.blocks.4.ln1-layer_norm_grad-547,
model.blocks.4.att.key-hierarchical_parallel_cast-235,
model.blocks.4.att-broadcast_mul-517,
model.blocks.3.ln2-layer_norm_grad-583,
model.blocks.1.ffn-broadcast_mul-722,
model.blocks.5-add_n-302,
model.blocks.5.att-broadcast_mul-441,
model.blocks.4.att-broadcast_mul-220,
model.blocks.4.att.output-broadcast_matmul-247,
model.blocks.4.att.receptance-broadcast_matmul-240,
model.blocks.0.ffn.receptance-broadcast_matmul-811,
model.blocks.3.ffn-add_n-569_out_0_pinned_identity,
model.blocks.4.att-broadcast_mul-223,
model.blocks.4.att.time_shift-pad-219,
model.blocks.1-add_n-109,
model.blocks.2.att.receptance-broadcast_matmul_grad_b-681,
model.blocks.2.ffn-add_n-671,
model.blocks.0.ffn.receptance-hierarchical_parallel_cast-51,
model.blocks.4.att-broadcast_mul-522,
model.blocks.0.att-add_n-886,
model.blocks.5.ffn-broadcast_mul-305,
model.blocks.0.ffn-add_n-832,
model.blocks.2.att-broadcast_mul-701,
model.blocks.1.ffn.value-broadcast_matmul-104,
model.blocks.1.ffn-broadcast_mul-92,
model.blocks.0.ffn-add_n-44,
model.blocks.2.att-reduce_sum_like-698,
model.blocks.2.ffn-add_n-671_out_0_pinned_identity,
model.blocks.1.ffn-broadcast_mul-108,
model.head_q-broadcast_matmul-328,
model.blocks.1.ffn-add_n-93,
model.blocks.2.att.value-broadcast_matmul_grad_b-685,
model.blocks.3.att.value-broadcast_matmul-599,
model.blocks.1.ln2-layer_norm-87,
model.head-broadcast_matmul-362,
model.blocks.5.ffn-broadcast_mul-401,
model.blocks.5.ffn.key-broadcast_matmul-316,
model.blocks.2.att.receptance-hierarchical_parallel_cast-131,
model.blocks.1.ffn-broadcast_mul-89,
model.blocks.2.att-broadcast_mul-689,
model.blocks.0.att-add_n-893,
model.blocks.1.att.value-hierarchical_parallel_cast-75,
model.blocks.5.ffn.time_shift-pad-408,
model.blocks.0.att.output-hierarchical_parallel_cast-30,
model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812_out_0_pinned_identity,
model.blocks.4.att-reduce_sum_like-528,
model.blocks.3.att-broadcast_mul-176,
model.blocks.5.ffn-broadcast_mul-382,
model.blocks.0.att-broadcast_mul-873,
model.blocks.2.ffn.time_shift-pad-663,
model.blocks.2.ffn-add_n-152,
model-transpose-370,
model.blocks.1.att.value-broadcast_matmul-76,
model.blocks.1.ffn-broadcast_mul-744,
model.blocks.1.att-broadcast_mul-63,
model.blocks.1.att-add_n-67,
model.blocks.4.ffn-relu_grad-474,
model.blocks.0.att-add_n-886_out_0_pinned_identity,
model.blocks.5.ln1-layer_norm_grad-462,
model.blocks.1.att-broadcast_mul-61,
model.blocks.1.att-broadcast_mul-68,
model.blocks.2.ffn-broadcast_mul-151,
model.blocks.0.att.value-hierarchical_parallel_cast-21,
model.blocks.1.ffn.time_shift-pad-748,
model.blocks.3.att.output-broadcast_matmul-193,
model.blocks.4.ffn.value-broadcast_matmul_grad_b-470_out_0_pinned_identity,
model.blocks.2.att.value-hierarchical_parallel_cast-129,
model.blocks.1.ffn.key-broadcast_matmul-100,
model.blocks.4.att-broadcast_mul-526,
model.ln_out-layer_norm_grad-380,
model.blocks.1.att.key-broadcast_matmul-767,
model.blocks.5.att.value-hierarchical_parallel_cast-291,
model.blocks.0.ffn-reduce_sum_like-827,
model.blocks.3.att-add_n-175,
model.blocks.1.ffn-add_n-747,
model.blocks.4.att-broadcast_mul-228,
model.blocks.4.att-add_n-224,
model.blocks.3.att-broadcast_mul-179,
model-transpose-331,
model.blocks.1.ffn.time_shift-pad-88,
model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812,
model.blocks.0.ffn-add_n-39,
model.blocks.1.att-broadcast_mul-71,
model.blocks.3.ln2-add_n-584,
model.blocks.3.ffn.time_shift-pad-196,
model.blocks.3.ffn-add_n-201,
model.blocks.3.ffn.receptance-broadcast_matmul-214,
model.blocks.2.ffn-add_n-147,
model.blocks.0.att.receptance-broadcast_matmul_grad_b-851,
model.blocks.1.att-add_n-796,
model.blocks.0.ffn-broadcast_mul-38,
model.blocks.3.att-reduce_sum_like-609,
model.blocks.0.ffn.time_shift-add_n-834,
model-reshape-361,
model.blocks.5.att-broadcast_mul-432,
model.blocks.4.att-broadcast_mul-531,
model.blocks.5.att-broadcast_mul-274,
model.blocks.3.att-broadcast_mul-174,
model.blocks.4.ffn.key-hierarchical_parallel_cast-261,
model.blocks.2.ffn-relu-155,
model.blocks.0.att.output-broadcast_matmul-31,
model.blocks.0.ffn.key-hierarchical_parallel_cast-45,
model.blocks.5.att.receptance-broadcast_matmul-294,
model.blocks.5.att-broadcast_mul-449,
model.blocks.4.ffn-broadcast_mul-256,
model.head_q-broadcast_matmul-368,
model.blocks.5.att-reduce_sum_like-433,
model.blocks.0.ffn-relu-47,
model.blocks.1.att.value-broadcast_matmul-769,
model.blocks.1.att.receptance-broadcast_matmul-78,
model.head_q-hierarchical_parallel_cast-327,
model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642,
model.blocks.2.att.time_shift-pad-708,
model.ln_out-layer_norm-326,
model.blocks.4.ffn.time_shift-pad-250,
model.blocks.2.ffn-broadcast_mul-148,
model.blocks.5.att.time_shift-pad-273,
model.blocks.0.att.time_shift-pad-3,
model.blocks.0.att-broadcast_mul-866,
model.blocks.3.att-reduce_sum_like-603,
model.blocks.1.att-broadcast_mul-785,
model.blocks.0.ffn-broadcast_mul-829,
model.blocks.2.ffn.key-broadcast_matmul-154,
model.blocks.5.att-broadcast_mul-277,
model.blocks.0.ffn.key-broadcast_matmul-46,
model.blocks.4.att.value-broadcast_matmul-238,
model.blocks.1.att.output-broadcast_matmul-85,
model.blocks.2.ln2-layer_norm_grad-668,
model.blocks.2.att-broadcast_mul-704,
model.blocks.0.att.value-broadcast_matmul-22,
model.blocks.0.ffn-add_n-828,
model.blocks.2.att-broadcast_mul-697,
model.blocks.2.ffn.receptance-hierarchical_parallel_cast-159,
model.blocks.1.ffn-broadcast_mul-730,
model.blocks.0.att.key-broadcast_matmul-20,
model.blocks.5.att-reduce_sum_like-436,
model.blocks.5.ffn.receptance-broadcast_matmul-322,
model.blocks.4.ffn-broadcast_mul-259,
model.blocks.1.att-add_n-62,
model.blocks.5.ffn-broadcast_mul-390,
model.blocks.2.ffn-broadcast_mul-146,
model.blocks.3.att-add_n-626_out_0_pinned_identity,
model.blocks.2.att-broadcast_mul-687,
model.blocks.2.att-broadcast_mul-120,
model.blocks.0.ffn-broadcast_mul-54,
model.blocks.4.ffn-broadcast_mul-478,
model.blocks.3.att-add_n-635,
model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727_out_0_pinned_identity,
model.blocks.4.ffn-relu-263,
model.blocks.2.att-broadcast_mul-122,
model.blocks.1.ffn.key-broadcast_matmul-736,
model.blocks.2.att-broadcast_mul-705,
model.blocks.1-add_n-86,
model.blocks.4.ffn-scalar_mul-483,
model.blocks.5.att-add_n-465_out_0_pinned_identity,
model.blocks.2.ffn.value-broadcast_matmul-639,
model.blocks.1.ffn-broadcast_mul-733,
model.blocks.1.ffn-broadcast_mul-94,
model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642_out_0_pinned_identity,
model.blocks.2.ln1-layer_norm-110,
model.blocks.3.ffn-broadcast_mul-197,
model.blocks.5.att-broadcast_mul-450,
model.blocks.1.att-broadcast_mul-66,
model.blocks.3.ffn-add_n-569,
model.blocks.1.att-broadcast_mul-777,
model.blocks.4.att-broadcast_mul-230,
model.blocks.1.att.receptance-broadcast_matmul_grad_b-766,
model.blocks.4.att.key-broadcast_matmul_grad_b-513_out_0_pinned_identity,
model.blocks.2.att.key-broadcast_matmul-128,
model.blocks.3.att.value-broadcast_matmul-184,
model.blocks.3.att-broadcast_mul-171,
model.blocks.2.att-broadcast_mul-125,
model.blocks.2.ffn.key-hierarchical_parallel_cast-153,
model.blocks.0.att.receptance-broadcast_matmul-850,
model.blocks.2.att.receptance-broadcast_matmul-132,
model.blocks.1.ffn.key-hierarchical_parallel_cast-99,
model.blocks.2.att-broadcast_mul-112,
model.blocks.3.att.value-hierarchical_parallel_cast-183,
model.blocks.2.att.key-hierarchical_parallel_cast-127,
model.blocks.0.ffn.key-broadcast_matmul_grad_b-822_out_0_pinned_identity,
model.blocks.0.att-broadcast_mul-874,
model.blocks.0-add_n-55,
model.blocks.2.att-broadcast_mul-117,
model.blocks.1.att-add_n-72,
model.blocks.2.att-add_n-121,
model.blocks.4.att.time_shift-add_n-539,
model.blocks.1.att.key-broadcast_matmul-74,
model.blocks.4.ffn-broadcast_mul-485,
model.blocks.5.ln2-add_n-414,
model.blocks.3.att.receptance-broadcast_matmul-595,
model.blocks.2.att.value-broadcast_matmul-130,
model.blocks.5.att.time_shift-pad-453,
model.blocks.5.att-add_n-278,
model.blocks.3-add_n-194,
model.blocks.0.ffn-broadcast_mul-40,
model.blocks.1.ffn-relu-101,
model.blocks.2.att.output-broadcast_matmul-139,
model.blocks.1.att-reduce_sum_like-776,
model.blocks.5.ffn-add_n-399_out_0_pinned_identity,
model.blocks.1.att.receptance-hierarchical_parallel_cast-77,
model.head_q-broadcast_matmul_grad_b-369,
model-batch_matmul-366,
model.blocks.2.ffn.time_shift-pad-142,
model.head_k-hierarchical_parallel_cast-375,
model.blocks.2.ffn-broadcast_mul-637,
model.blocks.2.ffn-broadcast_mul-143,
model.blocks.3.att.time_shift-pad-165,
model.blocks.2.att-reduce_sum_like-694,
model.blocks.1.ffn.receptance-broadcast_matmul-106,
model.blocks.2.att-add_n-126,
model.blocks.3.ffn.time_shift-add_n-579,
model.blocks.5.ffn.time_shift-add_n-409,
model.blocks.1.ln1-layer_norm-56,
model.blocks.2.att.receptance-broadcast_matmul-680,
model.blocks.1.att-add_n-805,
model.blocks.3.att.value-broadcast_matmul_grad_b-600_out_0_pinned_identity,
model.blocks.3.ln2-layer_norm-195,
model.blocks.4.att-broadcast_mul-534,
model.blocks.2.ffn.value-broadcast_matmul-158,
model.blocks.3.ffn-scalar_mul-568,
model.blocks.1.att-broadcast_mul-772,
model.blocks.1.ffn-broadcast_mul-97,
model.blocks.3.ffn-relu-209,
model.blocks.3.att.time_shift-add_n-624,
model.blocks.4.ffn.receptance-hierarchical_parallel_cast-267,
model.blocks.1.ffn-scalar_mul-738,
model.blocks.3.ln1-layer_norm-164,
model.blocks.0.att.receptance-broadcast_matmul_grad_b-851_out_0_pinned_identity,
model.blocks.3.att-add_n-170,
model.blocks.2.ffn.receptance-broadcast_matmul-641,
model.head_k-broadcast_matmul-373,
model.blocks.3.att-broadcast_mul-169,
model.blocks.1.ln2-add_n-754,
model.blocks.3.att.receptance-hierarchical_parallel_cast-185,
model.blocks.0.ln2-layer_norm-33,
model.blocks.5.ffn-relu_grad-389,
model.blocks.3.att.key-broadcast_matmul-182,
model.blocks.5.att.output-broadcast_matmul-417,
model.blocks.5.ffn-broadcast_mul-313,
model.blocks.5.att.key-broadcast_matmul_grad_b-428,
model.blocks.1.ffn.value-hierarchical_parallel_cast-103,
model.blocks.5.ffn-broadcast_mul-393,
model.blocks.0.att.receptance-broadcast_matmul-24,
model.blocks.3.ffn.key-broadcast_matmul-566,
model.blocks.2.att-broadcast_mul-115,
model.blocks.3.ffn-broadcast_mul-205,
model.blocks.2.ln2-layer_norm-141,
model.blocks.3.ffn.key-hierarchical_parallel_cast-207,
model.blocks.3.ffn.value-hierarchical_parallel_cast-211,
model.blocks.1.att-broadcast_mul-775,
model.blocks.4.att.time_shift-pad-538,
model.blocks.5.att-broadcast_mul-284,
model.blocks.3.ffn.key-broadcast_matmul_grad_b-567_out_0_pinned_identity,
model.blocks.3.ffn-broadcast_mul-202,
model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557,
model.blocks.0-add_n-32,
model.blocks.3.ffn-add_n-206,
model.blocks.0.att.value-broadcast_matmul_grad_b-855_out_0_pinned_identity,
model.blocks.3.ffn-broadcast_mul-216,
model.blocks.2.att.receptance-broadcast_matmul_grad_b-681_out_0_pinned_identity,
model.blocks.0.att-reduce_sum_like-875,
model.blocks.0.ffn-broadcast_mul-35,
model.blocks.2.att-add_n-695,
model.blocks.3.ffn.value-broadcast_matmul_grad_b-555_out_0_pinned_identity,
model.blocks.5.att.receptance-broadcast_matmul-425,
model.blocks.2.att-add_n-713_out_0_pinned_identity,
model-reshape-359,
model.blocks.3-add_n-217,
model.blocks.5.att-broadcast_mul-431,
model.blocks.5.att-scalar_mul-455,
model.blocks.1.att.output-hierarchical_parallel_cast-84,
model.blocks.4.att.value-hierarchical_parallel_cast-237,
model.blocks.5.att-add_n-456,
model.blocks.4.ln2-add_n-499,
model.blocks.5.att.receptance-broadcast_matmul_grad_b-426,
model-reshape-340,
model.blocks.5.att-broadcast_mul-445,
model.blocks.4.att-broadcast_mul-520,
model.blocks.5.att.receptance-broadcast_matmul_grad_b-426_out_0_pinned_identity,
model.blocks.5.ln1-add_n-463,
model.blocks.5.att-broadcast_mul-437,
model.blocks.5.ln1-layer_norm-272,
model.blocks.1.ffn-reduce_sum_like-735,
model.blocks.5.att-add_n-440,
model.blocks.4.ffn-add_n-484_out_0_pinned_identity,
model.blocks.1.att.output-broadcast_matmul_grad_b-758_out_0_pinned_identity,
model.blocks.5.att-reduce_sum_like-439,
model.blocks.0.ffn-reduce_sum_like-817,
model.blocks.5.att-add_n-444,
model.blocks.5.att-broadcast_mul-442,
model.blocks.0.att-add_n-872,
model.blocks.2.ffn-add_n-662,
model.blocks.3.ffn-reduce_sum_like-565,
model.blocks.5.att-add_n-458,
model.blocks.5.ffn-relu-317,
model.blocks.4.ffn-broadcast_mul-476,
model.blocks.5.att-add_n-458_out_0_pinned_identity,
model.blocks.5.att.key-broadcast_matmul_grad_b-428_out_0_pinned_identity,
model.blocks.3.ffn.value-broadcast_matmul-212,
model.blocks.4.att-broadcast_mul-519,
model.blocks.5.att.value-broadcast_matmul-429,
model.blocks.5.att-add_n-448,
model.blocks.3.att-broadcast_mul-620,
model.blocks.5.att-reduce_sum_like-447,
model.blocks.5.att-add_n-452,
model.blocks.4.ffn.receptance-broadcast_matmul-268,
model.blocks.5.att.time_shift-add_n-454,
model.blocks.3.att.key-hierarchical_parallel_cast-181,
model.blocks.4.ffn-reduce_sum_like-477,
model.blocks.4.ffn.time_shift-add_n-494,
model.blocks.4.ffn-broadcast_mul-479,
model.blocks.2.ffn.value-broadcast_matmul_grad_b-640,
model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472_out_0_pinned_identity,
model.blocks.4.att-reduce_sum_like-536,
model.blocks.0.ffn-broadcast_mul-815,
model.blocks.4.ffn-broadcast_mul-467,
model.blocks.2.att.output-broadcast_matmul-672,
model-add_n-339,
model.blocks.2.ffn-scalar_mul-653,
model.blocks.4.ffn.value-broadcast_matmul-469,
model.blocks.4.ffn.key-broadcast_matmul-481,
model.blocks.4.ffn-add_n-488,
model-batch_matmul-336,
model.blocks.4.ffn-broadcast_mul-486,
model.blocks.4.att-reduce_sum_like-524,
model.blocks.3.att.key-broadcast_matmul_grad_b-598,
model.blocks.4.ln1-add_n-548,
model.blocks.4.ffn-reduce_sum_like-487,
model.blocks.4.ffn-broadcast_mul-489,
model.blocks.1.ln1-add_n-803,
model.blocks.2.att-broadcast_mul-686,
model.blocks.5.ffn-scalar_mul-398,
model.blocks.4.ffn.time_shift-pad-493,
model.blocks.0.ffn-add_n-841,
model.blocks.4.att.output-broadcast_matmul-502,
model.blocks.4.att.receptance-broadcast_matmul-510,
model.blocks.4.att-broadcast_mul-516,
model.blocks.5.ffn-add_n-416,
model.blocks.0.att-scalar_mul-892,
model.blocks.4.ln2-layer_norm-249,
model.blocks.5.ffn-add_n-314,
model.blocks.4.att-reduce_sum_like-518,
model.blocks.4.att-reduce_sum_like-521,
model.blocks.4.att-scalar_mul-540,
model.blocks.4.att.receptance-broadcast_matmul_grad_b-511,
model.blocks.3.att-broadcast_mul-608,
model.blocks.4.att.receptance-broadcast_matmul_grad_b-511_out_0_pinned_identity,
model.blocks.2.att.output-broadcast_matmul_grad_b-673,
model.blocks.5.ffn-broadcast_mul-404,
model.blocks.4.att.key-broadcast_matmul-512,
model.blocks.4.att-broadcast_mul-523,
model.blocks.3.att.receptance-broadcast_matmul-186,
model.blocks.4.att-add_n-529,
model.blocks.4.ffn.key-broadcast_matmul_grad_b-482,
model.blocks.4.att-scalar_mul-542,
model.blocks.0.ln1-layer_norm_grad-890,
model.blocks.0.ffn.value-broadcast_matmul-809,
model.blocks.5.att-add_n-456_out_0_pinned_identity,
model.blocks.4.att-add_n-543_out_0_pinned_identity,
model.blocks.4.att.value-broadcast_matmul-514,
model.blocks.4.ffn.receptance-broadcast_matmul-471,
model.blocks.4.att-add_n-533,
model.blocks.4.att-reduce_sum_like-532,
model.blocks.3.ffn.receptance-broadcast_matmul-556,
model.blocks.4.att.output-hierarchical_parallel_cast-246,
model.blocks.5.ffn-add_n-399,
model.blocks.2.att-add_n-116,
model.blocks.3.ffn-broadcast_mul-560,
model.blocks.0.ffn-add_n-824,
model.blocks.3.ffn-broadcast_mul-563,
model.blocks.1.att-scalar_mul-795,
model.blocks.1.att.key-broadcast_matmul_grad_b-768,
model.blocks.5.ffn.value-broadcast_matmul-320,
model.blocks.3.ffn-broadcast_mul-564,
model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557_out_0_pinned_identity,
model.blocks.2.att-reduce_sum_like-702,
model.blocks.3.ffn-broadcast_mul-552,
model.blocks.4.att-broadcast_mul-527,
model.blocks.3.ffn-relu_grad-559,
model.blocks.1.att-add_n-798_out_0_pinned_identity,
model.blocks.3.ln1-add_n-633,
model.blocks.3.ffn-broadcast_mul-570,
model.blocks.0.att.key-broadcast_matmul_grad_b-853,
model.blocks.3.ffn-add_n-573,
model.blocks.3.ffn-reduce_sum_like-572,
model.blocks.3.ffn-broadcast_mul-574,
model.blocks.3.ffn-add_n-577,
model.blocks.2.att-add_n-711_out_0_pinned_identity,
model.blocks.3.ffn.time_shift-pad-578,
model.blocks.0.att-broadcast_mul-865,
model.blocks.3.att-broadcast_mul-602,
model.blocks.5.ffn-reduce_sum_like-392,
model.blocks.3.att-broadcast_mul-604,
model.blocks.0.att-add_n-868,
model.blocks.3.att-broadcast_mul-605,
model.blocks.3.att-reduce_sum_like-606,
model.blocks.1.ffn-broadcast_mul-734,
model.blocks.4.att-add_n-229,
model.blocks.3.att-add_n-626,
model.blocks.3.att.receptance-broadcast_matmul_grad_b-596,
model.blocks.5.att-add_n-288,
model.blocks.3.att.receptance-broadcast_matmul_grad_b-596_out_0_pinned_identity,
model.blocks.0.ffn.receptance-broadcast_matmul-52,
model.blocks.1.ln2-layer_norm_grad-753,
model.blocks.3.att.key-broadcast_matmul-597,
model.blocks.5.ffn.value-broadcast_matmul_grad_b-385_out_0_pinned_identity,
model.blocks.3.ffn-add_n-586_out_0_pinned_identity,
model-log_softmax_grad-358,
model.blocks.3.att-add_n-610,
model.blocks.3.att-broadcast_mul-611,
model.blocks.4.ln2-layer_norm_grad-498,
model.blocks.3.att-broadcast_mul-612,
model.blocks.5.ffn-broadcast_mul-405,
model.blocks.3.att-reduce_sum_like-613,
model.blocks.3.att-scalar_mul-627,
model.blocks.5.att.receptance-hierarchical_parallel_cast-293,
model.blocks.0.ffn.time_shift-pad-34,
model.blocks.3.ffn-broadcast_mul-200,
model.blocks.3.att-add_n-628,
model.blocks.2-add_n-140,
model.blocks.4.att-add_n-541_out_0_pinned_identity,
model.blocks.3.att-add_n-628_out_0_pinned_identity,
model.blocks.3.att.key-broadcast_matmul_grad_b-598_out_0_pinned_identity,
model.blocks.0.att-reduce_sum_like-879,
model.blocks.2.att-add_n-699,
model.blocks.3.att-broadcast_mul-615,
model.blocks.3.att-add_n-618,
model.blocks.2.ffn.key-broadcast_matmul_grad_b-652,
model.blocks.1.ffn-add_n-756_out_0_pinned_identity,
model.blocks.3.att-broadcast_mul-616,
model.blocks.2.ffn.value-hierarchical_parallel_cast-157,
model.blocks.2.att.output-broadcast_matmul_grad_b-673_out_0_pinned_identity,
model-log_softmax-344,
model.blocks.3.att-reduce_sum_like-617,
model.blocks.5.att-broadcast_mul-446,
model.blocks.3.att-broadcast_mul-619,
model.blocks.3.att-add_n-622,
model.blocks.3.att.time_shift-pad-623,
model.blocks.3.ln1-layer_norm_grad-632,
model.blocks.2.ffn-broadcast_mul-646,
model.blocks.2.ffn-broadcast_mul-648,
model.blocks.1.att-broadcast_mul-789,
model.blocks.2.ffn-broadcast_mul-649,
model.blocks.0.ffn-broadcast_mul-807,
model.blocks.2.ffn-reduce_sum_like-650,
model.blocks.2.ffn-add_n-654,
model.blocks.2.ffn-add_n-654_out_0_pinned_identity,
model.blocks.3.ffn.key-broadcast_matmul-208,
model.blocks.2.ffn-relu_grad-644,
model.head-hierarchical_parallel_cast-337,
model.blocks.2.ffn.key-broadcast_matmul-651,
model.blocks.3.ffn-reduce_sum_like-562,
model.blocks.2.ffn-broadcast_mul-655,
model.blocks.2.ffn-add_n-658,
model.blocks.2.ffn-reduce_sum_like-657,
model.blocks.4.att-add_n-541,
model.blocks.2.ffn-broadcast_mul-659,
model.blocks.2.ln2-add_n-669,
model.blocks.2.att-reduce_sum_like-688,
model.blocks.4.ffn-add_n-501_out_0_pinned_identity,
model.blocks.1.att-add_n-780,
model.blocks.2.att-broadcast_mul-690,
model.blocks.2.att-reduce_sum_like-691,
model.blocks.2.att.key-broadcast_matmul_grad_b-683_out_0_pinned_identity,
model.blocks.5.ffn-add_n-309,
model.blocks.5.ln2-layer_norm-303,
model.blocks.2.att-scalar_mul-710,
model.blocks.2.att-add_n-711,
model.blocks.2.ffn.receptance-broadcast_matmul-160,
model.blocks.2.att-broadcast_mul-692,
model.blocks.2.att-broadcast_mul-693,
model.blocks.1.att-broadcast_mul-58,
model.blocks.2.att-broadcast_mul-696,
model.blocks.2-add_n-163,
model-batch_matmul-367,
model.blocks.1.att-broadcast_mul-771,
model.blocks.5.att.value-broadcast_matmul-292,
model.blocks.2.att-scalar_mul-712,
model.blocks.5.att-broadcast_mul-287,
model.blocks.1.att-reduce_sum_like-791,
model.blocks.5.ffn
I20220824 02:43:24.346840 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_3-AutoMixedPrecision
I20220824 02:43:24.346880 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_4-PruneAmpWhiteIdentityOpPass
I20220824 02:43:24.450372 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_4-PruneAmpWhiteIdentityOpPass
I20220824 02:43:24.450409 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_5-OptimizerPlacementOptimizationPass
I20220824 02:43:24.450417 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_5-OptimizerPlacementOptimizationPass
I20220824 02:43:24.450421 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_6-FuseAddToOutputPass
I20220824 02:43:24.566179 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_6-FuseAddToOutputPass
I20220824 02:43:24.566218 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_7-IRRoundTripBeforeAD
I20220824 02:43:24.566227 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_7-IRRoundTripBeforeAD
I20220824 02:43:24.566232 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_8-DynamicLossScaleSchedulePass
I20220824 02:43:24.566237 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_8-DynamicLossScaleSchedulePass
I20220824 02:43:24.566241 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_9-AutoTrainStep
I20220824 02:43:24.672466 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_9-AutoTrainStep
I20220824 02:43:24.672508 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_10-AutoLearningRate
I20220824 02:43:24.762683 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_10-AutoLearningRate
I20220824 02:43:24.762724 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_11-QuantAwareTraining
I20220824 02:43:24.762740 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_11-QuantAwareTraining
I20220824 02:43:24.762745 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_12-GenerateOptimizerOpConfs
I20220824 02:43:24.871826 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_12-GenerateOptimizerOpConfs
I20220824 02:43:24.871874 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_13-PrunePinnedIdentityOpPass
I20220824 02:43:27.197641 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_13-PrunePinnedIdentityOpPass
I20220824 02:43:27.197705 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_14-ReplaceEmbeddingOps
I20220824 02:43:27.340008 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_14-ReplaceEmbeddingOps
I20220824 02:43:27.340045 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_15-FuseEmbeddingShuffleInteractionPass
I20220824 02:43:27.340065 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_15-FuseEmbeddingShuffleInteractionPass
I20220824 02:43:27.340068 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_16-FuseBCEReduceMeanFwBwPass
I20220824 02:43:27.340085 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_16-FuseBCEReduceMeanFwBwPass
I20220824 02:43:27.340088 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_17-AddSspVariableProxy
I20220824 02:43:27.340096 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_17-AddSspVariableProxy
I20220824 02:43:27.340099 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_18-CheckpointingPass
I20220824 02:43:27.477694 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_18-CheckpointingPass
I20220824 02:43:27.477732 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_19-CudnnFusedNormalizationAddReluPass
I20220824 02:43:27.620852 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_19-CudnnFusedNormalizationAddReluPass
I20220824 02:43:27.620894 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_20-PruneCastToStaticShapeOpsPass
I20220824 02:43:27.781328 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_20-PruneCastToStaticShapeOpsPass
I20220824 02:43:27.781368 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_21-IRRoundTrip
I20220824 02:43:27.781387 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_21-IRRoundTrip
I20220824 02:43:27.781391 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_22-FuseAddToOutputPass1
I20220824 02:43:27.958559 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_22-FuseAddToOutputPass1
I20220824 02:43:27.958609 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_23-FuseConsecutiveAddPass
I20220824 02:43:28.249280 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_23-FuseConsecutiveAddPass
I20220824 02:43:28.249325 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_24-IndexedSlicesOptimizerRewritePass
I20220824 02:43:28.249332 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_24-IndexedSlicesOptimizerRewritePass
I20220824 02:43:28.249336 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_25-SplitSparseSoftmaxCrossEntropyOpPass
I20220824 02:43:28.420624 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_25-SplitSparseSoftmaxCrossEntropyOpPass
I20220824 02:43:28.420662 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_26-DoParallelCastBeforeWideningTypeCast
I20220824 02:43:28.561056 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_26-DoParallelCastBeforeWideningTypeCast
I20220824 02:43:28.561098 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_27-FuseCastScalePass
I20220824 02:43:28.691248 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_27-FuseCastScalePass
I20220824 02:43:28.691303 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_28-PruneParallelCastOpsPass
I20220824 02:43:28.851759 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_28-PruneParallelCastOpsPass
I20220824 02:43:28.851804 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_29-FuseUpdateOpsPass
I20220824 02:43:29.024806 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_29-FuseUpdateOpsPass
I20220824 02:43:29.024847 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_30-FuseModelUpdateCastOpsPass
I20220824 02:43:29.024868 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_30-FuseModelUpdateCastOpsPass
I20220824 02:43:29.024884 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_31-MultiTensorModelUpdatePass
I20220824 02:43:29.024889 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_31-MultiTensorModelUpdatePass
I20220824 02:43:29.024893 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_32-FixPipelineStageIdPass
I20220824 02:43:29.134517 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_32-FixPipelineStageIdPass
I20220824 02:43:29.134559 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_33-PipelineBufferPass
I20220824 02:43:29.264134 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_33-PipelineBufferPass
I20220824 02:43:29.264182 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_34-DumpVariableInfoPass
I20220824 02:43:29.394918 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_34-DumpVariableInfoPass
I20220824 02:43:29.394951 1392289 job_build_and_infer_ctx.cpp:937] GraphBase_0 start compiling with pass pass_cnt_35-DumpBlobParallelConfPass
I20220824 02:43:29.515578 1392289 job_build_and_infer_ctx.cpp:951] GraphBase_0 finish compiling with pass pass_cnt_35-DumpBlobParallelConfPass
(GRAPH:GraphBase_0:GraphBase) end building graph with compile passes.
(GRAPH:GraphBase_0:GraphBase) start re-building graph outputs for optimizatioin.
(GRAPH:GraphBase_0:GraphBase) end re-building graph outputs for optimizatioin.
(GRAPH:GraphBase_0:GraphBase) building graph Done! Cost time: 6.16s.

(GRAPH:GraphBase_0:GraphBase) start building plan.
I20220824 02:43:32.177898 1392289 nn_graph.cpp:291] Graph name: GraphBase_0 compile time: 0.87588 seconds.
I20220824 02:43:32.660974 1392289 plan_util.cpp:965] 
 Graph name GraphBase_0 in Rank: 0, Device: 0 needs to allocate [ 7990.68 MiB ] device memory. 
   In general, Chunk id: 1  memory is [ 6480.66 MiB ] with mem_block_num = 121
        Unreused memory not eager var is  [ 372.368 MiB ] with mem_block_num = 1092
        Eager Variable Tensor total memory is [ 1137.65 MiB ] with mem_block_num = 348
I20220824 02:43:32.661118 1392289 plan_util.cpp:965] 
 Graph name GraphBase_0 in Rank: 1, Device: 1 needs to allocate [ 7990.68 MiB ] device memory. 
   In general, Chunk id: 0  memory is [ 6480.66 MiB ] with mem_block_num = 121
        Unreused memory not eager var is  [ 372.368 MiB ] with mem_block_num = 1093
        Eager Variable Tensor total memory is [ 1137.65 MiB ] with mem_block_num = 348
I20220824 02:43:32.661126 1392289 plan_util.cpp:1002] ========================= In Device : 0 Chunk Memory info details:
I20220824 02:43:32.661130 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 152 has num = 1219 tensor with mem size = 5585.02
I20220824 02:43:32.662009 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 15 has num = 1 tensor with mem size = 99.3526
I20220824 02:43:32.662017 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 4 has num = 1 tensor with mem size = 49.6763
I20220824 02:43:32.662029 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 14 has num = 1 tensor with mem size = 49.6763
I20220824 02:43:32.662035 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 3 has num = 1 tensor with mem size = 49.6763
I20220824 02:43:32.662040 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 2 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.662045 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 1 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.662050 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 81 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.662055 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 0 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.662060 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 157 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.662065 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 16 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.662070 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 29 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662078 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 240 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662083 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 44 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662088 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 111 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662092 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 32 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662097 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 34 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662102 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 46 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662107 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 71 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662113 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 73 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662118 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 97 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662123 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 99 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662128 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 109 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662133 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 236 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662138 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 23 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.662142 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 22 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662148 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 21 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662153 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 93 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662158 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 12 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662163 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 91 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662168 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 27 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662173 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 11 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662179 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 76 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662184 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 79 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662189 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 13 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.662194 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 72 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662199 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 40 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662204 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 77 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662209 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 86 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662214 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 98 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662220 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 104 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662225 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 105 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662230 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 33 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662235 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 37 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662240 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 38 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662245 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 39 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662250 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 56 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662253 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 8 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662258 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 67 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662263 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 45 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662267 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 65 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662272 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 58 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662277 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 52 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662282 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 54 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662287 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 106 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662292 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 10 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662297 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 238 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662302 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 17 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662307 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 7 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662312 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 6 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662317 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 18 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662323 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 9 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662328 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 184 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662333 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 180 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662338 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 178 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662343 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 110 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662348 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 5 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662351 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 103 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662356 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 66 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662364 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 182 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662367 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 94 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662372 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 85 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662377 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 87 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.662382 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 90 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.662387 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 88 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.662391 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 24 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.662396 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 89 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.662401 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 30 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.662406 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 92 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662411 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 20 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662416 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 80 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662421 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 78 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662426 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 19 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662431 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 25 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662436 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 26 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662441 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 31 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662446 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 28 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.662451 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 153 has num = 1 tensor with mem size = 0.032768
I20220824 02:43:32.662456 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 102 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662461 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 60 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662468 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 62 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662473 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 68 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662478 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 83 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662483 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 107 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662488 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 108 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662493 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 42 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662498 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 101 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662503 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 112 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662509 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 113 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662514 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 114 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662519 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 186 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662524 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 188 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662528 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 95 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662533 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 50 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662539 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 64 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662544 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 48 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662547 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 43 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662552 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 70 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662557 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 41 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662562 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 36 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662566 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 35 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662571 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 74 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662576 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 75 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662581 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 82 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662586 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 84 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662591 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 96 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662595 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 69 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662600 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 100 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.662607 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 156 has num = 1 tensor with mem size = 0.000512
I20220824 02:43:32.662613 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 154 has num = 1 tensor with mem size = 0.000512
I20220824 02:43:32.662618 1392289 plan_util.cpp:1007]      In Device: 0 Chunk id: 1 MemBlock id: 155 has num = 1 tensor with mem size = 0.000512
I20220824 02:43:32.662895 1392289 plan_util.cpp:1002] ========================= In Device : 1 Chunk Memory info details:
I20220824 02:43:32.662899 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 205 has num = 1219 tensor with mem size = 5585.02
I20220824 02:43:32.663754 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 196 has num = 1 tensor with mem size = 99.3526
I20220824 02:43:32.663764 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 197 has num = 1 tensor with mem size = 49.6763
I20220824 02:43:32.663769 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 214 has num = 1 tensor with mem size = 49.6763
I20220824 02:43:32.663777 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 215 has num = 1 tensor with mem size = 49.6763
I20220824 02:43:32.663781 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 210 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.663786 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 195 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.663791 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 212 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.663796 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 151 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.663801 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 213 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.663806 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 211 has num = 1 tensor with mem size = 24.8381
I20220824 02:43:32.663811 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 57 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663816 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 121 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663821 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 171 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663826 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 135 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663831 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 133 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663836 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 123 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663841 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 173 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663846 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 53 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663851 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 177 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663856 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 181 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663861 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 218 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663866 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 227 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663870 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 224 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663874 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 229 has num = 1 tensor with mem size = 16.7772
I20220824 02:43:32.663882 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 199 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663887 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 198 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663892 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 189 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663897 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 222 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663901 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 163 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663906 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 190 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663911 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 200 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663916 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 161 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663923 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 168 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663928 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 165 has num = 1 tensor with mem size = 8.38861
I20220824 02:43:32.663933 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 63 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663937 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 241 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663942 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 139 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663947 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 126 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663952 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 117 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663957 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 116 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663962 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 55 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663967 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 122 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663971 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 127 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663976 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 129 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663981 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 138 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663986 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 140 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663990 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 141 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.663995 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 145 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664000 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 146 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664005 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 167 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664011 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 172 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664014 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 179 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664021 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 228 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664026 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 232 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664032 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 234 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664037 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 235 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664041 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 239 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664047 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 203 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664052 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 237 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664057 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 204 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664063 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 217 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664067 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 216 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664072 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 134 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664077 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 233 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664081 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 193 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664086 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 147 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664091 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 202 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664096 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 194 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664100 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 115 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664105 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 174 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664110 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 201 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664114 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 128 has num = 1 tensor with mem size = 4.1943
I20220824 02:43:32.664119 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 159 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.664124 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 225 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.664129 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 219 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.664134 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 158 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.664139 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 160 has num = 1 tensor with mem size = 2.09715
I20220824 02:43:32.664144 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 191 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664149 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 164 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664153 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 192 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664158 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 226 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664166 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 221 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664171 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 220 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664176 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 166 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664181 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 162 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664186 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 223 has num = 1 tensor with mem size = 1.04858
I20220824 02:43:32.664191 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 206 has num = 1 tensor with mem size = 0.032768
I20220824 02:43:32.664196 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 47 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664201 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 176 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664207 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 61 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664212 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 59 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664217 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 185 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664222 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 183 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664227 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 187 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664232 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 51 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664237 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 49 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664242 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 175 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664247 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 230 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664252 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 231 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664256 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 118 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664261 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 132 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664266 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 170 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664270 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 143 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664275 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 131 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664280 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 137 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664285 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 130 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664290 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 149 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664295 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 142 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664300 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 144 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664306 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 124 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664311 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 148 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664316 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 150 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664321 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 120 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664326 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 169 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664330 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 119 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664335 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 136 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664340 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 125 has num = 1 tensor with mem size = 0.004096
I20220824 02:43:32.664347 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 208 has num = 1 tensor with mem size = 0.000512
I20220824 02:43:32.664352 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 207 has num = 1 tensor with mem size = 0.000512
I20220824 02:43:32.664357 1392289 plan_util.cpp:1007]      In Device: 1 Chunk id: 0 MemBlock id: 209 has num = 1 tensor with mem size = 0.000512
I20220824 02:43:32.843573 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_k.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.843961 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.844293 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.844568 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.844842 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.845124 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.845419 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.845707 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.845974 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.846269 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.846601 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.846872 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_first-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.847142 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_decay-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.847432 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_decay-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.847721 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.849579 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.849882 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.850176 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.850467 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.850769 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_v-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.851047 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_v-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.851346 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.851720 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.852056 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.852326 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.852596 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.852862 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.853647 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.854220 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.854905 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.855480 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.855770 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.856037 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.856294 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.856737 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.857002 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.857260 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_first-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.857518 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_decay-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.857772 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_decay-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.858034 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.858299 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.858595 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.858853 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.859259 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.859601 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.859915 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_v-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.860204 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.860479 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.860914 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.861342 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.861819 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.862108 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.862537 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.862845 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.863133 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.863409 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.863814 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.emb.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.864392 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.864802 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.865070 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.865330 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.865592 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.865851 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.866106 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.866410 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_decay-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.866719 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.866998 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_decay-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.867264 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.867530 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_k.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.867802 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.868134 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.868435 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_first-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.868687 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.869165 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.869453 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.869943 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.870275 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.870515 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.870777 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.871032 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.871295 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.871559 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.871963 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.872256 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.872579 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.872964 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.873200 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.873462 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_v-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.873746 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.873973 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_first-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.874203 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.874639 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.874900 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.875125 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.875370 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.875595 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_first-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.875866 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.876057 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.876288 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.876485 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.876690 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln0.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.876955 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_q.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.877305 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.877533 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_v-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.877735 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.877940 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.878141 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.878348 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.878551 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_v-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.878755 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.878942 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.879132 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.879339 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_v-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.879520 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.879725 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.879940 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.880165 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_first-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.880364 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.880589 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_first-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.880955 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.881162 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.881350 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.881690 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.881887 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.time_first-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.882072 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.882261 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_v-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.882442 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.882623 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.882807 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.882992 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.883194 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.883550 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.883791 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.883981 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.884169 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.884356 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.884541 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.884892 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.885087 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.885272 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.885458 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.885643 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.885825 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.886008 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.886446 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.886706 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.886963 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.887152 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.887495 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.887686 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.888051 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: System-Train-TrainStep created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.888267 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.888624 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.888984 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.889358 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.889556 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_v-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.889911 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.890108 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.ln_out.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.890298 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.890482 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.890674 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.891048 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.emb.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.891417 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.891616 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.891803 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.892001 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.892192 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.4.att.time_mix_v-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.892385 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.892589 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_v-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.892781 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.892994 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.893345 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.893561 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.893747 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.893930 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.894129 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_decay-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.894330 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.894522 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_decay-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.894729 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_decay-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.894944 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.time_first-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.895310 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.895722 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.att.output.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.895936 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.896147 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.896355 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.896713 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.896909 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ffn.receptance.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.897091 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.897289 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.897480 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_v-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.897828 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.898187 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.1.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.898380 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.898758 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.898952 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.899149 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.899343 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.899526 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.899714 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.key.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.900084 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.900285 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.900480 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.900664 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.900851 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_first-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.901059 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.head_q.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.901258 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_first-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.901463 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.5.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.901671 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.901877 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.902091 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_k-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.902293 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.902495 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.902705 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.902911 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.ln1.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.903136 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.903334 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.time_mix_r-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.903687 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.key.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.904069 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.value.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.904278 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.receptance.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.904464 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.904660 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln1.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.904846 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_mix_k-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.905043 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ffn.time_mix_r-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.905238 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_decay-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.905434 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ln2.bias-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.905660 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.time_first-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.905858 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.att.time_decay-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.906200 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.2.ffn.value.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.906400 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.att.output.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.906596 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.0.att.time_decay-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.906790 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.weight-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.907001 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.weight-v created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.907205 1392289 nn_graph.cpp:391] Lazy nn.Graph name GraphBase_0 op: model.blocks.3.ln2.bias-m created in JobPass, nn.Graph has created a eager tensor for this variable.
I20220824 02:43:32.995541 1392289 thread_manager.cpp:53]  Actor thread: 524299 created.
I20220824 02:43:32.995592 1392289 thread_manager.cpp:53]  Actor thread: 524355 created.
I20220824 02:43:32.995625 1392289 thread_manager.cpp:53]  Actor thread: 524296 created.
I20220824 02:43:32.995648 1392289 thread_manager.cpp:53]  Actor thread: 524354 created.
I20220824 02:43:32.995715 1392289 thread_manager.cpp:53]  Actor thread: 524295 created.
I20220824 02:43:32.995752 1392289 thread_manager.cpp:53]  Actor thread: 524294 created.
I20220824 02:43:32.995788 1392289 thread_manager.cpp:53]  Actor thread: 524353 created.
I20220824 02:43:32.995822 1392289 thread_manager.cpp:53]  Actor thread: 524291 created.
I20220824 02:43:32.995853 1392289 thread_manager.cpp:53]  Actor thread: 524292 created.
I20220824 02:43:32.995887 1392289 thread_manager.cpp:53]  Actor thread: 524352 created.
I20220824 02:43:32.995914 1392289 thread_manager.cpp:53]  Actor thread: 524293 created.
I20220824 02:43:32.995950 1392289 thread_manager.cpp:53]  Actor thread: 524290 created.
I20220824 02:43:32.995991 1392289 thread_manager.cpp:53]  Actor thread: 524288 created.
I20220824 02:43:32.996023 1392289 thread_manager.cpp:53]  Actor thread: 524305 created.
I20220824 02:43:32.997678 1392289 thread_manager.cpp:53]  Actor thread: 1048579 created.
I20220824 02:43:32.997723 1392289 thread_manager.cpp:53]  Actor thread: 524289 created.
I20220824 02:43:32.997747 1392289 thread_manager.cpp:53]  Actor thread: 524301 created.
I20220824 02:43:32.997778 1392289 thread_manager.cpp:53]  Actor thread: 524297 created.
I20220824 02:43:32.997809 1392289 thread_manager.cpp:53]  Actor thread: 524356 created.
I20220824 02:43:32.997835 1392289 thread_manager.cpp:53]  Actor thread: 524357 created.
I20220824 02:43:32.997865 1392289 thread_manager.cpp:53]  Actor thread: 524298 created.
I20220824 02:43:33.001901 1392289 thread_manager.cpp:53]  Actor thread: 1048577 created.
I20220824 02:43:33.002017 1392289 thread_manager.cpp:53]  Actor thread: 524303 created.
I20220824 02:43:33.005375 1392289 thread_manager.cpp:53]  Actor thread: 1048578 created.
I20220824 02:43:33.005463 1392289 thread_manager.cpp:53]  Actor thread: 524304 created.
I20220824 02:43:33.005528 1392289 thread_manager.cpp:53]  Actor thread: 524307 created.
I20220824 02:43:33.005589 1392289 thread_manager.cpp:53]  Actor thread: 524300 created.
I20220824 02:43:33.010568 1392289 thread_manager.cpp:53]  Actor thread: 1048576 created.
I20220824 02:43:33.010675 1392289 thread_manager.cpp:53]  Actor thread: 524302 created.
I20220824 02:43:33.010740 1392289 thread_manager.cpp:53]  Actor thread: 524306 created.
I20220824 02:43:33.010799 1392289 thread_manager.cpp:53]  Actor thread: 524308 created.
(GRAPH:GraphBase_0:GraphBase) building plan Done! Cost time: 3.87s.

GraphBase_0 with operators:
(GRAPH:GraphBase_0:GraphBase): (
  (CONFIG:config:GraphConfig(training=True, ))
  (INPUT:_GraphBase_0_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
         sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
  (INPUT:_GraphBase_0_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
         sbp=(oneflow.sbp.split(dim=0),), size=(8, 1024), dtype=oneflow.int64))
  (MODULE:model:GPT()): (
    (INPUT:_model_input.1.0_idx:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
           sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
           dtype=oneflow.int64))
    (INPUT:_model_input.1.1_targets:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0, 1]),
           sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
           dtype=oneflow.int64))
    (BUFFER:model.copy_mask:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
           sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32)): ()
    (MODULE:model.emb:VocabEmbedding(num_embeddings=6064, embedding_dim=1024)): (
      (INPUT:_model.emb_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024),
             dtype=oneflow.int64))
      (PARAMETER:model.emb.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.emb-gather-0(model.emb.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16)), System-Boxing-Identity-297/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64))) -> (model.emb-gather-0/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.emb-unsorted_segment_sum_like-898(model.blocks.0.ln0-layer_norm_grad-897/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), System-Boxing-Identity-297/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), model.emb.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.emb-unsorted_segment_sum_like-898/out_0:(sbp=(P), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.emb-gather-0-out_0-cast_h2f(model.emb-gather-0/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.emb-gather-0-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.emb-unsorted_segment_sum_like-898_out_0_pinned_identity-out_0-cast_h2f(model.emb-unsorted_segment_sum_like-898/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.emb-unsorted_segment_sum_like-898_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.emb_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
    )
    (MODULE:model.blocks:Sequential()): (
      (INPUT:_model.blocks_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
      (MODULE:model.blocks.0:Block()): (
        (INPUT:_model.blocks.0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
        (MODULE:model.blocks.0.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.0.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.0.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.0.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm-2(model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm_param_grad-889(model.blocks.0.att.time_shift-add_n-882-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1-layer_norm_param_grad-889/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm_param_grad-889/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm_grad-890(model.blocks.0.att.time_shift-add_n-882/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln0-layer_norm-1-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln1-layer_norm-2/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm-2/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln1-layer_norm_grad-890/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight-out-cast_f2h(model.blocks.0.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1-add_n-891-out_0-cast_h2f(model.blocks.0.ln1-layer_norm_grad-890/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln1-add_n-891-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.weight_optimizer(model.blocks.0.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm_param_grad-889/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln1.bias_optimizer(model.blocks.0.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1-layer_norm_param_grad-889/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.0.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.0.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.0.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.0.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2-layer_norm-33(model.blocks.0.att.output-broadcast_matmul-31/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln2-layer_norm-33/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-33/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-33/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2-layer_norm_param_grad-837(model.blocks.0.ffn.time_shift-add_n-834-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0-add_n-32-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-33/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-33/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln2-layer_norm_param_grad-837/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm_param_grad-837/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2-layer_norm_grad-838(model.blocks.0.ffn.time_shift-add_n-834/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.output-broadcast_matmul-31/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-33/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm-33/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln2-layer_norm_grad-838/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight-out-cast_f2h(model.blocks.0.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias-out-cast_f2h(model.blocks.0.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.weight_optimizer(model.blocks.0.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm_param_grad-837/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln2.bias_optimizer(model.blocks.0.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2-layer_norm_param_grad-837/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.0.ln0:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.0.ln0_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
          (PARAMETER:model.blocks.0.ln0.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ln0.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.0.ln0.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm-1(model.emb-gather-0-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm_param_grad-896(model.blocks.0.ln1-add_n-891-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.emb-gather-0-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0-layer_norm_param_grad-896/gamma_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm_param_grad-896/beta_diff_0:(sbp=(P), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm_grad-897(model.blocks.0.ln1-layer_norm_grad-890/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.emb-gather-0/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln0-layer_norm-1/mean_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm-1/inv_variance_0:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.ln0.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ln0-layer_norm_grad-897/dx_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0-layer_norm-1-y_0-cast_f2h(model.blocks.0.ln0-layer_norm-1/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0-layer_norm-1-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight-out-cast_f2h(model.blocks.0.ln0.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.ln0.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.weight_optimizer(model.blocks.0.ln0.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm_param_grad-896/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln0.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ln0.bias_optimizer(model.blocks.0.ln0.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0-layer_norm_param_grad-896/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ln0.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.ln0.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ln0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.0.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.0.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.0.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.0.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.0.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.0.att.time_shift-pad-3(model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-pad-881(model.blocks.0.att-add_n-880/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-pad-881/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-add_n-882([model.blocks.0.att.time_shift-pad-881/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-add_n-876/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att.time_shift-add_n-882/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f(model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.time_shift-add_n-882-out_0-cast_h2f(model.blocks.0.att.time_shift-add_n-882/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.time_shift-add_n-882-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.0.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-hierarchical_parallel_cast-19(model.blocks.0.att-add_n-8-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-broadcast_matmul-20(model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-broadcast_matmul-20/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-broadcast_matmul-852(model.blocks.0.att-wkv_grad-847-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-broadcast_matmul-852/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-broadcast_matmul_grad_b-853(model.blocks.0.att-wkv_grad-847-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.key-hierarchical_parallel_cast-19/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-broadcast_matmul_grad_b-853/out_0:(sbp=(S(1)), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-hierarchical_parallel_cast-857(model.blocks.0.att.key-broadcast_matmul-852/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key.weight-out-cast_f2h(model.blocks.0.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-broadcast_matmul-20-out_0-cast_h2f(model.blocks.0.att.key-broadcast_matmul-20/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-broadcast_matmul-20-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key-broadcast_matmul_grad_b-853_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att.key-broadcast_matmul_grad_b-853/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.key-broadcast_matmul_grad_b-853_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.key.weight_optimizer(model.blocks.0.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.key-broadcast_matmul_grad_b-853_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.key.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.key.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-hierarchical_parallel_cast-21(model.blocks.0.att-add_n-13-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-broadcast_matmul-22(model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-broadcast_matmul-22/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-broadcast_matmul-854(model.blocks.0.att-wkv_grad-847-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-broadcast_matmul-854/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-broadcast_matmul_grad_b-855(model.blocks.0.att-wkv_grad-847-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.value-hierarchical_parallel_cast-21/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-broadcast_matmul_grad_b-855/out_0:(sbp=(S(1)), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-hierarchical_parallel_cast-858(model.blocks.0.att.value-broadcast_matmul-854/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value.weight-out-cast_f2h(model.blocks.0.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-broadcast_matmul-22-out_0-cast_h2f(model.blocks.0.att.value-broadcast_matmul-22/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-broadcast_matmul-22-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value-broadcast_matmul_grad_b-855_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att.value-broadcast_matmul_grad_b-855/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.value-broadcast_matmul_grad_b-855_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.value.weight_optimizer(model.blocks.0.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.value-broadcast_matmul_grad_b-855_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.value.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.value.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-hierarchical_parallel_cast-23(model.blocks.0.att-add_n-18-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul-24(model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul-24/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul-850(model.blocks.0.att-sigmoid_v2_grad-846-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul-850/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul_grad_b-851(model.blocks.0.att-sigmoid_v2_grad-846-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.receptance-hierarchical_parallel_cast-23/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul_grad_b-851/out_0:(sbp=(S(1)), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-hierarchical_parallel_cast-856(model.blocks.0.att.receptance-broadcast_matmul-850/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight-out-cast_f2h(model.blocks.0.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f(model.blocks.0.att.receptance-broadcast_matmul-24/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance-broadcast_matmul_grad_b-851_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att.receptance-broadcast_matmul_grad_b-851/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.receptance-broadcast_matmul_grad_b-851_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.receptance.weight_optimizer(model.blocks.0.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.receptance-broadcast_matmul_grad_b-851_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.split(dim=0),), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.0.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
            (PARAMETER:model.blocks.0.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul-31(model.blocks.0.att-broadcast_mul-29-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul-31/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul-842(model.blocks.0.ln2-layer_norm_grad-838/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul-842/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul_grad_b-843(model.blocks.0.ln2-layer_norm_grad-838/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-broadcast_mul-29-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul_grad_b-843/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight-out-cast_f2h(model.blocks.0.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul-842-out_0-cast_h2f(model.blocks.0.att.output-broadcast_matmul-842/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul-842-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output-broadcast_matmul_grad_b-843_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att.output-broadcast_matmul_grad_b-843/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att.output-broadcast_matmul_grad_b-843_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.att.output.weight_optimizer(model.blocks.0.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.output-broadcast_matmul_grad_b-843_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.0.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-4(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-4/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-5(model.blocks.0.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-5/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-6(model.blocks.0.att-scalar_mul-5/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-6/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-7(model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-scalar_add-6/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-7/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-8([model.blocks.0.att-broadcast_mul-4/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-7/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))]) -> (model.blocks.0.att-add_n-8/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-9(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-9/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-10(model.blocks.0.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-10/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-11(model.blocks.0.att-scalar_mul-10/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-11/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-12(model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-scalar_add-11/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-12/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-13([model.blocks.0.att-broadcast_mul-9/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-12/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))]) -> (model.blocks.0.att-add_n-13/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-14(model.blocks.0.ln1-layer_norm-2/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-14/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-15(model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-15/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-16(model.blocks.0.att-scalar_mul-15/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-16/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-17(model.blocks.0.att.time_shift-pad-3-y_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-scalar_add-16/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-17/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-18([model.blocks.0.att-broadcast_mul-14/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-17/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))]) -> (model.blocks.0.att-add_n-18/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2-25(model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2-25/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_decay() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_first() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-wkv-26(model.blocks.0.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-300/out:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-298/out:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-wkv-26/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-exp-27(model.blocks.0.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-exp-27/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-28(model.blocks.0.att-exp-27/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_mul-28/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-29(System-Boxing-Identity-302/out:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-wkv-26/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-29/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-844(model.blocks.0.att.output-broadcast_matmul-842-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-wkv-26/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-844/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-845(model.blocks.0.att.output-broadcast_matmul-842-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-302/out:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-845/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2_grad-846(model.blocks.0.att.receptance-broadcast_matmul-24-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-844/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2_grad-846/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-wkv_grad-847(model.blocks.0.att-scalar_mul-28/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-300/out:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-298/out:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-broadcast_mul-845/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-wkv_grad-847/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.att-wkv_grad-847/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.0.att-wkv_grad-847/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.att-wkv_grad-847/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum-848(model.blocks.0.att-wkv_grad-847/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-reduce_sum-848/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum-849(model.blocks.0.att-wkv_grad-847/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-reduce_sum-849/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-859(model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-859/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-860(model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-860/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-861(model.blocks.0.att-broadcast_mul-860/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-861/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-862(model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-16-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-862/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-863(model.blocks.0.att.receptance-hierarchical_parallel_cast-856/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-863/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-864(model.blocks.0.att-broadcast_mul-863/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-16-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-864/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-865(model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-865/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-866(model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-866/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-867(model.blocks.0.att-broadcast_mul-866/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-867/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-869(model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-6-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-869/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-870(model.blocks.0.att.key-hierarchical_parallel_cast-857/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-870/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-871(model.blocks.0.att-broadcast_mul-870/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-6-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-871/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-873(model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-873/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-874(model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln1-layer_norm-2-y_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-874/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-875(model.blocks.0.att-broadcast_mul-874/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-875/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-876([model.blocks.0.att-broadcast_mul-873/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-broadcast_mul-865/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-broadcast_mul-859/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att-add_n-876/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-877(model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-11-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-877/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-878(model.blocks.0.att.value-hierarchical_parallel_cast-858/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att.time_shift-pad-3/y_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-broadcast_mul-878/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-reduce_sum_like-879(model.blocks.0.att-broadcast_mul-878/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-scalar_add-11-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-reduce_sum_like-879/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-880([model.blocks.0.att-broadcast_mul-877/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-broadcast_mul-869/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-broadcast_mul-862/z_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att-add_n-880/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-883(model.blocks.0.att-reduce_sum_like-864/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-scalar_mul-883/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-884([model.blocks.0.att-scalar_mul-883/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-reduce_sum_like-861/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att-add_n-884/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-885(model.blocks.0.att-reduce_sum_like-871/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-scalar_mul-885/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-886([model.blocks.0.att-scalar_mul-885/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-reduce_sum_like-867/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att-add_n-886/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_mul-892(model.blocks.0.att-reduce_sum_like-879/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-scalar_mul-892/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-893([model.blocks.0.att-scalar_mul-892/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.att-reduce_sum_like-875/y_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.att-add_n-893/out_0:(sbp=(P), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-8-out_0-cast_f2h(model.blocks.0.att-add_n-8/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-add_n-8-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-broadcast_mul-29-z_0-cast_f2h(model.blocks.0.att-broadcast_mul-29/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-broadcast_mul-29-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-13-out_0-cast_f2h(model.blocks.0.att-add_n-13/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-add_n-13-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_k-out-cast_f2h(model.blocks.0.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-11-out_0-cast_f2h(model.blocks.0.att-scalar_add-11/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-11-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-wkv_grad-847-gv_0-cast_f2h(model.blocks.0.att-wkv_grad-847/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-wkv_grad-847-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-16-out_0-cast_f2h(model.blocks.0.att-scalar_add-16/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-16-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-wkv_grad-847-gk_0-cast_f2h(model.blocks.0.att-wkv_grad-847/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-wkv_grad-847-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r-out-cast_f2h(model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-scalar_add-6-out_0-cast_f2h(model.blocks.0.att-scalar_add-6/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-scalar_add-6-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-18-out_0-cast_f2h(model.blocks.0.att-add_n-18/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-add_n-18-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-sigmoid_v2_grad-846-dx_0-cast_f2h(model.blocks.0.att-sigmoid_v2_grad-846/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att-sigmoid_v2_grad-846-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_v-out-cast_f2h(model.blocks.0.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-886_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att-add_n-886/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-add_n-886_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-884_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att-add_n-884/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-add_n-884_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att-add_n-893_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.att-add_n-893/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.att-add_n-893_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_k_optimizer(model.blocks.0.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att-add_n-886_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_v-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_v-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_v_optimizer(model.blocks.0.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att-add_n-893_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_v-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_v-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_mix_r_optimizer(model.blocks.0.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att-add_n-884_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_decay-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_decay-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_decay_optimizer(model.blocks.0.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.att-reduce_sum-848/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.time_decay-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.att.time_decay-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_first-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_first-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.att.time_first_optimizer(model.blocks.0.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.att-reduce_sum-849/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.att.time_first-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.0.att.time_first-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.0.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.0.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.0.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.0.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.0.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.0.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.0.ffn.time_shift-pad-34(model.blocks.0.ln2-layer_norm-33/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.time_shift-pad-34/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.time_shift-pad-833(model.blocks.0.ffn-add_n-832/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.time_shift-pad-833/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.time_shift-add_n-834([model.blocks.0.ffn.time_shift-pad-833/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-add_n-828/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn.time_shift-add_n-834/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.time_shift-add_n-834-out_0-cast_h2f(model.blocks.0.ffn.time_shift-add_n-834/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.time_shift-add_n-834-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.0.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul-46(model.blocks.0.ffn-add_n-39/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul-46/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul-821(model.blocks.0.ffn-relu_grad-814/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul-821/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul_grad_b-822(model.blocks.0.ffn-relu_grad-814/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-add_n-39/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul_grad_b-822/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight-out-cast_f2h(model.blocks.0.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key-broadcast_matmul_grad_b-822_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn.key-broadcast_matmul_grad_b-822/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.key-broadcast_matmul_grad_b-822_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.key.weight_optimizer(model.blocks.0.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.key-broadcast_matmul_grad_b-822_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.0.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.0.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul-52(model.blocks.0.ffn-add_n-44/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul-52/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul-811(model.blocks.0.ffn-sigmoid_v2_grad-808-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812(model.blocks.0.ffn-sigmoid_v2_grad-808-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-add_n-44/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight-out-cast_f2h(model.blocks.0.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul-52-out_0-cast_h2f(model.blocks.0.ffn.receptance-broadcast_matmul-52/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul-52-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.receptance.weight_optimizer(model.blocks.0.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.receptance-broadcast_matmul_grad_b-812_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.0.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.0.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.0.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.0.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-50(model.blocks.0.ffn-square-48-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-50/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-809(model.blocks.0.ffn-broadcast_mul-807/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-809/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul_grad_b-810(model.blocks.0.ffn-broadcast_mul-807/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-square-48-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul_grad_b-810/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight-out-cast_f2h(model.blocks.0.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-809-out_0-cast_h2f(model.blocks.0.ffn.value-broadcast_matmul-809/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-809-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul-50-out_0-cast_h2f(model.blocks.0.ffn.value-broadcast_matmul-50/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul-50-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value-broadcast_matmul_grad_b-810_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn.value-broadcast_matmul_grad_b-810/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn.value-broadcast_matmul_grad_b-810_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.0.ffn.value.weight_optimizer(model.blocks.0.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.0.ffn.value-broadcast_matmul_grad_b-810_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.0.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.0.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.0.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-35(model.blocks.0.ln2-layer_norm-33/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-35/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-36(model.blocks.0.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_mul-36/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-37(model.blocks.0.ffn-scalar_mul-36/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-37/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-38(model.blocks.0.ffn.time_shift-pad-34/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-37-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-38/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-39([model.blocks.0.ffn-broadcast_mul-35/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-38/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-39/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-40(model.blocks.0.ln2-layer_norm-33/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-40/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-41(model.blocks.0.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_mul-41/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-42(model.blocks.0.ffn-scalar_mul-41/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-42/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-43(model.blocks.0.ffn.time_shift-pad-34/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-42-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-43/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-44([model.blocks.0.ffn-broadcast_mul-40/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-43/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-44/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-relu-47(model.blocks.0.ffn.key-broadcast_matmul-46/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-relu-47/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square-48(model.blocks.0.ffn-relu-47-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square-48/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2-53(model.blocks.0.ffn.receptance-broadcast_matmul-52-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2-53/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-54(model.blocks.0.ffn-sigmoid_v2-53-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.value-broadcast_matmul-50/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-54/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-806(model.blocks.1.ln1-add_n-803-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.value-broadcast_matmul-50-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-broadcast_mul-806/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-807(model.blocks.1.ln1-layer_norm_grad-802/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-sigmoid_v2-53-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-807/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2_grad-808(model.blocks.0.ffn.receptance-broadcast_matmul-52-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn-broadcast_mul-806/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2_grad-808/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square_grad-813(model.blocks.0.ffn-relu-47-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.0.ffn.value-broadcast_matmul-809-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square_grad-813/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-relu_grad-814(model.blocks.0.ffn-square_grad-813-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-relu-47/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-relu_grad-814/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-815(model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-815/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-816(model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-33/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-816/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-817(model.blocks.0.ffn-broadcast_mul-816/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-817/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-818(model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-42-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-818/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-819(model.blocks.0.ffn.receptance-broadcast_matmul-811/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_shift-pad-34/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-819/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-820(model.blocks.0.ffn-broadcast_mul-819/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-42-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-820/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-823(model.blocks.0.ffn-reduce_sum_like-820/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-scalar_mul-823/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-824([model.blocks.0.ffn-scalar_mul-823/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-reduce_sum_like-817/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-824/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-825(model.blocks.0.ffn.key-broadcast_matmul-821/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-825/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-826(model.blocks.0.ffn.key-broadcast_matmul-821/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ln2-layer_norm-33/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-826/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-827(model.blocks.0.ffn-broadcast_mul-826/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-827/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-828([model.blocks.0.ffn-broadcast_mul-825/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-815/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-828/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-829(model.blocks.0.ffn.key-broadcast_matmul-821/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-37-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-829/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-broadcast_mul-830(model.blocks.0.ffn.key-broadcast_matmul-821/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn.time_shift-pad-34/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-broadcast_mul-830/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-reduce_sum_like-831(model.blocks.0.ffn-broadcast_mul-830/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-scalar_add-37-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-reduce_sum_like-831/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-832([model.blocks.0.ffn-broadcast_mul-829/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-818/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-832/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_mul-840(model.blocks.0.ffn-reduce_sum_like-831/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-scalar_mul-840/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-841([model.blocks.0.ffn-scalar_mul-840/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-reduce_sum_like-827/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0.ffn-add_n-841/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square_grad-813-dx_0-cast_f2h(model.blocks.0.ffn-square_grad-813/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square_grad-813-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k-out-cast_f2h(model.blocks.0.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-42-out_0-cast_f2h(model.blocks.0.ffn-scalar_add-42/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-42-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r-out-cast_f2h(model.blocks.0.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2-53-y_0-cast_f2h(model.blocks.0.ffn-sigmoid_v2-53/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2-53-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-sigmoid_v2_grad-808-dx_0-cast_f2h(model.blocks.0.ffn-sigmoid_v2_grad-808/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-sigmoid_v2_grad-808-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-square-48-y_0-cast_f2h(model.blocks.0.ffn-square-48/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-square-48-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-scalar_add-37-out_0-cast_f2h(model.blocks.0.ffn-scalar_add-37/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.0.ffn-scalar_add-37-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-relu-47-y_0-cast_h2f(model.blocks.0.ffn-relu-47/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-relu-47-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-841_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn-add_n-841/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-add_n-841_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn-add_n-824_out_0_pinned_identity-out_0-cast_h2f(model.blocks.0.ffn-add_n-824/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0.ffn-add_n-824_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_k_optimizer(model.blocks.0.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn-add_n-841_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.0.ffn.time_mix_r_optimizer(model.blocks.0.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn-add_n-824_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.0.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.0.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.0-add_n-55([model.blocks.0.att.output-broadcast_matmul-31/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0.ffn-broadcast_mul-54/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.0-add_n-55/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.0-add_n-55-out_0-cast_h2f(model.blocks.0-add_n-55/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0-add_n-55-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.0-add_n-32-out_0-cast_h2f(model.blocks.0.att.output-broadcast_matmul-31/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.0-add_n-32-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.0_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.1:Block()): (
        (INPUT:_model.blocks.1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.1.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.1.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.1.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.1.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-layer_norm-56(model.blocks.0-add_n-55/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-56/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-56/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-layer_norm_param_grad-801(model.blocks.1.att.time_shift-add_n-794-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.0-add_n-55-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-56/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-56/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln1-layer_norm_param_grad-801/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm_param_grad-801/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-layer_norm_grad-802(model.blocks.1.att.time_shift-add_n-794/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.0-add_n-55/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-56/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm-56/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln1-layer_norm_grad-802/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias-out-cast_f2h(model.blocks.1.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight-out-cast_f2h(model.blocks.1.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1-add_n-803-out_0-cast_h2f(model.blocks.1.ln1-layer_norm_grad-802/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln1-add_n-803-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.weight_optimizer(model.blocks.1.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm_param_grad-801/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln1.bias_optimizer(model.blocks.1.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1-layer_norm_param_grad-801/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.1.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.1.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.1.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.1.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2-layer_norm-87(model.blocks.1.att.output-broadcast_matmul-85/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln2-layer_norm-87/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-87/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-87/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2-layer_norm_param_grad-752(model.blocks.1.ffn.time_shift-add_n-749-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1-add_n-86-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-87/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-87/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln2-layer_norm_param_grad-752/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm_param_grad-752/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2-layer_norm_grad-753(model.blocks.1.ffn.time_shift-add_n-749/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.output-broadcast_matmul-85/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-87/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm-87/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ln2-layer_norm_grad-753/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias-out-cast_f2h(model.blocks.1.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight-out-cast_f2h(model.blocks.1.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.weight_optimizer(model.blocks.1.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm_param_grad-752/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ln2.bias_optimizer(model.blocks.1.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2-layer_norm_param_grad-752/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.1.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.1.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.1.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.1.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.1.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.1.att.time_shift-pad-57(model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.time_shift-pad-793(model.blocks.1.att-add_n-792/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.time_shift-pad-793/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.time_shift-add_n-794([model.blocks.1.att.time_shift-pad-793/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-add_n-788/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att.time_shift-add_n-794/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.time_shift-add_n-794-out_0-cast_h2f(model.blocks.1.att.time_shift-add_n-794/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.time_shift-add_n-794-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.1.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key-broadcast_matmul-74(model.blocks.1.att-add_n-62/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.key-broadcast_matmul-74/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key-broadcast_matmul-767(model.blocks.1.att-wkv_grad-762-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.key-broadcast_matmul-767/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key-broadcast_matmul_grad_b-768(model.blocks.1.att-wkv_grad-762-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-add_n-62/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.key-broadcast_matmul_grad_b-768/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key.weight-out-cast_f2h(model.blocks.1.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key-broadcast_matmul-74-out_0-cast_h2f(model.blocks.1.att.key-broadcast_matmul-74/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.key-broadcast_matmul-74-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key-broadcast_matmul_grad_b-768_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att.key-broadcast_matmul_grad_b-768/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.key-broadcast_matmul_grad_b-768_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.key.weight_optimizer(model.blocks.1.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.key-broadcast_matmul_grad_b-768_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.key.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.key.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value-broadcast_matmul-76(model.blocks.1.att-add_n-67/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.value-broadcast_matmul-76/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value-broadcast_matmul-769(model.blocks.1.att-wkv_grad-762-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.value-broadcast_matmul-769/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value-broadcast_matmul_grad_b-770(model.blocks.1.att-wkv_grad-762-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-add_n-67/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.value-broadcast_matmul_grad_b-770/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value.weight-out-cast_f2h(model.blocks.1.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value-broadcast_matmul-76-out_0-cast_h2f(model.blocks.1.att.value-broadcast_matmul-76/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.value-broadcast_matmul-76-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value-broadcast_matmul_grad_b-770_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att.value-broadcast_matmul_grad_b-770/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.value-broadcast_matmul_grad_b-770_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.value.weight_optimizer(model.blocks.1.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.value-broadcast_matmul_grad_b-770_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.value.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.value.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul-78(model.blocks.1.att-add_n-72/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul-78/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul-765(model.blocks.1.att-sigmoid_v2_grad-761-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul-765/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul_grad_b-766(model.blocks.1.att-sigmoid_v2_grad-761-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-add_n-72/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul_grad_b-766/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight-out-cast_f2h(model.blocks.1.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul-78-out_0-cast_h2f(model.blocks.1.att.receptance-broadcast_matmul-78/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul-78-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance-broadcast_matmul_grad_b-766_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att.receptance-broadcast_matmul_grad_b-766/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.receptance-broadcast_matmul_grad_b-766_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.receptance.weight_optimizer(model.blocks.1.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.receptance-broadcast_matmul_grad_b-766_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.1.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
            (PARAMETER:model.blocks.1.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul-85(model.blocks.1.att-broadcast_mul-83-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul-85/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul-757(model.blocks.1.ln2-layer_norm_grad-753/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul-757/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul_grad_b-758(model.blocks.1.ln2-layer_norm_grad-753/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-83-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul_grad_b-758/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight-out-cast_f2h(model.blocks.1.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul-757-out_0-cast_h2f(model.blocks.1.att.output-broadcast_matmul-757/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul-757-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output-broadcast_matmul_grad_b-758_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att.output-broadcast_matmul_grad_b-758/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att.output-broadcast_matmul_grad_b-758_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.att.output.weight_optimizer(model.blocks.1.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.output-broadcast_matmul_grad_b-758_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.1.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-58(model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-58/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-59(model.blocks.1.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-59/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-60(model.blocks.1.att-scalar_mul-59/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-60/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-61(model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-60-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-61/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-62([model.blocks.1.att-broadcast_mul-58/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-61/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-62/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-63(model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-63/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-64(model.blocks.1.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-64/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-65(model.blocks.1.att-scalar_mul-64/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-65/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-66(model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-65-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-66/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-67([model.blocks.1.att-broadcast_mul-63/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-66/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-67/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-68(model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-68/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-69(model.blocks.1.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-69/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-70(model.blocks.1.att-scalar_mul-69/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-70/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-71(model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-70-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-71/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-72([model.blocks.1.att-broadcast_mul-68/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-71/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-72/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2-79(model.blocks.1.att.receptance-broadcast_matmul-78-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2-79/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_decay() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_first() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-wkv-80(model.blocks.1.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att.key-broadcast_matmul-74-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.value-broadcast_matmul-76-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-wkv-80/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-exp-81(model.blocks.1.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-exp-81/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-82(model.blocks.1.att-exp-81/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_mul-82/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-83(model.blocks.1.att-sigmoid_v2-79/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att-wkv-80/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-broadcast_mul-83/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-759(model.blocks.1.att.output-broadcast_matmul-757-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att-wkv-80/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-broadcast_mul-759/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-760(model.blocks.1.att.output-broadcast_matmul-757-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att-sigmoid_v2-79/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-broadcast_mul-760/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2_grad-761(model.blocks.1.att.receptance-broadcast_matmul-78-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att-broadcast_mul-759/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2_grad-761/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-wkv_grad-762(model.blocks.1.att-scalar_mul-82/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att.key-broadcast_matmul-74-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att.value-broadcast_matmul-76-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att-broadcast_mul-760/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-wkv_grad-762/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.att-wkv_grad-762/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.1.att-wkv_grad-762/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.att-wkv_grad-762/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum-763(model.blocks.1.att-wkv_grad-762/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-reduce_sum-763/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum-764(model.blocks.1.att-wkv_grad-762/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-reduce_sum-764/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-771(model.blocks.1.att.receptance-broadcast_matmul-765/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-771/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-772(model.blocks.1.att.receptance-broadcast_matmul-765/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-772/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-773(model.blocks.1.att-broadcast_mul-772/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-773/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-774(model.blocks.1.att.receptance-broadcast_matmul-765/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-70-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-774/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-775(model.blocks.1.att.receptance-broadcast_matmul-765/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-775/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-776(model.blocks.1.att-broadcast_mul-775/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-70-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-776/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-777(model.blocks.1.att.key-broadcast_matmul-767/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-777/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-778(model.blocks.1.att.key-broadcast_matmul-767/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-778/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-779(model.blocks.1.att-broadcast_mul-778/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-779/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-781(model.blocks.1.att.key-broadcast_matmul-767/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-60-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-781/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-782(model.blocks.1.att.key-broadcast_matmul-767/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-782/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-783(model.blocks.1.att-broadcast_mul-782/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-60-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-783/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-785(model.blocks.1.att.value-broadcast_matmul-769/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-785/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-786(model.blocks.1.att.value-broadcast_matmul-769/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln1-layer_norm-56/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-786/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-787(model.blocks.1.att-broadcast_mul-786/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-787/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-788([model.blocks.1.att-broadcast_mul-785/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-777/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-771/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-788/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-789(model.blocks.1.att.value-broadcast_matmul-769/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-65-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-789/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-790(model.blocks.1.att.value-broadcast_matmul-769/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att.time_shift-pad-57/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-broadcast_mul-790/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-reduce_sum_like-791(model.blocks.1.att-broadcast_mul-790/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-scalar_add-65-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-reduce_sum_like-791/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-792([model.blocks.1.att-broadcast_mul-789/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-781/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-broadcast_mul-774/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-792/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-795(model.blocks.1.att-reduce_sum_like-776/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-scalar_mul-795/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-796([model.blocks.1.att-scalar_mul-795/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-reduce_sum_like-773/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-796/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-797(model.blocks.1.att-reduce_sum_like-783/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-scalar_mul-797/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-798([model.blocks.1.att-scalar_mul-797/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-reduce_sum_like-779/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-798/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_mul-804(model.blocks.1.att-reduce_sum_like-791/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-scalar_mul-804/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-805([model.blocks.1.att-scalar_mul-804/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.att-reduce_sum_like-787/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.att-add_n-805/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-broadcast_mul-83-z_0-cast_f2h(model.blocks.1.att-broadcast_mul-83/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-broadcast_mul-83-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-wkv_grad-762-gv_0-cast_f2h(model.blocks.1.att-wkv_grad-762/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-wkv_grad-762-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-wkv_grad-762-gk_0-cast_f2h(model.blocks.1.att-wkv_grad-762/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-wkv_grad-762-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-60-out_0-cast_f2h(model.blocks.1.att-scalar_add-60/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-60-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-65-out_0-cast_f2h(model.blocks.1.att-scalar_add-65/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-65-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-scalar_add-70-out_0-cast_f2h(model.blocks.1.att-scalar_add-70/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-scalar_add-70-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r-out-cast_f2h(model.blocks.1.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-sigmoid_v2_grad-761-dx_0-cast_f2h(model.blocks.1.att-sigmoid_v2_grad-761/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att-sigmoid_v2_grad-761-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v-out-cast_f2h(model.blocks.1.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_k-out-cast_f2h(model.blocks.1.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-805_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att-add_n-805/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-add_n-805_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-796_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att-add_n-796/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-add_n-796_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att-add_n-798_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.att-add_n-798/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.att-add_n-798_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_k_optimizer(model.blocks.1.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att-add_n-798_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_v_optimizer(model.blocks.1.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att-add_n-805_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_v-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_v-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_mix_r_optimizer(model.blocks.1.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att-add_n-796_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_decay-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_decay-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_decay_optimizer(model.blocks.1.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att-reduce_sum-763/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.time_decay-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att.time_decay-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_first-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_first-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.att.time_first_optimizer(model.blocks.1.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att-reduce_sum-764/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.att.time_first-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.1.att.time_first-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.1.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.1.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.1.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.1.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.1.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.1.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.1.ffn.time_shift-pad-88(model.blocks.1.ln2-layer_norm-87/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.time_shift-pad-88/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.time_shift-pad-748(model.blocks.1.ffn-add_n-747/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.time_shift-pad-748/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.time_shift-add_n-749([model.blocks.1.ffn.time_shift-pad-748/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-add_n-743/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn.time_shift-add_n-749/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.time_shift-add_n-749-out_0-cast_h2f(model.blocks.1.ffn.time_shift-add_n-749/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.time_shift-add_n-749-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.1.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul-100(model.blocks.1.ffn-add_n-93/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul-100/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul-736(model.blocks.1.ffn-relu_grad-729/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul-736/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul_grad_b-737(model.blocks.1.ffn-relu_grad-729/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-add_n-93/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul_grad_b-737/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight-out-cast_f2h(model.blocks.1.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key-broadcast_matmul_grad_b-737_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn.key-broadcast_matmul_grad_b-737/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.key-broadcast_matmul_grad_b-737_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.key.weight_optimizer(model.blocks.1.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.key-broadcast_matmul_grad_b-737_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.1.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.1.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul-106(model.blocks.1.ffn-add_n-98/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul-106/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul-726(model.blocks.1.ffn-sigmoid_v2_grad-723-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727(model.blocks.1.ffn-sigmoid_v2_grad-723-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-add_n-98/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight-out-cast_f2h(model.blocks.1.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul-106-out_0-cast_h2f(model.blocks.1.ffn.receptance-broadcast_matmul-106/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul-106-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.receptance.weight_optimizer(model.blocks.1.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.receptance-broadcast_matmul_grad_b-727_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.1.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.1.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.1.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.1.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-104(model.blocks.1.ffn-square-102-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-104/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-724(model.blocks.1.ffn-broadcast_mul-722/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-724/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul_grad_b-725(model.blocks.1.ffn-broadcast_mul-722/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-square-102-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul_grad_b-725/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight-out-cast_f2h(model.blocks.1.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-724-out_0-cast_h2f(model.blocks.1.ffn.value-broadcast_matmul-724/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-724-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul-104-out_0-cast_h2f(model.blocks.1.ffn.value-broadcast_matmul-104/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul-104-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value-broadcast_matmul_grad_b-725_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn.value-broadcast_matmul_grad_b-725/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn.value-broadcast_matmul_grad_b-725_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.1.ffn.value.weight_optimizer(model.blocks.1.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.1.ffn.value-broadcast_matmul_grad_b-725_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.1.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.1.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.1.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-89(model.blocks.1.ln2-layer_norm-87/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-89/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-90(model.blocks.1.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_mul-90/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-91(model.blocks.1.ffn-scalar_mul-90/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-91/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-92(model.blocks.1.ffn.time_shift-pad-88/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-91-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-92/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-93([model.blocks.1.ffn-broadcast_mul-89/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-92/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-93/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-94(model.blocks.1.ln2-layer_norm-87/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-94/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-95(model.blocks.1.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_mul-95/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-96(model.blocks.1.ffn-scalar_mul-95/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-96/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-97(model.blocks.1.ffn.time_shift-pad-88/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-96-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-97/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-98([model.blocks.1.ffn-broadcast_mul-94/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-97/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-98/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-relu-101(model.blocks.1.ffn.key-broadcast_matmul-100/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-relu-101/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square-102(model.blocks.1.ffn-relu-101-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square-102/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2-107(model.blocks.1.ffn.receptance-broadcast_matmul-106-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2-107/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-108(model.blocks.1.ffn-sigmoid_v2-107-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.value-broadcast_matmul-104/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-108/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-721(model.blocks.2.ln1-add_n-718-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.value-broadcast_matmul-104-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-broadcast_mul-721/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-722(model.blocks.2.ln1-layer_norm_grad-717/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-sigmoid_v2-107-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-722/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2_grad-723(model.blocks.1.ffn.receptance-broadcast_matmul-106-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn-broadcast_mul-721/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2_grad-723/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square_grad-728(model.blocks.1.ffn-relu-101-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.1.ffn.value-broadcast_matmul-724-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square_grad-728/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-relu_grad-729(model.blocks.1.ffn-square_grad-728-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-relu-101/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-relu_grad-729/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-730(model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-730/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-731(model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-87/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-731/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-732(model.blocks.1.ffn-broadcast_mul-731/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-732/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-733(model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-96-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-733/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-734(model.blocks.1.ffn.receptance-broadcast_matmul-726/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_shift-pad-88/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-734/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-735(model.blocks.1.ffn-broadcast_mul-734/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-96-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-735/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-738(model.blocks.1.ffn-reduce_sum_like-735/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-scalar_mul-738/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-739([model.blocks.1.ffn-scalar_mul-738/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-reduce_sum_like-732/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-739/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-740(model.blocks.1.ffn.key-broadcast_matmul-736/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-740/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-741(model.blocks.1.ffn.key-broadcast_matmul-736/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ln2-layer_norm-87/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-741/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-742(model.blocks.1.ffn-broadcast_mul-741/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-742/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-743([model.blocks.1.ffn-broadcast_mul-740/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-730/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-743/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-744(model.blocks.1.ffn.key-broadcast_matmul-736/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-91-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-744/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-broadcast_mul-745(model.blocks.1.ffn.key-broadcast_matmul-736/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn.time_shift-pad-88/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-broadcast_mul-745/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-reduce_sum_like-746(model.blocks.1.ffn-broadcast_mul-745/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-scalar_add-91-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-reduce_sum_like-746/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-747([model.blocks.1.ffn-broadcast_mul-744/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-733/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-747/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_mul-755(model.blocks.1.ffn-reduce_sum_like-746/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-scalar_mul-755/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-756([model.blocks.1.ffn-scalar_mul-755/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-reduce_sum_like-742/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1.ffn-add_n-756/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square-102-y_0-cast_f2h(model.blocks.1.ffn-square-102/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square-102-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2-107-y_0-cast_f2h(model.blocks.1.ffn-sigmoid_v2-107/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2-107-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-sigmoid_v2_grad-723-dx_0-cast_f2h(model.blocks.1.ffn-sigmoid_v2_grad-723/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-sigmoid_v2_grad-723-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k-out-cast_f2h(model.blocks.1.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-91-out_0-cast_f2h(model.blocks.1.ffn-scalar_add-91/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-91-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-scalar_add-96-out_0-cast_f2h(model.blocks.1.ffn-scalar_add-96/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-scalar_add-96-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r-out-cast_f2h(model.blocks.1.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.1.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-square_grad-728-dx_0-cast_f2h(model.blocks.1.ffn-square_grad-728/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.1.ffn-square_grad-728-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-relu-101-y_0-cast_h2f(model.blocks.1.ffn-relu-101/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-relu-101-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-739_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn-add_n-739/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-add_n-739_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn-add_n-756_out_0_pinned_identity-out_0-cast_h2f(model.blocks.1.ffn-add_n-756/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1.ffn-add_n-756_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_k_optimizer(model.blocks.1.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn-add_n-756_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.1.ffn.time_mix_r_optimizer(model.blocks.1.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn-add_n-739_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.1.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.1.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.1-add_n-109([model.blocks.1.att.output-broadcast_matmul-85/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1.ffn-broadcast_mul-108/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.1-add_n-109/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.1-add_n-86-out_0-cast_h2f(model.blocks.1.att.output-broadcast_matmul-85/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1-add_n-86-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.1-add_n-109-out_0-cast_h2f(model.blocks.1-add_n-109/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.1-add_n-109-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.2:Block()): (
        (INPUT:_model.blocks.2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.2.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.2.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.2.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.2.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-layer_norm-110(model.blocks.1-add_n-109/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-110/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-110/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-layer_norm_param_grad-716(model.blocks.2.att.time_shift-add_n-709-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.1-add_n-109-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-110/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-110/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln1-layer_norm_param_grad-716/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm_param_grad-716/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-layer_norm_grad-717(model.blocks.2.att.time_shift-add_n-709/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.1-add_n-109/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-110/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm-110/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln1-layer_norm_grad-717/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight-out-cast_f2h(model.blocks.2.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias-out-cast_f2h(model.blocks.2.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1-add_n-718-out_0-cast_h2f(model.blocks.2.ln1-layer_norm_grad-717/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln1-add_n-718-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.weight_optimizer(model.blocks.2.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm_param_grad-716/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln1.bias_optimizer(model.blocks.2.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1-layer_norm_param_grad-716/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.2.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.2.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.2.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.2.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2-layer_norm-141(model.blocks.2.att.output-broadcast_matmul-139/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln2-layer_norm-141/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-141/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-141/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2-layer_norm_param_grad-667(model.blocks.2.ffn.time_shift-add_n-664-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2-add_n-140-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-141/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-141/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln2-layer_norm_param_grad-667/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm_param_grad-667/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2-layer_norm_grad-668(model.blocks.2.ffn.time_shift-add_n-664/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.output-broadcast_matmul-139/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-141/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm-141/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ln2-layer_norm_grad-668/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias-out-cast_f2h(model.blocks.2.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight-out-cast_f2h(model.blocks.2.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.weight_optimizer(model.blocks.2.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm_param_grad-667/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ln2.bias_optimizer(model.blocks.2.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2-layer_norm_param_grad-667/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.2.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.2.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.2.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.2.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.2.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.2.att.time_shift-pad-111(model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.time_shift-pad-708(model.blocks.2.att-add_n-707/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.time_shift-pad-708/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.time_shift-add_n-709([model.blocks.2.att.time_shift-pad-708/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-add_n-703/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att.time_shift-add_n-709/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.time_shift-add_n-709-out_0-cast_h2f(model.blocks.2.att.time_shift-add_n-709/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.time_shift-add_n-709-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.2.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key-broadcast_matmul-128(model.blocks.2.att-add_n-116/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.key-broadcast_matmul-128/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key-broadcast_matmul-682(model.blocks.2.att-wkv_grad-677-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.key-broadcast_matmul-682/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key-broadcast_matmul_grad_b-683(model.blocks.2.att-wkv_grad-677-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-add_n-116/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.key-broadcast_matmul_grad_b-683/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key.weight-out-cast_f2h(model.blocks.2.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key-broadcast_matmul-128-out_0-cast_h2f(model.blocks.2.att.key-broadcast_matmul-128/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.key-broadcast_matmul-128-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key-broadcast_matmul_grad_b-683_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att.key-broadcast_matmul_grad_b-683/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.key-broadcast_matmul_grad_b-683_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.key.weight_optimizer(model.blocks.2.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.key-broadcast_matmul_grad_b-683_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.key.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.key.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value-broadcast_matmul-130(model.blocks.2.att-add_n-121/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.value-broadcast_matmul-130/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value-broadcast_matmul-684(model.blocks.2.att-wkv_grad-677-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.value-broadcast_matmul-684/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value-broadcast_matmul_grad_b-685(model.blocks.2.att-wkv_grad-677-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-add_n-121/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.value-broadcast_matmul_grad_b-685/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value.weight-out-cast_f2h(model.blocks.2.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value-broadcast_matmul-130-out_0-cast_h2f(model.blocks.2.att.value-broadcast_matmul-130/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.value-broadcast_matmul-130-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value-broadcast_matmul_grad_b-685_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att.value-broadcast_matmul_grad_b-685/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.value-broadcast_matmul_grad_b-685_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.value.weight_optimizer(model.blocks.2.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.value-broadcast_matmul_grad_b-685_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.value.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.value.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul-132(model.blocks.2.att-add_n-126/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul-132/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul-680(model.blocks.2.att-sigmoid_v2_grad-676-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul-680/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul_grad_b-681(model.blocks.2.att-sigmoid_v2_grad-676-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-add_n-126/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul_grad_b-681/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight-out-cast_f2h(model.blocks.2.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul-132-out_0-cast_h2f(model.blocks.2.att.receptance-broadcast_matmul-132/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul-132-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance-broadcast_matmul_grad_b-681_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att.receptance-broadcast_matmul_grad_b-681/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.receptance-broadcast_matmul_grad_b-681_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.receptance.weight_optimizer(model.blocks.2.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.receptance-broadcast_matmul_grad_b-681_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.2.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
            (PARAMETER:model.blocks.2.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul-139(model.blocks.2.att-broadcast_mul-137-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul-139/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul-672(model.blocks.2.ln2-layer_norm_grad-668/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul-672/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul_grad_b-673(model.blocks.2.ln2-layer_norm_grad-668/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-137-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul_grad_b-673/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight-out-cast_f2h(model.blocks.2.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul-672-out_0-cast_h2f(model.blocks.2.att.output-broadcast_matmul-672/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul-672-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output-broadcast_matmul_grad_b-673_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att.output-broadcast_matmul_grad_b-673/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att.output-broadcast_matmul_grad_b-673_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.att.output.weight_optimizer(model.blocks.2.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.output-broadcast_matmul_grad_b-673_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.2.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-112(model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-112/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-113(model.blocks.2.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-113/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-114(model.blocks.2.att-scalar_mul-113/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-114/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-115(model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-114-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-115/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-116([model.blocks.2.att-broadcast_mul-112/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-115/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-116/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-117(model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-117/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-118(model.blocks.2.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-118/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-119(model.blocks.2.att-scalar_mul-118/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-119/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-120(model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-119-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-120/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-121([model.blocks.2.att-broadcast_mul-117/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-120/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-121/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-122(model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-122/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-123(model.blocks.2.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-123/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-124(model.blocks.2.att-scalar_mul-123/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-124/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-125(model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-124-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-125/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-126([model.blocks.2.att-broadcast_mul-122/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-125/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-126/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2-133(model.blocks.2.att.receptance-broadcast_matmul-132-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2-133/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_decay() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_first() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-wkv-134(model.blocks.2.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att.key-broadcast_matmul-128-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.value-broadcast_matmul-130-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-wkv-134/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-exp-135(model.blocks.2.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-exp-135/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-136(model.blocks.2.att-exp-135/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_mul-136/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-137(model.blocks.2.att-sigmoid_v2-133/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att-wkv-134/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-broadcast_mul-137/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-674(model.blocks.2.att.output-broadcast_matmul-672-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att-wkv-134/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-broadcast_mul-674/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-675(model.blocks.2.att.output-broadcast_matmul-672-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att-sigmoid_v2-133/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-broadcast_mul-675/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2_grad-676(model.blocks.2.att.receptance-broadcast_matmul-132-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att-broadcast_mul-674/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2_grad-676/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-wkv_grad-677(model.blocks.2.att-scalar_mul-136/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att.key-broadcast_matmul-128-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att.value-broadcast_matmul-130-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att-broadcast_mul-675/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-wkv_grad-677/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.att-wkv_grad-677/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.2.att-wkv_grad-677/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.att-wkv_grad-677/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum-678(model.blocks.2.att-wkv_grad-677/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-reduce_sum-678/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum-679(model.blocks.2.att-wkv_grad-677/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-reduce_sum-679/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-686(model.blocks.2.att.receptance-broadcast_matmul-680/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-686/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-687(model.blocks.2.att.receptance-broadcast_matmul-680/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-687/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-688(model.blocks.2.att-broadcast_mul-687/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-688/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-689(model.blocks.2.att.receptance-broadcast_matmul-680/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-124-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-689/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-690(model.blocks.2.att.receptance-broadcast_matmul-680/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-690/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-691(model.blocks.2.att-broadcast_mul-690/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-124-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-691/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-692(model.blocks.2.att.key-broadcast_matmul-682/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-692/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-693(model.blocks.2.att.key-broadcast_matmul-682/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-693/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-694(model.blocks.2.att-broadcast_mul-693/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-694/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-696(model.blocks.2.att.key-broadcast_matmul-682/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-114-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-696/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-697(model.blocks.2.att.key-broadcast_matmul-682/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-697/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-698(model.blocks.2.att-broadcast_mul-697/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-114-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-698/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-700(model.blocks.2.att.value-broadcast_matmul-684/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-700/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-701(model.blocks.2.att.value-broadcast_matmul-684/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln1-layer_norm-110/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-701/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-702(model.blocks.2.att-broadcast_mul-701/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-702/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-703([model.blocks.2.att-broadcast_mul-700/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-692/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-686/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-703/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-704(model.blocks.2.att.value-broadcast_matmul-684/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-119-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-704/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-705(model.blocks.2.att.value-broadcast_matmul-684/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att.time_shift-pad-111/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-broadcast_mul-705/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-reduce_sum_like-706(model.blocks.2.att-broadcast_mul-705/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-scalar_add-119-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-reduce_sum_like-706/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-707([model.blocks.2.att-broadcast_mul-704/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-696/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-broadcast_mul-689/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-707/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-710(model.blocks.2.att-reduce_sum_like-691/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-scalar_mul-710/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-711([model.blocks.2.att-scalar_mul-710/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-reduce_sum_like-688/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-711/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-712(model.blocks.2.att-reduce_sum_like-698/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-scalar_mul-712/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-713([model.blocks.2.att-scalar_mul-712/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-reduce_sum_like-694/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-713/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_mul-719(model.blocks.2.att-reduce_sum_like-706/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-scalar_mul-719/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-720([model.blocks.2.att-scalar_mul-719/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.att-reduce_sum_like-702/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.att-add_n-720/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-broadcast_mul-137-z_0-cast_f2h(model.blocks.2.att-broadcast_mul-137/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-broadcast_mul-137-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-wkv_grad-677-gk_0-cast_f2h(model.blocks.2.att-wkv_grad-677/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-wkv_grad-677-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-sigmoid_v2_grad-676-dx_0-cast_f2h(model.blocks.2.att-sigmoid_v2_grad-676/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-sigmoid_v2_grad-676-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-124-out_0-cast_f2h(model.blocks.2.att-scalar_add-124/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-124-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-119-out_0-cast_f2h(model.blocks.2.att-scalar_add-119/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-119-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-wkv_grad-677-gv_0-cast_f2h(model.blocks.2.att-wkv_grad-677/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-wkv_grad-677-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_k-out-cast_f2h(model.blocks.2.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v-out-cast_f2h(model.blocks.2.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r-out-cast_f2h(model.blocks.2.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-scalar_add-114-out_0-cast_f2h(model.blocks.2.att-scalar_add-114/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.att-scalar_add-114-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-711_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att-add_n-711/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-add_n-711_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-720_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att-add_n-720/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-add_n-720_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att-add_n-713_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.att-add_n-713/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.att-add_n-713_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_k_optimizer(model.blocks.2.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att-add_n-713_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_v_optimizer(model.blocks.2.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att-add_n-720_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_v-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_v-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_mix_r_optimizer(model.blocks.2.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att-add_n-711_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_decay-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_decay-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_decay_optimizer(model.blocks.2.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att-reduce_sum-678/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.time_decay-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att.time_decay-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_first-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_first-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.att.time_first_optimizer(model.blocks.2.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att-reduce_sum-679/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.att.time_first-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.2.att.time_first-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.2.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.2.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.2.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.2.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.2.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.2.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.2.ffn.time_shift-pad-142(model.blocks.2.ln2-layer_norm-141/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.time_shift-pad-142/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.time_shift-pad-663(model.blocks.2.ffn-add_n-662/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.time_shift-pad-663/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.time_shift-add_n-664([model.blocks.2.ffn.time_shift-pad-663/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-add_n-658/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn.time_shift-add_n-664/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.time_shift-add_n-664-out_0-cast_h2f(model.blocks.2.ffn.time_shift-add_n-664/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.time_shift-add_n-664-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.2.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul-154(model.blocks.2.ffn-add_n-147/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul-154/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul-651(model.blocks.2.ffn-relu_grad-644/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul-651/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul_grad_b-652(model.blocks.2.ffn-relu_grad-644/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-add_n-147/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul_grad_b-652/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight-out-cast_f2h(model.blocks.2.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key-broadcast_matmul_grad_b-652_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn.key-broadcast_matmul_grad_b-652/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.key-broadcast_matmul_grad_b-652_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.key.weight_optimizer(model.blocks.2.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.key-broadcast_matmul_grad_b-652_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.2.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.2.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul-160(model.blocks.2.ffn-add_n-152/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul-160/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul-641(model.blocks.2.ffn-sigmoid_v2_grad-638-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642(model.blocks.2.ffn-sigmoid_v2_grad-638-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-add_n-152/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight-out-cast_f2h(model.blocks.2.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul-160-out_0-cast_h2f(model.blocks.2.ffn.receptance-broadcast_matmul-160/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul-160-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.receptance.weight_optimizer(model.blocks.2.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.receptance-broadcast_matmul_grad_b-642_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.2.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.2.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.2.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.2.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-158(model.blocks.2.ffn-square-156-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-158/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-639(model.blocks.2.ffn-broadcast_mul-637/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-639/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul_grad_b-640(model.blocks.2.ffn-broadcast_mul-637/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-square-156-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul_grad_b-640/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight-out-cast_f2h(model.blocks.2.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-158-out_0-cast_h2f(model.blocks.2.ffn.value-broadcast_matmul-158/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-158-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul-639-out_0-cast_h2f(model.blocks.2.ffn.value-broadcast_matmul-639/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul-639-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value-broadcast_matmul_grad_b-640_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn.value-broadcast_matmul_grad_b-640/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn.value-broadcast_matmul_grad_b-640_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.2.ffn.value.weight_optimizer(model.blocks.2.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.2.ffn.value-broadcast_matmul_grad_b-640_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.2.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.2.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.2.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-143(model.blocks.2.ln2-layer_norm-141/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-143/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-144(model.blocks.2.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_mul-144/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-145(model.blocks.2.ffn-scalar_mul-144/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-145/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-146(model.blocks.2.ffn.time_shift-pad-142/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-145-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-146/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-147([model.blocks.2.ffn-broadcast_mul-143/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-146/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-147/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-148(model.blocks.2.ln2-layer_norm-141/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-148/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-149(model.blocks.2.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_mul-149/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-150(model.blocks.2.ffn-scalar_mul-149/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-150/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-151(model.blocks.2.ffn.time_shift-pad-142/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-150-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-151/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-152([model.blocks.2.ffn-broadcast_mul-148/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-151/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-152/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-relu-155(model.blocks.2.ffn.key-broadcast_matmul-154/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-relu-155/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square-156(model.blocks.2.ffn-relu-155-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square-156/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2-161(model.blocks.2.ffn.receptance-broadcast_matmul-160-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2-161/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-162(model.blocks.2.ffn-sigmoid_v2-161-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.value-broadcast_matmul-158/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-162/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-636(model.blocks.3.ln1-add_n-633-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.value-broadcast_matmul-158-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-broadcast_mul-636/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-637(model.blocks.3.ln1-layer_norm_grad-632/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-sigmoid_v2-161-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-637/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2_grad-638(model.blocks.2.ffn.receptance-broadcast_matmul-160-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn-broadcast_mul-636/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2_grad-638/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square_grad-643(model.blocks.2.ffn-relu-155-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.2.ffn.value-broadcast_matmul-639-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square_grad-643/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-relu_grad-644(model.blocks.2.ffn-square_grad-643-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-relu-155/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-relu_grad-644/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-645(model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-645/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-646(model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-141/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-646/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-647(model.blocks.2.ffn-broadcast_mul-646/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-647/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-648(model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-150-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-648/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-649(model.blocks.2.ffn.receptance-broadcast_matmul-641/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_shift-pad-142/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-649/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-650(model.blocks.2.ffn-broadcast_mul-649/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-150-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-650/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-653(model.blocks.2.ffn-reduce_sum_like-650/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-scalar_mul-653/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-654([model.blocks.2.ffn-scalar_mul-653/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-reduce_sum_like-647/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-654/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-655(model.blocks.2.ffn.key-broadcast_matmul-651/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-655/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-656(model.blocks.2.ffn.key-broadcast_matmul-651/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ln2-layer_norm-141/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-656/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-657(model.blocks.2.ffn-broadcast_mul-656/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-657/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-658([model.blocks.2.ffn-broadcast_mul-655/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-645/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-658/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-659(model.blocks.2.ffn.key-broadcast_matmul-651/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-145-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-659/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-broadcast_mul-660(model.blocks.2.ffn.key-broadcast_matmul-651/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn.time_shift-pad-142/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-broadcast_mul-660/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-reduce_sum_like-661(model.blocks.2.ffn-broadcast_mul-660/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-scalar_add-145-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-reduce_sum_like-661/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-662([model.blocks.2.ffn-broadcast_mul-659/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-648/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-662/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_mul-670(model.blocks.2.ffn-reduce_sum_like-661/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-scalar_mul-670/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-671([model.blocks.2.ffn-scalar_mul-670/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-reduce_sum_like-657/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2.ffn-add_n-671/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r-out-cast_f2h(model.blocks.2.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k-out-cast_f2h(model.blocks.2.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-145-out_0-cast_f2h(model.blocks.2.ffn-scalar_add-145/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-145-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2_grad-638-dx_0-cast_f2h(model.blocks.2.ffn-sigmoid_v2_grad-638/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2_grad-638-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-scalar_add-150-out_0-cast_f2h(model.blocks.2.ffn-scalar_add-150/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-scalar_add-150-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-sigmoid_v2-161-y_0-cast_f2h(model.blocks.2.ffn-sigmoid_v2-161/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-sigmoid_v2-161-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square-156-y_0-cast_f2h(model.blocks.2.ffn-square-156/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square-156-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-square_grad-643-dx_0-cast_f2h(model.blocks.2.ffn-square_grad-643/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.2.ffn-square_grad-643-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-relu-155-y_0-cast_h2f(model.blocks.2.ffn-relu-155/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-relu-155-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-671_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn-add_n-671/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-add_n-671_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn-add_n-654_out_0_pinned_identity-out_0-cast_h2f(model.blocks.2.ffn-add_n-654/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2.ffn-add_n-654_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_k_optimizer(model.blocks.2.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn-add_n-671_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.2.ffn.time_mix_r_optimizer(model.blocks.2.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn-add_n-654_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.2.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.2.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.2-add_n-163([model.blocks.2.att.output-broadcast_matmul-139/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2.ffn-broadcast_mul-162/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.2-add_n-163/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.2-add_n-163-out_0-cast_h2f(model.blocks.2-add_n-163/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2-add_n-163-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.2-add_n-140-out_0-cast_h2f(model.blocks.2.att.output-broadcast_matmul-139/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.2-add_n-140-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.3:Block()): (
        (INPUT:_model.blocks.3_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.3.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.3.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.3.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.3.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-layer_norm-164(model.blocks.2-add_n-163/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-164/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-164/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-layer_norm_param_grad-631(model.blocks.3.att.time_shift-add_n-624-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.2-add_n-163-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-164/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-164/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln1-layer_norm_param_grad-631/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm_param_grad-631/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-layer_norm_grad-632(model.blocks.3.att.time_shift-add_n-624/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.2-add_n-163/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-164/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm-164/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln1-layer_norm_grad-632/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight-out-cast_f2h(model.blocks.3.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias-out-cast_f2h(model.blocks.3.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1-add_n-633-out_0-cast_h2f(model.blocks.3.ln1-layer_norm_grad-632/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln1-add_n-633-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.weight_optimizer(model.blocks.3.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm_param_grad-631/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln1.bias_optimizer(model.blocks.3.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1-layer_norm_param_grad-631/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.3.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.3.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.3.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.3.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2-layer_norm-195(model.blocks.3.att.output-broadcast_matmul-193/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln2-layer_norm-195/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-195/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-195/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2-layer_norm_param_grad-582(model.blocks.3.ffn.time_shift-add_n-579-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3-add_n-194-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-195/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-195/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln2-layer_norm_param_grad-582/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm_param_grad-582/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2-layer_norm_grad-583(model.blocks.3.ffn.time_shift-add_n-579/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.output-broadcast_matmul-193/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-195/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm-195/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ln2-layer_norm_grad-583/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias-out-cast_f2h(model.blocks.3.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight-out-cast_f2h(model.blocks.3.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.weight_optimizer(model.blocks.3.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm_param_grad-582/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ln2.bias_optimizer(model.blocks.3.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2-layer_norm_param_grad-582/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.3.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.3.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.3.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.3.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.3.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.3.att.time_shift-pad-165(model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.time_shift-pad-623(model.blocks.3.att-add_n-622/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.time_shift-pad-623/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.time_shift-add_n-624([model.blocks.3.att.time_shift-pad-623/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-add_n-618/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att.time_shift-add_n-624/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.time_shift-add_n-624-out_0-cast_h2f(model.blocks.3.att.time_shift-add_n-624/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.time_shift-add_n-624-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.3.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key-broadcast_matmul-182(model.blocks.3.att-add_n-170/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.key-broadcast_matmul-182/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key-broadcast_matmul-597(model.blocks.3.att-wkv_grad-592-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.key-broadcast_matmul-597/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key-broadcast_matmul_grad_b-598(model.blocks.3.att-wkv_grad-592-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-add_n-170/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.key-broadcast_matmul_grad_b-598/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key.weight-out-cast_f2h(model.blocks.3.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key-broadcast_matmul-182-out_0-cast_h2f(model.blocks.3.att.key-broadcast_matmul-182/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.key-broadcast_matmul-182-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key-broadcast_matmul_grad_b-598_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att.key-broadcast_matmul_grad_b-598/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.key-broadcast_matmul_grad_b-598_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.key.weight_optimizer(model.blocks.3.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.key-broadcast_matmul_grad_b-598_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.key.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.key.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value-broadcast_matmul-184(model.blocks.3.att-add_n-175/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.value-broadcast_matmul-184/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value-broadcast_matmul-599(model.blocks.3.att-wkv_grad-592-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.value-broadcast_matmul-599/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value-broadcast_matmul_grad_b-600(model.blocks.3.att-wkv_grad-592-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-add_n-175/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.value-broadcast_matmul_grad_b-600/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value.weight-out-cast_f2h(model.blocks.3.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value-broadcast_matmul-184-out_0-cast_h2f(model.blocks.3.att.value-broadcast_matmul-184/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.value-broadcast_matmul-184-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value-broadcast_matmul_grad_b-600_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att.value-broadcast_matmul_grad_b-600/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.value-broadcast_matmul_grad_b-600_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.value.weight_optimizer(model.blocks.3.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.value-broadcast_matmul_grad_b-600_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.value.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.value.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul-186(model.blocks.3.att-add_n-180/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul-186/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul-595(model.blocks.3.att-sigmoid_v2_grad-591-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul-595/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul_grad_b-596(model.blocks.3.att-sigmoid_v2_grad-591-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-add_n-180/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul_grad_b-596/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight-out-cast_f2h(model.blocks.3.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul-186-out_0-cast_h2f(model.blocks.3.att.receptance-broadcast_matmul-186/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul-186-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance-broadcast_matmul_grad_b-596_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att.receptance-broadcast_matmul_grad_b-596/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.receptance-broadcast_matmul_grad_b-596_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.receptance.weight_optimizer(model.blocks.3.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.receptance-broadcast_matmul_grad_b-596_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.3.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
            (PARAMETER:model.blocks.3.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul-193(model.blocks.3.att-broadcast_mul-191-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul-193/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul-587(model.blocks.3.ln2-layer_norm_grad-583/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul-587/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul_grad_b-588(model.blocks.3.ln2-layer_norm_grad-583/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-191-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul_grad_b-588/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight-out-cast_f2h(model.blocks.3.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul-587-out_0-cast_h2f(model.blocks.3.att.output-broadcast_matmul-587/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul-587-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output-broadcast_matmul_grad_b-588_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att.output-broadcast_matmul_grad_b-588/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att.output-broadcast_matmul_grad_b-588_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.att.output.weight_optimizer(model.blocks.3.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.output-broadcast_matmul_grad_b-588_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.3.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-166(model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-166/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-167(model.blocks.3.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-167/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-168(model.blocks.3.att-scalar_mul-167/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-168/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-169(model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-168-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-169/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-170([model.blocks.3.att-broadcast_mul-166/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-169/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-170/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-171(model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-171/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-172(model.blocks.3.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-172/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-173(model.blocks.3.att-scalar_mul-172/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-173/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-174(model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-173-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-174/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-175([model.blocks.3.att-broadcast_mul-171/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-174/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-175/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-176(model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-176/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-177(model.blocks.3.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-177/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-178(model.blocks.3.att-scalar_mul-177/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-178/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-179(model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-178-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-179/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-180([model.blocks.3.att-broadcast_mul-176/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-179/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-180/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2-187(model.blocks.3.att.receptance-broadcast_matmul-186-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2-187/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_decay() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_first() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-wkv-188(model.blocks.3.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att.key-broadcast_matmul-182-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.value-broadcast_matmul-184-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-wkv-188/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-exp-189(model.blocks.3.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-exp-189/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-190(model.blocks.3.att-exp-189/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_mul-190/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-191(model.blocks.3.att-sigmoid_v2-187/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att-wkv-188/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-broadcast_mul-191/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-589(model.blocks.3.att.output-broadcast_matmul-587-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att-wkv-188/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-broadcast_mul-589/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-590(model.blocks.3.att.output-broadcast_matmul-587-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att-sigmoid_v2-187/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-broadcast_mul-590/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2_grad-591(model.blocks.3.att.receptance-broadcast_matmul-186-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att-broadcast_mul-589/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2_grad-591/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-wkv_grad-592(model.blocks.3.att-scalar_mul-190/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att.key-broadcast_matmul-182-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att.value-broadcast_matmul-184-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att-broadcast_mul-590/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-wkv_grad-592/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.att-wkv_grad-592/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.3.att-wkv_grad-592/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.att-wkv_grad-592/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum-593(model.blocks.3.att-wkv_grad-592/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-reduce_sum-593/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum-594(model.blocks.3.att-wkv_grad-592/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-reduce_sum-594/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-601(model.blocks.3.att.receptance-broadcast_matmul-595/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-601/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-602(model.blocks.3.att.receptance-broadcast_matmul-595/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-602/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-603(model.blocks.3.att-broadcast_mul-602/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-603/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-604(model.blocks.3.att.receptance-broadcast_matmul-595/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-178-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-604/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-605(model.blocks.3.att.receptance-broadcast_matmul-595/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-605/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-606(model.blocks.3.att-broadcast_mul-605/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-178-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-606/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-607(model.blocks.3.att.key-broadcast_matmul-597/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-607/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-608(model.blocks.3.att.key-broadcast_matmul-597/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-608/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-609(model.blocks.3.att-broadcast_mul-608/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-609/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-611(model.blocks.3.att.key-broadcast_matmul-597/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-168-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-611/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-612(model.blocks.3.att.key-broadcast_matmul-597/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-612/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-613(model.blocks.3.att-broadcast_mul-612/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-168-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-613/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-615(model.blocks.3.att.value-broadcast_matmul-599/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-615/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-616(model.blocks.3.att.value-broadcast_matmul-599/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln1-layer_norm-164/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-616/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-617(model.blocks.3.att-broadcast_mul-616/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-617/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-618([model.blocks.3.att-broadcast_mul-615/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-607/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-601/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-618/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-619(model.blocks.3.att.value-broadcast_matmul-599/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-173-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-619/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-620(model.blocks.3.att.value-broadcast_matmul-599/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att.time_shift-pad-165/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-broadcast_mul-620/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-reduce_sum_like-621(model.blocks.3.att-broadcast_mul-620/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-scalar_add-173-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-reduce_sum_like-621/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-622([model.blocks.3.att-broadcast_mul-619/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-611/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-broadcast_mul-604/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-622/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-625(model.blocks.3.att-reduce_sum_like-606/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-scalar_mul-625/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-626([model.blocks.3.att-scalar_mul-625/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-reduce_sum_like-603/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-626/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-627(model.blocks.3.att-reduce_sum_like-613/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-scalar_mul-627/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-628([model.blocks.3.att-scalar_mul-627/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-reduce_sum_like-609/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-628/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_mul-634(model.blocks.3.att-reduce_sum_like-621/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-scalar_mul-634/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-635([model.blocks.3.att-scalar_mul-634/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.att-reduce_sum_like-617/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.att-add_n-635/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-sigmoid_v2_grad-591-dx_0-cast_f2h(model.blocks.3.att-sigmoid_v2_grad-591/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-sigmoid_v2_grad-591-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-wkv_grad-592-gv_0-cast_f2h(model.blocks.3.att-wkv_grad-592/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-wkv_grad-592-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-wkv_grad-592-gk_0-cast_f2h(model.blocks.3.att-wkv_grad-592/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-wkv_grad-592-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-broadcast_mul-191-z_0-cast_f2h(model.blocks.3.att-broadcast_mul-191/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-broadcast_mul-191-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-168-out_0-cast_f2h(model.blocks.3.att-scalar_add-168/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-168-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_k-out-cast_f2h(model.blocks.3.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v-out-cast_f2h(model.blocks.3.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-178-out_0-cast_f2h(model.blocks.3.att-scalar_add-178/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-178-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-scalar_add-173-out_0-cast_f2h(model.blocks.3.att-scalar_add-173/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att-scalar_add-173-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r-out-cast_f2h(model.blocks.3.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-626_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att-add_n-626/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-add_n-626_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-628_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att-add_n-628/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-add_n-628_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att-add_n-635_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.att-add_n-635/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.att-add_n-635_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_k_optimizer(model.blocks.3.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att-add_n-628_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_v_optimizer(model.blocks.3.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att-add_n-635_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_v-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_v-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_mix_r_optimizer(model.blocks.3.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att-add_n-626_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_decay-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_decay-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_decay_optimizer(model.blocks.3.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att-reduce_sum-593/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.time_decay-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att.time_decay-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_first-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_first-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.att.time_first_optimizer(model.blocks.3.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att-reduce_sum-594/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.att.time_first-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.3.att.time_first-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.3.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.3.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.3.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.3.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.3.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.3.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.3.ffn.time_shift-pad-196(model.blocks.3.ln2-layer_norm-195/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.time_shift-pad-196/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.time_shift-pad-578(model.blocks.3.ffn-add_n-577/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.time_shift-pad-578/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.time_shift-add_n-579([model.blocks.3.ffn.time_shift-pad-578/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-add_n-573/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn.time_shift-add_n-579/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.time_shift-add_n-579-out_0-cast_h2f(model.blocks.3.ffn.time_shift-add_n-579/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.time_shift-add_n-579-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.3.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul-208(model.blocks.3.ffn-add_n-201/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul-208/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul-566(model.blocks.3.ffn-relu_grad-559/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul-566/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul_grad_b-567(model.blocks.3.ffn-relu_grad-559/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-add_n-201/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul_grad_b-567/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight-out-cast_f2h(model.blocks.3.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key-broadcast_matmul_grad_b-567_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn.key-broadcast_matmul_grad_b-567/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.key-broadcast_matmul_grad_b-567_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.key.weight_optimizer(model.blocks.3.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.key-broadcast_matmul_grad_b-567_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.3.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.3.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul-214(model.blocks.3.ffn-add_n-206/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul-214/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul-556(model.blocks.3.ffn-sigmoid_v2_grad-553-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557(model.blocks.3.ffn-sigmoid_v2_grad-553-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-add_n-206/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight-out-cast_f2h(model.blocks.3.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul-214-out_0-cast_h2f(model.blocks.3.ffn.receptance-broadcast_matmul-214/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul-214-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.receptance.weight_optimizer(model.blocks.3.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.receptance-broadcast_matmul_grad_b-557_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.3.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.3.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.3.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.3.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-212(model.blocks.3.ffn-square-210-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-212/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-554(model.blocks.3.ffn-broadcast_mul-552/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-554/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul_grad_b-555(model.blocks.3.ffn-broadcast_mul-552/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-square-210-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul_grad_b-555/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight-out-cast_f2h(model.blocks.3.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-554-out_0-cast_h2f(model.blocks.3.ffn.value-broadcast_matmul-554/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-554-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul-212-out_0-cast_h2f(model.blocks.3.ffn.value-broadcast_matmul-212/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul-212-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value-broadcast_matmul_grad_b-555_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn.value-broadcast_matmul_grad_b-555/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn.value-broadcast_matmul_grad_b-555_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.3.ffn.value.weight_optimizer(model.blocks.3.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.3.ffn.value-broadcast_matmul_grad_b-555_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.3.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.3.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.3.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-197(model.blocks.3.ln2-layer_norm-195/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-197/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-198(model.blocks.3.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_mul-198/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-199(model.blocks.3.ffn-scalar_mul-198/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-199/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-200(model.blocks.3.ffn.time_shift-pad-196/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-199-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-200/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-201([model.blocks.3.ffn-broadcast_mul-197/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-200/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-201/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-202(model.blocks.3.ln2-layer_norm-195/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-202/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-203(model.blocks.3.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_mul-203/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-204(model.blocks.3.ffn-scalar_mul-203/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-204/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-205(model.blocks.3.ffn.time_shift-pad-196/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-204-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-205/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-206([model.blocks.3.ffn-broadcast_mul-202/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-205/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-206/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-relu-209(model.blocks.3.ffn.key-broadcast_matmul-208/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-relu-209/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square-210(model.blocks.3.ffn-relu-209-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square-210/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2-215(model.blocks.3.ffn.receptance-broadcast_matmul-214-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2-215/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-216(model.blocks.3.ffn-sigmoid_v2-215-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.value-broadcast_matmul-212/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-216/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-551(model.blocks.4.ln1-add_n-548-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.value-broadcast_matmul-212-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-broadcast_mul-551/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-552(model.blocks.4.ln1-layer_norm_grad-547/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-sigmoid_v2-215-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-552/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2_grad-553(model.blocks.3.ffn.receptance-broadcast_matmul-214-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn-broadcast_mul-551/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2_grad-553/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square_grad-558(model.blocks.3.ffn-relu-209-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.3.ffn.value-broadcast_matmul-554-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square_grad-558/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-relu_grad-559(model.blocks.3.ffn-square_grad-558-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-relu-209/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-relu_grad-559/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-560(model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-560/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-561(model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-195/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-561/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-562(model.blocks.3.ffn-broadcast_mul-561/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-562/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-563(model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-204-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-563/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-564(model.blocks.3.ffn.receptance-broadcast_matmul-556/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_shift-pad-196/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-564/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-565(model.blocks.3.ffn-broadcast_mul-564/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-204-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-565/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-568(model.blocks.3.ffn-reduce_sum_like-565/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-scalar_mul-568/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-569([model.blocks.3.ffn-scalar_mul-568/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-reduce_sum_like-562/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-569/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-570(model.blocks.3.ffn.key-broadcast_matmul-566/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-570/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-571(model.blocks.3.ffn.key-broadcast_matmul-566/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ln2-layer_norm-195/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-571/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-572(model.blocks.3.ffn-broadcast_mul-571/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-572/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-573([model.blocks.3.ffn-broadcast_mul-570/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-560/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-573/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-574(model.blocks.3.ffn.key-broadcast_matmul-566/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-199-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-574/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-broadcast_mul-575(model.blocks.3.ffn.key-broadcast_matmul-566/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn.time_shift-pad-196/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-broadcast_mul-575/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-reduce_sum_like-576(model.blocks.3.ffn-broadcast_mul-575/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-scalar_add-199-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-reduce_sum_like-576/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-577([model.blocks.3.ffn-broadcast_mul-574/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-563/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-577/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_mul-585(model.blocks.3.ffn-reduce_sum_like-576/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-scalar_mul-585/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-586([model.blocks.3.ffn-scalar_mul-585/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-reduce_sum_like-572/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3.ffn-add_n-586/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-204-out_0-cast_f2h(model.blocks.3.ffn-scalar_add-204/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-204-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square-210-y_0-cast_f2h(model.blocks.3.ffn-square-210/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square-210-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r-out-cast_f2h(model.blocks.3.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-square_grad-558-dx_0-cast_f2h(model.blocks.3.ffn-square_grad-558/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-square_grad-558-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-scalar_add-199-out_0-cast_f2h(model.blocks.3.ffn-scalar_add-199/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-scalar_add-199-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2_grad-553-dx_0-cast_f2h(model.blocks.3.ffn-sigmoid_v2_grad-553/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2_grad-553-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k-out-cast_f2h(model.blocks.3.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-sigmoid_v2-215-y_0-cast_f2h(model.blocks.3.ffn-sigmoid_v2-215/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.3.ffn-sigmoid_v2-215-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-relu-209-y_0-cast_h2f(model.blocks.3.ffn-relu-209/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-relu-209-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-586_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn-add_n-586/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-add_n-586_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn-add_n-569_out_0_pinned_identity-out_0-cast_h2f(model.blocks.3.ffn-add_n-569/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3.ffn-add_n-569_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_k_optimizer(model.blocks.3.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn-add_n-586_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.3.ffn.time_mix_r_optimizer(model.blocks.3.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn-add_n-569_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.3.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.3.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.3-add_n-217([model.blocks.3.att.output-broadcast_matmul-193/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3.ffn-broadcast_mul-216/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.3-add_n-217/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.3-add_n-217-out_0-cast_h2f(model.blocks.3-add_n-217/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3-add_n-217-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.3-add_n-194-out_0-cast_h2f(model.blocks.3.att.output-broadcast_matmul-193/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.3-add_n-194-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.3_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.4:Block()): (
        (INPUT:_model.blocks.4_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.4.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.4.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.4.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.4.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-layer_norm-218(model.blocks.3-add_n-217/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-218/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-218/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-layer_norm_param_grad-546(model.blocks.4.att.time_shift-add_n-539-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.3-add_n-217-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-218/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-218/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln1-layer_norm_param_grad-546/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm_param_grad-546/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-layer_norm_grad-547(model.blocks.4.att.time_shift-add_n-539/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.3-add_n-217/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-218/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm-218/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln1-layer_norm_grad-547/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight-out-cast_f2h(model.blocks.4.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias-out-cast_f2h(model.blocks.4.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1-add_n-548-out_0-cast_h2f(model.blocks.4.ln1-layer_norm_grad-547/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln1-add_n-548-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.weight_optimizer(model.blocks.4.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm_param_grad-546/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln1.bias_optimizer(model.blocks.4.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1-layer_norm_param_grad-546/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.4.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.4.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.4.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.4.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2-layer_norm-249(model.blocks.4.att.output-broadcast_matmul-247/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln2-layer_norm-249/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-249/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-249/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2-layer_norm_param_grad-497(model.blocks.4.ffn.time_shift-add_n-494-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4-add_n-248-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-249/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-249/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln2-layer_norm_param_grad-497/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm_param_grad-497/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2-layer_norm_grad-498(model.blocks.4.ffn.time_shift-add_n-494/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.output-broadcast_matmul-247/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-249/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm-249/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ln2-layer_norm_grad-498/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias-out-cast_f2h(model.blocks.4.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight-out-cast_f2h(model.blocks.4.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.weight_optimizer(model.blocks.4.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm_param_grad-497/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ln2.bias_optimizer(model.blocks.4.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2-layer_norm_param_grad-497/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.4.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.4.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.4.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.4.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.4.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.4.att.time_shift-pad-219(model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.time_shift-pad-538(model.blocks.4.att-add_n-537/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.time_shift-pad-538/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.time_shift-add_n-539([model.blocks.4.att.time_shift-pad-538/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-add_n-533/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att.time_shift-add_n-539/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.time_shift-add_n-539-out_0-cast_h2f(model.blocks.4.att.time_shift-add_n-539/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.time_shift-add_n-539-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.4.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key-broadcast_matmul-236(model.blocks.4.att-add_n-224/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.key-broadcast_matmul-236/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key-broadcast_matmul-512(model.blocks.4.att-wkv_grad-507-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.key-broadcast_matmul-512/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key-broadcast_matmul_grad_b-513(model.blocks.4.att-wkv_grad-507-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-add_n-224/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.key-broadcast_matmul_grad_b-513/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key.weight-out-cast_f2h(model.blocks.4.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key-broadcast_matmul-236-out_0-cast_h2f(model.blocks.4.att.key-broadcast_matmul-236/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.key-broadcast_matmul-236-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key-broadcast_matmul_grad_b-513_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att.key-broadcast_matmul_grad_b-513/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.key-broadcast_matmul_grad_b-513_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.key.weight_optimizer(model.blocks.4.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.key-broadcast_matmul_grad_b-513_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.key.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.key.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value-broadcast_matmul-238(model.blocks.4.att-add_n-229/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.value-broadcast_matmul-238/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value-broadcast_matmul-514(model.blocks.4.att-wkv_grad-507-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.value-broadcast_matmul-514/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value-broadcast_matmul_grad_b-515(model.blocks.4.att-wkv_grad-507-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-add_n-229/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.value-broadcast_matmul_grad_b-515/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value.weight-out-cast_f2h(model.blocks.4.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value-broadcast_matmul-238-out_0-cast_h2f(model.blocks.4.att.value-broadcast_matmul-238/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.value-broadcast_matmul-238-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value-broadcast_matmul_grad_b-515_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att.value-broadcast_matmul_grad_b-515/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.value-broadcast_matmul_grad_b-515_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.value.weight_optimizer(model.blocks.4.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.value-broadcast_matmul_grad_b-515_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.value.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.value.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul-240(model.blocks.4.att-add_n-234/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul-240/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul-510(model.blocks.4.att-sigmoid_v2_grad-506-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul-510/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul_grad_b-511(model.blocks.4.att-sigmoid_v2_grad-506-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-add_n-234/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul_grad_b-511/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight-out-cast_f2h(model.blocks.4.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul-240-out_0-cast_h2f(model.blocks.4.att.receptance-broadcast_matmul-240/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul-240-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance-broadcast_matmul_grad_b-511_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att.receptance-broadcast_matmul_grad_b-511/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.receptance-broadcast_matmul_grad_b-511_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.receptance.weight_optimizer(model.blocks.4.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.receptance-broadcast_matmul_grad_b-511_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.4.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
            (PARAMETER:model.blocks.4.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul-247(model.blocks.4.att-broadcast_mul-245-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul-247/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul-502(model.blocks.4.ln2-layer_norm_grad-498/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul-502/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul_grad_b-503(model.blocks.4.ln2-layer_norm_grad-498/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-245-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul_grad_b-503/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight-out-cast_f2h(model.blocks.4.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul-502-out_0-cast_h2f(model.blocks.4.att.output-broadcast_matmul-502/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul-502-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output-broadcast_matmul_grad_b-503_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att.output-broadcast_matmul_grad_b-503/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att.output-broadcast_matmul_grad_b-503_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.att.output.weight_optimizer(model.blocks.4.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.output-broadcast_matmul_grad_b-503_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.4.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-220(model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-220/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-221(model.blocks.4.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-221/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-222(model.blocks.4.att-scalar_mul-221/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-222/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-223(model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-222-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-223/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-224([model.blocks.4.att-broadcast_mul-220/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-223/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-224/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-225(model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-225/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-226(model.blocks.4.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-226/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-227(model.blocks.4.att-scalar_mul-226/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-227/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-228(model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-227-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-228/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-229([model.blocks.4.att-broadcast_mul-225/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-228/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-229/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-230(model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-230/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-231(model.blocks.4.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-231/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-232(model.blocks.4.att-scalar_mul-231/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-232/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-233(model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-232-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-233/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-234([model.blocks.4.att-broadcast_mul-230/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-233/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-234/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2-241(model.blocks.4.att.receptance-broadcast_matmul-240-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2-241/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_decay() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_first() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-wkv-242(model.blocks.4.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att.key-broadcast_matmul-236-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.value-broadcast_matmul-238-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-wkv-242/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-exp-243(model.blocks.4.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-exp-243/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-244(model.blocks.4.att-exp-243/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_mul-244/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-245(model.blocks.4.att-sigmoid_v2-241/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att-wkv-242/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-broadcast_mul-245/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-504(model.blocks.4.att.output-broadcast_matmul-502-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att-wkv-242/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-broadcast_mul-504/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-505(model.blocks.4.att.output-broadcast_matmul-502-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att-sigmoid_v2-241/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-broadcast_mul-505/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2_grad-506(model.blocks.4.att.receptance-broadcast_matmul-240-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att-broadcast_mul-504/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2_grad-506/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-wkv_grad-507(model.blocks.4.att-scalar_mul-244/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att.key-broadcast_matmul-236-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att.value-broadcast_matmul-238-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att-broadcast_mul-505/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-wkv_grad-507/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.att-wkv_grad-507/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.4.att-wkv_grad-507/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.att-wkv_grad-507/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum-508(model.blocks.4.att-wkv_grad-507/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-reduce_sum-508/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum-509(model.blocks.4.att-wkv_grad-507/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-reduce_sum-509/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-516(model.blocks.4.att.receptance-broadcast_matmul-510/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-516/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-517(model.blocks.4.att.receptance-broadcast_matmul-510/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-517/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-518(model.blocks.4.att-broadcast_mul-517/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-518/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-519(model.blocks.4.att.receptance-broadcast_matmul-510/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-232-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-519/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-520(model.blocks.4.att.receptance-broadcast_matmul-510/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-520/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-521(model.blocks.4.att-broadcast_mul-520/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-232-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-521/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-522(model.blocks.4.att.key-broadcast_matmul-512/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-522/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-523(model.blocks.4.att.key-broadcast_matmul-512/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-523/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-524(model.blocks.4.att-broadcast_mul-523/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-524/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-526(model.blocks.4.att.key-broadcast_matmul-512/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-222-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-526/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-527(model.blocks.4.att.key-broadcast_matmul-512/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-527/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-528(model.blocks.4.att-broadcast_mul-527/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-222-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-528/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-530(model.blocks.4.att.value-broadcast_matmul-514/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-530/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-531(model.blocks.4.att.value-broadcast_matmul-514/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln1-layer_norm-218/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-531/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-532(model.blocks.4.att-broadcast_mul-531/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-532/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-533([model.blocks.4.att-broadcast_mul-530/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-522/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-516/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-533/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-534(model.blocks.4.att.value-broadcast_matmul-514/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-227-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-534/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-535(model.blocks.4.att.value-broadcast_matmul-514/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att.time_shift-pad-219/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-broadcast_mul-535/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-reduce_sum_like-536(model.blocks.4.att-broadcast_mul-535/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-scalar_add-227-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-reduce_sum_like-536/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-537([model.blocks.4.att-broadcast_mul-534/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-526/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-broadcast_mul-519/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-537/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-540(model.blocks.4.att-reduce_sum_like-521/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-scalar_mul-540/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-541([model.blocks.4.att-scalar_mul-540/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-reduce_sum_like-518/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-541/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-542(model.blocks.4.att-reduce_sum_like-528/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-scalar_mul-542/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-543([model.blocks.4.att-scalar_mul-542/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-reduce_sum_like-524/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-543/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_mul-549(model.blocks.4.att-reduce_sum_like-536/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-scalar_mul-549/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-550([model.blocks.4.att-scalar_mul-549/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.att-reduce_sum_like-532/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.att-add_n-550/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-wkv_grad-507-gv_0-cast_f2h(model.blocks.4.att-wkv_grad-507/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-wkv_grad-507-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-wkv_grad-507-gk_0-cast_f2h(model.blocks.4.att-wkv_grad-507/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-wkv_grad-507-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-sigmoid_v2_grad-506-dx_0-cast_f2h(model.blocks.4.att-sigmoid_v2_grad-506/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-sigmoid_v2_grad-506-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r-out-cast_f2h(model.blocks.4.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-227-out_0-cast_f2h(model.blocks.4.att-scalar_add-227/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-227-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-broadcast_mul-245-z_0-cast_f2h(model.blocks.4.att-broadcast_mul-245/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-broadcast_mul-245-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-222-out_0-cast_f2h(model.blocks.4.att-scalar_add-222/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-222-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-scalar_add-232-out_0-cast_f2h(model.blocks.4.att-scalar_add-232/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att-scalar_add-232-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v-out-cast_f2h(model.blocks.4.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_k-out-cast_f2h(model.blocks.4.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-550_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att-add_n-550/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-add_n-550_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-541_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att-add_n-541/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-add_n-541_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att-add_n-543_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.att-add_n-543/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.att-add_n-543_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_k_optimizer(model.blocks.4.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att-add_n-543_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_v_optimizer(model.blocks.4.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att-add_n-550_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_v-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_v-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_mix_r_optimizer(model.blocks.4.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att-add_n-541_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_decay-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_decay-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_decay_optimizer(model.blocks.4.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att-reduce_sum-508/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.time_decay-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att.time_decay-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_first-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_first-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.att.time_first_optimizer(model.blocks.4.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att-reduce_sum-509/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.att.time_first-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.4.att.time_first-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.4.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.4.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.4.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.4.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.4.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.4.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.4.ffn.time_shift-pad-250(model.blocks.4.ln2-layer_norm-249/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.time_shift-pad-250/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.time_shift-pad-493(model.blocks.4.ffn-add_n-492/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.time_shift-pad-493/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.time_shift-add_n-494([model.blocks.4.ffn.time_shift-pad-493/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-add_n-488/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn.time_shift-add_n-494/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.time_shift-add_n-494-out_0-cast_h2f(model.blocks.4.ffn.time_shift-add_n-494/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.time_shift-add_n-494-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.4.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul-262(model.blocks.4.ffn-add_n-255/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul-262/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul-481(model.blocks.4.ffn-relu_grad-474/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul-481/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul_grad_b-482(model.blocks.4.ffn-relu_grad-474/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-add_n-255/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul_grad_b-482/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight-out-cast_f2h(model.blocks.4.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key-broadcast_matmul_grad_b-482_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn.key-broadcast_matmul_grad_b-482/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.key-broadcast_matmul_grad_b-482_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.key.weight_optimizer(model.blocks.4.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.key-broadcast_matmul_grad_b-482_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.4.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.4.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul-268(model.blocks.4.ffn-add_n-260/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul-268/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul-471(model.blocks.4.ffn-sigmoid_v2_grad-468-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472(model.blocks.4.ffn-sigmoid_v2_grad-468-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-add_n-260/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight-out-cast_f2h(model.blocks.4.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul-268-out_0-cast_h2f(model.blocks.4.ffn.receptance-broadcast_matmul-268/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul-268-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.receptance.weight_optimizer(model.blocks.4.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.receptance-broadcast_matmul_grad_b-472_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.4.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.4.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.4.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.4.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-266(model.blocks.4.ffn-square-264-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-266/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-469(model.blocks.4.ffn-broadcast_mul-467/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-469/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul_grad_b-470(model.blocks.4.ffn-broadcast_mul-467/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-square-264-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul_grad_b-470/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight-out-cast_f2h(model.blocks.4.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-469-out_0-cast_h2f(model.blocks.4.ffn.value-broadcast_matmul-469/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-469-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul-266-out_0-cast_h2f(model.blocks.4.ffn.value-broadcast_matmul-266/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul-266-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value-broadcast_matmul_grad_b-470_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn.value-broadcast_matmul_grad_b-470/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn.value-broadcast_matmul_grad_b-470_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.4.ffn.value.weight_optimizer(model.blocks.4.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.4.ffn.value-broadcast_matmul_grad_b-470_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.4.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.4.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.4.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-251(model.blocks.4.ln2-layer_norm-249/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-251/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-252(model.blocks.4.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_mul-252/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-253(model.blocks.4.ffn-scalar_mul-252/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-253/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-254(model.blocks.4.ffn.time_shift-pad-250/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-253-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-254/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-255([model.blocks.4.ffn-broadcast_mul-251/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-254/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-255/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-256(model.blocks.4.ln2-layer_norm-249/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-256/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-257(model.blocks.4.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_mul-257/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-258(model.blocks.4.ffn-scalar_mul-257/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-258/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-259(model.blocks.4.ffn.time_shift-pad-250/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-258-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-259/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-260([model.blocks.4.ffn-broadcast_mul-256/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-259/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-260/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-relu-263(model.blocks.4.ffn.key-broadcast_matmul-262/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-relu-263/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square-264(model.blocks.4.ffn-relu-263-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square-264/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2-269(model.blocks.4.ffn.receptance-broadcast_matmul-268-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2-269/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-270(model.blocks.4.ffn-sigmoid_v2-269-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.value-broadcast_matmul-266/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-270/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-466(model.blocks.5.ln1-add_n-463-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.value-broadcast_matmul-266-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-broadcast_mul-466/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-467(model.blocks.5.ln1-layer_norm_grad-462/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-sigmoid_v2-269-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-467/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2_grad-468(model.blocks.4.ffn.receptance-broadcast_matmul-268-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn-broadcast_mul-466/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2_grad-468/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square_grad-473(model.blocks.4.ffn-relu-263-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.4.ffn.value-broadcast_matmul-469-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square_grad-473/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-relu_grad-474(model.blocks.4.ffn-square_grad-473-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-relu-263/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-relu_grad-474/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-475(model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-475/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-476(model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-249/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-476/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-477(model.blocks.4.ffn-broadcast_mul-476/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-477/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-478(model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-258-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-478/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-479(model.blocks.4.ffn.receptance-broadcast_matmul-471/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_shift-pad-250/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-479/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-480(model.blocks.4.ffn-broadcast_mul-479/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-258-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-480/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-483(model.blocks.4.ffn-reduce_sum_like-480/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-scalar_mul-483/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-484([model.blocks.4.ffn-scalar_mul-483/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-reduce_sum_like-477/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-484/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-485(model.blocks.4.ffn.key-broadcast_matmul-481/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-485/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-486(model.blocks.4.ffn.key-broadcast_matmul-481/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ln2-layer_norm-249/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-486/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-487(model.blocks.4.ffn-broadcast_mul-486/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-487/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-488([model.blocks.4.ffn-broadcast_mul-485/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-475/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-488/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-489(model.blocks.4.ffn.key-broadcast_matmul-481/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-253-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-489/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-broadcast_mul-490(model.blocks.4.ffn.key-broadcast_matmul-481/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn.time_shift-pad-250/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-broadcast_mul-490/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-reduce_sum_like-491(model.blocks.4.ffn-broadcast_mul-490/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-scalar_add-253-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-reduce_sum_like-491/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-492([model.blocks.4.ffn-broadcast_mul-489/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-478/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-492/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_mul-500(model.blocks.4.ffn-reduce_sum_like-491/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-scalar_mul-500/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-501([model.blocks.4.ffn-scalar_mul-500/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-reduce_sum_like-487/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4.ffn-add_n-501/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square_grad-473-dx_0-cast_f2h(model.blocks.4.ffn-square_grad-473/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square_grad-473-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k-out-cast_f2h(model.blocks.4.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r-out-cast_f2h(model.blocks.4.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2_grad-468-dx_0-cast_f2h(model.blocks.4.ffn-sigmoid_v2_grad-468/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2_grad-468-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-sigmoid_v2-269-y_0-cast_f2h(model.blocks.4.ffn-sigmoid_v2-269/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-sigmoid_v2-269-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-square-264-y_0-cast_f2h(model.blocks.4.ffn-square-264/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-square-264-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-253-out_0-cast_f2h(model.blocks.4.ffn-scalar_add-253/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-253-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-scalar_add-258-out_0-cast_f2h(model.blocks.4.ffn-scalar_add-258/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.4.ffn-scalar_add-258-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-relu-263-y_0-cast_h2f(model.blocks.4.ffn-relu-263/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-relu-263-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-484_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn-add_n-484/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-add_n-484_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn-add_n-501_out_0_pinned_identity-out_0-cast_h2f(model.blocks.4.ffn-add_n-501/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4.ffn-add_n-501_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_k_optimizer(model.blocks.4.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn-add_n-501_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.4.ffn.time_mix_r_optimizer(model.blocks.4.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn-add_n-484_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.4.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.4.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.4-add_n-271([model.blocks.4.att.output-broadcast_matmul-247/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4.ffn-broadcast_mul-270/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.4-add_n-271/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.4-add_n-271-out_0-cast_h2f(model.blocks.4-add_n-271/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4-add_n-271-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.4-add_n-248-out_0-cast_h2f(model.blocks.4.att.output-broadcast_matmul-247/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.4-add_n-248-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.4_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (MODULE:model.blocks.5:Block()): (
        (INPUT:_model.blocks.5_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
        (MODULE:model.blocks.5.ln1:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.5.ln1_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.5.ln1.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.ln1.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.5.ln1.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-layer_norm-272(model.blocks.4-add_n-271/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-272/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-272/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-layer_norm_param_grad-461(model.blocks.5.att.time_shift-add_n-454-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.4-add_n-271-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-272/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-272/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln1-layer_norm_param_grad-461/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm_param_grad-461/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-layer_norm_grad-462(model.blocks.5.att.time_shift-add_n-454/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.4-add_n-271/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-272/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm-272/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln1-layer_norm_grad-462/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight-out-cast_f2h(model.blocks.5.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln1.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias-out-cast_f2h(model.blocks.5.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln1.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1-add_n-463-out_0-cast_h2f(model.blocks.5.ln1-layer_norm_grad-462/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln1-add_n-463-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.weight_optimizer(model.blocks.5.ln1.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm_param_grad-461/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln1.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln1.bias_optimizer(model.blocks.5.ln1.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1-layer_norm_param_grad-461/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln1.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln1.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.ln1_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.5.ln2:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
          (INPUT:_model.blocks.5.ln2_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<add_n_backward>))
          (PARAMETER:model.blocks.5.ln2.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.ln2.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (OPERATOR: model.blocks.5.ln2.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2-layer_norm-303(model.blocks.5.att.output-broadcast_matmul-301/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln2-layer_norm-303/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-303/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-303/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2-layer_norm_param_grad-412(model.blocks.5.ffn.time_shift-add_n-409-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5-add_n-302-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-303/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-303/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln2-layer_norm_param_grad-412/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm_param_grad-412/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2-layer_norm_grad-413(model.blocks.5.ffn.time_shift-add_n-409/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.output-broadcast_matmul-301/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-303/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm-303/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ln2-layer_norm_grad-413/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias-out-cast_f2h(model.blocks.5.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln2.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight-out-cast_f2h(model.blocks.5.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.ln2.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.weight_optimizer(model.blocks.5.ln2.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm_param_grad-412/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln2.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ln2.bias_optimizer(model.blocks.5.ln2.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2-layer_norm_param_grad-412/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ln2.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.ln2.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.ln2_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
        )
        (MODULE:model.blocks.5.att:RWKV_TimeMix()): (
          (INPUT:_model.blocks.5.att_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.5.att.time_decay:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.att.time_first:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.att.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.att.time_mix_v:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.att.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.5.att.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.5.att.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.5.att.time_shift-pad-273(model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.time_shift-pad-453(model.blocks.5.att-add_n-452/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.time_shift-pad-453/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.time_shift-add_n-454([model.blocks.5.att.time_shift-pad-453/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-add_n-448/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att.time_shift-add_n-454/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.time_shift-add_n-454-out_0-cast_h2f(model.blocks.5.att.time_shift-add_n-454/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.time_shift-add_n-454-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.5.att.key:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.att.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.att.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.key.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key-broadcast_matmul-290(model.blocks.5.att-add_n-278/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.key-broadcast_matmul-290/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key-broadcast_matmul-427(model.blocks.5.att-wkv_grad-422-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.key-broadcast_matmul-427/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key-broadcast_matmul_grad_b-428(model.blocks.5.att-wkv_grad-422-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-add_n-278/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.key-broadcast_matmul_grad_b-428/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key.weight-out-cast_f2h(model.blocks.5.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key-broadcast_matmul-290-out_0-cast_h2f(model.blocks.5.att.key-broadcast_matmul-290/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.key-broadcast_matmul-290-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key-broadcast_matmul_grad_b-428_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att.key-broadcast_matmul_grad_b-428/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.key-broadcast_matmul_grad_b-428_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.key.weight_optimizer(model.blocks.5.att.key.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.key-broadcast_matmul_grad_b-428_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.key.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.key.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.att.value:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.att.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.att.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.value.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value-broadcast_matmul-292(model.blocks.5.att-add_n-283/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.value-broadcast_matmul-292/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value-broadcast_matmul-429(model.blocks.5.att-wkv_grad-422-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.value-broadcast_matmul-429/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value-broadcast_matmul_grad_b-430(model.blocks.5.att-wkv_grad-422-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-add_n-283/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.value-broadcast_matmul_grad_b-430/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value.weight-out-cast_f2h(model.blocks.5.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value-broadcast_matmul-292-out_0-cast_h2f(model.blocks.5.att.value-broadcast_matmul-292/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.value-broadcast_matmul-292-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value-broadcast_matmul_grad_b-430_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att.value-broadcast_matmul_grad_b-430/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.value-broadcast_matmul_grad_b-430_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.value.weight_optimizer(model.blocks.5.att.value.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.value-broadcast_matmul_grad_b-430_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.value.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.value.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.att.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.att.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.att.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul-294(model.blocks.5.att-add_n-288/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul-294/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul-425(model.blocks.5.att-sigmoid_v2_grad-421-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul-425/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul_grad_b-426(model.blocks.5.att-sigmoid_v2_grad-421-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-add_n-288/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul_grad_b-426/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight-out-cast_f2h(model.blocks.5.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul-294-out_0-cast_h2f(model.blocks.5.att.receptance-broadcast_matmul-294/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul-294-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance-broadcast_matmul_grad_b-426_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att.receptance-broadcast_matmul_grad_b-426/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.receptance-broadcast_matmul_grad_b-426_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.receptance.weight_optimizer(model.blocks.5.att.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.receptance-broadcast_matmul_grad_b-426_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.att.output:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.5.att.output_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
            (PARAMETER:model.blocks.5.att.output.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.att.output.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul-301(model.blocks.5.att-broadcast_mul-299-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul-301/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul-417(model.blocks.5.ln2-layer_norm_grad-413/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul-417/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul_grad_b-418(model.blocks.5.ln2-layer_norm_grad-413/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-299-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul_grad_b-418/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight-out-cast_f2h(model.blocks.5.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.output.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul-417-out_0-cast_h2f(model.blocks.5.att.output-broadcast_matmul-417/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul-417-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output-broadcast_matmul_grad_b-418_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att.output-broadcast_matmul_grad_b-418/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att.output-broadcast_matmul_grad_b-418_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.att.output.weight_optimizer(model.blocks.5.att.output.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.output-broadcast_matmul_grad_b-418_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.output.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.output.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.att.output_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.5.att.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-274(model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-274/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-275(model.blocks.5.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-275/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-276(model.blocks.5.att-scalar_mul-275/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-276/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-277(model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-276-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-277/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-278([model.blocks.5.att-broadcast_mul-274/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-277/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-278/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-279(model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-279/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-280(model.blocks.5.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-280/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-281(model.blocks.5.att-scalar_mul-280/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-281/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-282(model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-281-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-282/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-283([model.blocks.5.att-broadcast_mul-279/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-282/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-283/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-284(model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-284/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-285(model.blocks.5.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-285/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-286(model.blocks.5.att-scalar_mul-285/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-286/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-287(model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-286-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-287/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-288([model.blocks.5.att-broadcast_mul-284/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-287/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-288/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2-295(model.blocks.5.att.receptance-broadcast_matmul-294-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2-295/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_decay() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_first() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-wkv-296(model.blocks.5.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att.key-broadcast_matmul-290-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.value-broadcast_matmul-292-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-wkv-296/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-exp-297(model.blocks.5.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-exp-297/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-298(model.blocks.5.att-exp-297/y_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_mul-298/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-299(model.blocks.5.att-sigmoid_v2-295/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att-wkv-296/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-broadcast_mul-299/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-419(model.blocks.5.att.output-broadcast_matmul-417-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att-wkv-296/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-broadcast_mul-419/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-420(model.blocks.5.att.output-broadcast_matmul-417-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att-sigmoid_v2-295/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-broadcast_mul-420/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2_grad-421(model.blocks.5.att.receptance-broadcast_matmul-294-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att-broadcast_mul-419/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2_grad-421/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-wkv_grad-422(model.blocks.5.att-scalar_mul-298/out_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att.key-broadcast_matmul-290-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att.value-broadcast_matmul-292-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att-broadcast_mul-420/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-wkv_grad-422/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.att-wkv_grad-422/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.blocks.5.att-wkv_grad-422/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.att-wkv_grad-422/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum-423(model.blocks.5.att-wkv_grad-422/gw_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-reduce_sum-423/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum-424(model.blocks.5.att-wkv_grad-422/gu_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-reduce_sum-424/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-431(model.blocks.5.att.receptance-broadcast_matmul-425/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-431/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-432(model.blocks.5.att.receptance-broadcast_matmul-425/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-432/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-433(model.blocks.5.att-broadcast_mul-432/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-433/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-434(model.blocks.5.att.receptance-broadcast_matmul-425/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-286-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-434/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-435(model.blocks.5.att.receptance-broadcast_matmul-425/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-435/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-436(model.blocks.5.att-broadcast_mul-435/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-286-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-436/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-437(model.blocks.5.att.key-broadcast_matmul-427/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-437/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-438(model.blocks.5.att.key-broadcast_matmul-427/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-438/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-439(model.blocks.5.att-broadcast_mul-438/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-439/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-441(model.blocks.5.att.key-broadcast_matmul-427/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-276-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-441/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-442(model.blocks.5.att.key-broadcast_matmul-427/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-442/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-443(model.blocks.5.att-broadcast_mul-442/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-276-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-443/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-445(model.blocks.5.att.value-broadcast_matmul-429/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-445/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-446(model.blocks.5.att.value-broadcast_matmul-429/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln1-layer_norm-272/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-446/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-447(model.blocks.5.att-broadcast_mul-446/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-447/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-448([model.blocks.5.att-broadcast_mul-445/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-437/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-431/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-448/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-449(model.blocks.5.att.value-broadcast_matmul-429/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-281-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-449/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-450(model.blocks.5.att.value-broadcast_matmul-429/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att.time_shift-pad-273/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-broadcast_mul-450/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-reduce_sum_like-451(model.blocks.5.att-broadcast_mul-450/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-scalar_add-281-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-reduce_sum_like-451/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-452([model.blocks.5.att-broadcast_mul-449/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-441/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-broadcast_mul-434/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-452/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-455(model.blocks.5.att-reduce_sum_like-436/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-scalar_mul-455/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-456([model.blocks.5.att-scalar_mul-455/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-reduce_sum_like-433/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-456/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-457(model.blocks.5.att-reduce_sum_like-443/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-scalar_mul-457/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-458([model.blocks.5.att-scalar_mul-457/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-reduce_sum_like-439/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-458/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_mul-464(model.blocks.5.att-reduce_sum_like-451/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-scalar_mul-464/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-465([model.blocks.5.att-scalar_mul-464/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.att-reduce_sum_like-447/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.att-add_n-465/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-broadcast_mul-299-z_0-cast_f2h(model.blocks.5.att-broadcast_mul-299/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-broadcast_mul-299-z_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-sigmoid_v2_grad-421-dx_0-cast_f2h(model.blocks.5.att-sigmoid_v2_grad-421/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-sigmoid_v2_grad-421-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r-out-cast_f2h(model.blocks.5.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_k-out-cast_f2h(model.blocks.5.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v-out-cast_f2h(model.blocks.5.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att.time_mix_v-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-wkv_grad-422-gv_0-cast_f2h(model.blocks.5.att-wkv_grad-422/gv_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-wkv_grad-422-gv_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-281-out_0-cast_f2h(model.blocks.5.att-scalar_add-281/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-281-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-286-out_0-cast_f2h(model.blocks.5.att-scalar_add-286/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-286-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-wkv_grad-422-gk_0-cast_f2h(model.blocks.5.att-wkv_grad-422/gk_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-wkv_grad-422-gk_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-scalar_add-276-out_0-cast_f2h(model.blocks.5.att-scalar_add-276/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.att-scalar_add-276-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-465_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att-add_n-465/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-add_n-465_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-458_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att-add_n-458/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-add_n-458_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att-add_n-456_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.att-add_n-456/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.att-add_n-456_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_k_optimizer(model.blocks.5.att.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att-add_n-458_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_v_optimizer(model.blocks.5.att.time_mix_v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att-add_n-465_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_v-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_v-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_mix_r_optimizer(model.blocks.5.att.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att-add_n-456_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.att.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_decay-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_decay-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_decay_optimizer(model.blocks.5.att.time_decay/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att-reduce_sum-423/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.time_decay-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att.time_decay-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_first-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_first-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.att.time_first_optimizer(model.blocks.5.att.time_first/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att-reduce_sum-424/output_tensor_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.att.time_first-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.blocks.5.att.time_first-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.att_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
        )
        (MODULE:model.blocks.5.ffn:RWKV_ChannelMix()): (
          (INPUT:_model.blocks.5.ffn_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
          (PARAMETER:model.blocks.5.ffn.time_mix_k:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (PARAMETER:model.blocks.5.ffn.time_mix_r:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), size=(1, 1, 1024), dtype=oneflow.float32,
                 grad_fn=<accumulate_grad>)): ()
          (MODULE:model.blocks.5.ffn.time_shift:ZeroPad2d()): (
            (INPUT:_model.blocks.5.ffn.time_shift_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
            (OPERATOR: model.blocks.5.ffn.time_shift-pad-304(model.blocks.5.ln2-layer_norm-303/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.time_shift-pad-304/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.time_shift-pad-408(model.blocks.5.ffn-add_n-407/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.time_shift-pad-408/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.time_shift-add_n-409([model.blocks.5.ffn.time_shift-pad-408/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-add_n-403/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn.time_shift-add_n-409/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.time_shift-add_n-409-out_0-cast_h2f(model.blocks.5.ffn.time_shift-add_n-409/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.time_shift-add_n-409-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.time_shift_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<pad_backward>))
          )
          (MODULE:model.blocks.5.ffn.key:Linear1D(in_features=1024, out_features=4096, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.ffn.key_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.ffn.key.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(4096, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.ffn.key.weight() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul-316(model.blocks.5.ffn-add_n-309/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul-316/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul-396(model.blocks.5.ffn-relu_grad-389/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul-396/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul_grad_b-397(model.blocks.5.ffn-relu_grad-389/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-add_n-309/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul_grad_b-397/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight-out-cast_f2h(model.blocks.5.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.key.weight-out-cast_f2h/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key-broadcast_matmul_grad_b-397_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn.key-broadcast_matmul_grad_b-397/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.key-broadcast_matmul_grad_b-397_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight-m() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight-v() -> (out:sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.key.weight_optimizer(model.blocks.5.ffn.key.weight/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.key-broadcast_matmul_grad_b-397_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.key.weight-m/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.key.weight-v/out:(sbp=(B), size=(4096, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.key_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.ffn.receptance:Linear1D(in_features=1024, out_features=1024, bias=False, parallel=col)): (
            (INPUT:_model.blocks.5.ffn.receptance_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<add_n_backward>))
            (PARAMETER:model.blocks.5.ffn.receptance.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 1024), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.ffn.receptance.weight() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul-322(model.blocks.5.ffn-add_n-314/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul-322/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul-386(model.blocks.5.ffn-sigmoid_v2_grad-383-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387(model.blocks.5.ffn-sigmoid_v2_grad-383-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-add_n-314/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight-out-cast_f2h(model.blocks.5.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.receptance.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul-322-out_0-cast_h2f(model.blocks.5.ffn.receptance-broadcast_matmul-322/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul-322-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight-m() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight-v() -> (out:sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.receptance.weight_optimizer(model.blocks.5.ffn.receptance.weight/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.receptance-broadcast_matmul_grad_b-387_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.receptance.weight-m/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.receptance.weight-v/out:(sbp=(B), size=(1024, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.receptance_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (MODULE:model.blocks.5.ffn.value:Linear1D(in_features=4096, out_features=1024, bias=False, parallel=row)): (
            (INPUT:_model.blocks.5.ffn.value_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 4096),
                   dtype=oneflow.float32, grad_fn=<square_backward>))
            (PARAMETER:model.blocks.5.ffn.value.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), size=(1024, 4096), dtype=oneflow.float32,
                   grad_fn=<accumulate_grad>)): ()
            (OPERATOR: model.blocks.5.ffn.value.weight() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-320(model.blocks.5.ffn-square-318-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-320/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-384(model.blocks.5.ffn-broadcast_mul-382/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-384/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul_grad_b-385(model.blocks.5.ffn-broadcast_mul-382/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-square-318-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul_grad_b-385/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight-out-cast_f2h(model.blocks.5.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.value.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-320-out_0-cast_h2f(model.blocks.5.ffn.value-broadcast_matmul-320/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-320-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul-384-out_0-cast_h2f(model.blocks.5.ffn.value-broadcast_matmul-384/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul-384-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value-broadcast_matmul_grad_b-385_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn.value-broadcast_matmul_grad_b-385/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn.value-broadcast_matmul_grad_b-385_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight-m() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight-v() -> (out:sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OPERATOR: model.blocks.5.ffn.value.weight_optimizer(model.blocks.5.ffn.value.weight/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.5.ffn.value-broadcast_matmul_grad_b-385_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.value.weight-m/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), model.blocks.5.ffn.value.weight-v/out:(sbp=(B), size=(1024, 4096), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
            (OUTPUT:_model.blocks.5.ffn.value_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                   sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                   dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
          )
          (OPERATOR: model.blocks.5.ffn.time_mix_k() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-305(model.blocks.5.ln2-layer_norm-303/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-305/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-306(model.blocks.5.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_mul-306/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-307(model.blocks.5.ffn-scalar_mul-306/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-307/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-308(model.blocks.5.ffn.time_shift-pad-304/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-307-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-308/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-309([model.blocks.5.ffn-broadcast_mul-305/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-308/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-309/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-310(model.blocks.5.ln2-layer_norm-303/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-310/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-311(model.blocks.5.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_mul-311/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-312(model.blocks.5.ffn-scalar_mul-311/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-312/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-313(model.blocks.5.ffn.time_shift-pad-304/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-312-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-313/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-314([model.blocks.5.ffn-broadcast_mul-310/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-313/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-314/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-relu-317(model.blocks.5.ffn.key-broadcast_matmul-316/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-relu-317/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square-318(model.blocks.5.ffn-relu-317-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square-318/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2-323(model.blocks.5.ffn.receptance-broadcast_matmul-322-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2-323/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-324(model.blocks.5.ffn-sigmoid_v2-323-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.value-broadcast_matmul-320/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-324/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-381(model.ln_out-layer_norm_grad-380-dx_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.value-broadcast_matmul-320-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-broadcast_mul-381/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-382(model.ln_out-layer_norm_grad-380/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-sigmoid_v2-323-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-382/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2_grad-383(model.blocks.5.ffn.receptance-broadcast_matmul-322-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn-broadcast_mul-381/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2_grad-383/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square_grad-388(model.blocks.5.ffn-relu-317-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32)), model.blocks.5.ffn.value-broadcast_matmul-384-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square_grad-388/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-relu_grad-389(model.blocks.5.ffn-square_grad-388-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-relu-317/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-relu_grad-389/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-390(model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-390/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-391(model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-303/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-391/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-392(model.blocks.5.ffn-broadcast_mul-391/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-392/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-393(model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-312-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-393/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-394(model.blocks.5.ffn.receptance-broadcast_matmul-386/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_shift-pad-304/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-394/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-395(model.blocks.5.ffn-broadcast_mul-394/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-312-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-395/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-398(model.blocks.5.ffn-reduce_sum_like-395/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-scalar_mul-398/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-399([model.blocks.5.ffn-scalar_mul-398/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-reduce_sum_like-392/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-399/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-400(model.blocks.5.ffn.key-broadcast_matmul-396/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-400/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-401(model.blocks.5.ffn.key-broadcast_matmul-396/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ln2-layer_norm-303/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-401/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-402(model.blocks.5.ffn-broadcast_mul-401/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-402/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-403([model.blocks.5.ffn-broadcast_mul-400/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-390/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-403/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-404(model.blocks.5.ffn.key-broadcast_matmul-396/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-307-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-404/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-broadcast_mul-405(model.blocks.5.ffn.key-broadcast_matmul-396/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn.time_shift-pad-304/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-broadcast_mul-405/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-reduce_sum_like-406(model.blocks.5.ffn-broadcast_mul-405/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-scalar_add-307-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-reduce_sum_like-406/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-407([model.blocks.5.ffn-broadcast_mul-404/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-393/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-407/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_mul-415(model.blocks.5.ffn-reduce_sum_like-406/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-scalar_mul-415/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-416([model.blocks.5.ffn-scalar_mul-415/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-reduce_sum_like-402/y_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5.ffn-add_n-416/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-307-out_0-cast_f2h(model.blocks.5.ffn-scalar_add-307/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-307-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r-out-cast_f2h(model.blocks.5.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.time_mix_r-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square_grad-388-dx_0-cast_f2h(model.blocks.5.ffn-square_grad-388/dx_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square_grad-388-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-scalar_add-312-out_0-cast_f2h(model.blocks.5.ffn-scalar_add-312/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-scalar_add-312-out_0-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k-out-cast_f2h(model.blocks.5.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn.time_mix_k-out-cast_f2h/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2-323-y_0-cast_f2h(model.blocks.5.ffn-sigmoid_v2-323/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2-323-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-sigmoid_v2_grad-383-dx_0-cast_f2h(model.blocks.5.ffn-sigmoid_v2_grad-383/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-sigmoid_v2_grad-383-dx_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-square-318-y_0-cast_f2h(model.blocks.5.ffn-square-318/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))) -> (model.blocks.5.ffn-square-318-y_0-cast_f2h/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-relu-317-y_0-cast_h2f(model.blocks.5.ffn-relu-317/y_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-relu-317-y_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 4096), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-416_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn-add_n-416/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-add_n-416_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn-add_n-399_out_0_pinned_identity-out_0-cast_h2f(model.blocks.5.ffn-add_n-399/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5.ffn-add_n-399_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_k_optimizer(model.blocks.5.ffn.time_mix_k/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn-add_n-416_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_k-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_k-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r-m() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r-v() -> (out:sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OPERATOR: model.blocks.5.ffn.time_mix_r_optimizer(model.blocks.5.ffn.time_mix_r/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn-add_n-399_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_r-m/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), model.blocks.5.ffn.time_mix_r-v/out:(sbp=(B), size=(1, 1, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
          (OUTPUT:_model.blocks.5.ffn_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
                 sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
                 dtype=oneflow.float32, grad_fn=<broadcast_mul_backward>))
        )
        (OPERATOR: model.blocks.5-add_n-325([model.blocks.5.att.output-broadcast_matmul-301/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5.ffn-broadcast_mul-324/z_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.blocks.5-add_n-325/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.5-add_n-302-out_0-cast_h2f(model.blocks.5.att.output-broadcast_matmul-301/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5-add_n-302-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OPERATOR: model.blocks.5-add_n-325-out_0-cast_h2f(model.blocks.5-add_n-325/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.blocks.5-add_n-325-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
        (OUTPUT:_model.blocks.5_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
               sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
               dtype=oneflow.float32, grad_fn=<add_n_backward>))
      )
      (OUTPUT:_model.blocks_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<add_n_backward>))
    )
    (MODULE:model.ln_out:LayerNorm((1024,), eps=1e-05, elementwise_affine=True)): (
      (INPUT:_model.ln_out_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<add_n_backward>))
      (PARAMETER:model.ln_out.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (PARAMETER:model.ln_out.bias:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(1024,), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.ln_out.weight() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm-326(model.blocks.5-add_n-325/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.ln_out.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16)), model.ln_out.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-326/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-326/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm_param_grad-379(model.head_k-add_n-376-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.blocks.5-add_n-325-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-326/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-326/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32))) -> (model.ln_out-layer_norm_param_grad-379/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out-layer_norm_param_grad-379/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm_grad-380(model.head_k-add_n-376/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.blocks.5-add_n-325/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-326/mean_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out-layer_norm-326/inv_variance_0:(sbp=(B), size=(8, 1024), dtype=(oneflow.float32)), model.ln_out.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))) -> (model.ln_out-layer_norm_grad-380/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias-out-cast_f2h(model.ln_out.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.ln_out.bias-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight-out-cast_f2h(model.ln_out.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32))) -> (model.ln_out.weight-out-cast_f2h/out_0:(sbp=(B), size=(1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out-layer_norm_grad-380-dx_0-cast_h2f(model.ln_out-layer_norm_grad-380/dx_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.ln_out-layer_norm_grad-380-dx_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.weight_optimizer(model.ln_out.weight/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out-layer_norm_param_grad-379/gamma_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.ln_out.weight-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out.weight-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias-m() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias-v() -> (out:sbp=(B), size=(1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.ln_out.bias_optimizer(model.ln_out.bias/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out-layer_norm_param_grad-379/beta_diff_0:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.ln_out.bias-m/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), model.ln_out.bias-v/out:(sbp=(B), size=(1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.ln_out_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
    )
    (MODULE:model.head:Linear1D(in_features=1024, out_features=6064, bias=False, parallel=row)): (
      (INPUT:_model.head_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
      (PARAMETER:model.head.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(6064, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.head.weight() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul-338(model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul-338/out_0:(sbp=(B), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul-362(model-reshape-361/out_0:(sbp=(B), size=(8, 1024, 6064), dtype=(oneflow.bfloat16)), model.head.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul-362/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul_grad_b-363(model-reshape-361/out_0:(sbp=(B), size=(8, 1024, 6064), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul_grad_b-363/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight-out-cast_f2h(model.head.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))) -> (model.head.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head-broadcast_matmul_grad_b-363_out_0_pinned_identity-out_0-cast_h2f(model.head-broadcast_matmul_grad_b-363/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))) -> (model.head-broadcast_matmul_grad_b-363_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight-m() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight-v() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head.weight_optimizer(model.head.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.head-broadcast_matmul_grad_b-363_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.head.weight-m/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.head.weight-v/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.head_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 6064),
             dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
    )
    (MODULE:model.head_q:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)): (
      (INPUT:_model.head_q_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
      (PARAMETER:model.head_q.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.head_q.weight() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul-328(model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head_q.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul-328/out_0:(sbp=(B), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul-368(model-batch_matmul-366/out_0:(sbp=(S(1)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.head_q.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul-368/out_0:(sbp=(S(1)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul_grad_b-369(model-batch_matmul-366/out_0:(sbp=(S(2)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul_grad_b-369/out_0:(sbp=(S(0)), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight-out-cast_f2h(model.head_q.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))) -> (model.head_q.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q-broadcast_matmul_grad_b-369_out_0_pinned_identity-out_0-cast_h2f(model.head_q-broadcast_matmul_grad_b-369/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_q-broadcast_matmul_grad_b-369_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight-m() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight-v() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_q.weight_optimizer(model.head_q.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_q-broadcast_matmul_grad_b-369_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.head_q.weight-m/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_q.weight-v/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.head_q_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 256),
             dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
    )
    (MODULE:model.head_k:Linear1D(in_features=1024, out_features=256, bias=False, parallel=col)): (
      (INPUT:_model.head_k_input.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 1024),
             dtype=oneflow.float32, grad_fn=<layer_norm_backward>))
      (PARAMETER:model.head_k.weight:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), size=(256, 1024), dtype=oneflow.float32,
             grad_fn=<accumulate_grad>)): ()
      (OPERATOR: model.head_k.weight() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul-330(model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head_k.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul-330/out_0:(sbp=(B), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul-373(model-transpose-370/output_0:(sbp=(S(1)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.head_k.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul-373/out_0:(sbp=(S(1)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul_grad_b-374(model-transpose-370/output_0:(sbp=(S(2)), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model.ln_out-layer_norm-326/y_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul_grad_b-374/out_0:(sbp=(S(0)), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-add_n-376([model.head_k-broadcast_matmul-373/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model.head-broadcast_matmul-362/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))]) -> (model.head_k-add_n-376/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight-out-cast_f2h(model.head_k.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))) -> (model.head_k.weight-out-cast_f2h/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-add_n-376-out_0-cast_h2f(model.head_k-add_n-376/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-add_n-376-out_0-cast_h2f/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k-broadcast_matmul_grad_b-374_out_0_pinned_identity-out_0-cast_h2f(model.head_k-broadcast_matmul_grad_b-374/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.bfloat16))) -> (model.head_k-broadcast_matmul_grad_b-374_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight-m() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight-v() -> (out:sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OPERATOR: model.head_k.weight_optimizer(model.head_k.weight/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_k-broadcast_matmul_grad_b-374_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.head_k.weight-m/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), model.head_k.weight-v/out:(sbp=(B), size=(256, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
      (OUTPUT:_model.head_k_output.0.0_2:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
             sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(8, 1024, 256),
             dtype=oneflow.float32, grad_fn=<broadcast_matmul_backward>))
    )
    (OPERATOR: model.emb.weight() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-331(model.head_k-broadcast_matmul-330/out_0:(sbp=(B), size=(8, 1024, 256), dtype=(oneflow.bfloat16))) -> (model-transpose-331/output_0:(sbp=(B), size=(8, 256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-332(model.head_q-broadcast_matmul-328/out_0:(sbp=(B), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model-transpose-331/output_0:(sbp=(B), size=(8, 256, 1024), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-332/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-scalar_mul-333(model-batch_matmul-332/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model-scalar_mul-333/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-one_hot-334(System-Boxing-Identity-297/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64))) -> (model-one_hot-334/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.int64))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-cast-335(model-one_hot-334/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.int64))) -> (model-cast-335/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-336(model-scalar_mul-333/out_0:(sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model-cast-335-out_0-cast_f2h/out_0:(sbp=(S(2)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-336/out_0:(sbp=(S(2)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-340(model.head-broadcast_matmul-338/out_0:(sbp=(B), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-340/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-341(_GraphBase_0_input.1.1_targets/out:(sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64))) -> (model-reshape-341/out_0:(sbp=(S(0)), size=(8192), dtype=(oneflow.int64))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-342(model-reshape-340/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-transpose-342/output_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-343(model-transpose-342/output_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-343/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-log_softmax-344(model-reshape-343/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-log_softmax-344/prob_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-nll-345(model-log_softmax-344-prob_0-cast_h2f/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.float32)), System-Boxing-Identity-301/out:(sbp=(B), size=(8192), dtype=(oneflow.int64))) -> (model-nll-345/output_0:(sbp=(B), size=(8192), dtype=(oneflow.float32)), model-nll-345/out_weight_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-346(model-nll-345/output_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))) -> (model-reshape-346/out_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reduce_sum-347(model-reshape-346/out_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))) -> (model-reduce_sum-347/output_tensor_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reduce_sum-348(model-nll-345/out_weight_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))) -> (model-reduce_sum-348/output_tensor_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_div-349(model-reduce_sum-347/output_tensor_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-reduce_sum-348/output_tensor_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (model-broadcast_div-349/z_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_div-352(ones_like-351/out_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-reduce_sum-348/output_tensor_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (model-broadcast_div-352/z_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_div_grad-353(ones_like-351/out_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-broadcast_div-349/z_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-reduce_sum-348/output_tensor_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (model-broadcast_div_grad-353/dy_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_like-354(model-broadcast_div-352/z_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-reshape-346/out_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))) -> (model-broadcast_like-354/y_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-broadcast_like-355(model-broadcast_div_grad-353/dy_0:(sbp=(B), size=(), dtype=(oneflow.float32)), model-nll-345/out_weight_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))) -> (model-broadcast_like-355/y_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-356(model-broadcast_like-354/y_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))) -> (model-reshape-356/out_0:(sbp=(B), size=(8192), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-nll_grad-357(model-reshape-356/out_0:(sbp=(B), size=(8192), dtype=(oneflow.float32)), model-log_softmax-344-prob_0-cast_h2f/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.float32)), System-Boxing-Identity-301/out:(sbp=(B), size=(8192), dtype=(oneflow.int64))) -> (model-nll_grad-357/in_grad_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-log_softmax_grad-358(model-log_softmax-344/prob_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16)), model-nll_grad-357-in_grad_0-cast_f2h/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-log_softmax_grad-358/dx_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-359(model-log_softmax_grad-358/dx_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-359/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-360(model-reshape-359/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-transpose-360/output_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-reshape-361(model-transpose-360/output_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-reshape-361/out_0:(sbp=(B), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-364(model-reshape-361/out_0:(sbp=(B), size=(8, 1024, 6064), dtype=(oneflow.bfloat16)), model-cast-335-out_0-cast_f2h/out_0:(sbp=(S(1)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-364/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-scalar_mul-365(model-batch_matmul-364/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model-scalar_mul-365/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-366(model-scalar_mul-365/out_0:(sbp=(S(1)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16)), model-transpose-331/output_0:(sbp=(B), size=(8, 256, 1024), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-366/out_0:(sbp=(S(1)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-batch_matmul-367(model.head_q-broadcast_matmul-328/out_0:(sbp=(B), size=(8, 1024, 256), dtype=(oneflow.bfloat16)), model-scalar_mul-365/out_0:(sbp=(S(2)), size=(8, 1024, 1024), dtype=(oneflow.bfloat16))) -> (model-batch_matmul-367/out_0:(sbp=(S(2)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-transpose-370(model-batch_matmul-367/out_0:(sbp=(S(2)), size=(8, 256, 1024), dtype=(oneflow.bfloat16))) -> (model-transpose-370/output_0:(sbp=(S(1)), size=(8, 1024, 256), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-nll_grad-357-in_grad_0-cast_f2h(model-nll_grad-357/in_grad_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.float32))) -> (model-nll_grad-357-in_grad_0-cast_f2h/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weight-out-cast_f2h(model.emb.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32))) -> (model.emb.weight-out-cast_f2h/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-cast-335-out_0-cast_f2h(model-cast-335/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.float32))) -> (model-cast-335-out_0-cast_f2h/out_0:(sbp=(S(0)), size=(8, 1024, 6064), dtype=(oneflow.bfloat16))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model-log_softmax-344-prob_0-cast_h2f(model-log_softmax-344/prob_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.bfloat16))) -> (model-log_softmax-344-prob_0-cast_h2f/out_0:(sbp=(B), size=(8192, 6064), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weight-m() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weight-v() -> (out:sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OPERATOR: model.emb.weightadam_bias_correction_factor1(System-Train-TrainStep-Identity/out:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (model.emb.weightadam_bias_correction_factor1/out_0:(sbp=(B), size=(1), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cpu", ranks=[0])))
    (OPERATOR: model.emb.weightadam_bias_correction_factor2(System-Train-TrainStep-Identity/out:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (model.emb.weightadam_bias_correction_factor2/out_0:(sbp=(B), size=(1), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cpu", ranks=[0])))
    (OPERATOR: model.emb.weight_optimizer(model.emb.weight/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.emb-unsorted_segment_sum_like-898_out_0_pinned_identity-out_0-cast_h2f/out_0:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-299/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), model.emb.weight-m/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), model.emb.weight-v/out:(sbp=(B), size=(6064, 1024), dtype=(oneflow.float32)), System-Boxing-Identity-296/out:(sbp=(B), size=(1), dtype=(oneflow.float32)), System-Boxing-Identity-303/out:(sbp=(B), size=(1), dtype=(oneflow.float32))) -> (), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
    (OUTPUT:_model_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cuda", ranks=[0, 1]),
           sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(),
           dtype=oneflow.float32, grad_fn=<broadcast_div_backward>))
  )
  (OPERATOR: _GraphBase_0_input.1.0_idx() -> (_GraphBase_0_input.1.0_idx/out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0, 1])))
  (OPERATOR: _GraphBase_0_input.1.1_targets() -> (_GraphBase_0_input.1.1_targets/out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0, 1])))
  (OPERATOR: scalar_add-350(model-broadcast_div-349/z_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (scalar_add-350/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: ones_like-351(scalar_add-350/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))) -> (ones_like-351/out_0:(sbp=(B), size=(), dtype=(oneflow.float32))), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: _GraphBase_0_output.0.0.0_loss(model-broadcast_div-349/z_0:sbp=(B), size=(), dtype=(oneflow.float32)) -> (_GraphBase_0_output.0.0.0_loss/out:sbp=(B), size=(), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep() -> (out:sbp=(B), size=(1), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep-Identity(System-Train-TrainStep/out:sbp=(B), size=(1), dtype=(oneflow.int64)) -> (out:sbp=(B), size=(1), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep-ScalarAdd(System-Train-TrainStep-Identity/out:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (System-Train-TrainStep-ScalarAdd/out_0:(sbp=(B), size=(1), dtype=(oneflow.int64))), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Train-TrainStep-Assign(System-Train-TrainStep/out:(sbp=(B), size=(1), dtype=(oneflow.int64)), System-Train-TrainStep-ScalarAdd/out_0:(sbp=(B), size=(1), dtype=(oneflow.int64))) -> (), placement=(oneflow.placement(type="cpu", ranks=[0])))
  (OPERATOR: System-Boxing-Identity-296(model.emb.weightadam_bias_correction_factor1/out_0:sbp=(B), size=(1), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(1), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-297(_GraphBase_0_input.1.0_idx/out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)) -> (out:sbp=(S(0)), size=(8, 1024), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-298(model.blocks.0.att.value-broadcast_matmul-22-out_0-cast_h2f/out_0:sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-299(System-Train-LearningRate-Scheduler_63/out:sbp=(B), size=(1), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(1), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-300(model.blocks.0.att.key-broadcast_matmul-20-out_0-cast_h2f/out_0:sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-301(model-reshape-341/out_0:sbp=(B), size=(8192), dtype=(oneflow.int64)) -> (out:sbp=(B), size=(8192), dtype=(oneflow.int64)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-302(model.blocks.0.att-sigmoid_v2-25/y_0:sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(8, 1024, 1024), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OPERATOR: System-Boxing-Identity-303(model.emb.weightadam_bias_correction_factor2/out_0:sbp=(B), size=(1), dtype=(oneflow.float32)) -> (out:sbp=(B), size=(1), dtype=(oneflow.float32)), placement=(oneflow.placement(type="cuda", ranks=[0, 1])))
  (OUTPUT:_GraphBase_0_output.0.0.0_loss:tensor(..., placement=oneflow.placement(type="cpu", ranks=[0]),
         sbp=(oneflow.sbp.broadcast,), is_lazy='True', size=(),
         dtype=oneflow.float32, grad_fn=<global_to_global_backward>))
)
[32m[08/24 02:43:44 lb.utils.events]: [0m eta: 0:26:36  iteration: 19/10000  consumed_samples: 160  total_loss: 9.68  time: 0.1600 s/iter  data_time: 0.0022 s/iter total_throughput: 50.00 samples/s lr: 8.00e-04  
[32m[08/24 02:43:48 lb.utils.events]: [0m eta: 0:26:33  iteration: 39/10000  consumed_samples: 320  total_loss: 5.177  time: 0.1598 s/iter  data_time: 0.0023 s/iter total_throughput: 50.05 samples/s lr: 8.00e-04  
[32m[08/24 02:43:51 lb.utils.events]: [0m eta: 0:26:32  iteration: 59/10000  consumed_samples: 480  total_loss: 3.965  time: 0.1600 s/iter  data_time: 0.0020 s/iter total_throughput: 50.01 samples/s lr: 8.00e-04  
[32m[08/24 02:43:54 lb.utils.events]: [0m eta: 0:26:30  iteration: 79/10000  consumed_samples: 640  total_loss: 3.216  time: 0.1601 s/iter  data_time: 0.0021 s/iter total_throughput: 49.96 samples/s lr: 8.00e-04  
[32m[08/24 02:43:57 lb.utils.events]: [0m eta: 0:26:26  iteration: 99/10000  consumed_samples: 800  total_loss: 2.893  time: 0.1601 s/iter  data_time: 0.0022 s/iter total_throughput: 49.97 samples/s lr: 8.00e-04  
[32m[08/24 02:44:00 lb.utils.events]: [0m eta: 0:26:23  iteration: 119/10000  consumed_samples: 960  total_loss: 2.754  time: 0.1602 s/iter  data_time: 0.0022 s/iter total_throughput: 49.95 samples/s lr: 8.00e-04  
[32m[08/24 02:44:04 lb.utils.events]: [0m eta: 0:26:20  iteration: 139/10000  consumed_samples: 1120  total_loss: 2.608  time: 0.1601 s/iter  data_time: 0.0023 s/iter total_throughput: 49.96 samples/s lr: 8.00e-04  
[32m[08/24 02:44:07 lb.utils.events]: [0m eta: 0:26:17  iteration: 159/10000  consumed_samples: 1280  total_loss: 2.426  time: 0.1602 s/iter  data_time: 0.0022 s/iter total_throughput: 49.95 samples/s lr: 8.00e-04  
[32m[08/24 02:44:10 lb.utils.events]: [0m eta: 0:26:14  iteration: 179/10000  consumed_samples: 1440  total_loss: 2.22  time: 0.1602 s/iter  data_time: 0.0023 s/iter total_throughput: 49.95 samples/s lr: 8.00e-04  
[32m[08/24 02:44:13 lb.utils.events]: [0m eta: 0:26:11  iteration: 199/10000  consumed_samples: 1600  total_loss: 1.967  time: 0.1602 s/iter  data_time: 0.0022 s/iter total_throughput: 49.93 samples/s lr: 8.00e-04  
[32m[08/24 02:44:16 lb.utils.events]: [0m eta: 0:26:09  iteration: 219/10000  consumed_samples: 1760  total_loss: 1.461  time: 0.1603 s/iter  data_time: 0.0024 s/iter total_throughput: 49.91 samples/s lr: 8.00e-04  
[32m[08/24 02:44:20 lb.utils.events]: [0m eta: 0:26:06  iteration: 239/10000  consumed_samples: 1920  total_loss: 1.008  time: 0.1603 s/iter  data_time: 0.0023 s/iter total_throughput: 49.89 samples/s lr: 8.00e-04  
[32m[08/24 02:44:23 lb.utils.events]: [0m eta: 0:26:03  iteration: 259/10000  consumed_samples: 2080  total_loss: 0.8413  time: 0.1604 s/iter  data_time: 0.0021 s/iter total_throughput: 49.88 samples/s lr: 8.00e-04  
[32m[08/24 02:44:26 lb.utils.events]: [0m eta: 0:26:00  iteration: 279/10000  consumed_samples: 2240  total_loss: 0.5917  time: 0.1604 s/iter  data_time: 0.0021 s/iter total_throughput: 49.87 samples/s lr: 8.00e-04  
[32m[08/24 02:44:29 lb.utils.events]: [0m eta: 0:25:57  iteration: 299/10000  consumed_samples: 2400  total_loss: 0.4054  time: 0.1605 s/iter  data_time: 0.0020 s/iter total_throughput: 49.84 samples/s lr: 8.00e-04  
[32m[08/24 02:44:33 lb.utils.events]: [0m eta: 0:25:55  iteration: 319/10000  consumed_samples: 2560  total_loss: 0.2196  time: 0.1606 s/iter  data_time: 0.0023 s/iter total_throughput: 49.82 samples/s lr: 8.00e-04  
[32m[08/24 02:44:36 lb.utils.events]: [0m eta: 0:25:52  iteration: 339/10000  consumed_samples: 2720  total_loss: 0.1112  time: 0.1606 s/iter  data_time: 0.0023 s/iter total_throughput: 49.80 samples/s lr: 8.00e-04  
[32m[08/24 02:44:39 lb.utils.events]: [0m eta: 0:25:49  iteration: 359/10000  consumed_samples: 2880  total_loss: 0.05964  time: 0.1607 s/iter  data_time: 0.0021 s/iter total_throughput: 49.78 samples/s lr: 8.00e-04  
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Killing subprocess 1392289
Killing subprocess 1392290
Main process received SIGINT, exiting
F20220824 02:44:41.830744 1393121 cuda_stream.cpp:153] Check failed: cudaEventRecord(cuda_event->cuda_event(), cuda_stream_) : driver shutting down (4) 
*** Check failure stack trace: ***
F20220824 02:44:41.830747 1393138 cuda_stream_context.cpp:80] driver shutting down
  File "../oneflow/core/lazy/stream_context/cuda/cuda_stream_context.cpp", line 80, in operator()
    cb_event.first->Sync()
Error Type: oneflow.ErrorProto.runtime_error
*** Check failure stack trace: ***
    @     0x7fb539b019cc  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fb539b019cc  google::LogMessageFatal::~LogMessageFatal()
    @     0x7fb566bc0c2d  std::thread::_State_impl<>::_M_run()
    @     0x7fb5645f15ac  oneflow::ep::CudaStream::RecordEvent()
    @     0x7fb538f7ade4  (unknown)
    @     0x7fb566bc0461  oneflow::(anonymous namespace)::CudaStreamContext::AddCallback()
    @     0x7fb5bd933609  start_thread
    @     0x7fb566b62c7c  oneflow::GenericActorContext::AddCallback()
    @     0x7fb5bd858133  clone
