[01/19 11:19:47] libai INFO: Rank of current process: 0. World size: 1
[01/19 11:19:47] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 11:19:47] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mbias_dropout_fusion[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 11:19:47] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 11:19:47] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 11:19:47] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 11:20:00] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 11:20:00] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 11:20:00] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 11:20:00] lb.data.build INFO:     train:      16000
[01/19 11:20:00] lb.data.build INFO:     validation: 160000
[01/19 11:20:00] lb.data.build INFO:     test:       160000
[01/19 11:20:00] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 11:20:00] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 11:20:00] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 11:20:00] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 11:20:00] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 11:20:00] lb.data.indexed_dataset INFO:     reading document index...
[01/19 11:20:00] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 11:20:01] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 11:20:01] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 11:20:01] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.835868 seconds
[01/19 11:20:01] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 11:20:01] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 11:20:01] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 11:20:01] lb.data.dataset_utils INFO:  > dataset split:
[01/19 11:20:01] lb.data.dataset_utils INFO:     train:
[01/19 11:20:01] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 11:20:01] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 11:20:01] lb.data.dataset_utils INFO:     validation:
[01/19 11:20:01] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 11:20:01] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 11:20:01] lb.data.dataset_utils INFO:     test:
[01/19 11:20:01] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 11:20:01] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 11:20:01] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 11:20:01] lb.data.dataset_utils INFO:     loaded indexed file in 0.034 seconds
[01/19 11:20:01] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 11:20:01] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:20:01] lb.data.dataset_utils INFO:     loaded indexed file in 0.013 seconds
[01/19 11:20:01] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 11:20:01] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:20:01] lb.data.dataset_utils INFO:     loaded indexed file in 0.010 seconds
[01/19 11:20:01] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 11:20:01] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 11:20:02] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 11:20:06] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 10.7  data_time: 0.0051  lr: 0.00e+00  
[01/19 11:20:07] lb.utils.events INFO:  eta: 0:03:53  iteration: 2/1000  consumed samples: 32  total_loss: 10.7  data_time: 0.0068  lr: 1.00e-06  
[01/19 11:20:07] lb.utils.events INFO:  eta: 0:03:09  iteration: 3/1000  consumed samples: 48  total_loss: 10.7  time: 0.1902(84.10)  data_time: 0.0057  lr: 2.00e-06  
[01/19 11:20:07] lb.utils.events INFO:  eta: 0:03:16  iteration: 4/1000  consumed samples: 64  total_loss: 10.7  time: 0.1978(80.87)  data_time: 0.0047  lr: 3.00e-06  
[01/19 11:20:07] lb.utils.events INFO:  eta: 0:03:22  iteration: 5/1000  consumed samples: 80  total_loss: 10.7  time: 0.2000(80.02)  data_time: 0.0043  lr: 4.00e-06  
[01/19 11:20:07] lb.utils.events INFO:  eta: 0:03:23  iteration: 6/1000  consumed samples: 96  total_loss: 10.7  time: 0.2017(79.33)  data_time: 0.0043  lr: 5.00e-06  
[01/19 11:20:08] lb.utils.events INFO:  eta: 0:03:22  iteration: 7/1000  consumed samples: 112  total_loss: 10.7  time: 0.2012(79.51)  data_time: 0.0039  lr: 6.00e-06  
[01/19 11:20:08] lb.utils.events INFO:  eta: 0:03:22  iteration: 8/1000  consumed samples: 128  total_loss: 10.69  time: 0.2021(79.15)  data_time: 0.0036  lr: 7.00e-06  
[01/19 11:20:08] lb.utils.events INFO:  eta: 0:03:22  iteration: 9/1000  consumed samples: 144  total_loss: 10.69  time: 0.2018(79.30)  data_time: 0.0035  lr: 8.00e-06  
[01/19 11:20:08] lb.utils.events INFO:  eta: 0:03:22  iteration: 10/1000  consumed samples: 160  total_loss: 10.67  time: 0.2024(79.05)  data_time: 0.0036  lr: 9.00e-06  
[01/19 11:20:09] lb.utils.events INFO:  eta: 0:03:21  iteration: 11/1000  consumed samples: 176  total_loss: 10.66  time: 0.2020(79.22)  data_time: 0.0034  lr: 1.00e-05  
[01/19 11:20:09] lb.utils.events INFO:  eta: 0:03:22  iteration: 12/1000  consumed samples: 192  total_loss: 10.65  time: 0.2025(79.00)  data_time: 0.0033  lr: 1.10e-05  
[01/19 11:20:09] lb.utils.events INFO:  eta: 0:03:21  iteration: 13/1000  consumed samples: 208  total_loss: 10.65  time: 0.2022(79.11)  data_time: 0.0032  lr: 1.20e-05  
[01/19 11:20:09] lb.utils.events INFO:  eta: 0:03:19  iteration: 14/1000  consumed samples: 224  total_loss: 10.62  time: 0.2022(79.13)  data_time: 0.0033  lr: 1.30e-05  
[01/19 11:20:09] lb.utils.events INFO:  eta: 0:03:20  iteration: 15/1000  consumed samples: 240  total_loss: 10.59  time: 0.2025(79.01)  data_time: 0.0032  lr: 1.40e-05  
[01/19 11:20:10] lb.utils.events INFO:  eta: 0:03:19  iteration: 16/1000  consumed samples: 256  total_loss: 10.57  time: 0.2023(79.09)  data_time: 0.0031  lr: 1.50e-05  
[01/19 11:20:10] lb.utils.events INFO:  eta: 0:03:20  iteration: 17/1000  consumed samples: 272  total_loss: 10.55  time: 0.2032(78.75)  data_time: 0.0031  lr: 1.60e-05  
[01/19 11:20:10] lb.utils.events INFO:  eta: 0:03:20  iteration: 18/1000  consumed samples: 288  total_loss: 10.55  time: 0.2036(78.59)  data_time: 0.0031  lr: 1.70e-05  
[01/19 11:20:10] lb.utils.events INFO:  eta: 0:03:21  iteration: 19/1000  consumed samples: 304  total_loss: 10.54  time: 0.2038(78.52)  data_time: 0.0031  lr: 1.80e-05  
[01/19 11:20:11] lb.utils.events INFO:  eta: 0:03:20  iteration: 20/1000  consumed samples: 320  total_loss: 10.52  time: 0.2036(78.58)  data_time: 0.0031  lr: 1.90e-05  
[01/19 11:20:11] lb.utils.events INFO:  eta: 0:03:20  iteration: 21/1000  consumed samples: 336  total_loss: 10.49  time: 0.2038(78.50)  data_time: 0.0030  lr: 2.00e-05  
[01/19 11:20:11] lb.utils.events INFO:  eta: 0:03:20  iteration: 22/1000  consumed samples: 352  total_loss: 10.49  time: 0.2036(78.57)  data_time: 0.0033  lr: 2.10e-05  
[01/19 11:20:11] lb.utils.events INFO:  eta: 0:03:20  iteration: 23/1000  consumed samples: 368  total_loss: 10.48  time: 0.2038(78.51)  data_time: 0.0032  lr: 2.20e-05  
[01/19 11:20:11] lb.utils.events INFO:  eta: 0:03:20  iteration: 24/1000  consumed samples: 384  total_loss: 10.48  time: 0.2041(78.41)  data_time: 0.0032  lr: 2.30e-05  
[01/19 11:20:12] lb.utils.events INFO:  eta: 0:03:20  iteration: 25/1000  consumed samples: 400  total_loss: 10.48  time: 0.2042(78.36)  data_time: 0.0031  lr: 2.40e-05  
[01/19 11:20:12] lb.utils.events INFO:  eta: 0:03:20  iteration: 26/1000  consumed samples: 416  total_loss: 10.48  time: 0.2044(78.30)  data_time: 0.0031  lr: 2.50e-05  
[01/19 11:20:12] lb.utils.events INFO:  eta: 0:03:20  iteration: 27/1000  consumed samples: 432  total_loss: 10.47  time: 0.2042(78.34)  data_time: 0.0031  lr: 2.60e-05  
[01/19 11:20:12] lb.utils.events INFO:  eta: 0:03:19  iteration: 28/1000  consumed samples: 448  total_loss: 10.46  time: 0.2040(78.42)  data_time: 0.0031  lr: 2.70e-05  
[01/19 11:20:13] lb.utils.events INFO:  eta: 0:03:20  iteration: 29/1000  consumed samples: 464  total_loss: 10.45  time: 0.2042(78.36)  data_time: 0.0030  lr: 2.80e-05  
[01/19 11:20:13] lb.utils.events INFO:  eta: 0:03:20  iteration: 30/1000  consumed samples: 480  total_loss: 10.44  time: 0.2043(78.31)  data_time: 0.0030  lr: 2.90e-05  
[01/19 11:20:13] lb.utils.events INFO:  eta: 0:03:20  iteration: 31/1000  consumed samples: 496  total_loss: 10.44  time: 0.2044(78.27)  data_time: 0.0030  lr: 3.00e-05  
[01/19 11:20:13] lb.utils.events INFO:  eta: 0:03:19  iteration: 32/1000  consumed samples: 512  total_loss: 10.44  time: 0.2043(78.33)  data_time: 0.0031  lr: 3.10e-05  
[01/19 11:20:13] lb.utils.events INFO:  eta: 0:03:19  iteration: 33/1000  consumed samples: 528  total_loss: 10.44  time: 0.2043(78.30)  data_time: 0.0033  lr: 3.20e-05  
[01/19 11:20:14] lb.utils.events INFO:  eta: 0:03:19  iteration: 34/1000  consumed samples: 544  total_loss: 10.43  time: 0.2036(78.58)  data_time: 0.0034  lr: 3.30e-05  
[01/19 11:20:14] lb.utils.events INFO:  eta: 0:03:18  iteration: 35/1000  consumed samples: 560  total_loss: 10.41  time: 0.2031(78.78)  data_time: 0.0035  lr: 3.40e-05  
[01/19 11:20:14] lb.utils.events INFO:  eta: 0:03:18  iteration: 36/1000  consumed samples: 576  total_loss: 10.4  time: 0.2029(78.84)  data_time: 0.0035  lr: 3.50e-05  
[01/19 11:20:14] lb.utils.events INFO:  eta: 0:03:17  iteration: 37/1000  consumed samples: 592  total_loss: 10.4  time: 0.2023(79.08)  data_time: 0.0036  lr: 3.60e-05  
[01/19 11:20:15] lb.utils.events INFO:  eta: 0:03:17  iteration: 38/1000  consumed samples: 608  total_loss: 10.38  time: 0.2025(79.02)  data_time: 0.0036  lr: 3.70e-05  
[01/19 11:20:15] lb.utils.events INFO:  eta: 0:03:17  iteration: 39/1000  consumed samples: 624  total_loss: 10.36  time: 0.2024(79.07)  data_time: 0.0036  lr: 3.80e-05  
[01/19 11:20:15] lb.utils.events INFO:  eta: 0:03:16  iteration: 40/1000  consumed samples: 640  total_loss: 10.36  time: 0.2021(79.19)  data_time: 0.0036  lr: 3.90e-05  
[01/19 11:20:15] lb.utils.events INFO:  eta: 0:03:15  iteration: 41/1000  consumed samples: 656  total_loss: 10.35  time: 0.2017(79.31)  data_time: 0.0035  lr: 4.00e-05  
[01/19 11:20:15] lb.utils.events INFO:  eta: 0:03:16  iteration: 42/1000  consumed samples: 672  total_loss: 10.34  time: 0.2021(79.18)  data_time: 0.0030  lr: 4.10e-05  
[01/19 11:20:16] lb.utils.events INFO:  eta: 0:03:16  iteration: 43/1000  consumed samples: 688  total_loss: 10.33  time: 0.2022(79.12)  data_time: 0.0031  lr: 4.20e-05  
[01/19 11:20:16] lb.utils.events INFO:  eta: 0:03:16  iteration: 44/1000  consumed samples: 704  total_loss: 10.33  time: 0.2023(79.10)  data_time: 0.0031  lr: 4.30e-05  
[01/19 11:20:16] lb.utils.events INFO:  eta: 0:03:15  iteration: 45/1000  consumed samples: 720  total_loss: 10.33  time: 0.2024(79.05)  data_time: 0.0032  lr: 4.40e-05  
[01/19 11:20:16] lb.utils.events INFO:  eta: 0:03:15  iteration: 46/1000  consumed samples: 736  total_loss: 10.33  time: 0.2024(79.05)  data_time: 0.0034  lr: 4.50e-05  
[01/19 11:20:17] lb.utils.events INFO:  eta: 0:03:15  iteration: 47/1000  consumed samples: 752  total_loss: 10.32  time: 0.2025(79.00)  data_time: 0.0034  lr: 4.60e-05  
[01/19 11:20:17] lb.utils.events INFO:  eta: 0:03:15  iteration: 48/1000  consumed samples: 768  total_loss: 10.32  time: 0.2025(79.00)  data_time: 0.0035  lr: 4.70e-05  
[01/19 11:20:17] lb.utils.events INFO:  eta: 0:03:15  iteration: 49/1000  consumed samples: 784  total_loss: 10.32  time: 0.2026(78.96)  data_time: 0.0035  lr: 4.80e-05  
[01/19 11:20:17] lb.utils.events INFO:  eta: 0:03:14  iteration: 50/1000  consumed samples: 800  total_loss: 10.31  time: 0.2026(78.96)  data_time: 0.0035  lr: 4.90e-05  
[01/19 11:20:17] lb.utils.events INFO:  eta: 0:03:14  iteration: 51/1000  consumed samples: 816  total_loss: 10.31  time: 0.2027(78.92)  data_time: 0.0035  lr: 5.00e-05  
[01/19 11:20:18] lb.utils.events INFO:  eta: 0:03:14  iteration: 52/1000  consumed samples: 832  total_loss: 10.3  time: 0.2027(78.92)  data_time: 0.0035  lr: 5.10e-05  
[01/19 11:20:18] lb.utils.events INFO:  eta: 0:03:14  iteration: 53/1000  consumed samples: 848  total_loss: 10.29  time: 0.2028(78.88)  data_time: 0.0033  lr: 5.20e-05  
[01/19 11:20:18] lb.utils.events INFO:  eta: 0:03:14  iteration: 54/1000  consumed samples: 864  total_loss: 10.27  time: 0.2028(78.88)  data_time: 0.0032  lr: 5.30e-05  
[01/19 11:20:18] lb.utils.events INFO:  eta: 0:03:13  iteration: 55/1000  consumed samples: 880  total_loss: 10.25  time: 0.2031(78.78)  data_time: 0.0032  lr: 5.40e-05  
[01/19 11:20:19] lb.utils.events INFO:  eta: 0:03:13  iteration: 56/1000  consumed samples: 896  total_loss: 10.23  time: 0.2031(78.79)  data_time: 0.0032  lr: 5.50e-05  
[01/19 11:20:19] lb.utils.events INFO:  eta: 0:03:13  iteration: 57/1000  consumed samples: 912  total_loss: 10.21  time: 0.2032(78.75)  data_time: 0.0031  lr: 5.60e-05  
[01/19 11:20:19] lb.utils.events INFO:  eta: 0:03:13  iteration: 58/1000  consumed samples: 928  total_loss: 10.21  time: 0.2032(78.76)  data_time: 0.0030  lr: 5.70e-05  
[01/19 11:20:19] lb.utils.events INFO:  eta: 0:03:13  iteration: 59/1000  consumed samples: 944  total_loss: 10.2  time: 0.2032(78.72)  data_time: 0.0030  lr: 5.80e-05  
[01/19 11:20:19] lb.utils.events INFO:  eta: 0:03:12  iteration: 60/1000  consumed samples: 960  total_loss: 10.2  time: 0.2032(78.73)  data_time: 0.0030  lr: 5.90e-05  
[01/19 11:20:20] lb.utils.events INFO:  eta: 0:03:12  iteration: 61/1000  consumed samples: 976  total_loss: 10.19  time: 0.2033(78.70)  data_time: 0.0030  lr: 6.00e-05  
[01/19 11:20:20] lb.utils.events INFO:  eta: 0:03:12  iteration: 62/1000  consumed samples: 992  total_loss: 10.18  time: 0.2034(78.66)  data_time: 0.0029  lr: 6.10e-05  
[01/19 11:20:20] lb.utils.events INFO:  eta: 0:03:13  iteration: 63/1000  consumed samples: 1008  total_loss: 10.16  time: 0.2035(78.63)  data_time: 0.0030  lr: 6.20e-05  
[01/19 11:20:20] lb.utils.events INFO:  eta: 0:03:12  iteration: 64/1000  consumed samples: 1024  total_loss: 10.15  time: 0.2035(78.64)  data_time: 0.0030  lr: 6.30e-05  
[01/19 11:20:21] lb.utils.events INFO:  eta: 0:03:12  iteration: 65/1000  consumed samples: 1040  total_loss: 10.14  time: 0.2035(78.61)  data_time: 0.0029  lr: 6.40e-05  
[01/19 11:20:21] lb.utils.events INFO:  eta: 0:03:12  iteration: 66/1000  consumed samples: 1056  total_loss: 10.14  time: 0.2035(78.62)  data_time: 0.0027  lr: 6.50e-05  
[01/19 11:20:21] lb.utils.events INFO:  eta: 0:03:12  iteration: 67/1000  consumed samples: 1072  total_loss: 10.14  time: 0.2037(78.54)  data_time: 0.0027  lr: 6.60e-05  
[01/19 11:20:21] lb.utils.events INFO:  eta: 0:03:11  iteration: 68/1000  consumed samples: 1088  total_loss: 10.13  time: 0.2037(78.55)  data_time: 0.0027  lr: 6.70e-05  
[01/19 11:20:21] lb.utils.events INFO:  eta: 0:03:11  iteration: 69/1000  consumed samples: 1104  total_loss: 10.12  time: 0.2037(78.53)  data_time: 0.0026  lr: 6.80e-05  
[01/19 11:20:22] lb.utils.events INFO:  eta: 0:03:11  iteration: 70/1000  consumed samples: 1120  total_loss: 10.11  time: 0.2038(78.49)  data_time: 0.0026  lr: 6.90e-05  
[01/19 11:20:22] lb.utils.events INFO:  eta: 0:03:11  iteration: 71/1000  consumed samples: 1136  total_loss: 10.11  time: 0.2039(78.46)  data_time: 0.0026  lr: 7.00e-05  
[01/19 11:20:22] lb.utils.events INFO:  eta: 0:03:11  iteration: 72/1000  consumed samples: 1152  total_loss: 10.11  time: 0.2039(78.48)  data_time: 0.0027  lr: 7.10e-05  
[01/19 11:20:22] lb.utils.events INFO:  eta: 0:03:11  iteration: 73/1000  consumed samples: 1168  total_loss: 10.1  time: 0.2040(78.45)  data_time: 0.0026  lr: 7.20e-05  
[01/19 11:20:23] lb.utils.events INFO:  eta: 0:03:10  iteration: 74/1000  consumed samples: 1184  total_loss: 10.09  time: 0.2035(78.64)  data_time: 0.0026  lr: 7.30e-05  
[01/19 11:20:23] lb.utils.events INFO:  eta: 0:03:10  iteration: 75/1000  consumed samples: 1200  total_loss: 10.09  time: 0.2032(78.75)  data_time: 0.0027  lr: 7.40e-05  
[01/19 11:20:23] lb.utils.events INFO:  eta: 0:03:10  iteration: 76/1000  consumed samples: 1216  total_loss: 10.08  time: 0.2032(78.72)  data_time: 0.0027  lr: 7.50e-05  
[01/19 11:20:23] lb.utils.events INFO:  eta: 0:03:10  iteration: 77/1000  consumed samples: 1232  total_loss: 10.08  time: 0.2032(78.73)  data_time: 0.0027  lr: 7.60e-05  
[01/19 11:20:23] lb.utils.events INFO:  eta: 0:03:09  iteration: 78/1000  consumed samples: 1248  total_loss: 10.06  time: 0.2033(78.69)  data_time: 0.0027  lr: 7.70e-05  
[01/19 11:20:24] lb.utils.events INFO:  eta: 0:03:09  iteration: 79/1000  consumed samples: 1264  total_loss: 10.05  time: 0.2033(78.71)  data_time: 0.0027  lr: 7.80e-05  
[01/19 11:20:24] lb.utils.events INFO:  eta: 0:03:09  iteration: 80/1000  consumed samples: 1280  total_loss: 10  time: 0.2034(78.68)  data_time: 0.0027  lr: 7.90e-05  
[01/19 11:20:24] lb.utils.events INFO:  eta: 0:03:09  iteration: 81/1000  consumed samples: 1296  total_loss: 9.957  time: 0.2034(78.68)  data_time: 0.0027  lr: 8.00e-05  
[01/19 11:20:24] lb.utils.events INFO:  eta: 0:03:09  iteration: 82/1000  consumed samples: 1312  total_loss: 9.956  time: 0.2034(78.65)  data_time: 0.0028  lr: 8.10e-05  
[01/19 11:20:25] lb.utils.events INFO:  eta: 0:03:08  iteration: 83/1000  consumed samples: 1328  total_loss: 9.955  time: 0.2034(78.66)  data_time: 0.0028  lr: 8.20e-05  
[01/19 11:20:25] lb.utils.events INFO:  eta: 0:03:08  iteration: 84/1000  consumed samples: 1344  total_loss: 9.931  time: 0.2036(78.59)  data_time: 0.0028  lr: 8.30e-05  
[01/19 11:20:25] lb.utils.events INFO:  eta: 0:03:08  iteration: 85/1000  consumed samples: 1360  total_loss: 9.907  time: 0.2037(78.57)  data_time: 0.0028  lr: 8.40e-05  
[01/19 11:20:25] lb.utils.events INFO:  eta: 0:03:08  iteration: 86/1000  consumed samples: 1376  total_loss: 9.892  time: 0.2036(78.57)  data_time: 0.0028  lr: 8.50e-05  
[01/19 11:20:25] lb.utils.events INFO:  eta: 0:03:08  iteration: 87/1000  consumed samples: 1392  total_loss: 9.877  time: 0.2037(78.54)  data_time: 0.0028  lr: 8.60e-05  
[01/19 11:20:26] lb.utils.events INFO:  eta: 0:03:08  iteration: 88/1000  consumed samples: 1408  total_loss: 9.874  time: 0.2038(78.52)  data_time: 0.0028  lr: 8.70e-05  
[01/19 11:20:26] lb.utils.events INFO:  eta: 0:03:08  iteration: 89/1000  consumed samples: 1424  total_loss: 9.871  time: 0.2039(78.46)  data_time: 0.0028  lr: 8.80e-05  
[01/19 11:20:26] lb.utils.events INFO:  eta: 0:03:07  iteration: 90/1000  consumed samples: 1440  total_loss: 9.865  time: 0.2039(78.46)  data_time: 0.0028  lr: 8.90e-05  
[01/19 11:20:26] lb.utils.events INFO:  eta: 0:03:07  iteration: 91/1000  consumed samples: 1456  total_loss: 9.858  time: 0.2041(78.40)  data_time: 0.0028  lr: 9.00e-05  
[01/19 11:20:27] lb.utils.events INFO:  eta: 0:03:07  iteration: 92/1000  consumed samples: 1472  total_loss: 9.857  time: 0.2040(78.42)  data_time: 0.0028  lr: 9.10e-05  
[01/19 11:20:27] lb.utils.events INFO:  eta: 0:03:07  iteration: 93/1000  consumed samples: 1488  total_loss: 9.856  time: 0.2041(78.39)  data_time: 0.0028  lr: 9.20e-05  
[01/19 11:20:27] lb.utils.events INFO:  eta: 0:03:06  iteration: 94/1000  consumed samples: 1504  total_loss: 9.853  time: 0.2041(78.40)  data_time: 0.0028  lr: 9.30e-05  
[01/19 11:20:27] lb.utils.events INFO:  eta: 0:03:06  iteration: 95/1000  consumed samples: 1520  total_loss: 9.85  time: 0.2041(78.38)  data_time: 0.0029  lr: 9.40e-05  
[01/19 11:20:27] lb.utils.events INFO:  eta: 0:03:06  iteration: 96/1000  consumed samples: 1536  total_loss: 9.833  time: 0.2041(78.38)  data_time: 0.0029  lr: 9.50e-05  
[01/19 11:20:28] lb.utils.events INFO:  eta: 0:03:06  iteration: 97/1000  consumed samples: 1552  total_loss: 9.816  time: 0.2043(78.33)  data_time: 0.0030  lr: 9.60e-05  
[01/19 11:20:28] lb.utils.events INFO:  eta: 0:03:06  iteration: 98/1000  consumed samples: 1568  total_loss: 9.808  time: 0.2042(78.34)  data_time: 0.0030  lr: 9.70e-05  
[01/19 11:20:28] lb.utils.events INFO:  eta: 0:03:05  iteration: 99/1000  consumed samples: 1584  total_loss: 9.801  time: 0.2043(78.31)  data_time: 0.0030  lr: 9.80e-05  
[01/19 11:20:28] lb.utils.events INFO:  eta: 0:03:05  iteration: 100/1000  consumed samples: 1600  total_loss: 9.779  time: 0.2043(78.32)  data_time: 0.0031  lr: 9.90e-05  
[01/19 11:20:29] lb.utils.events INFO:  eta: 0:03:05  iteration: 101/1000  consumed samples: 1616  total_loss: 9.758  time: 0.2043(78.30)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:20:29] lb.utils.events INFO:  eta: 0:03:05  iteration: 102/1000  consumed samples: 1632  total_loss: 9.754  time: 0.2043(78.31)  data_time: 0.0043  lr: 9.90e-05  
[01/19 11:20:29] lb.utils.events INFO:  eta: 0:03:04  iteration: 103/1000  consumed samples: 1648  total_loss: 9.75  time: 0.2032(78.73)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:29] lb.utils.events INFO:  eta: 0:03:04  iteration: 104/1000  consumed samples: 1664  total_loss: 9.738  time: 0.2033(78.71)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:29] lb.utils.events INFO:  eta: 0:03:04  iteration: 105/1000  consumed samples: 1680  total_loss: 9.726  time: 0.2033(78.69)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:30] lb.utils.events INFO:  eta: 0:03:04  iteration: 106/1000  consumed samples: 1696  total_loss: 9.715  time: 0.2033(78.70)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:30] lb.utils.events INFO:  eta: 0:03:04  iteration: 107/1000  consumed samples: 1712  total_loss: 9.704  time: 0.2034(78.68)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:30] lb.utils.events INFO:  eta: 0:03:03  iteration: 108/1000  consumed samples: 1728  total_loss: 9.689  time: 0.2034(78.68)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:30] lb.utils.events INFO:  eta: 0:03:03  iteration: 109/1000  consumed samples: 1744  total_loss: 9.674  time: 0.2034(78.65)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:31] lb.utils.events INFO:  eta: 0:03:03  iteration: 110/1000  consumed samples: 1760  total_loss: 9.658  time: 0.2034(78.65)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:20:31] lb.utils.events INFO:  eta: 0:03:03  iteration: 111/1000  consumed samples: 1776  total_loss: 9.641  time: 0.2035(78.63)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:20:31] lb.utils.events INFO:  eta: 0:03:03  iteration: 112/1000  consumed samples: 1792  total_loss: 9.608  time: 0.2035(78.63)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:20:31] lb.utils.events INFO:  eta: 0:03:03  iteration: 113/1000  consumed samples: 1808  total_loss: 9.574  time: 0.2035(78.61)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:20:31] lb.utils.events INFO:  eta: 0:03:02  iteration: 114/1000  consumed samples: 1824  total_loss: 9.569  time: 0.2035(78.61)  data_time: 0.0043  lr: 9.90e-05  
[01/19 11:20:32] lb.utils.events INFO:  eta: 0:03:02  iteration: 115/1000  consumed samples: 1840  total_loss: 9.564  time: 0.2036(78.59)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:20:32] lb.utils.events INFO:  eta: 0:03:02  iteration: 116/1000  consumed samples: 1856  total_loss: 9.559  time: 0.2037(78.56)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:20:32] lb.utils.events INFO:  eta: 0:03:02  iteration: 117/1000  consumed samples: 1872  total_loss: 9.555  time: 0.2037(78.54)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:20:32] lb.utils.events INFO:  eta: 0:03:02  iteration: 118/1000  consumed samples: 1888  total_loss: 9.543  time: 0.2037(78.54)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:20:33] lb.utils.events INFO:  eta: 0:03:01  iteration: 119/1000  consumed samples: 1904  total_loss: 9.532  time: 0.2038(78.51)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:20:33] lb.utils.events INFO:  eta: 0:03:01  iteration: 120/1000  consumed samples: 1920  total_loss: 9.509  time: 0.2038(78.52)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:20:33] lb.utils.events INFO:  eta: 0:03:01  iteration: 121/1000  consumed samples: 1936  total_loss: 9.485  time: 0.2038(78.49)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:20:33] lb.utils.events INFO:  eta: 0:03:01  iteration: 122/1000  consumed samples: 1952  total_loss: 9.475  time: 0.2038(78.50)  data_time: 0.0027  lr: 9.90e-05  
[01/19 11:20:33] lb.utils.events INFO:  eta: 0:03:01  iteration: 123/1000  consumed samples: 1968  total_loss: 9.466  time: 0.2039(78.48)  data_time: 0.0028  lr: 9.90e-05  
[01/19 11:20:34] lb.utils.events INFO:  eta: 0:03:00  iteration: 124/1000  consumed samples: 1984  total_loss: 9.46  time: 0.2040(78.43)  data_time: 0.0028  lr: 9.90e-05  
[01/19 11:20:34] lb.utils.events INFO:  eta: 0:03:00  iteration: 125/1000  consumed samples: 2000  total_loss: 9.455  time: 0.2040(78.43)  data_time: 0.0028  lr: 9.90e-05  
[01/19 11:20:34] lb.utils.events INFO:  eta: 0:03:00  iteration: 126/1000  consumed samples: 2016  total_loss: 9.445  time: 0.2041(78.41)  data_time: 0.0028  lr: 9.90e-05  
[01/19 11:20:34] lb.utils.events INFO:  eta: 0:03:00  iteration: 127/1000  consumed samples: 2032  total_loss: 9.434  time: 0.2040(78.41)  data_time: 0.0030  lr: 9.90e-05  
[01/19 11:20:35] lb.utils.events INFO:  eta: 0:03:00  iteration: 128/1000  consumed samples: 2048  total_loss: 9.432  time: 0.2041(78.39)  data_time: 0.0030  lr: 9.90e-05  
[01/19 11:20:35] lb.utils.events INFO:  eta: 0:02:59  iteration: 129/1000  consumed samples: 2064  total_loss: 9.429  time: 0.2041(78.39)  data_time: 0.0030  lr: 9.90e-05  
[01/19 11:20:35] lb.utils.events INFO:  eta: 0:02:59  iteration: 130/1000  consumed samples: 2080  total_loss: 9.423  time: 0.2041(78.37)  data_time: 0.0031  lr: 9.90e-05  
[01/19 11:20:35] lb.utils.events INFO:  eta: 0:02:59  iteration: 131/1000  consumed samples: 2096  total_loss: 9.416  time: 0.2042(78.37)  data_time: 0.0031  lr: 9.90e-05  
[01/19 11:49:18] libai INFO: Rank of current process: 0. World size: 1
[01/19 11:49:18] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 11:49:18] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mbias_dropout_fusion[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 11:49:18] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 11:49:18] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 11:49:18] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 11:49:21] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 11:49:21] libai INFO: Loadding megatron weight
[01/19 11:49:21] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 11:49:24] lb.utils.load_megatron_weight INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbert.encoders.0.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.0.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.0.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.0.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.0.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.0.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.1.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.1.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.1.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.1.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.1.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.1.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.2.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.2.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.2.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.2.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.2.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.2.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.3.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.3.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.3.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.3.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.3.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.3.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.4.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.4.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.4.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.4.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.4.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.4.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.final_layernorm.{weight, bias}[0m
[01/19 11:49:24] lb.utils.load_megatron_weight INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mbert.encoder.layers_0.input_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_0.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoder.layers_0.self_attention.dense.{weight, bias}[0m
  [35mbert.encoder.layers_0.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_0.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoder.layers_0.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoder.layers_1.input_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_1.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoder.layers_1.self_attention.dense.{weight, bias}[0m
  [35mbert.encoder.layers_1.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_1.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoder.layers_1.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoder.layers_2.input_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_2.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoder.layers_2.self_attention.dense.{weight, bias}[0m
  [35mbert.encoder.layers_2.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_2.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoder.layers_2.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoder.layers_3.input_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_3.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoder.layers_3.self_attention.dense.{weight, bias}[0m
  [35mbert.encoder.layers_3.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_3.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoder.layers_3.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoder.layers_4.input_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_4.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoder.layers_4.self_attention.dense.{weight, bias}[0m
  [35mbert.encoder.layers_4.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoder.layers_4.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoder.layers_4.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoder.final_layernorm.{weight, bias}[0m
[01/19 11:49:24] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 11:49:24] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 11:49:24] lb.data.build INFO:     train:      16000
[01/19 11:49:24] lb.data.build INFO:     validation: 160000
[01/19 11:49:24] lb.data.build INFO:     test:       160000
[01/19 11:49:24] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 11:49:24] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     reading document index...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 11:49:24] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 11:49:24] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.137605 seconds
[01/19 11:49:24] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 11:49:24] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 11:49:24] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 11:49:24] lb.data.dataset_utils INFO:  > dataset split:
[01/19 11:49:24] lb.data.dataset_utils INFO:     train:
[01/19 11:49:24] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 11:49:24] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 11:49:24] lb.data.dataset_utils INFO:     validation:
[01/19 11:49:24] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 11:49:24] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 11:49:24] lb.data.dataset_utils INFO:     test:
[01/19 11:49:24] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 11:49:24] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 11:49:24] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 11:49:24] lb.data.dataset_utils INFO:     loaded indexed file in 0.006 seconds
[01/19 11:49:24] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 11:49:24] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:49:24] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:49:24] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 11:49:24] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:49:24] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:49:24] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 11:49:24] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 11:49:25] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 11:49:29] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 10.73  data_time: 0.1318  lr: 0.00e+00  
[01/19 11:49:29] lb.utils.events INFO:  eta: 0:03:54  iteration: 2/1000  consumed samples: 32  total_loss: 10.74  data_time: 0.0697  lr: 1.00e-06  
[01/19 11:49:30] lb.utils.events INFO:  eta: 0:03:15  iteration: 3/1000  consumed samples: 48  total_loss: 10.73  time: 0.1966(81.38)  data_time: 0.0474  lr: 2.00e-06  
[01/19 11:49:30] lb.utils.events INFO:  eta: 0:03:20  iteration: 4/1000  consumed samples: 64  total_loss: 10.72  time: 0.2020(79.22)  data_time: 0.0360  lr: 3.00e-06  
[01/19 11:49:30] lb.utils.events INFO:  eta: 0:03:17  iteration: 5/1000  consumed samples: 80  total_loss: 10.7  time: 0.2009(79.64)  data_time: 0.0296  lr: 4.00e-06  
[01/19 11:49:30] lb.utils.events INFO:  eta: 0:03:21  iteration: 6/1000  consumed samples: 96  total_loss: 10.65  time: 0.2043(78.32)  data_time: 0.0256  lr: 5.00e-06  
[01/19 11:49:31] lb.utils.events INFO:  eta: 0:03:18  iteration: 7/1000  consumed samples: 112  total_loss: 10.6  time: 0.2034(78.67)  data_time: 0.0223  lr: 6.00e-06  
[01/19 11:49:31] lb.utils.events INFO:  eta: 0:03:21  iteration: 8/1000  consumed samples: 128  total_loss: 10.51  time: 0.2052(77.98)  data_time: 0.0198  lr: 7.00e-06  
[01/19 11:49:31] lb.utils.events INFO:  eta: 0:03:17  iteration: 9/1000  consumed samples: 144  total_loss: 10.41  time: 0.2044(78.29)  data_time: 0.0181  lr: 8.00e-06  
[01/19 11:49:31] lb.utils.events INFO:  eta: 0:03:21  iteration: 10/1000  consumed samples: 160  total_loss: 10.33  time: 0.2057(77.79)  data_time: 0.0170  lr: 9.00e-06  
[01/19 11:49:31] lb.utils.events INFO:  eta: 0:03:24  iteration: 11/1000  consumed samples: 176  total_loss: 10.26  time: 0.2058(77.76)  data_time: 0.0156  lr: 1.00e-05  
[01/19 11:49:32] lb.utils.events INFO:  eta: 0:03:23  iteration: 12/1000  consumed samples: 192  total_loss: 10.14  time: 0.2059(77.72)  data_time: 0.0145  lr: 1.10e-05  
[01/19 11:49:32] lb.utils.events INFO:  eta: 0:03:23  iteration: 13/1000  consumed samples: 208  total_loss: 10.01  time: 0.2054(77.91)  data_time: 0.0137  lr: 1.20e-05  
[01/19 11:49:32] lb.utils.events INFO:  eta: 0:03:23  iteration: 14/1000  consumed samples: 224  total_loss: 9.881  time: 0.2055(77.84)  data_time: 0.0132  lr: 1.30e-05  
[01/19 11:49:32] lb.utils.events INFO:  eta: 0:03:23  iteration: 15/1000  consumed samples: 240  total_loss: 9.749  time: 0.2057(77.77)  data_time: 0.0125  lr: 1.40e-05  
[01/19 11:49:33] lb.utils.events INFO:  eta: 0:03:23  iteration: 16/1000  consumed samples: 256  total_loss: 9.639  time: 0.2058(77.73)  data_time: 0.0119  lr: 1.50e-05  
[01/19 11:49:33] lb.utils.events INFO:  eta: 0:03:23  iteration: 17/1000  consumed samples: 272  total_loss: 9.53  time: 0.2055(77.86)  data_time: 0.0114  lr: 1.60e-05  
[01/19 11:49:33] lb.utils.events INFO:  eta: 0:03:22  iteration: 18/1000  consumed samples: 288  total_loss: 9.496  time: 0.2056(77.82)  data_time: 0.0112  lr: 1.70e-05  
[01/19 11:49:33] lb.utils.events INFO:  eta: 0:03:22  iteration: 19/1000  consumed samples: 304  total_loss: 9.461  time: 0.2053(77.94)  data_time: 0.0108  lr: 1.80e-05  
[01/19 11:49:33] lb.utils.events INFO:  eta: 0:03:22  iteration: 20/1000  consumed samples: 320  total_loss: 9.399  time: 0.2018(79.27)  data_time: 0.0104  lr: 1.90e-05  
[01/19 11:49:34] lb.utils.events INFO:  eta: 0:03:21  iteration: 21/1000  consumed samples: 336  total_loss: 9.337  time: 0.2017(79.31)  data_time: 0.0040  lr: 2.00e-05  
[01/19 11:49:34] lb.utils.events INFO:  eta: 0:03:21  iteration: 22/1000  consumed samples: 352  total_loss: 9.231  time: 0.2020(79.21)  data_time: 0.0041  lr: 2.10e-05  
[01/19 11:49:34] lb.utils.events INFO:  eta: 0:03:21  iteration: 23/1000  consumed samples: 368  total_loss: 9.124  time: 0.2019(79.25)  data_time: 0.0041  lr: 2.20e-05  
[01/19 11:49:34] lb.utils.events INFO:  eta: 0:03:21  iteration: 24/1000  consumed samples: 384  total_loss: 9.098  time: 0.2021(79.16)  data_time: 0.0042  lr: 2.30e-05  
[01/19 11:49:35] lb.utils.events INFO:  eta: 0:03:21  iteration: 25/1000  consumed samples: 400  total_loss: 9.072  time: 0.2021(79.18)  data_time: 0.0042  lr: 2.40e-05  
[01/19 11:49:35] lb.utils.events INFO:  eta: 0:03:21  iteration: 26/1000  consumed samples: 416  total_loss: 9.058  time: 0.2023(79.09)  data_time: 0.0043  lr: 2.50e-05  
[01/19 11:49:35] lb.utils.events INFO:  eta: 0:03:20  iteration: 27/1000  consumed samples: 432  total_loss: 9.044  time: 0.2023(79.10)  data_time: 0.0044  lr: 2.60e-05  
[01/19 11:49:35] lb.utils.events INFO:  eta: 0:03:20  iteration: 28/1000  consumed samples: 448  total_loss: 9.02  time: 0.2025(79.02)  data_time: 0.0044  lr: 2.70e-05  
[01/19 11:49:35] lb.utils.events INFO:  eta: 0:03:20  iteration: 29/1000  consumed samples: 464  total_loss: 8.995  time: 0.2024(79.04)  data_time: 0.0044  lr: 2.80e-05  
[01/19 11:49:36] lb.utils.events INFO:  eta: 0:03:20  iteration: 30/1000  consumed samples: 480  total_loss: 8.938  time: 0.2026(78.97)  data_time: 0.0044  lr: 2.90e-05  
[01/19 11:49:36] lb.utils.events INFO:  eta: 0:03:20  iteration: 31/1000  consumed samples: 496  total_loss: 8.881  time: 0.2028(78.88)  data_time: 0.0044  lr: 3.00e-05  
[01/19 11:49:36] lb.utils.events INFO:  eta: 0:03:19  iteration: 32/1000  consumed samples: 512  total_loss: 8.87  time: 0.2027(78.93)  data_time: 0.0044  lr: 3.10e-05  
[01/19 11:49:36] lb.utils.events INFO:  eta: 0:03:19  iteration: 33/1000  consumed samples: 528  total_loss: 8.858  time: 0.2026(78.97)  data_time: 0.0044  lr: 3.20e-05  
[01/19 11:49:37] lb.utils.events INFO:  eta: 0:03:19  iteration: 34/1000  consumed samples: 544  total_loss: 8.843  time: 0.2027(78.92)  data_time: 0.0044  lr: 3.30e-05  
[01/19 11:49:37] lb.utils.events INFO:  eta: 0:03:19  iteration: 35/1000  consumed samples: 560  total_loss: 8.829  time: 0.2026(78.98)  data_time: 0.0044  lr: 3.40e-05  
[01/19 11:49:37] lb.utils.events INFO:  eta: 0:03:16  iteration: 36/1000  consumed samples: 576  total_loss: 8.803  time: 0.2025(79.01)  data_time: 0.0044  lr: 3.50e-05  
[01/19 11:49:37] lb.utils.events INFO:  eta: 0:03:14  iteration: 37/1000  consumed samples: 592  total_loss: 8.777  time: 0.2025(79.02)  data_time: 0.0044  lr: 3.60e-05  
[01/19 11:49:37] lb.utils.events INFO:  eta: 0:03:16  iteration: 38/1000  consumed samples: 608  total_loss: 8.776  time: 0.2026(78.96)  data_time: 0.0068  lr: 3.70e-05  
[01/19 11:49:38] lb.utils.events INFO:  eta: 0:03:13  iteration: 39/1000  consumed samples: 624  total_loss: 8.776  time: 0.2026(78.97)  data_time: 0.0068  lr: 3.80e-05  
[01/19 11:49:38] lb.utils.events INFO:  eta: 0:03:13  iteration: 40/1000  consumed samples: 640  total_loss: 8.766  time: 0.2025(79.00)  data_time: 0.0068  lr: 3.90e-05  
[01/19 11:49:38] lb.utils.events INFO:  eta: 0:03:13  iteration: 41/1000  consumed samples: 656  total_loss: 8.756  time: 0.2025(79.01)  data_time: 0.0067  lr: 4.00e-05  
[01/19 11:49:38] lb.utils.events INFO:  eta: 0:03:12  iteration: 42/1000  consumed samples: 672  total_loss: 8.745  time: 0.2026(78.97)  data_time: 0.0065  lr: 4.10e-05  
[01/19 11:49:39] lb.utils.events INFO:  eta: 0:03:12  iteration: 43/1000  consumed samples: 688  total_loss: 8.733  time: 0.2026(78.98)  data_time: 0.0065  lr: 4.20e-05  
[01/19 11:49:39] lb.utils.events INFO:  eta: 0:03:12  iteration: 44/1000  consumed samples: 704  total_loss: 8.727  time: 0.2027(78.93)  data_time: 0.0065  lr: 4.30e-05  
[01/19 11:49:39] lb.utils.events INFO:  eta: 0:03:12  iteration: 45/1000  consumed samples: 720  total_loss: 8.72  time: 0.2027(78.94)  data_time: 0.0064  lr: 4.40e-05  
[01/19 11:49:39] lb.utils.events INFO:  eta: 0:03:12  iteration: 46/1000  consumed samples: 736  total_loss: 8.707  time: 0.2026(78.97)  data_time: 0.0064  lr: 4.50e-05  
[01/19 11:49:39] lb.utils.events INFO:  eta: 0:03:11  iteration: 47/1000  consumed samples: 752  total_loss: 8.694  time: 0.2026(78.99)  data_time: 0.0063  lr: 4.60e-05  
[01/19 11:49:40] lb.utils.events INFO:  eta: 0:03:11  iteration: 48/1000  consumed samples: 768  total_loss: 8.69  time: 0.2027(78.95)  data_time: 0.0063  lr: 4.70e-05  
[01/19 11:49:40] lb.utils.events INFO:  eta: 0:03:11  iteration: 49/1000  consumed samples: 784  total_loss: 8.686  time: 0.2028(78.91)  data_time: 0.0063  lr: 4.80e-05  
[01/19 11:49:40] lb.utils.events INFO:  eta: 0:03:11  iteration: 50/1000  consumed samples: 800  total_loss: 8.667  time: 0.2029(78.86)  data_time: 0.0062  lr: 4.90e-05  
[01/19 11:49:40] lb.utils.events INFO:  eta: 0:03:11  iteration: 51/1000  consumed samples: 816  total_loss: 8.647  time: 0.2029(78.86)  data_time: 0.0062  lr: 5.00e-05  
[01/19 11:49:40] lb.utils.events INFO:  eta: 0:03:11  iteration: 52/1000  consumed samples: 832  total_loss: 8.644  time: 0.2030(78.82)  data_time: 0.0062  lr: 5.10e-05  
[01/19 11:49:41] lb.utils.events INFO:  eta: 0:03:10  iteration: 53/1000  consumed samples: 848  total_loss: 8.641  time: 0.2029(78.85)  data_time: 0.0062  lr: 5.20e-05  
[01/19 11:49:41] lb.utils.events INFO:  eta: 0:03:10  iteration: 54/1000  consumed samples: 864  total_loss: 8.634  time: 0.2030(78.81)  data_time: 0.0063  lr: 5.30e-05  
[01/19 11:49:41] lb.utils.events INFO:  eta: 0:03:10  iteration: 55/1000  consumed samples: 880  total_loss: 8.627  time: 0.2030(78.82)  data_time: 0.0062  lr: 5.40e-05  
[01/19 11:49:41] lb.utils.events INFO:  eta: 0:03:10  iteration: 56/1000  consumed samples: 896  total_loss: 8.625  time: 0.2031(78.78)  data_time: 0.0063  lr: 5.50e-05  
[01/19 11:49:42] lb.utils.events INFO:  eta: 0:03:10  iteration: 57/1000  consumed samples: 912  total_loss: 8.622  time: 0.2031(78.79)  data_time: 0.0067  lr: 5.60e-05  
[01/19 11:49:42] lb.utils.events INFO:  eta: 0:03:10  iteration: 58/1000  consumed samples: 928  total_loss: 8.619  time: 0.2032(78.76)  data_time: 0.0041  lr: 5.70e-05  
[01/19 11:49:42] lb.utils.events INFO:  eta: 0:03:10  iteration: 59/1000  consumed samples: 944  total_loss: 8.615  time: 0.2031(78.76)  data_time: 0.0041  lr: 5.80e-05  
[01/19 11:49:42] lb.utils.events INFO:  eta: 0:03:10  iteration: 60/1000  consumed samples: 960  total_loss: 8.612  time: 0.2032(78.73)  data_time: 0.0041  lr: 5.90e-05  
[01/19 11:49:42] lb.utils.events INFO:  eta: 0:03:09  iteration: 61/1000  consumed samples: 976  total_loss: 8.609  time: 0.2032(78.74)  data_time: 0.0042  lr: 6.00e-05  
[01/19 11:49:43] lb.utils.events INFO:  eta: 0:03:09  iteration: 62/1000  consumed samples: 992  total_loss: 8.605  time: 0.2033(78.71)  data_time: 0.0043  lr: 6.10e-05  
[01/19 11:49:43] lb.utils.events INFO:  eta: 0:03:09  iteration: 63/1000  consumed samples: 1008  total_loss: 8.6  time: 0.2033(78.71)  data_time: 0.0043  lr: 6.20e-05  
[01/19 11:49:43] lb.utils.events INFO:  eta: 0:03:09  iteration: 64/1000  consumed samples: 1024  total_loss: 8.598  time: 0.2034(78.68)  data_time: 0.0042  lr: 6.30e-05  
[01/19 11:49:43] lb.utils.events INFO:  eta: 0:03:09  iteration: 65/1000  consumed samples: 1040  total_loss: 8.595  time: 0.2033(78.68)  data_time: 0.0041  lr: 6.40e-05  
[01/19 11:49:44] lb.utils.events INFO:  eta: 0:03:08  iteration: 66/1000  consumed samples: 1056  total_loss: 8.594  time: 0.2034(78.64)  data_time: 0.0041  lr: 6.50e-05  
[01/19 11:49:44] lb.utils.events INFO:  eta: 0:03:08  iteration: 67/1000  consumed samples: 1072  total_loss: 8.592  time: 0.2034(78.66)  data_time: 0.0041  lr: 6.60e-05  
[01/19 11:49:44] lb.utils.events INFO:  eta: 0:03:08  iteration: 68/1000  consumed samples: 1088  total_loss: 8.582  time: 0.2036(78.58)  data_time: 0.0041  lr: 6.70e-05  
[01/19 11:49:44] lb.utils.events INFO:  eta: 0:03:08  iteration: 69/1000  consumed samples: 1104  total_loss: 8.572  time: 0.2036(78.59)  data_time: 0.0041  lr: 6.80e-05  
[01/19 11:49:44] lb.utils.events INFO:  eta: 0:03:08  iteration: 70/1000  consumed samples: 1120  total_loss: 8.557  time: 0.2036(78.57)  data_time: 0.0042  lr: 6.90e-05  
[01/19 11:49:45] lb.utils.events INFO:  eta: 0:03:07  iteration: 71/1000  consumed samples: 1136  total_loss: 8.542  time: 0.2036(78.58)  data_time: 0.0042  lr: 7.00e-05  
[01/19 11:49:45] lb.utils.events INFO:  eta: 0:03:07  iteration: 72/1000  consumed samples: 1152  total_loss: 8.54  time: 0.2037(78.55)  data_time: 0.0041  lr: 7.10e-05  
[01/19 11:49:45] lb.utils.events INFO:  eta: 0:03:07  iteration: 73/1000  consumed samples: 1168  total_loss: 8.538  time: 0.2039(78.48)  data_time: 0.0041  lr: 7.20e-05  
[01/19 11:49:45] lb.utils.events INFO:  eta: 0:03:09  iteration: 74/1000  consumed samples: 1184  total_loss: 8.527  time: 0.2039(78.45)  data_time: 0.0040  lr: 7.30e-05  
[01/19 11:49:46] lb.utils.events INFO:  eta: 0:03:10  iteration: 75/1000  consumed samples: 1200  total_loss: 8.516  time: 0.2040(78.42)  data_time: 0.0040  lr: 7.40e-05  
[01/19 11:49:46] lb.utils.events INFO:  eta: 0:03:08  iteration: 76/1000  consumed samples: 1216  total_loss: 8.51  time: 0.2040(78.43)  data_time: 0.0040  lr: 7.50e-05  
[01/19 11:49:46] lb.utils.events INFO:  eta: 0:03:06  iteration: 77/1000  consumed samples: 1232  total_loss: 8.505  time: 0.2040(78.44)  data_time: 0.0036  lr: 7.60e-05  
[01/19 11:49:46] lb.utils.events INFO:  eta: 0:03:06  iteration: 78/1000  consumed samples: 1248  total_loss: 8.492  time: 0.2039(78.48)  data_time: 0.0037  lr: 7.70e-05  
[01/19 11:49:46] lb.utils.events INFO:  eta: 0:03:06  iteration: 79/1000  consumed samples: 1264  total_loss: 8.479  time: 0.2039(78.49)  data_time: 0.0037  lr: 7.80e-05  
[01/19 11:49:47] lb.utils.events INFO:  eta: 0:03:06  iteration: 80/1000  consumed samples: 1280  total_loss: 8.474  time: 0.2039(78.46)  data_time: 0.0037  lr: 7.90e-05  
[01/19 11:49:47] lb.utils.events INFO:  eta: 0:03:06  iteration: 81/1000  consumed samples: 1296  total_loss: 8.47  time: 0.2039(78.47)  data_time: 0.0037  lr: 8.00e-05  
[01/19 11:49:47] lb.utils.events INFO:  eta: 0:03:05  iteration: 82/1000  consumed samples: 1312  total_loss: 8.459  time: 0.2040(78.44)  data_time: 0.0036  lr: 8.10e-05  
[01/19 11:49:47] lb.utils.events INFO:  eta: 0:03:05  iteration: 83/1000  consumed samples: 1328  total_loss: 8.448  time: 0.2040(78.43)  data_time: 0.0037  lr: 8.20e-05  
[01/19 11:49:48] lb.utils.events INFO:  eta: 0:03:07  iteration: 84/1000  consumed samples: 1344  total_loss: 8.446  time: 0.2041(78.40)  data_time: 0.0037  lr: 8.30e-05  
[01/19 11:49:48] lb.utils.events INFO:  eta: 0:03:05  iteration: 85/1000  consumed samples: 1360  total_loss: 8.445  time: 0.2041(78.40)  data_time: 0.0038  lr: 8.40e-05  
[01/19 11:49:48] lb.utils.events INFO:  eta: 0:03:07  iteration: 86/1000  consumed samples: 1376  total_loss: 8.444  time: 0.2041(78.38)  data_time: 0.0038  lr: 8.50e-05  
[01/19 11:49:48] lb.utils.events INFO:  eta: 0:03:05  iteration: 87/1000  consumed samples: 1392  total_loss: 8.443  time: 0.2041(78.39)  data_time: 0.0038  lr: 8.60e-05  
[01/19 11:49:48] lb.utils.events INFO:  eta: 0:03:06  iteration: 88/1000  consumed samples: 1408  total_loss: 8.436  time: 0.2042(78.36)  data_time: 0.0038  lr: 8.70e-05  
[01/19 11:49:49] lb.utils.events INFO:  eta: 0:03:04  iteration: 89/1000  consumed samples: 1424  total_loss: 8.428  time: 0.2042(78.37)  data_time: 0.0038  lr: 8.80e-05  
[01/19 11:49:49] lb.utils.events INFO:  eta: 0:03:06  iteration: 90/1000  consumed samples: 1440  total_loss: 8.422  time: 0.2042(78.35)  data_time: 0.0038  lr: 8.90e-05  
[01/19 11:49:49] lb.utils.events INFO:  eta: 0:03:04  iteration: 91/1000  consumed samples: 1456  total_loss: 8.415  time: 0.2042(78.36)  data_time: 0.0037  lr: 9.00e-05  
[01/19 11:49:49] lb.utils.events INFO:  eta: 0:03:05  iteration: 92/1000  consumed samples: 1472  total_loss: 8.405  time: 0.2042(78.34)  data_time: 0.0038  lr: 9.10e-05  
[01/19 11:49:50] lb.utils.events INFO:  eta: 0:03:04  iteration: 93/1000  consumed samples: 1488  total_loss: 8.394  time: 0.2042(78.34)  data_time: 0.0039  lr: 9.20e-05  
[01/19 11:49:50] lb.utils.events INFO:  eta: 0:03:05  iteration: 94/1000  consumed samples: 1504  total_loss: 8.394  time: 0.2043(78.32)  data_time: 0.0039  lr: 9.30e-05  
[01/19 11:49:50] lb.utils.events INFO:  eta: 0:03:03  iteration: 95/1000  consumed samples: 1520  total_loss: 8.393  time: 0.2043(78.33)  data_time: 0.0039  lr: 9.40e-05  
[01/19 11:49:50] lb.utils.events INFO:  eta: 0:03:05  iteration: 96/1000  consumed samples: 1536  total_loss: 8.39  time: 0.2043(78.30)  data_time: 0.0039  lr: 9.50e-05  
[01/19 11:49:50] lb.utils.events INFO:  eta: 0:03:03  iteration: 97/1000  consumed samples: 1552  total_loss: 8.387  time: 0.2043(78.31)  data_time: 0.0039  lr: 9.60e-05  
[01/19 11:49:51] lb.utils.events INFO:  eta: 0:03:04  iteration: 98/1000  consumed samples: 1568  total_loss: 8.385  time: 0.2044(78.29)  data_time: 0.0039  lr: 9.70e-05  
[01/19 11:49:51] lb.utils.events INFO:  eta: 0:03:05  iteration: 99/1000  consumed samples: 1584  total_loss: 8.383  time: 0.2044(78.27)  data_time: 0.0040  lr: 9.80e-05  
[01/19 11:49:51] lb.utils.events INFO:  eta: 0:03:05  iteration: 100/1000  consumed samples: 1600  total_loss: 8.362  time: 0.2045(78.25)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:51] lb.utils.events INFO:  eta: 0:03:05  iteration: 101/1000  consumed samples: 1616  total_loss: 8.341  time: 0.2045(78.25)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:49:52] lb.utils.events INFO:  eta: 0:03:05  iteration: 102/1000  consumed samples: 1632  total_loss: 8.33  time: 0.2045(78.24)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:49:52] lb.utils.events INFO:  eta: 0:03:05  iteration: 103/1000  consumed samples: 1648  total_loss: 8.319  time: 0.2046(78.22)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:49:52] lb.utils.events INFO:  eta: 0:03:04  iteration: 104/1000  consumed samples: 1664  total_loss: 8.318  time: 0.2045(78.23)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:49:52] lb.utils.events INFO:  eta: 0:03:04  iteration: 105/1000  consumed samples: 1680  total_loss: 8.318  time: 0.2046(78.21)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:49:52] lb.utils.events INFO:  eta: 0:03:04  iteration: 106/1000  consumed samples: 1696  total_loss: 8.318  time: 0.2046(78.22)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:49:53] lb.utils.events INFO:  eta: 0:03:04  iteration: 107/1000  consumed samples: 1712  total_loss: 8.318  time: 0.2046(78.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:49:53] lb.utils.events INFO:  eta: 0:03:04  iteration: 108/1000  consumed samples: 1728  total_loss: 8.317  time: 0.2046(78.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:49:53] lb.utils.events INFO:  eta: 0:03:04  iteration: 109/1000  consumed samples: 1744  total_loss: 8.316  time: 0.2046(78.18)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:53] lb.utils.events INFO:  eta: 0:03:03  iteration: 110/1000  consumed samples: 1760  total_loss: 8.316  time: 0.2046(78.19)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:54] lb.utils.events INFO:  eta: 0:03:03  iteration: 111/1000  consumed samples: 1776  total_loss: 8.316  time: 0.2047(78.17)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:54] lb.utils.events INFO:  eta: 0:03:03  iteration: 112/1000  consumed samples: 1792  total_loss: 8.307  time: 0.2047(78.18)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:54] lb.utils.events INFO:  eta: 0:03:03  iteration: 113/1000  consumed samples: 1808  total_loss: 8.299  time: 0.2047(78.16)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:54] lb.utils.events INFO:  eta: 0:03:02  iteration: 114/1000  consumed samples: 1824  total_loss: 8.293  time: 0.2047(78.16)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:55] lb.utils.events INFO:  eta: 0:03:02  iteration: 115/1000  consumed samples: 1840  total_loss: 8.288  time: 0.2047(78.15)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:55] lb.utils.events INFO:  eta: 0:03:02  iteration: 116/1000  consumed samples: 1856  total_loss: 8.286  time: 0.2047(78.15)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:55] lb.utils.events INFO:  eta: 0:03:02  iteration: 117/1000  consumed samples: 1872  total_loss: 8.284  time: 0.2048(78.13)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:55] lb.utils.events INFO:  eta: 0:03:02  iteration: 118/1000  consumed samples: 1888  total_loss: 8.281  time: 0.2048(78.14)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:55] lb.utils.events INFO:  eta: 0:03:01  iteration: 119/1000  consumed samples: 1904  total_loss: 8.277  time: 0.2048(78.12)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:56] lb.utils.events INFO:  eta: 0:03:01  iteration: 120/1000  consumed samples: 1920  total_loss: 8.274  time: 0.2048(78.13)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:49:56] lb.utils.events INFO:  eta: 0:03:01  iteration: 121/1000  consumed samples: 1936  total_loss: 8.27  time: 0.2048(78.11)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:56] lb.utils.events INFO:  eta: 0:03:01  iteration: 122/1000  consumed samples: 1952  total_loss: 8.268  time: 0.2048(78.11)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:56] lb.utils.events INFO:  eta: 0:03:01  iteration: 123/1000  consumed samples: 1968  total_loss: 8.266  time: 0.2049(78.10)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:57] lb.utils.events INFO:  eta: 0:03:00  iteration: 124/1000  consumed samples: 1984  total_loss: 8.266  time: 0.2049(78.08)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:57] lb.utils.events INFO:  eta: 0:03:00  iteration: 125/1000  consumed samples: 2000  total_loss: 8.266  time: 0.2050(78.06)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:57] lb.utils.events INFO:  eta: 0:03:00  iteration: 126/1000  consumed samples: 2016  total_loss: 8.252  time: 0.2050(78.06)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:57] lb.utils.events INFO:  eta: 0:03:00  iteration: 127/1000  consumed samples: 2032  total_loss: 8.238  time: 0.2050(78.05)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:49:57] lb.utils.events INFO:  eta: 0:03:00  iteration: 128/1000  consumed samples: 2048  total_loss: 8.236  time: 0.2050(78.05)  data_time: 0.0049  lr: 9.90e-05  
[01/19 11:49:58] lb.utils.events INFO:  eta: 0:02:59  iteration: 129/1000  consumed samples: 2064  total_loss: 8.234  time: 0.2050(78.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:49:58] lb.utils.events INFO:  eta: 0:02:59  iteration: 130/1000  consumed samples: 2080  total_loss: 8.232  time: 0.2050(78.04)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:58] lb.utils.events INFO:  eta: 0:02:59  iteration: 131/1000  consumed samples: 2096  total_loss: 8.229  time: 0.2051(78.02)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:58] lb.utils.events INFO:  eta: 0:02:59  iteration: 132/1000  consumed samples: 2112  total_loss: 8.228  time: 0.2051(78.02)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:59] lb.utils.events INFO:  eta: 0:02:59  iteration: 133/1000  consumed samples: 2128  total_loss: 8.226  time: 0.2051(78.01)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:59] lb.utils.events INFO:  eta: 0:02:58  iteration: 134/1000  consumed samples: 2144  total_loss: 8.217  time: 0.2051(78.01)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:59] lb.utils.events INFO:  eta: 0:02:58  iteration: 135/1000  consumed samples: 2160  total_loss: 8.208  time: 0.2051(77.99)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:49:59] lb.utils.events INFO:  eta: 0:02:58  iteration: 136/1000  consumed samples: 2176  total_loss: 8.207  time: 0.2051(77.99)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:49:59] lb.utils.events INFO:  eta: 0:02:58  iteration: 137/1000  consumed samples: 2192  total_loss: 8.206  time: 0.2052(77.98)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:00] lb.utils.events INFO:  eta: 0:02:58  iteration: 138/1000  consumed samples: 2208  total_loss: 8.203  time: 0.2052(77.98)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:00] lb.utils.events INFO:  eta: 0:02:57  iteration: 139/1000  consumed samples: 2224  total_loss: 8.2  time: 0.2052(77.97)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:00] lb.utils.events INFO:  eta: 0:02:57  iteration: 140/1000  consumed samples: 2240  total_loss: 8.198  time: 0.2052(77.97)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:00] lb.utils.events INFO:  eta: 0:02:57  iteration: 141/1000  consumed samples: 2256  total_loss: 8.195  time: 0.2053(77.95)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:01] lb.utils.events INFO:  eta: 0:02:57  iteration: 142/1000  consumed samples: 2272  total_loss: 8.195  time: 0.2052(77.95)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:01] lb.utils.events INFO:  eta: 0:02:57  iteration: 143/1000  consumed samples: 2288  total_loss: 8.194  time: 0.2053(77.94)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:01] lb.utils.events INFO:  eta: 0:02:56  iteration: 144/1000  consumed samples: 2304  total_loss: 8.191  time: 0.2053(77.94)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:01] lb.utils.events INFO:  eta: 0:02:56  iteration: 145/1000  consumed samples: 2320  total_loss: 8.187  time: 0.2053(77.92)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:01] lb.utils.events INFO:  eta: 0:02:56  iteration: 146/1000  consumed samples: 2336  total_loss: 8.191  time: 0.2053(77.93)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:02] lb.utils.events INFO:  eta: 0:02:56  iteration: 147/1000  consumed samples: 2352  total_loss: 8.187  time: 0.2053(77.93)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:02] lb.utils.events INFO:  eta: 0:02:56  iteration: 148/1000  consumed samples: 2368  total_loss: 8.186  time: 0.2053(77.92)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:02] lb.utils.events INFO:  eta: 0:02:55  iteration: 149/1000  consumed samples: 2384  total_loss: 8.185  time: 0.2054(77.90)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:02] lb.utils.events INFO:  eta: 0:02:55  iteration: 150/1000  consumed samples: 2400  total_loss: 8.182  time: 0.2054(77.89)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:03] lb.utils.events INFO:  eta: 0:02:55  iteration: 151/1000  consumed samples: 2416  total_loss: 8.18  time: 0.2054(77.89)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:03] lb.utils.events INFO:  eta: 0:02:55  iteration: 152/1000  consumed samples: 2432  total_loss: 8.176  time: 0.2054(77.88)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:03] lb.utils.events INFO:  eta: 0:02:55  iteration: 153/1000  consumed samples: 2448  total_loss: 8.172  time: 0.2054(77.89)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:03] lb.utils.events INFO:  eta: 0:02:54  iteration: 154/1000  consumed samples: 2464  total_loss: 8.17  time: 0.2055(77.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:03] lb.utils.events INFO:  eta: 0:02:54  iteration: 155/1000  consumed samples: 2480  total_loss: 8.169  time: 0.2055(77.87)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:04] lb.utils.events INFO:  eta: 0:02:54  iteration: 156/1000  consumed samples: 2496  total_loss: 8.167  time: 0.2055(77.88)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:04] lb.utils.events INFO:  eta: 0:02:54  iteration: 157/1000  consumed samples: 2512  total_loss: 8.166  time: 0.2055(77.86)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:04] lb.utils.events INFO:  eta: 0:02:54  iteration: 158/1000  consumed samples: 2528  total_loss: 8.163  time: 0.2055(77.85)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:04] lb.utils.events INFO:  eta: 0:02:53  iteration: 159/1000  consumed samples: 2544  total_loss: 8.159  time: 0.2056(77.83)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:05] lb.utils.events INFO:  eta: 0:02:53  iteration: 160/1000  consumed samples: 2560  total_loss: 8.157  time: 0.2056(77.83)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:05] lb.utils.events INFO:  eta: 0:02:53  iteration: 161/1000  consumed samples: 2576  total_loss: 8.155  time: 0.2056(77.83)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:05] lb.utils.events INFO:  eta: 0:02:53  iteration: 162/1000  consumed samples: 2592  total_loss: 8.154  time: 0.2056(77.82)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:05] lb.utils.events INFO:  eta: 0:02:52  iteration: 163/1000  consumed samples: 2608  total_loss: 8.153  time: 0.2056(77.82)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:05] lb.utils.events INFO:  eta: 0:02:52  iteration: 164/1000  consumed samples: 2624  total_loss: 8.153  time: 0.2057(77.80)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:06] lb.utils.events INFO:  eta: 0:02:52  iteration: 165/1000  consumed samples: 2640  total_loss: 8.152  time: 0.2056(77.80)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:06] lb.utils.events INFO:  eta: 0:02:52  iteration: 166/1000  consumed samples: 2656  total_loss: 8.146  time: 0.2057(77.79)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:06] lb.utils.events INFO:  eta: 0:02:52  iteration: 167/1000  consumed samples: 2672  total_loss: 8.14  time: 0.2057(77.79)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:06] lb.utils.events INFO:  eta: 0:02:51  iteration: 168/1000  consumed samples: 2688  total_loss: 8.138  time: 0.2057(77.77)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:07] lb.utils.events INFO:  eta: 0:02:51  iteration: 169/1000  consumed samples: 2704  total_loss: 8.137  time: 0.2057(77.78)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:07] lb.utils.events INFO:  eta: 0:02:51  iteration: 170/1000  consumed samples: 2720  total_loss: 8.134  time: 0.2058(77.76)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:07] lb.utils.events INFO:  eta: 0:02:51  iteration: 171/1000  consumed samples: 2736  total_loss: 8.132  time: 0.2058(77.75)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:07] lb.utils.events INFO:  eta: 0:02:51  iteration: 172/1000  consumed samples: 2752  total_loss: 8.13  time: 0.2058(77.73)  data_time: 0.0054  lr: 9.90e-05  
[01/19 11:50:08] lb.utils.events INFO:  eta: 0:02:50  iteration: 173/1000  consumed samples: 2768  total_loss: 8.128  time: 0.2058(77.73)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:08] lb.utils.events INFO:  eta: 0:02:50  iteration: 174/1000  consumed samples: 2784  total_loss: 8.128  time: 0.2058(77.74)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:08] lb.utils.events INFO:  eta: 0:02:50  iteration: 175/1000  consumed samples: 2800  total_loss: 8.128  time: 0.2059(77.72)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:08] lb.utils.events INFO:  eta: 0:02:50  iteration: 176/1000  consumed samples: 2816  total_loss: 8.124  time: 0.2059(77.69)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:08] lb.utils.events INFO:  eta: 0:02:50  iteration: 177/1000  consumed samples: 2832  total_loss: 8.121  time: 0.2059(77.69)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:09] lb.utils.events INFO:  eta: 0:02:49  iteration: 178/1000  consumed samples: 2848  total_loss: 8.124  time: 0.2059(77.69)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:09] lb.utils.events INFO:  eta: 0:02:49  iteration: 179/1000  consumed samples: 2864  total_loss: 8.121  time: 0.2060(77.68)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:09] lb.utils.events INFO:  eta: 0:02:49  iteration: 180/1000  consumed samples: 2880  total_loss: 8.119  time: 0.2060(77.66)  data_time: 0.0053  lr: 9.90e-05  
[01/19 11:50:09] lb.utils.events INFO:  eta: 0:02:49  iteration: 181/1000  consumed samples: 2896  total_loss: 8.117  time: 0.2061(77.64)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:10] lb.utils.events INFO:  eta: 0:02:49  iteration: 182/1000  consumed samples: 2912  total_loss: 8.114  time: 0.2061(77.65)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:10] lb.utils.events INFO:  eta: 0:02:48  iteration: 183/1000  consumed samples: 2928  total_loss: 8.111  time: 0.2061(77.65)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:10] lb.utils.events INFO:  eta: 0:02:48  iteration: 184/1000  consumed samples: 2944  total_loss: 8.11  time: 0.2061(77.63)  data_time: 0.0055  lr: 9.90e-05  
[01/19 11:50:10] lb.utils.events INFO:  eta: 0:02:48  iteration: 185/1000  consumed samples: 2960  total_loss: 8.11  time: 0.2061(77.62)  data_time: 0.0054  lr: 9.90e-05  
[01/19 11:50:10] lb.utils.events INFO:  eta: 0:02:48  iteration: 186/1000  consumed samples: 2976  total_loss: 8.11  time: 0.2061(77.62)  data_time: 0.0054  lr: 9.90e-05  
[01/19 11:50:11] lb.utils.events INFO:  eta: 0:02:48  iteration: 187/1000  consumed samples: 2992  total_loss: 8.11  time: 0.2061(77.62)  data_time: 0.0054  lr: 9.90e-05  
[01/19 11:50:11] lb.utils.events INFO:  eta: 0:02:47  iteration: 188/1000  consumed samples: 3008  total_loss: 8.108  time: 0.2061(77.62)  data_time: 0.0054  lr: 9.90e-05  
[01/19 11:50:11] lb.utils.events INFO:  eta: 0:02:47  iteration: 189/1000  consumed samples: 3024  total_loss: 8.107  time: 0.2062(77.61)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:11] lb.utils.events INFO:  eta: 0:02:47  iteration: 190/1000  consumed samples: 3040  total_loss: 8.103  time: 0.2062(77.59)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:12] lb.utils.events INFO:  eta: 0:02:47  iteration: 191/1000  consumed samples: 3056  total_loss: 8.1  time: 0.2062(77.61)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:12] lb.utils.events INFO:  eta: 0:02:47  iteration: 192/1000  consumed samples: 3072  total_loss: 8.098  time: 0.2062(77.60)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:12] lb.utils.events INFO:  eta: 0:02:46  iteration: 193/1000  consumed samples: 3088  total_loss: 8.097  time: 0.2062(77.58)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:12] lb.utils.events INFO:  eta: 0:02:46  iteration: 194/1000  consumed samples: 3104  total_loss: 8.097  time: 0.2063(77.57)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:12] lb.utils.events INFO:  eta: 0:02:46  iteration: 195/1000  consumed samples: 3120  total_loss: 8.097  time: 0.2063(77.57)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:13] lb.utils.events INFO:  eta: 0:02:46  iteration: 196/1000  consumed samples: 3136  total_loss: 8.095  time: 0.2063(77.57)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:13] lb.utils.events INFO:  eta: 0:02:46  iteration: 197/1000  consumed samples: 3152  total_loss: 8.094  time: 0.2063(77.56)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:13] lb.utils.events INFO:  eta: 0:02:45  iteration: 198/1000  consumed samples: 3168  total_loss: 8.089  time: 0.2063(77.56)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:13] lb.utils.events INFO:  eta: 0:02:45  iteration: 199/1000  consumed samples: 3184  total_loss: 8.085  time: 0.2063(77.54)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:14] lb.utils.events INFO:  eta: 0:02:45  iteration: 200/1000  consumed samples: 3200  total_loss: 8.085  time: 0.2064(77.53)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:14] lb.utils.events INFO:  eta: 0:02:45  iteration: 201/1000  consumed samples: 3216  total_loss: 8.085  time: 0.2064(77.52)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:14] lb.utils.events INFO:  eta: 0:02:45  iteration: 202/1000  consumed samples: 3232  total_loss: 8.082  time: 0.2064(77.51)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:14] lb.utils.events INFO:  eta: 0:02:44  iteration: 203/1000  consumed samples: 3248  total_loss: 8.082  time: 0.2064(77.51)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:15] lb.utils.events INFO:  eta: 0:02:44  iteration: 204/1000  consumed samples: 3264  total_loss: 8.075  time: 0.2064(77.51)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:15] lb.utils.events INFO:  eta: 0:02:44  iteration: 205/1000  consumed samples: 3280  total_loss: 8.069  time: 0.2065(77.50)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:15] lb.utils.events INFO:  eta: 0:02:44  iteration: 206/1000  consumed samples: 3296  total_loss: 8.063  time: 0.2065(77.48)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:15] lb.utils.events INFO:  eta: 0:02:43  iteration: 207/1000  consumed samples: 3312  total_loss: 8.057  time: 0.2065(77.47)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:15] lb.utils.events INFO:  eta: 0:02:43  iteration: 208/1000  consumed samples: 3328  total_loss: 8.054  time: 0.2065(77.47)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:16] lb.utils.events INFO:  eta: 0:02:43  iteration: 209/1000  consumed samples: 3344  total_loss: 8.054  time: 0.2066(77.46)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:16] lb.utils.events INFO:  eta: 0:02:43  iteration: 210/1000  consumed samples: 3360  total_loss: 8.053  time: 0.2066(77.46)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:16] lb.utils.events INFO:  eta: 0:02:43  iteration: 211/1000  consumed samples: 3376  total_loss: 8.052  time: 0.2066(77.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:16] lb.utils.events INFO:  eta: 0:02:43  iteration: 212/1000  consumed samples: 3392  total_loss: 8.047  time: 0.2066(77.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:17] lb.utils.events INFO:  eta: 0:02:42  iteration: 213/1000  consumed samples: 3408  total_loss: 8.043  time: 0.2066(77.44)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:17] lb.utils.events INFO:  eta: 0:02:42  iteration: 214/1000  consumed samples: 3424  total_loss: 8.043  time: 0.2066(77.43)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:17] lb.utils.events INFO:  eta: 0:02:42  iteration: 215/1000  consumed samples: 3440  total_loss: 8.043  time: 0.2066(77.43)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:17] lb.utils.events INFO:  eta: 0:02:42  iteration: 216/1000  consumed samples: 3456  total_loss: 8.042  time: 0.2067(77.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:17] lb.utils.events INFO:  eta: 0:02:42  iteration: 217/1000  consumed samples: 3472  total_loss: 8.041  time: 0.2067(77.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:18] lb.utils.events INFO:  eta: 0:02:41  iteration: 218/1000  consumed samples: 3488  total_loss: 8.04  time: 0.2067(77.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:18] lb.utils.events INFO:  eta: 0:02:41  iteration: 219/1000  consumed samples: 3504  total_loss: 8.037  time: 0.2067(77.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:18] lb.utils.events INFO:  eta: 0:02:41  iteration: 220/1000  consumed samples: 3520  total_loss: 8.036  time: 0.2067(77.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:18] lb.utils.events INFO:  eta: 0:02:41  iteration: 221/1000  consumed samples: 3536  total_loss: 8.036  time: 0.2068(77.38)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:19] lb.utils.events INFO:  eta: 0:02:41  iteration: 222/1000  consumed samples: 3552  total_loss: 8.034  time: 0.2068(77.37)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:19] lb.utils.events INFO:  eta: 0:02:40  iteration: 223/1000  consumed samples: 3568  total_loss: 8.031  time: 0.2069(77.34)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:19] lb.utils.events INFO:  eta: 0:02:40  iteration: 224/1000  consumed samples: 3584  total_loss: 8.025  time: 0.2069(77.33)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:19] lb.utils.events INFO:  eta: 0:02:40  iteration: 225/1000  consumed samples: 3600  total_loss: 8.021  time: 0.2069(77.32)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:20] lb.utils.events INFO:  eta: 0:02:40  iteration: 226/1000  consumed samples: 3616  total_loss: 8.02  time: 0.2069(77.32)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:20] lb.utils.events INFO:  eta: 0:02:40  iteration: 227/1000  consumed samples: 3632  total_loss: 8.016  time: 0.2070(77.31)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:20] lb.utils.events INFO:  eta: 0:02:39  iteration: 228/1000  consumed samples: 3648  total_loss: 8.013  time: 0.2070(77.30)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:20] lb.utils.events INFO:  eta: 0:02:39  iteration: 229/1000  consumed samples: 3664  total_loss: 8.013  time: 0.2070(77.28)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:20] lb.utils.events INFO:  eta: 0:02:39  iteration: 230/1000  consumed samples: 3680  total_loss: 8.011  time: 0.2070(77.28)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:21] lb.utils.events INFO:  eta: 0:02:39  iteration: 231/1000  consumed samples: 3696  total_loss: 8.009  time: 0.2071(77.27)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:21] lb.utils.events INFO:  eta: 0:02:39  iteration: 232/1000  consumed samples: 3712  total_loss: 8.008  time: 0.2071(77.27)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:21] lb.utils.events INFO:  eta: 0:02:38  iteration: 233/1000  consumed samples: 3728  total_loss: 8.007  time: 0.2071(77.26)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:21] lb.utils.events INFO:  eta: 0:02:38  iteration: 234/1000  consumed samples: 3744  total_loss: 8.005  time: 0.2071(77.26)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:22] lb.utils.events INFO:  eta: 0:02:38  iteration: 235/1000  consumed samples: 3760  total_loss: 8.003  time: 0.2071(77.25)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:22] lb.utils.events INFO:  eta: 0:02:38  iteration: 236/1000  consumed samples: 3776  total_loss: 8.002  time: 0.2071(77.25)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:22] lb.utils.events INFO:  eta: 0:02:38  iteration: 237/1000  consumed samples: 3792  total_loss: 8.002  time: 0.2072(77.23)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:22] lb.utils.events INFO:  eta: 0:02:37  iteration: 238/1000  consumed samples: 3808  total_loss: 8.001  time: 0.2072(77.23)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:22] lb.utils.events INFO:  eta: 0:02:37  iteration: 239/1000  consumed samples: 3824  total_loss: 8.001  time: 0.2072(77.22)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:23] lb.utils.events INFO:  eta: 0:02:37  iteration: 240/1000  consumed samples: 3840  total_loss: 8.001  time: 0.2072(77.21)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:23] lb.utils.events INFO:  eta: 0:02:37  iteration: 241/1000  consumed samples: 3856  total_loss: 8.001  time: 0.2072(77.21)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:23] lb.utils.events INFO:  eta: 0:02:37  iteration: 242/1000  consumed samples: 3872  total_loss: 7.999  time: 0.2073(77.20)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:23] lb.utils.events INFO:  eta: 0:02:36  iteration: 243/1000  consumed samples: 3888  total_loss: 7.996  time: 0.2073(77.19)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:24] lb.utils.events INFO:  eta: 0:02:36  iteration: 244/1000  consumed samples: 3904  total_loss: 7.994  time: 0.2073(77.18)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:24] lb.utils.events INFO:  eta: 0:02:36  iteration: 245/1000  consumed samples: 3920  total_loss: 7.994  time: 0.2073(77.17)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:24] lb.utils.events INFO:  eta: 0:02:36  iteration: 246/1000  consumed samples: 3936  total_loss: 7.994  time: 0.2074(77.15)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:24] lb.utils.events INFO:  eta: 0:02:36  iteration: 247/1000  consumed samples: 3952  total_loss: 7.994  time: 0.2074(77.14)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:25] lb.utils.events INFO:  eta: 0:02:35  iteration: 248/1000  consumed samples: 3968  total_loss: 7.993  time: 0.2075(77.13)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:25] lb.utils.events INFO:  eta: 0:02:35  iteration: 249/1000  consumed samples: 3984  total_loss: 7.99  time: 0.2075(77.12)  data_time: 0.0031  lr: 9.90e-05  
[01/19 11:50:25] lb.utils.events INFO:  eta: 0:02:35  iteration: 250/1000  consumed samples: 4000  total_loss: 7.987  time: 0.2075(77.11)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:25] lb.utils.events INFO:  eta: 0:02:35  iteration: 251/1000  consumed samples: 4016  total_loss: 7.986  time: 0.2075(77.10)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:25] lb.utils.events INFO:  eta: 0:02:35  iteration: 252/1000  consumed samples: 4032  total_loss: 7.986  time: 0.2076(77.08)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:26] lb.utils.events INFO:  eta: 0:02:34  iteration: 253/1000  consumed samples: 4048  total_loss: 7.983  time: 0.2076(77.08)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:26] lb.utils.events INFO:  eta: 0:02:34  iteration: 254/1000  consumed samples: 4064  total_loss: 7.979  time: 0.2076(77.07)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:26] lb.utils.events INFO:  eta: 0:02:34  iteration: 255/1000  consumed samples: 4080  total_loss: 7.977  time: 0.2076(77.07)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:26] lb.utils.events INFO:  eta: 0:02:34  iteration: 256/1000  consumed samples: 4096  total_loss: 7.976  time: 0.2076(77.05)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:27] lb.utils.events INFO:  eta: 0:02:34  iteration: 257/1000  consumed samples: 4112  total_loss: 7.975  time: 0.2076(77.05)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:27] lb.utils.events INFO:  eta: 0:02:34  iteration: 258/1000  consumed samples: 4128  total_loss: 7.975  time: 0.2077(77.04)  data_time: 0.0058  lr: 9.90e-05  
[01/19 11:50:27] lb.utils.events INFO:  eta: 0:02:33  iteration: 259/1000  consumed samples: 4144  total_loss: 7.974  time: 0.2077(77.03)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:27] lb.utils.events INFO:  eta: 0:02:33  iteration: 260/1000  consumed samples: 4160  total_loss: 7.974  time: 0.2077(77.03)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:28] lb.utils.events INFO:  eta: 0:02:33  iteration: 261/1000  consumed samples: 4176  total_loss: 7.973  time: 0.2077(77.02)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:28] lb.utils.events INFO:  eta: 0:02:33  iteration: 262/1000  consumed samples: 4192  total_loss: 7.971  time: 0.2078(77.01)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:28] lb.utils.events INFO:  eta: 0:02:33  iteration: 263/1000  consumed samples: 4208  total_loss: 7.97  time: 0.2078(77.00)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:28] lb.utils.events INFO:  eta: 0:02:32  iteration: 264/1000  consumed samples: 4224  total_loss: 7.968  time: 0.2078(77.00)  data_time: 0.0059  lr: 9.90e-05  
[01/19 11:50:28] lb.utils.events INFO:  eta: 0:02:32  iteration: 265/1000  consumed samples: 4240  total_loss: 7.967  time: 0.2078(76.99)  data_time: 0.0060  lr: 9.90e-05  
[01/19 11:50:29] lb.utils.events INFO:  eta: 0:02:32  iteration: 266/1000  consumed samples: 4256  total_loss: 7.967  time: 0.2078(76.99)  data_time: 0.0060  lr: 9.90e-05  
[01/19 11:50:29] lb.utils.events INFO:  eta: 0:02:32  iteration: 267/1000  consumed samples: 4272  total_loss: 7.967  time: 0.2079(76.98)  data_time: 0.0060  lr: 9.90e-05  
[01/19 11:50:29] lb.utils.events INFO:  eta: 0:02:32  iteration: 268/1000  consumed samples: 4288  total_loss: 7.966  time: 0.2079(76.98)  data_time: 0.0060  lr: 9.90e-05  
[01/19 11:50:29] lb.utils.events INFO:  eta: 0:02:31  iteration: 269/1000  consumed samples: 4304  total_loss: 7.964  time: 0.2079(76.96)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:30] lb.utils.events INFO:  eta: 0:02:31  iteration: 270/1000  consumed samples: 4320  total_loss: 7.962  time: 0.2079(76.95)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:30] lb.utils.events INFO:  eta: 0:02:31  iteration: 271/1000  consumed samples: 4336  total_loss: 7.962  time: 0.2080(76.94)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:30] lb.utils.events INFO:  eta: 0:02:31  iteration: 272/1000  consumed samples: 4352  total_loss: 7.961  time: 0.2080(76.94)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:30] lb.utils.events INFO:  eta: 0:02:31  iteration: 273/1000  consumed samples: 4368  total_loss: 7.96  time: 0.2080(76.93)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:30] lb.utils.events INFO:  eta: 0:02:30  iteration: 274/1000  consumed samples: 4384  total_loss: 7.957  time: 0.2080(76.93)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:31] lb.utils.events INFO:  eta: 0:02:30  iteration: 275/1000  consumed samples: 4400  total_loss: 7.953  time: 0.2080(76.92)  data_time: 0.0061  lr: 9.90e-05  
[01/19 11:50:31] lb.utils.events INFO:  eta: 0:02:30  iteration: 276/1000  consumed samples: 4416  total_loss: 7.953  time: 0.2080(76.92)  data_time: 0.0060  lr: 9.90e-05  
[01/19 11:50:31] lb.utils.events INFO:  eta: 0:02:30  iteration: 277/1000  consumed samples: 4432  total_loss: 7.953  time: 0.2080(76.92)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:31] lb.utils.events INFO:  eta: 0:02:30  iteration: 278/1000  consumed samples: 4448  total_loss: 7.951  time: 0.2080(76.92)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:32] lb.utils.events INFO:  eta: 0:02:29  iteration: 279/1000  consumed samples: 4464  total_loss: 7.95  time: 0.2080(76.91)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:32] lb.utils.events INFO:  eta: 0:02:29  iteration: 280/1000  consumed samples: 4480  total_loss: 7.95  time: 0.2080(76.91)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:32] lb.utils.events INFO:  eta: 0:02:29  iteration: 281/1000  consumed samples: 4496  total_loss: 7.95  time: 0.2081(76.90)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:32] lb.utils.events INFO:  eta: 0:02:29  iteration: 282/1000  consumed samples: 4512  total_loss: 7.949  time: 0.2081(76.90)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:33] lb.utils.events INFO:  eta: 0:02:29  iteration: 283/1000  consumed samples: 4528  total_loss: 7.947  time: 0.2081(76.89)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:33] lb.utils.events INFO:  eta: 0:02:28  iteration: 284/1000  consumed samples: 4544  total_loss: 7.945  time: 0.2081(76.89)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:33] lb.utils.events INFO:  eta: 0:02:28  iteration: 285/1000  consumed samples: 4560  total_loss: 7.945  time: 0.2081(76.89)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:33] lb.utils.events INFO:  eta: 0:02:28  iteration: 286/1000  consumed samples: 4576  total_loss: 7.944  time: 0.2081(76.89)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:33] lb.utils.events INFO:  eta: 0:02:28  iteration: 287/1000  consumed samples: 4592  total_loss: 7.942  time: 0.2081(76.88)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:34] lb.utils.events INFO:  eta: 0:02:28  iteration: 288/1000  consumed samples: 4608  total_loss: 7.942  time: 0.2081(76.88)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:34] lb.utils.events INFO:  eta: 0:02:27  iteration: 289/1000  consumed samples: 4624  total_loss: 7.94  time: 0.2081(76.88)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:34] lb.utils.events INFO:  eta: 0:02:27  iteration: 290/1000  consumed samples: 4640  total_loss: 7.942  time: 0.2081(76.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:34] lb.utils.events INFO:  eta: 0:02:27  iteration: 291/1000  consumed samples: 4656  total_loss: 7.94  time: 0.2082(76.86)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:35] lb.utils.events INFO:  eta: 0:02:27  iteration: 292/1000  consumed samples: 4672  total_loss: 7.938  time: 0.2082(76.86)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:35] lb.utils.events INFO:  eta: 0:02:27  iteration: 293/1000  consumed samples: 4688  total_loss: 7.937  time: 0.2082(76.85)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:35] lb.utils.events INFO:  eta: 0:02:26  iteration: 294/1000  consumed samples: 4704  total_loss: 7.935  time: 0.2082(76.85)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:35] lb.utils.events INFO:  eta: 0:02:26  iteration: 295/1000  consumed samples: 4720  total_loss: 7.937  time: 0.2082(76.84)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:35] lb.utils.events INFO:  eta: 0:02:26  iteration: 296/1000  consumed samples: 4736  total_loss: 7.935  time: 0.2082(76.83)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:36] lb.utils.events INFO:  eta: 0:02:26  iteration: 297/1000  consumed samples: 4752  total_loss: 7.933  time: 0.2083(76.83)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:36] lb.utils.events INFO:  eta: 0:02:26  iteration: 298/1000  consumed samples: 4768  total_loss: 7.933  time: 0.2083(76.82)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:36] lb.utils.events INFO:  eta: 0:02:25  iteration: 299/1000  consumed samples: 4784  total_loss: 7.93  time: 0.2083(76.82)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:36] lb.utils.events INFO:  eta: 0:02:25  iteration: 300/1000  consumed samples: 4800  total_loss: 7.93  time: 0.2083(76.81)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:37] lb.utils.events INFO:  eta: 0:02:25  iteration: 301/1000  consumed samples: 4816  total_loss: 7.933  time: 0.2083(76.81)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:37] lb.utils.events INFO:  eta: 0:02:25  iteration: 302/1000  consumed samples: 4832  total_loss: 7.93  time: 0.2083(76.80)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:37] lb.utils.events INFO:  eta: 0:02:25  iteration: 303/1000  consumed samples: 4848  total_loss: 7.93  time: 0.2083(76.80)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:37] lb.utils.events INFO:  eta: 0:02:25  iteration: 304/1000  consumed samples: 4864  total_loss: 7.93  time: 0.2084(76.79)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:38] lb.utils.events INFO:  eta: 0:02:24  iteration: 305/1000  consumed samples: 4880  total_loss: 7.933  time: 0.2084(76.78)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:38] lb.utils.events INFO:  eta: 0:02:24  iteration: 306/1000  consumed samples: 4896  total_loss: 7.933  time: 0.2084(76.77)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:38] lb.utils.events INFO:  eta: 0:02:24  iteration: 307/1000  consumed samples: 4912  total_loss: 7.933  time: 0.2084(76.76)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:38] lb.utils.events INFO:  eta: 0:02:24  iteration: 308/1000  consumed samples: 4928  total_loss: 7.933  time: 0.2085(76.75)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:38] lb.utils.events INFO:  eta: 0:02:24  iteration: 309/1000  consumed samples: 4944  total_loss: 7.933  time: 0.2085(76.75)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:39] lb.utils.events INFO:  eta: 0:02:23  iteration: 310/1000  consumed samples: 4960  total_loss: 7.933  time: 0.2085(76.74)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:39] lb.utils.events INFO:  eta: 0:02:23  iteration: 311/1000  consumed samples: 4976  total_loss: 7.93  time: 0.2085(76.74)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:39] lb.utils.events INFO:  eta: 0:02:23  iteration: 312/1000  consumed samples: 4992  total_loss: 7.928  time: 0.2085(76.74)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:39] lb.utils.events INFO:  eta: 0:02:23  iteration: 313/1000  consumed samples: 5008  total_loss: 7.93  time: 0.2085(76.73)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:40] lb.utils.events INFO:  eta: 0:02:23  iteration: 314/1000  consumed samples: 5024  total_loss: 7.928  time: 0.2085(76.73)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:40] lb.utils.events INFO:  eta: 0:02:22  iteration: 315/1000  consumed samples: 5040  total_loss: 7.928  time: 0.2085(76.73)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:40] lb.utils.events INFO:  eta: 0:02:22  iteration: 316/1000  consumed samples: 5056  total_loss: 7.926  time: 0.2086(76.71)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:40] lb.utils.events INFO:  eta: 0:02:22  iteration: 317/1000  consumed samples: 5072  total_loss: 7.924  time: 0.2086(76.72)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:41] lb.utils.events INFO:  eta: 0:02:22  iteration: 318/1000  consumed samples: 5088  total_loss: 7.924  time: 0.2086(76.71)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:41] lb.utils.events INFO:  eta: 0:02:22  iteration: 319/1000  consumed samples: 5104  total_loss: 7.924  time: 0.2086(76.71)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:41] lb.utils.events INFO:  eta: 0:02:21  iteration: 320/1000  consumed samples: 5120  total_loss: 7.923  time: 0.2086(76.71)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:41] lb.utils.events INFO:  eta: 0:02:21  iteration: 321/1000  consumed samples: 5136  total_loss: 7.922  time: 0.2086(76.71)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:41] lb.utils.events INFO:  eta: 0:02:21  iteration: 322/1000  consumed samples: 5152  total_loss: 7.922  time: 0.2086(76.70)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:42] lb.utils.events INFO:  eta: 0:02:21  iteration: 323/1000  consumed samples: 5168  total_loss: 7.921  time: 0.2086(76.69)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:42] lb.utils.events INFO:  eta: 0:02:21  iteration: 324/1000  consumed samples: 5184  total_loss: 7.921  time: 0.2086(76.68)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:42] lb.utils.events INFO:  eta: 0:02:20  iteration: 325/1000  consumed samples: 5200  total_loss: 7.919  time: 0.2087(76.68)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:42] lb.utils.events INFO:  eta: 0:02:20  iteration: 326/1000  consumed samples: 5216  total_loss: 7.919  time: 0.2087(76.67)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:43] lb.utils.events INFO:  eta: 0:02:20  iteration: 327/1000  consumed samples: 5232  total_loss: 7.919  time: 0.2087(76.67)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:43] lb.utils.events INFO:  eta: 0:02:20  iteration: 328/1000  consumed samples: 5248  total_loss: 7.919  time: 0.2087(76.66)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:43] lb.utils.events INFO:  eta: 0:02:19  iteration: 329/1000  consumed samples: 5264  total_loss: 7.919  time: 0.2087(76.66)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:43] lb.utils.events INFO:  eta: 0:02:19  iteration: 330/1000  consumed samples: 5280  total_loss: 7.919  time: 0.2087(76.66)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:43] lb.utils.events INFO:  eta: 0:02:19  iteration: 331/1000  consumed samples: 5296  total_loss: 7.917  time: 0.2087(76.66)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:44] lb.utils.events INFO:  eta: 0:02:19  iteration: 332/1000  consumed samples: 5312  total_loss: 7.915  time: 0.2087(76.66)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:44] lb.utils.events INFO:  eta: 0:02:19  iteration: 333/1000  consumed samples: 5328  total_loss: 7.915  time: 0.2087(76.65)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:44] lb.utils.events INFO:  eta: 0:02:18  iteration: 334/1000  consumed samples: 5344  total_loss: 7.915  time: 0.2087(76.65)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:44] lb.utils.events INFO:  eta: 0:02:18  iteration: 335/1000  consumed samples: 5360  total_loss: 7.914  time: 0.2088(76.64)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:45] lb.utils.events INFO:  eta: 0:02:18  iteration: 336/1000  consumed samples: 5376  total_loss: 7.913  time: 0.2088(76.64)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:45] lb.utils.events INFO:  eta: 0:02:18  iteration: 337/1000  consumed samples: 5392  total_loss: 7.91  time: 0.2088(76.63)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:45] lb.utils.events INFO:  eta: 0:02:18  iteration: 338/1000  consumed samples: 5408  total_loss: 7.91  time: 0.2088(76.63)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:45] lb.utils.events INFO:  eta: 0:02:17  iteration: 339/1000  consumed samples: 5424  total_loss: 7.91  time: 0.2088(76.62)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:46] lb.utils.events INFO:  eta: 0:02:17  iteration: 340/1000  consumed samples: 5440  total_loss: 7.913  time: 0.2088(76.62)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:46] lb.utils.events INFO:  eta: 0:02:17  iteration: 341/1000  consumed samples: 5456  total_loss: 7.913  time: 0.2088(76.63)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:46] lb.utils.events INFO:  eta: 0:02:17  iteration: 342/1000  consumed samples: 5472  total_loss: 7.914  time: 0.2088(76.63)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:46] lb.utils.events INFO:  eta: 0:02:17  iteration: 343/1000  consumed samples: 5488  total_loss: 7.913  time: 0.2088(76.63)  data_time: 0.0032  lr: 9.90e-05  
[01/19 11:50:46] lb.utils.events INFO:  eta: 0:02:16  iteration: 344/1000  consumed samples: 5504  total_loss: 7.907  time: 0.2088(76.63)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:47] lb.utils.events INFO:  eta: 0:02:16  iteration: 345/1000  consumed samples: 5520  total_loss: 7.907  time: 0.2088(76.63)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:47] lb.utils.events INFO:  eta: 0:02:16  iteration: 346/1000  consumed samples: 5536  total_loss: 7.902  time: 0.2088(76.62)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:47] lb.utils.events INFO:  eta: 0:02:16  iteration: 347/1000  consumed samples: 5552  total_loss: 7.9  time: 0.2088(76.62)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:47] lb.utils.events INFO:  eta: 0:02:16  iteration: 348/1000  consumed samples: 5568  total_loss: 7.898  time: 0.2089(76.61)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:48] lb.utils.events INFO:  eta: 0:02:15  iteration: 349/1000  consumed samples: 5584  total_loss: 7.898  time: 0.2088(76.61)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:48] lb.utils.events INFO:  eta: 0:02:15  iteration: 350/1000  consumed samples: 5600  total_loss: 7.898  time: 0.2089(76.60)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:48] lb.utils.events INFO:  eta: 0:02:15  iteration: 351/1000  consumed samples: 5616  total_loss: 7.897  time: 0.2089(76.60)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:48] lb.utils.events INFO:  eta: 0:02:15  iteration: 352/1000  consumed samples: 5632  total_loss: 7.897  time: 0.2089(76.60)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:49] lb.utils.events INFO:  eta: 0:02:15  iteration: 353/1000  consumed samples: 5648  total_loss: 7.898  time: 0.2089(76.59)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:49] lb.utils.events INFO:  eta: 0:02:14  iteration: 354/1000  consumed samples: 5664  total_loss: 7.898  time: 0.2089(76.59)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:49] lb.utils.events INFO:  eta: 0:02:14  iteration: 355/1000  consumed samples: 5680  total_loss: 7.897  time: 0.2089(76.59)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:49] lb.utils.events INFO:  eta: 0:02:14  iteration: 356/1000  consumed samples: 5696  total_loss: 7.896  time: 0.2089(76.58)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:50:49] lb.utils.events INFO:  eta: 0:02:14  iteration: 357/1000  consumed samples: 5712  total_loss: 7.896  time: 0.2089(76.58)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:50] lb.utils.events INFO:  eta: 0:02:14  iteration: 358/1000  consumed samples: 5728  total_loss: 7.895  time: 0.2089(76.58)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:50] lb.utils.events INFO:  eta: 0:02:13  iteration: 359/1000  consumed samples: 5744  total_loss: 7.896  time: 0.2089(76.58)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:50:50] lb.utils.events INFO:  eta: 0:02:13  iteration: 360/1000  consumed samples: 5760  total_loss: 7.896  time: 0.2089(76.58)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:50:50] lb.utils.events INFO:  eta: 0:02:13  iteration: 361/1000  consumed samples: 5776  total_loss: 7.892  time: 0.2089(76.58)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:51] lb.utils.events INFO:  eta: 0:02:13  iteration: 362/1000  consumed samples: 5792  total_loss: 7.892  time: 0.2089(76.57)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:51] lb.utils.events INFO:  eta: 0:02:13  iteration: 363/1000  consumed samples: 5808  total_loss: 7.888  time: 0.2089(76.58)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:51] lb.utils.events INFO:  eta: 0:02:12  iteration: 364/1000  consumed samples: 5824  total_loss: 7.888  time: 0.2090(76.57)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:51] lb.utils.events INFO:  eta: 0:02:12  iteration: 365/1000  consumed samples: 5840  total_loss: 7.888  time: 0.2090(76.56)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:51] lb.utils.events INFO:  eta: 0:02:12  iteration: 366/1000  consumed samples: 5856  total_loss: 7.885  time: 0.2090(76.56)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:52] lb.utils.events INFO:  eta: 0:02:12  iteration: 367/1000  consumed samples: 5872  total_loss: 7.884  time: 0.2090(76.56)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:52] lb.utils.events INFO:  eta: 0:02:11  iteration: 368/1000  consumed samples: 5888  total_loss: 7.884  time: 0.2090(76.55)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:52] lb.utils.events INFO:  eta: 0:02:11  iteration: 369/1000  consumed samples: 5904  total_loss: 7.884  time: 0.2090(76.55)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:52] lb.utils.events INFO:  eta: 0:02:11  iteration: 370/1000  consumed samples: 5920  total_loss: 7.883  time: 0.2090(76.55)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:53] lb.utils.events INFO:  eta: 0:02:11  iteration: 371/1000  consumed samples: 5936  total_loss: 7.883  time: 0.2090(76.54)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:53] lb.utils.events INFO:  eta: 0:02:11  iteration: 372/1000  consumed samples: 5952  total_loss: 7.882  time: 0.2090(76.54)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:53] lb.utils.events INFO:  eta: 0:02:10  iteration: 373/1000  consumed samples: 5968  total_loss: 7.88  time: 0.2090(76.54)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:53] lb.utils.events INFO:  eta: 0:02:10  iteration: 374/1000  consumed samples: 5984  total_loss: 7.878  time: 0.2090(76.54)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:54] lb.utils.events INFO:  eta: 0:02:10  iteration: 375/1000  consumed samples: 6000  total_loss: 7.88  time: 0.2091(76.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:54] lb.utils.events INFO:  eta: 0:02:10  iteration: 376/1000  consumed samples: 6016  total_loss: 7.878  time: 0.2091(76.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:54] lb.utils.events INFO:  eta: 0:02:10  iteration: 377/1000  consumed samples: 6032  total_loss: 7.88  time: 0.2091(76.52)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:50:54] lb.utils.events INFO:  eta: 0:02:09  iteration: 378/1000  consumed samples: 6048  total_loss: 7.878  time: 0.2091(76.52)  data_time: 0.0042  lr: 9.90e-05  
[01/19 11:50:54] lb.utils.events INFO:  eta: 0:02:09  iteration: 379/1000  consumed samples: 6064  total_loss: 7.88  time: 0.2091(76.52)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:50:55] lb.utils.events INFO:  eta: 0:02:09  iteration: 380/1000  consumed samples: 6080  total_loss: 7.88  time: 0.2091(76.52)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:50:55] lb.utils.events INFO:  eta: 0:02:09  iteration: 381/1000  consumed samples: 6096  total_loss: 7.88  time: 0.2091(76.51)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:50:55] lb.utils.events INFO:  eta: 0:02:09  iteration: 382/1000  consumed samples: 6112  total_loss: 7.88  time: 0.2091(76.51)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:50:55] lb.utils.events INFO:  eta: 0:02:08  iteration: 383/1000  consumed samples: 6128  total_loss: 7.878  time: 0.2091(76.50)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:50:56] lb.utils.events INFO:  eta: 0:02:08  iteration: 384/1000  consumed samples: 6144  total_loss: 7.878  time: 0.2091(76.50)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:50:56] lb.utils.events INFO:  eta: 0:02:08  iteration: 385/1000  consumed samples: 6160  total_loss: 7.877  time: 0.2092(76.49)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:50:56] lb.utils.events INFO:  eta: 0:02:08  iteration: 386/1000  consumed samples: 6176  total_loss: 7.877  time: 0.2092(76.49)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:50:56] lb.utils.events INFO:  eta: 0:02:08  iteration: 387/1000  consumed samples: 6192  total_loss: 7.875  time: 0.2092(76.48)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:57] lb.utils.events INFO:  eta: 0:02:07  iteration: 388/1000  consumed samples: 6208  total_loss: 7.872  time: 0.2092(76.48)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:57] lb.utils.events INFO:  eta: 0:02:07  iteration: 389/1000  consumed samples: 6224  total_loss: 7.869  time: 0.2092(76.48)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:57] lb.utils.events INFO:  eta: 0:02:07  iteration: 390/1000  consumed samples: 6240  total_loss: 7.867  time: 0.2092(76.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:57] lb.utils.events INFO:  eta: 0:02:07  iteration: 391/1000  consumed samples: 6256  total_loss: 7.866  time: 0.2093(76.46)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:57] lb.utils.events INFO:  eta: 0:02:07  iteration: 392/1000  consumed samples: 6272  total_loss: 7.866  time: 0.2093(76.46)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:50:58] lb.utils.events INFO:  eta: 0:02:06  iteration: 393/1000  consumed samples: 6288  total_loss: 7.867  time: 0.2093(76.46)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:58] lb.utils.events INFO:  eta: 0:02:06  iteration: 394/1000  consumed samples: 6304  total_loss: 7.867  time: 0.2093(76.46)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:58] lb.utils.events INFO:  eta: 0:02:06  iteration: 395/1000  consumed samples: 6320  total_loss: 7.867  time: 0.2093(76.46)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:58] lb.utils.events INFO:  eta: 0:02:06  iteration: 396/1000  consumed samples: 6336  total_loss: 7.867  time: 0.2093(76.46)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:50:59] lb.utils.events INFO:  eta: 0:02:05  iteration: 397/1000  consumed samples: 6352  total_loss: 7.866  time: 0.2093(76.46)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:50:59] lb.utils.events INFO:  eta: 0:02:05  iteration: 398/1000  consumed samples: 6368  total_loss: 7.866  time: 0.2093(76.46)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:59] lb.utils.events INFO:  eta: 0:02:05  iteration: 399/1000  consumed samples: 6384  total_loss: 7.865  time: 0.2093(76.45)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:59] lb.utils.events INFO:  eta: 0:02:05  iteration: 400/1000  consumed samples: 6400  total_loss: 7.865  time: 0.2093(76.45)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:50:59] lb.utils.events INFO:  eta: 0:02:05  iteration: 401/1000  consumed samples: 6416  total_loss: 7.865  time: 0.2093(76.45)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:00] lb.utils.events INFO:  eta: 0:02:04  iteration: 402/1000  consumed samples: 6432  total_loss: 7.865  time: 0.2093(76.45)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:51:00] lb.utils.events INFO:  eta: 0:02:04  iteration: 403/1000  consumed samples: 6448  total_loss: 7.864  time: 0.2093(76.45)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:51:00] lb.utils.events INFO:  eta: 0:02:04  iteration: 404/1000  consumed samples: 6464  total_loss: 7.864  time: 0.2093(76.45)  data_time: 0.0033  lr: 9.90e-05  
[01/19 11:51:00] lb.utils.events INFO:  eta: 0:02:04  iteration: 405/1000  consumed samples: 6480  total_loss: 7.864  time: 0.2093(76.44)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:51:01] lb.utils.events INFO:  eta: 0:02:04  iteration: 406/1000  consumed samples: 6496  total_loss: 7.864  time: 0.2093(76.44)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:01] lb.utils.events INFO:  eta: 0:02:03  iteration: 407/1000  consumed samples: 6512  total_loss: 7.864  time: 0.2093(76.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:01] lb.utils.events INFO:  eta: 0:02:03  iteration: 408/1000  consumed samples: 6528  total_loss: 7.863  time: 0.2093(76.43)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:01] lb.utils.events INFO:  eta: 0:02:03  iteration: 409/1000  consumed samples: 6544  total_loss: 7.863  time: 0.2093(76.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:02] lb.utils.events INFO:  eta: 0:02:03  iteration: 410/1000  consumed samples: 6560  total_loss: 7.864  time: 0.2094(76.42)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:02] lb.utils.events INFO:  eta: 0:02:03  iteration: 411/1000  consumed samples: 6576  total_loss: 7.863  time: 0.2094(76.42)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:02] lb.utils.events INFO:  eta: 0:02:02  iteration: 412/1000  consumed samples: 6592  total_loss: 7.863  time: 0.2094(76.42)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:02] lb.utils.events INFO:  eta: 0:02:02  iteration: 413/1000  consumed samples: 6608  total_loss: 7.861  time: 0.2094(76.41)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:51:02] lb.utils.events INFO:  eta: 0:02:02  iteration: 414/1000  consumed samples: 6624  total_loss: 7.861  time: 0.2094(76.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:03] lb.utils.events INFO:  eta: 0:02:02  iteration: 415/1000  consumed samples: 6640  total_loss: 7.859  time: 0.2094(76.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:03] lb.utils.events INFO:  eta: 0:02:02  iteration: 416/1000  consumed samples: 6656  total_loss: 7.859  time: 0.2094(76.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:03] lb.utils.events INFO:  eta: 0:02:01  iteration: 417/1000  consumed samples: 6672  total_loss: 7.859  time: 0.2094(76.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:03] lb.utils.events INFO:  eta: 0:02:01  iteration: 418/1000  consumed samples: 6688  total_loss: 7.859  time: 0.2094(76.39)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 419/1000  consumed samples: 6704  total_loss: 7.858  time: 0.2094(76.39)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 420/1000  consumed samples: 6720  total_loss: 7.856  time: 0.2094(76.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:51:04] lb.utils.events INFO:  eta: 0:02:00  iteration: 421/1000  consumed samples: 6736  total_loss: 7.856  time: 0.2094(76.40)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:51:04] lb.utils.events INFO:  eta: 0:02:00  iteration: 422/1000  consumed samples: 6752  total_loss: 7.854  time: 0.2095(76.39)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:51:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 423/1000  consumed samples: 6768  total_loss: 7.853  time: 0.2094(76.40)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:51:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 424/1000  consumed samples: 6784  total_loss: 7.852  time: 0.2095(76.39)  data_time: 0.0048  lr: 9.90e-05  
[01/19 11:51:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 425/1000  consumed samples: 6800  total_loss: 7.852  time: 0.2095(76.38)  data_time: 0.0047  lr: 9.90e-05  
[01/19 11:51:05] lb.utils.events INFO:  eta: 0:01:59  iteration: 426/1000  consumed samples: 6816  total_loss: 7.853  time: 0.2095(76.38)  data_time: 0.0046  lr: 9.90e-05  
[01/19 11:51:05] lb.utils.events INFO:  eta: 0:01:59  iteration: 427/1000  consumed samples: 6832  total_loss: 7.852  time: 0.2095(76.38)  data_time: 0.0046  lr: 9.90e-05  
[01/19 11:51:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 428/1000  consumed samples: 6848  total_loss: 7.852  time: 0.2095(76.36)  data_time: 0.0046  lr: 9.90e-05  
[01/19 11:51:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 429/1000  consumed samples: 6864  total_loss: 7.852  time: 0.2095(76.36)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 430/1000  consumed samples: 6880  total_loss: 7.852  time: 0.2095(76.36)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:06] lb.utils.events INFO:  eta: 0:01:58  iteration: 431/1000  consumed samples: 6896  total_loss: 7.851  time: 0.2095(76.36)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 432/1000  consumed samples: 6912  total_loss: 7.851  time: 0.2096(76.35)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 433/1000  consumed samples: 6928  total_loss: 7.851  time: 0.2096(76.35)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:51:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 434/1000  consumed samples: 6944  total_loss: 7.85  time: 0.2096(76.35)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 435/1000  consumed samples: 6960  total_loss: 7.85  time: 0.2096(76.35)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 436/1000  consumed samples: 6976  total_loss: 7.85  time: 0.2096(76.34)  data_time: 0.0045  lr: 9.90e-05  
[01/19 11:51:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 437/1000  consumed samples: 6992  total_loss: 7.85  time: 0.2096(76.35)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:51:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 438/1000  consumed samples: 7008  total_loss: 7.85  time: 0.2096(76.34)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:51:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 439/1000  consumed samples: 7024  total_loss: 7.85  time: 0.2096(76.34)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:51:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 440/1000  consumed samples: 7040  total_loss: 7.849  time: 0.2096(76.33)  data_time: 0.0044  lr: 9.90e-05  
[01/19 11:51:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 441/1000  consumed samples: 7056  total_loss: 7.849  time: 0.2096(76.33)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 442/1000  consumed samples: 7072  total_loss: 7.849  time: 0.2096(76.32)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 443/1000  consumed samples: 7088  total_loss: 7.847  time: 0.2096(76.33)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 444/1000  consumed samples: 7104  total_loss: 7.846  time: 0.2096(76.32)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 445/1000  consumed samples: 7120  total_loss: 7.844  time: 0.2096(76.32)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 446/1000  consumed samples: 7136  total_loss: 7.843  time: 0.2097(76.31)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 447/1000  consumed samples: 7152  total_loss: 7.843  time: 0.2097(76.31)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 448/1000  consumed samples: 7168  total_loss: 7.843  time: 0.2097(76.31)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 449/1000  consumed samples: 7184  total_loss: 7.843  time: 0.2097(76.30)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:51:11] lb.utils.events INFO:  eta: 0:01:54  iteration: 450/1000  consumed samples: 7200  total_loss: 7.843  time: 0.2097(76.30)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:51:11] lb.utils.events INFO:  eta: 0:01:54  iteration: 451/1000  consumed samples: 7216  total_loss: 7.843  time: 0.2097(76.30)  data_time: 0.0041  lr: 9.90e-05  
[01/19 11:51:11] lb.utils.events INFO:  eta: 0:01:54  iteration: 452/1000  consumed samples: 7232  total_loss: 7.841  time: 0.2097(76.29)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:51:11] lb.utils.events INFO:  eta: 0:01:54  iteration: 453/1000  consumed samples: 7248  total_loss: 7.84  time: 0.2097(76.29)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:51:12] lb.utils.events INFO:  eta: 0:01:54  iteration: 454/1000  consumed samples: 7264  total_loss: 7.837  time: 0.2098(76.28)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:12] lb.utils.events INFO:  eta: 0:01:53  iteration: 455/1000  consumed samples: 7280  total_loss: 7.833  time: 0.2098(76.28)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:12] lb.utils.events INFO:  eta: 0:01:53  iteration: 456/1000  consumed samples: 7296  total_loss: 7.831  time: 0.2098(76.28)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:12] lb.utils.events INFO:  eta: 0:01:53  iteration: 457/1000  consumed samples: 7312  total_loss: 7.829  time: 0.2098(76.28)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:13] lb.utils.events INFO:  eta: 0:01:53  iteration: 458/1000  consumed samples: 7328  total_loss: 7.826  time: 0.2098(76.27)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:13] lb.utils.events INFO:  eta: 0:01:53  iteration: 459/1000  consumed samples: 7344  total_loss: 7.824  time: 0.2098(76.27)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:13] lb.utils.events INFO:  eta: 0:01:52  iteration: 460/1000  consumed samples: 7360  total_loss: 7.823  time: 0.2098(76.27)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:13] lb.utils.events INFO:  eta: 0:01:52  iteration: 461/1000  consumed samples: 7376  total_loss: 7.824  time: 0.2098(76.27)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:13] lb.utils.events INFO:  eta: 0:01:52  iteration: 462/1000  consumed samples: 7392  total_loss: 7.824  time: 0.2098(76.26)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:14] lb.utils.events INFO:  eta: 0:01:52  iteration: 463/1000  consumed samples: 7408  total_loss: 7.824  time: 0.2098(76.26)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:14] lb.utils.events INFO:  eta: 0:01:52  iteration: 464/1000  consumed samples: 7424  total_loss: 7.823  time: 0.2098(76.26)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:14] lb.utils.events INFO:  eta: 0:01:51  iteration: 465/1000  consumed samples: 7440  total_loss: 7.823  time: 0.2098(76.26)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:14] lb.utils.events INFO:  eta: 0:01:51  iteration: 466/1000  consumed samples: 7456  total_loss: 7.823  time: 0.2098(76.25)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:15] lb.utils.events INFO:  eta: 0:01:51  iteration: 467/1000  consumed samples: 7472  total_loss: 7.822  time: 0.2098(76.25)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:15] lb.utils.events INFO:  eta: 0:01:51  iteration: 468/1000  consumed samples: 7488  total_loss: 7.822  time: 0.2099(76.24)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:15] lb.utils.events INFO:  eta: 0:01:51  iteration: 469/1000  consumed samples: 7504  total_loss: 7.822  time: 0.2099(76.24)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:51:15] lb.utils.events INFO:  eta: 0:01:50  iteration: 470/1000  consumed samples: 7520  total_loss: 7.822  time: 0.2099(76.24)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:51:16] lb.utils.events INFO:  eta: 0:01:50  iteration: 471/1000  consumed samples: 7536  total_loss: 7.82  time: 0.2099(76.24)  data_time: 0.0034  lr: 9.90e-05  
[01/19 11:51:16] lb.utils.events INFO:  eta: 0:01:50  iteration: 472/1000  consumed samples: 7552  total_loss: 7.82  time: 0.2099(76.24)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:51:16] lb.utils.events INFO:  eta: 0:01:50  iteration: 473/1000  consumed samples: 7568  total_loss: 7.82  time: 0.2099(76.24)  data_time: 0.0035  lr: 9.90e-05  
[01/19 11:51:16] lb.utils.events INFO:  eta: 0:01:49  iteration: 474/1000  consumed samples: 7584  total_loss: 7.819  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:16] lb.utils.events INFO:  eta: 0:01:49  iteration: 475/1000  consumed samples: 7600  total_loss: 7.818  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:17] lb.utils.events INFO:  eta: 0:01:49  iteration: 476/1000  consumed samples: 7616  total_loss: 7.816  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:17] lb.utils.events INFO:  eta: 0:01:49  iteration: 477/1000  consumed samples: 7632  total_loss: 7.816  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:17] lb.utils.events INFO:  eta: 0:01:49  iteration: 478/1000  consumed samples: 7648  total_loss: 7.815  time: 0.2099(76.22)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 479/1000  consumed samples: 7664  total_loss: 7.815  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 480/1000  consumed samples: 7680  total_loss: 7.814  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 481/1000  consumed samples: 7696  total_loss: 7.814  time: 0.2099(76.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 482/1000  consumed samples: 7712  total_loss: 7.814  time: 0.2099(76.22)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 483/1000  consumed samples: 7728  total_loss: 7.814  time: 0.2099(76.22)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:18] lb.utils.events INFO:  eta: 0:01:47  iteration: 484/1000  consumed samples: 7744  total_loss: 7.814  time: 0.2099(76.22)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 485/1000  consumed samples: 7760  total_loss: 7.814  time: 0.2099(76.22)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 486/1000  consumed samples: 7776  total_loss: 7.813  time: 0.2099(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 487/1000  consumed samples: 7792  total_loss: 7.812  time: 0.2099(76.22)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 488/1000  consumed samples: 7808  total_loss: 7.812  time: 0.2099(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 489/1000  consumed samples: 7824  total_loss: 7.812  time: 0.2099(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 490/1000  consumed samples: 7840  total_loss: 7.812  time: 0.2100(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 491/1000  consumed samples: 7856  total_loss: 7.812  time: 0.2099(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 492/1000  consumed samples: 7872  total_loss: 7.811  time: 0.2099(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:21] lb.utils.events INFO:  eta: 0:01:46  iteration: 493/1000  consumed samples: 7888  total_loss: 7.811  time: 0.2099(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 494/1000  consumed samples: 7904  total_loss: 7.81  time: 0.2100(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 495/1000  consumed samples: 7920  total_loss: 7.807  time: 0.2100(76.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 496/1000  consumed samples: 7936  total_loss: 7.805  time: 0.2100(76.20)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 497/1000  consumed samples: 7952  total_loss: 7.805  time: 0.2100(76.20)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 498/1000  consumed samples: 7968  total_loss: 7.805  time: 0.2100(76.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 499/1000  consumed samples: 7984  total_loss: 7.801  time: 0.2100(76.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 500/1000  consumed samples: 8000  total_loss: 7.796  time: 0.2100(76.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 501/1000  consumed samples: 8016  total_loss: 7.795  time: 0.2100(76.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:23] lb.utils.events INFO:  eta: 0:01:44  iteration: 502/1000  consumed samples: 8032  total_loss: 7.795  time: 0.2100(76.20)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:23] lb.utils.events INFO:  eta: 0:01:43  iteration: 503/1000  consumed samples: 8048  total_loss: 7.795  time: 0.2100(76.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:23] lb.utils.events INFO:  eta: 0:01:43  iteration: 504/1000  consumed samples: 8064  total_loss: 7.795  time: 0.2100(76.20)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:23] lb.utils.events INFO:  eta: 0:01:43  iteration: 505/1000  consumed samples: 8080  total_loss: 7.794  time: 0.2100(76.19)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:24] lb.utils.events INFO:  eta: 0:01:43  iteration: 506/1000  consumed samples: 8096  total_loss: 7.794  time: 0.2100(76.19)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:24] lb.utils.events INFO:  eta: 0:01:43  iteration: 507/1000  consumed samples: 8112  total_loss: 7.794  time: 0.2100(76.19)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:24] lb.utils.events INFO:  eta: 0:01:42  iteration: 508/1000  consumed samples: 8128  total_loss: 7.791  time: 0.2100(76.19)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:24] lb.utils.events INFO:  eta: 0:01:42  iteration: 509/1000  consumed samples: 8144  total_loss: 7.791  time: 0.2100(76.19)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:24] lb.utils.events INFO:  eta: 0:01:42  iteration: 510/1000  consumed samples: 8160  total_loss: 7.791  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:25] lb.utils.events INFO:  eta: 0:01:42  iteration: 511/1000  consumed samples: 8176  total_loss: 7.791  time: 0.2100(76.19)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:25] lb.utils.events INFO:  eta: 0:01:42  iteration: 512/1000  consumed samples: 8192  total_loss: 7.788  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:25] lb.utils.events INFO:  eta: 0:01:41  iteration: 513/1000  consumed samples: 8208  total_loss: 7.788  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:25] lb.utils.events INFO:  eta: 0:01:41  iteration: 514/1000  consumed samples: 8224  total_loss: 7.788  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:26] lb.utils.events INFO:  eta: 0:01:41  iteration: 515/1000  consumed samples: 8240  total_loss: 7.786  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:26] lb.utils.events INFO:  eta: 0:01:41  iteration: 516/1000  consumed samples: 8256  total_loss: 7.786  time: 0.2100(76.17)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:51:26] lb.utils.events INFO:  eta: 0:01:41  iteration: 517/1000  consumed samples: 8272  total_loss: 7.786  time: 0.2100(76.17)  data_time: 0.0040  lr: 9.90e-05  
[01/19 11:51:26] lb.utils.events INFO:  eta: 0:01:40  iteration: 518/1000  consumed samples: 8288  total_loss: 7.786  time: 0.2101(76.17)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:26] lb.utils.events INFO:  eta: 0:01:40  iteration: 519/1000  consumed samples: 8304  total_loss: 7.785  time: 0.2101(76.17)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:27] lb.utils.events INFO:  eta: 0:01:40  iteration: 520/1000  consumed samples: 8320  total_loss: 7.785  time: 0.2101(76.16)  data_time: 0.0039  lr: 9.90e-05  
[01/19 11:51:27] lb.utils.events INFO:  eta: 0:01:40  iteration: 521/1000  consumed samples: 8336  total_loss: 7.785  time: 0.2101(76.16)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:27] lb.utils.events INFO:  eta: 0:01:39  iteration: 522/1000  consumed samples: 8352  total_loss: 7.785  time: 0.2101(76.16)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:27] lb.utils.events INFO:  eta: 0:01:39  iteration: 523/1000  consumed samples: 8368  total_loss: 7.781  time: 0.2101(76.16)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:28] lb.utils.events INFO:  eta: 0:01:39  iteration: 524/1000  consumed samples: 8384  total_loss: 7.776  time: 0.2101(76.15)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:28] lb.utils.events INFO:  eta: 0:01:39  iteration: 525/1000  consumed samples: 8400  total_loss: 7.774  time: 0.2101(76.15)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:28] lb.utils.events INFO:  eta: 0:01:39  iteration: 526/1000  consumed samples: 8416  total_loss: 7.774  time: 0.2101(76.14)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:28] lb.utils.events INFO:  eta: 0:01:38  iteration: 527/1000  consumed samples: 8432  total_loss: 7.776  time: 0.2101(76.14)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:29] lb.utils.events INFO:  eta: 0:01:38  iteration: 528/1000  consumed samples: 8448  total_loss: 7.774  time: 0.2101(76.14)  data_time: 0.0038  lr: 9.90e-05  
[01/19 11:51:29] lb.utils.events INFO:  eta: 0:01:38  iteration: 529/1000  consumed samples: 8464  total_loss: 7.771  time: 0.2101(76.14)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:29] lb.utils.events INFO:  eta: 0:01:38  iteration: 530/1000  consumed samples: 8480  total_loss: 7.768  time: 0.2101(76.14)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:29] lb.utils.events INFO:  eta: 0:01:38  iteration: 531/1000  consumed samples: 8496  total_loss: 7.768  time: 0.2101(76.14)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:29] lb.utils.events INFO:  eta: 0:01:37  iteration: 532/1000  consumed samples: 8512  total_loss: 7.768  time: 0.2102(76.13)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:30] lb.utils.events INFO:  eta: 0:01:37  iteration: 533/1000  consumed samples: 8528  total_loss: 7.768  time: 0.2102(76.13)  data_time: 0.0037  lr: 9.90e-05  
[01/19 11:51:30] lb.utils.events INFO:  eta: 0:01:37  iteration: 534/1000  consumed samples: 8544  total_loss: 7.767  time: 0.2102(76.13)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:30] lb.utils.events INFO:  eta: 0:01:37  iteration: 535/1000  consumed samples: 8560  total_loss: 7.767  time: 0.2102(76.13)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:30] lb.utils.events INFO:  eta: 0:01:37  iteration: 536/1000  consumed samples: 8576  total_loss: 7.767  time: 0.2102(76.12)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:31] lb.utils.events INFO:  eta: 0:01:36  iteration: 537/1000  consumed samples: 8592  total_loss: 7.767  time: 0.2102(76.12)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:31] lb.utils.events INFO:  eta: 0:01:36  iteration: 538/1000  consumed samples: 8608  total_loss: 7.768  time: 0.2102(76.12)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:31] lb.utils.events INFO:  eta: 0:01:36  iteration: 539/1000  consumed samples: 8624  total_loss: 7.767  time: 0.2102(76.12)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:31] lb.utils.events INFO:  eta: 0:01:36  iteration: 540/1000  consumed samples: 8640  total_loss: 7.767  time: 0.2102(76.12)  data_time: 0.0036  lr: 9.90e-05  
[01/19 11:51:32] lb.utils.events INFO:  eta: 0:01:36  iteration: 541/1000  consumed samples: 8656  total_loss: 7.767  time: 0.2102(76.12)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:32] lb.utils.events INFO:  eta: 0:01:35  iteration: 542/1000  consumed samples: 8672  total_loss: 7.766  time: 0.2102(76.11)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:32] lb.utils.events INFO:  eta: 0:01:35  iteration: 543/1000  consumed samples: 8688  total_loss: 7.766  time: 0.2102(76.11)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:32] lb.utils.events INFO:  eta: 0:01:35  iteration: 544/1000  consumed samples: 8704  total_loss: 7.766  time: 0.2102(76.12)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:32] lb.utils.events INFO:  eta: 0:01:35  iteration: 545/1000  consumed samples: 8720  total_loss: 7.765  time: 0.2102(76.11)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:33] lb.utils.events INFO:  eta: 0:01:35  iteration: 546/1000  consumed samples: 8736  total_loss: 7.764  time: 0.2102(76.11)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:33] lb.utils.events INFO:  eta: 0:01:34  iteration: 547/1000  consumed samples: 8752  total_loss: 7.764  time: 0.2102(76.11)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:33] lb.utils.events INFO:  eta: 0:01:34  iteration: 548/1000  consumed samples: 8768  total_loss: 7.759  time: 0.2102(76.11)  data_time: 0.0080  lr: 9.90e-05  
[01/19 11:51:33] lb.utils.events INFO:  eta: 0:01:34  iteration: 549/1000  consumed samples: 8784  total_loss: 7.759  time: 0.2102(76.10)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:34] lb.utils.events INFO:  eta: 0:01:34  iteration: 550/1000  consumed samples: 8800  total_loss: 7.758  time: 0.2103(76.10)  data_time: 0.0083  lr: 9.90e-05  
[01/19 11:51:34] lb.utils.events INFO:  eta: 0:01:33  iteration: 551/1000  consumed samples: 8816  total_loss: 7.754  time: 0.2103(76.09)  data_time: 0.0083  lr: 9.90e-05  
[01/19 11:51:34] lb.utils.events INFO:  eta: 0:01:33  iteration: 552/1000  consumed samples: 8832  total_loss: 7.754  time: 0.2103(76.09)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:34] lb.utils.events INFO:  eta: 0:01:33  iteration: 553/1000  consumed samples: 8848  total_loss: 7.752  time: 0.2103(76.09)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:35] lb.utils.events INFO:  eta: 0:01:33  iteration: 554/1000  consumed samples: 8864  total_loss: 7.751  time: 0.2103(76.09)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:35] lb.utils.events INFO:  eta: 0:01:33  iteration: 555/1000  consumed samples: 8880  total_loss: 7.751  time: 0.2103(76.10)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:35] lb.utils.events INFO:  eta: 0:01:32  iteration: 556/1000  consumed samples: 8896  total_loss: 7.751  time: 0.2103(76.09)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:35] lb.utils.events INFO:  eta: 0:01:32  iteration: 557/1000  consumed samples: 8912  total_loss: 7.746  time: 0.2103(76.09)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:35] lb.utils.events INFO:  eta: 0:01:32  iteration: 558/1000  consumed samples: 8928  total_loss: 7.746  time: 0.2103(76.08)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:51:36] lb.utils.events INFO:  eta: 0:01:32  iteration: 559/1000  consumed samples: 8944  total_loss: 7.741  time: 0.2103(76.09)  data_time: 0.0082  lr: 9.90e-05  
[01/19 11:51:36] lb.utils.events INFO:  eta: 0:01:32  iteration: 560/1000  consumed samples: 8960  total_loss: 7.741  time: 0.2103(76.08)  data_time: 0.0081  lr: 9.90e-05  
[01/19 11:52:20] libai INFO: Rank of current process: 0. World size: 1
[01/19 11:52:20] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 11:52:20] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mbias_dropout_fusion[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 11:52:20] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 11:52:20] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 11:52:20] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 11:52:23] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 11:52:23] libai INFO: Loadding megatron weight
[01/19 11:52:23] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 11:52:23] lb.utils.load_megatron_weight INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbert.encoders.0.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.0.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.0.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.0.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.0.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.0.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.1.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.1.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.1.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.1.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.1.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.1.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.2.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.2.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.2.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.2.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.2.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.2.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.3.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.3.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.3.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.3.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.3.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.3.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.4.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.4.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.4.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.4.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.4.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.4.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.final_layernorm.{weight, bias}[0m
[01/19 11:52:23] lb.utils.load_megatron_weight INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mbert.encoders.layers_0.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_0.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_0.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_0.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_0.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_0.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_1.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_1.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_1.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_1.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_1.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_1.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_2.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_2.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_2.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_2.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_2.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_2.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_3.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_3.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_3.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_3.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_3.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_3.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_4.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_4.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_4.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_4.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_4.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_4.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.final_layernorm.{weight, bias}[0m
[01/19 11:52:23] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 11:52:23] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 11:52:23] lb.data.build INFO:     train:      16000
[01/19 11:52:23] lb.data.build INFO:     validation: 160000
[01/19 11:52:23] lb.data.build INFO:     test:       160000
[01/19 11:52:23] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 11:52:23] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 11:52:23] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 11:52:24] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 11:52:24] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 11:52:24] lb.data.indexed_dataset INFO:     reading document index...
[01/19 11:52:24] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 11:52:24] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 11:52:24] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 11:52:24] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.124857 seconds
[01/19 11:52:24] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 11:52:24] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 11:52:24] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 11:52:24] lb.data.dataset_utils INFO:  > dataset split:
[01/19 11:52:24] lb.data.dataset_utils INFO:     train:
[01/19 11:52:24] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 11:52:24] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 11:52:24] lb.data.dataset_utils INFO:     validation:
[01/19 11:52:24] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 11:52:24] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 11:52:24] lb.data.dataset_utils INFO:     test:
[01/19 11:52:24] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 11:52:24] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 11:52:24] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 11:52:24] lb.data.dataset_utils INFO:     loaded indexed file in 0.005 seconds
[01/19 11:52:24] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 11:52:24] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:52:24] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:52:24] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 11:52:24] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:52:24] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:52:24] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 11:52:24] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 11:52:24] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 11:52:54] libai INFO: Rank of current process: 0. World size: 1
[01/19 11:52:54] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 11:52:54] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mbias_dropout_fusion[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 11:52:54] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 11:52:55] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 11:52:55] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 11:52:57] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 11:52:57] libai INFO: Loadding megatron weight
[01/19 11:52:57] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 11:52:58] lb.utils.load_megatron_weight INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbert.encoders.0.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.0.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.0.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.0.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.0.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.0.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.1.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.1.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.1.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.1.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.1.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.1.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.2.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.2.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.2.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.2.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.2.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.2.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.3.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.3.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.3.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.3.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.3.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.3.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.encoders.4.input_layernorm.{weight, bias}[0m
  [34mbert.encoders.4.self_attention.query_key_value.{weight, bias}[0m
  [34mbert.encoders.4.self_attention.dense.{weight, bias}[0m
  [34mbert.encoders.4.post_attention_layernorm.{weight, bias}[0m
  [34mbert.encoders.4.mlp.dense_h_to_4h.{weight, bias}[0m
  [34mbert.encoders.4.mlp.dense_4h_to_h.{weight, bias}[0m
  [34mbert.final_layernorm.{weight, bias}[0m
[01/19 11:52:58] lb.utils.load_megatron_weight INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mbert.encoders.layers_0.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_0.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_0.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_0.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_0.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_0.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_1.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_1.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_1.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_1.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_1.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_1.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_2.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_2.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_2.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_2.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_2.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_2.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_3.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_3.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_3.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_3.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_3.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_3.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.layers_4.input_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_4.self_attention.query_key_value.{weight, bias}[0m
  [35mbert.encoders.layers_4.self_attention.dense.{weight, bias}[0m
  [35mbert.encoders.layers_4.post_attention_layernorm.{weight, bias}[0m
  [35mbert.encoders.layers_4.mlp.dense_h_to_4h.{weight, bias}[0m
  [35mbert.encoders.layers_4.mlp.dense_4h_to_h.{weight, bias}[0m
  [35mbert.encoders.final_layernorm.{weight, bias}[0m
[01/19 11:52:58] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 11:52:58] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 11:52:58] lb.data.build INFO:     train:      16000
[01/19 11:52:58] lb.data.build INFO:     validation: 160000
[01/19 11:52:58] lb.data.build INFO:     test:       160000
[01/19 11:52:58] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 11:52:58] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     reading document index...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 11:52:58] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 11:52:58] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.116534 seconds
[01/19 11:52:58] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 11:52:58] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 11:52:58] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 11:52:58] lb.data.dataset_utils INFO:  > dataset split:
[01/19 11:52:58] lb.data.dataset_utils INFO:     train:
[01/19 11:52:58] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 11:52:58] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 11:52:58] lb.data.dataset_utils INFO:     validation:
[01/19 11:52:58] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 11:52:58] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 11:52:58] lb.data.dataset_utils INFO:     test:
[01/19 11:52:58] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 11:52:58] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 11:52:58] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 11:52:58] lb.data.dataset_utils INFO:     loaded indexed file in 0.005 seconds
[01/19 11:52:58] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 11:52:58] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:52:58] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:52:58] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 11:52:58] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:52:58] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:52:58] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 11:52:58] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 11:52:58] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 11:53:03] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 10.52  data_time: 0.2545  lr: 0.00e+00  
[01/19 11:53:03] lb.utils.events INFO:  eta: 0:03:58  iteration: 2/1000  consumed samples: 32  total_loss: 10.55  data_time: 0.1300  lr: 1.00e-06  
[01/19 11:53:03] lb.utils.events INFO:  eta: 0:03:21  iteration: 3/1000  consumed samples: 48  total_loss: 10.57  time: 0.2026(78.98)  data_time: 0.0875  lr: 2.00e-06  
[01/19 11:53:04] lb.utils.events INFO:  eta: 0:03:25  iteration: 4/1000  consumed samples: 64  total_loss: 10.55  time: 0.2061(77.62)  data_time: 0.0664  lr: 3.00e-06  
[01/19 11:53:04] lb.utils.events INFO:  eta: 0:03:21  iteration: 5/1000  consumed samples: 80  total_loss: 10.52  time: 0.2051(78.00)  data_time: 0.0614  lr: 4.00e-06  
[01/19 11:53:04] lb.utils.events INFO:  eta: 0:03:24  iteration: 6/1000  consumed samples: 96  total_loss: 10.5  time: 0.2063(77.56)  data_time: 0.0519  lr: 5.00e-06  
[01/19 11:53:04] lb.utils.events INFO:  eta: 0:03:21  iteration: 7/1000  consumed samples: 112  total_loss: 10.47  time: 0.2051(78.00)  data_time: 0.0448  lr: 6.00e-06  
[01/19 11:53:04] lb.utils.events INFO:  eta: 0:03:21  iteration: 8/1000  consumed samples: 128  total_loss: 10.37  time: 0.2046(78.21)  data_time: 0.0396  lr: 7.00e-06  
[01/19 11:53:05] lb.utils.events INFO:  eta: 0:03:21  iteration: 9/1000  consumed samples: 144  total_loss: 10.27  time: 0.2053(77.92)  data_time: 0.0358  lr: 8.00e-06  
[01/19 11:53:05] lb.utils.events INFO:  eta: 0:03:24  iteration: 10/1000  consumed samples: 160  total_loss: 10.2  time: 0.2059(77.72)  data_time: 0.0328  lr: 9.00e-06  
[01/19 11:53:05] lb.utils.events INFO:  eta: 0:03:21  iteration: 11/1000  consumed samples: 176  total_loss: 10.14  time: 0.2057(77.80)  data_time: 0.0300  lr: 1.00e-05  
[01/19 11:53:05] lb.utils.events INFO:  eta: 0:03:20  iteration: 12/1000  consumed samples: 192  total_loss: 10.01  time: 0.2053(77.93)  data_time: 0.0278  lr: 1.10e-05  
[01/19 11:53:06] lb.utils.events INFO:  eta: 0:03:20  iteration: 13/1000  consumed samples: 208  total_loss: 9.88  time: 0.2051(78.01)  data_time: 0.0267  lr: 1.20e-05  
[01/19 11:55:27] libai INFO: Rank of current process: 0. World size: 1
[01/19 11:55:27] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 11:55:27] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mbias_dropout_fusion[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 11:55:27] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 11:55:27] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 11:55:27] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 11:55:30] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 11:55:30] libai INFO: Loadding megatron weight
[01/19 11:55:30] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 11:55:31] lb.utils.load_megatron_weight INFO: Some model parameters or buffers are not found in the checkpoint:
  [34mbert.final_layernorm.{weight, bias}[0m
[01/19 11:55:31] lb.utils.load_megatron_weight INFO: The checkpoint state_dict contains keys that are not used by the model:
  [35mbert.encoders.final_layernorm.{weight, bias}[0m
[01/19 11:55:31] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 11:55:31] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 11:55:31] lb.data.build INFO:     train:      16000
[01/19 11:55:31] lb.data.build INFO:     validation: 160000
[01/19 11:55:31] lb.data.build INFO:     test:       160000
[01/19 11:55:31] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 11:55:31] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     reading document index...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 11:55:31] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 11:55:31] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.116585 seconds
[01/19 11:55:31] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 11:55:31] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 11:55:31] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 11:55:31] lb.data.dataset_utils INFO:  > dataset split:
[01/19 11:55:31] lb.data.dataset_utils INFO:     train:
[01/19 11:55:31] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 11:55:31] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 11:55:31] lb.data.dataset_utils INFO:     validation:
[01/19 11:55:31] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 11:55:31] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 11:55:31] lb.data.dataset_utils INFO:     test:
[01/19 11:55:31] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 11:55:31] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 11:55:31] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 11:55:31] lb.data.dataset_utils INFO:     loaded indexed file in 0.006 seconds
[01/19 11:55:31] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 11:55:31] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:55:31] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:55:31] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 11:55:31] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 11:55:31] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 11:55:31] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 11:55:31] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 11:55:31] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 12:00:36] libai INFO: Rank of current process: 0. World size: 1
[01/19 12:00:36] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 12:00:36] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mbias_dropout_fusion[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 12:00:36] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 12:00:36] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 12:00:36] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 12:00:39] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 12:00:39] libai INFO: Loadding megatron weight
[01/19 12:00:39] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 12:00:39] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 12:00:39] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 12:00:39] lb.data.build INFO:     train:      16000
[01/19 12:00:39] lb.data.build INFO:     validation: 160000
[01/19 12:00:39] lb.data.build INFO:     test:       160000
[01/19 12:00:39] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 12:00:39] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     reading document index...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 12:00:39] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 12:00:39] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.119670 seconds
[01/19 12:00:39] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 12:00:39] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 12:00:39] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 12:00:39] lb.data.dataset_utils INFO:  > dataset split:
[01/19 12:00:39] lb.data.dataset_utils INFO:     train:
[01/19 12:00:39] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 12:00:39] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 12:00:39] lb.data.dataset_utils INFO:     validation:
[01/19 12:00:39] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 12:00:39] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 12:00:39] lb.data.dataset_utils INFO:     test:
[01/19 12:00:39] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 12:00:39] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 12:00:39] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 12:00:39] lb.data.dataset_utils INFO:     loaded indexed file in 0.006 seconds
[01/19 12:00:39] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 12:00:39] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 12:00:39] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 12:00:39] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 12:00:39] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 12:00:39] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 12:00:39] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 12:00:39] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 12:00:40] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 12:00:44] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.2473  lr: 0.00e+00  
[01/19 12:00:44] lb.utils.events INFO:  eta: 0:03:54  iteration: 2/1000  consumed samples: 32  total_loss: 8.65  data_time: 0.1275  lr: 1.00e-06  
[01/19 12:00:45] lb.utils.events INFO:  eta: 0:03:25  iteration: 3/1000  consumed samples: 48  total_loss: 8.663  time: 0.2058(77.73)  data_time: 0.0859  lr: 2.00e-06  
[01/19 12:00:45] lb.utils.events INFO:  eta: 0:03:25  iteration: 4/1000  consumed samples: 64  total_loss: 8.65  time: 0.2060(77.65)  data_time: 0.0652  lr: 3.00e-06  
[01/19 12:00:45] lb.utils.events INFO:  eta: 0:03:24  iteration: 5/1000  consumed samples: 80  total_loss: 8.663  time: 0.2038(78.52)  data_time: 0.0576  lr: 4.00e-06  
[01/19 12:00:45] lb.utils.events INFO:  eta: 0:03:24  iteration: 6/1000  consumed samples: 96  total_loss: 8.68  time: 0.2045(78.24)  data_time: 0.0487  lr: 5.00e-06  
[01/19 12:00:46] lb.utils.events INFO:  eta: 0:03:24  iteration: 7/1000  consumed samples: 112  total_loss: 8.663  time: 0.2036(78.57)  data_time: 0.0421  lr: 6.00e-06  
[01/19 12:00:46] lb.utils.events INFO:  eta: 0:03:24  iteration: 8/1000  consumed samples: 128  total_loss: 8.658  time: 0.2042(78.35)  data_time: 0.0372  lr: 7.00e-06  
[01/19 12:00:46] lb.utils.events INFO:  eta: 0:03:23  iteration: 9/1000  consumed samples: 144  total_loss: 8.652  time: 0.2036(78.59)  data_time: 0.0336  lr: 8.00e-06  
[01/19 12:00:46] lb.utils.events INFO:  eta: 0:03:23  iteration: 10/1000  consumed samples: 160  total_loss: 8.657  time: 0.2039(78.45)  data_time: 0.0306  lr: 9.00e-06  
[01/19 12:00:46] lb.utils.events INFO:  eta: 0:03:23  iteration: 11/1000  consumed samples: 176  total_loss: 8.662  time: 0.2033(78.72)  data_time: 0.0281  lr: 1.00e-05  
[01/19 12:00:47] lb.utils.events INFO:  eta: 0:03:23  iteration: 12/1000  consumed samples: 192  total_loss: 8.657  time: 0.2036(78.57)  data_time: 0.0260  lr: 1.10e-05  
[01/19 12:00:47] lb.utils.events INFO:  eta: 0:03:22  iteration: 13/1000  consumed samples: 208  total_loss: 8.652  time: 0.2034(78.67)  data_time: 0.0244  lr: 1.20e-05  
[01/19 12:00:47] lb.utils.events INFO:  eta: 0:03:22  iteration: 14/1000  consumed samples: 224  total_loss: 8.657  time: 0.2037(78.56)  data_time: 0.0230  lr: 1.30e-05  
[01/19 12:00:47] lb.utils.events INFO:  eta: 0:03:22  iteration: 15/1000  consumed samples: 240  total_loss: 8.662  time: 0.2041(78.40)  data_time: 0.0217  lr: 1.40e-05  
[01/19 12:00:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 16/1000  consumed samples: 256  total_loss: 8.657  time: 0.2044(78.29)  data_time: 0.0205  lr: 1.50e-05  
[01/19 12:00:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 17/1000  consumed samples: 272  total_loss: 8.652  time: 0.2041(78.39)  data_time: 0.0198  lr: 1.60e-05  
[01/19 12:00:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 18/1000  consumed samples: 288  total_loss: 8.646  time: 0.2048(78.14)  data_time: 0.0190  lr: 1.70e-05  
[01/19 12:00:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 19/1000  consumed samples: 304  total_loss: 8.652  time: 0.2049(78.09)  data_time: 0.0181  lr: 1.80e-05  
[01/19 12:00:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 20/1000  consumed samples: 320  total_loss: 8.657  time: 0.2047(78.18)  data_time: 0.0174  lr: 1.90e-05  
[01/19 12:00:49] lb.utils.events INFO:  eta: 0:03:21  iteration: 21/1000  consumed samples: 336  total_loss: 8.652  time: 0.2048(78.14)  data_time: 0.0053  lr: 2.00e-05  
[01/19 12:00:49] lb.utils.events INFO:  eta: 0:03:21  iteration: 22/1000  consumed samples: 352  total_loss: 8.646  time: 0.2046(78.21)  data_time: 0.0051  lr: 2.10e-05  
[01/19 12:00:49] lb.utils.events INFO:  eta: 0:03:21  iteration: 23/1000  consumed samples: 368  total_loss: 8.64  time: 0.2047(78.15)  data_time: 0.0051  lr: 2.20e-05  
[01/19 12:00:49] lb.utils.events INFO:  eta: 0:03:21  iteration: 24/1000  consumed samples: 384  total_loss: 8.646  time: 0.2046(78.20)  data_time: 0.0051  lr: 2.30e-05  
[01/19 12:00:50] lb.utils.events INFO:  eta: 0:03:21  iteration: 25/1000  consumed samples: 400  total_loss: 8.652  time: 0.2047(78.16)  data_time: 0.0040  lr: 2.40e-05  
[01/19 12:00:50] lb.utils.events INFO:  eta: 0:03:20  iteration: 26/1000  consumed samples: 416  total_loss: 8.646  time: 0.2046(78.21)  data_time: 0.0041  lr: 2.50e-05  
[01/19 12:00:50] lb.utils.events INFO:  eta: 0:03:20  iteration: 27/1000  consumed samples: 432  total_loss: 8.64  time: 0.2047(78.18)  data_time: 0.0040  lr: 2.60e-05  
[01/19 12:00:50] lb.utils.events INFO:  eta: 0:03:20  iteration: 28/1000  consumed samples: 448  total_loss: 8.638  time: 0.2045(78.23)  data_time: 0.0040  lr: 2.70e-05  
[01/19 12:00:50] lb.utils.events INFO:  eta: 0:03:20  iteration: 29/1000  consumed samples: 464  total_loss: 8.637  time: 0.2046(78.19)  data_time: 0.0041  lr: 2.80e-05  
[01/19 12:00:51] lb.utils.events INFO:  eta: 0:03:19  iteration: 30/1000  consumed samples: 480  total_loss: 8.634  time: 0.2045(78.24)  data_time: 0.0042  lr: 2.90e-05  
[01/19 12:00:51] lb.utils.events INFO:  eta: 0:03:19  iteration: 31/1000  consumed samples: 496  total_loss: 8.632  time: 0.2046(78.20)  data_time: 0.0042  lr: 3.00e-05  
[01/19 12:00:51] lb.utils.events INFO:  eta: 0:03:19  iteration: 32/1000  consumed samples: 512  total_loss: 8.631  time: 0.2045(78.24)  data_time: 0.0042  lr: 3.10e-05  
[01/19 12:00:51] lb.utils.events INFO:  eta: 0:03:19  iteration: 33/1000  consumed samples: 528  total_loss: 8.631  time: 0.2046(78.19)  data_time: 0.0042  lr: 3.20e-05  
[01/19 12:00:52] lb.utils.events INFO:  eta: 0:03:19  iteration: 34/1000  consumed samples: 544  total_loss: 8.628  time: 0.2045(78.24)  data_time: 0.0042  lr: 3.30e-05  
[01/19 12:00:52] lb.utils.events INFO:  eta: 0:03:18  iteration: 35/1000  consumed samples: 560  total_loss: 8.625  time: 0.2048(78.11)  data_time: 0.0042  lr: 3.40e-05  
[01/19 12:00:52] lb.utils.events INFO:  eta: 0:03:18  iteration: 36/1000  consumed samples: 576  total_loss: 8.624  time: 0.2047(78.15)  data_time: 0.0042  lr: 3.50e-05  
[01/19 12:00:52] lb.utils.events INFO:  eta: 0:03:18  iteration: 37/1000  consumed samples: 592  total_loss: 8.623  time: 0.2048(78.12)  data_time: 0.0041  lr: 3.60e-05  
[01/19 12:00:52] lb.utils.events INFO:  eta: 0:03:18  iteration: 38/1000  consumed samples: 608  total_loss: 8.621  time: 0.2047(78.15)  data_time: 0.0040  lr: 3.70e-05  
[01/19 12:00:53] lb.utils.events INFO:  eta: 0:03:18  iteration: 39/1000  consumed samples: 624  total_loss: 8.618  time: 0.2048(78.12)  data_time: 0.0041  lr: 3.80e-05  
[01/19 12:00:53] lb.utils.events INFO:  eta: 0:03:17  iteration: 40/1000  consumed samples: 640  total_loss: 8.615  time: 0.2047(78.17)  data_time: 0.0041  lr: 3.90e-05  
[01/19 12:00:53] lb.utils.events INFO:  eta: 0:03:17  iteration: 41/1000  consumed samples: 656  total_loss: 8.612  time: 0.2048(78.13)  data_time: 0.0041  lr: 4.00e-05  
[01/19 12:00:53] lb.utils.events INFO:  eta: 0:03:17  iteration: 42/1000  consumed samples: 672  total_loss: 8.611  time: 0.2047(78.15)  data_time: 0.0041  lr: 4.10e-05  
[01/19 12:00:54] lb.utils.events INFO:  eta: 0:03:17  iteration: 43/1000  consumed samples: 688  total_loss: 8.61  time: 0.2048(78.12)  data_time: 0.0041  lr: 4.20e-05  
[01/19 12:00:54] lb.utils.events INFO:  eta: 0:03:17  iteration: 44/1000  consumed samples: 704  total_loss: 8.609  time: 0.2047(78.15)  data_time: 0.0041  lr: 4.30e-05  
[01/19 12:00:54] lb.utils.events INFO:  eta: 0:03:16  iteration: 45/1000  consumed samples: 720  total_loss: 8.607  time: 0.2048(78.12)  data_time: 0.0057  lr: 4.40e-05  
[01/19 12:00:54] lb.utils.events INFO:  eta: 0:03:16  iteration: 46/1000  consumed samples: 736  total_loss: 8.605  time: 0.2047(78.15)  data_time: 0.0056  lr: 4.50e-05  
[01/19 12:00:54] lb.utils.events INFO:  eta: 0:03:16  iteration: 47/1000  consumed samples: 752  total_loss: 8.603  time: 0.2050(78.06)  data_time: 0.0057  lr: 4.60e-05  
[01/19 12:00:55] lb.utils.events INFO:  eta: 0:03:16  iteration: 48/1000  consumed samples: 768  total_loss: 8.602  time: 0.2049(78.09)  data_time: 0.0056  lr: 4.70e-05  
[01/19 12:00:55] lb.utils.events INFO:  eta: 0:03:16  iteration: 49/1000  consumed samples: 784  total_loss: 8.602  time: 0.2050(78.06)  data_time: 0.0055  lr: 4.80e-05  
[01/19 12:00:55] lb.utils.events INFO:  eta: 0:03:16  iteration: 50/1000  consumed samples: 800  total_loss: 8.593  time: 0.2050(78.03)  data_time: 0.0055  lr: 4.90e-05  
[01/19 12:00:55] lb.utils.events INFO:  eta: 0:03:15  iteration: 51/1000  consumed samples: 816  total_loss: 8.585  time: 0.2051(78.01)  data_time: 0.0055  lr: 5.00e-05  
[01/19 12:00:56] lb.utils.events INFO:  eta: 0:03:15  iteration: 52/1000  consumed samples: 832  total_loss: 8.579  time: 0.2050(78.03)  data_time: 0.0056  lr: 5.10e-05  
[01/19 12:00:56] lb.utils.events INFO:  eta: 0:03:15  iteration: 53/1000  consumed samples: 848  total_loss: 8.572  time: 0.2051(78.01)  data_time: 0.0055  lr: 5.20e-05  
[01/19 12:00:56] lb.utils.events INFO:  eta: 0:03:15  iteration: 54/1000  consumed samples: 864  total_loss: 8.571  time: 0.2050(78.03)  data_time: 0.0055  lr: 5.30e-05  
[01/19 12:00:56] lb.utils.events INFO:  eta: 0:03:15  iteration: 55/1000  consumed samples: 880  total_loss: 8.571  time: 0.2051(78.01)  data_time: 0.0055  lr: 5.40e-05  
[01/19 12:00:56] lb.utils.events INFO:  eta: 0:03:14  iteration: 56/1000  consumed samples: 896  total_loss: 8.569  time: 0.2051(78.03)  data_time: 0.0056  lr: 5.50e-05  
[01/19 12:00:57] lb.utils.events INFO:  eta: 0:03:14  iteration: 57/1000  consumed samples: 912  total_loss: 8.568  time: 0.2051(78.01)  data_time: 0.0055  lr: 5.60e-05  
[01/19 12:00:57] lb.utils.events INFO:  eta: 0:03:14  iteration: 58/1000  consumed samples: 928  total_loss: 8.559  time: 0.2051(78.02)  data_time: 0.0055  lr: 5.70e-05  
[01/19 12:00:57] lb.utils.events INFO:  eta: 0:03:14  iteration: 59/1000  consumed samples: 944  total_loss: 8.55  time: 0.2051(78.00)  data_time: 0.0054  lr: 5.80e-05  
[01/19 12:00:57] lb.utils.events INFO:  eta: 0:03:14  iteration: 60/1000  consumed samples: 960  total_loss: 8.55  time: 0.2052(77.97)  data_time: 0.0054  lr: 5.90e-05  
[01/19 12:00:58] lb.utils.events INFO:  eta: 0:03:13  iteration: 61/1000  consumed samples: 976  total_loss: 8.55  time: 0.2054(77.90)  data_time: 0.0053  lr: 6.00e-05  
[01/19 12:00:58] lb.utils.events INFO:  eta: 0:03:13  iteration: 62/1000  consumed samples: 992  total_loss: 8.541  time: 0.2053(77.93)  data_time: 0.0052  lr: 6.10e-05  
[01/19 12:00:58] lb.utils.events INFO:  eta: 0:03:13  iteration: 63/1000  consumed samples: 1008  total_loss: 8.533  time: 0.2052(77.96)  data_time: 0.0052  lr: 6.20e-05  
[01/19 12:00:58] lb.utils.events INFO:  eta: 0:03:13  iteration: 64/1000  consumed samples: 1024  total_loss: 8.533  time: 0.2052(77.97)  data_time: 0.0052  lr: 6.30e-05  
[01/19 12:00:58] lb.utils.events INFO:  eta: 0:03:13  iteration: 65/1000  consumed samples: 1040  total_loss: 8.533  time: 0.2053(77.95)  data_time: 0.0035  lr: 6.40e-05  
[01/19 12:00:59] lb.utils.events INFO:  eta: 0:03:12  iteration: 66/1000  consumed samples: 1056  total_loss: 8.529  time: 0.2052(77.96)  data_time: 0.0034  lr: 6.50e-05  
[01/19 12:00:59] lb.utils.events INFO:  eta: 0:03:12  iteration: 67/1000  consumed samples: 1072  total_loss: 8.526  time: 0.2053(77.94)  data_time: 0.0034  lr: 6.60e-05  
[01/19 12:00:59] lb.utils.events INFO:  eta: 0:03:12  iteration: 68/1000  consumed samples: 1088  total_loss: 8.523  time: 0.2052(77.96)  data_time: 0.0035  lr: 6.70e-05  
[01/19 12:00:59] lb.utils.events INFO:  eta: 0:03:12  iteration: 69/1000  consumed samples: 1104  total_loss: 8.519  time: 0.2054(77.90)  data_time: 0.0035  lr: 6.80e-05  
[01/19 12:01:00] lb.utils.events INFO:  eta: 0:03:11  iteration: 70/1000  consumed samples: 1120  total_loss: 8.517  time: 0.2054(77.91)  data_time: 0.0035  lr: 6.90e-05  
[01/19 12:01:00] lb.utils.events INFO:  eta: 0:03:11  iteration: 71/1000  consumed samples: 1136  total_loss: 8.515  time: 0.2054(77.90)  data_time: 0.0035  lr: 7.00e-05  
[01/19 12:01:00] lb.utils.events INFO:  eta: 0:03:11  iteration: 72/1000  consumed samples: 1152  total_loss: 8.514  time: 0.2053(77.92)  data_time: 0.0035  lr: 7.10e-05  
[01/19 12:01:00] lb.utils.events INFO:  eta: 0:03:11  iteration: 73/1000  consumed samples: 1168  total_loss: 8.513  time: 0.2054(77.90)  data_time: 0.0035  lr: 7.20e-05  
[01/19 12:01:00] lb.utils.events INFO:  eta: 0:03:11  iteration: 74/1000  consumed samples: 1184  total_loss: 8.511  time: 0.2054(77.91)  data_time: 0.0035  lr: 7.30e-05  
[01/19 12:01:01] lb.utils.events INFO:  eta: 0:03:10  iteration: 75/1000  consumed samples: 1200  total_loss: 8.509  time: 0.2054(77.88)  data_time: 0.0035  lr: 7.40e-05  
[01/19 12:01:01] lb.utils.events INFO:  eta: 0:03:10  iteration: 76/1000  consumed samples: 1216  total_loss: 8.505  time: 0.2054(77.91)  data_time: 0.0035  lr: 7.50e-05  
[01/19 12:01:01] lb.utils.events INFO:  eta: 0:03:10  iteration: 77/1000  consumed samples: 1232  total_loss: 8.502  time: 0.2054(77.89)  data_time: 0.0035  lr: 7.60e-05  
[01/19 12:01:01] lb.utils.events INFO:  eta: 0:03:10  iteration: 78/1000  consumed samples: 1248  total_loss: 8.488  time: 0.2054(77.90)  data_time: 0.0035  lr: 7.70e-05  
[01/19 12:01:02] lb.utils.events INFO:  eta: 0:03:10  iteration: 79/1000  consumed samples: 1264  total_loss: 8.474  time: 0.2054(77.88)  data_time: 0.0036  lr: 7.80e-05  
[01/19 12:01:02] lb.utils.events INFO:  eta: 0:03:09  iteration: 80/1000  consumed samples: 1280  total_loss: 8.466  time: 0.2054(77.90)  data_time: 0.0037  lr: 7.90e-05  
[01/19 12:01:02] lb.utils.events INFO:  eta: 0:03:09  iteration: 81/1000  consumed samples: 1296  total_loss: 8.458  time: 0.2054(77.88)  data_time: 0.0037  lr: 8.00e-05  
[01/19 12:01:02] lb.utils.events INFO:  eta: 0:03:09  iteration: 82/1000  consumed samples: 1312  total_loss: 8.455  time: 0.2054(77.89)  data_time: 0.0038  lr: 8.10e-05  
[01/19 12:01:02] lb.utils.events INFO:  eta: 0:03:09  iteration: 83/1000  consumed samples: 1328  total_loss: 8.451  time: 0.2054(77.88)  data_time: 0.0038  lr: 8.20e-05  
[01/19 12:01:03] lb.utils.events INFO:  eta: 0:03:09  iteration: 84/1000  consumed samples: 1344  total_loss: 8.45  time: 0.2055(77.85)  data_time: 0.0039  lr: 8.30e-05  
[01/19 12:01:03] lb.utils.events INFO:  eta: 0:03:08  iteration: 85/1000  consumed samples: 1360  total_loss: 8.448  time: 0.2056(77.84)  data_time: 0.0039  lr: 8.40e-05  
[01/19 12:01:03] lb.utils.events INFO:  eta: 0:03:08  iteration: 86/1000  consumed samples: 1376  total_loss: 8.445  time: 0.2055(77.85)  data_time: 0.0039  lr: 8.50e-05  
[01/19 12:01:03] lb.utils.events INFO:  eta: 0:03:08  iteration: 87/1000  consumed samples: 1392  total_loss: 8.441  time: 0.2056(77.83)  data_time: 0.0038  lr: 8.60e-05  
[01/19 12:01:04] lb.utils.events INFO:  eta: 0:03:08  iteration: 88/1000  consumed samples: 1408  total_loss: 8.441  time: 0.2055(77.84)  data_time: 0.0038  lr: 8.70e-05  
[01/19 12:01:04] lb.utils.events INFO:  eta: 0:03:08  iteration: 89/1000  consumed samples: 1424  total_loss: 8.44  time: 0.2056(77.82)  data_time: 0.0037  lr: 8.80e-05  
[01/19 12:01:04] lb.utils.events INFO:  eta: 0:03:07  iteration: 90/1000  consumed samples: 1440  total_loss: 8.439  time: 0.2056(77.83)  data_time: 0.0037  lr: 8.90e-05  
[01/19 12:01:04] lb.utils.events INFO:  eta: 0:03:07  iteration: 91/1000  consumed samples: 1456  total_loss: 8.437  time: 0.2056(77.81)  data_time: 0.0036  lr: 9.00e-05  
[01/19 12:01:04] lb.utils.events INFO:  eta: 0:03:07  iteration: 92/1000  consumed samples: 1472  total_loss: 8.434  time: 0.2056(77.82)  data_time: 0.0035  lr: 9.10e-05  
[01/19 12:01:05] lb.utils.events INFO:  eta: 0:03:07  iteration: 93/1000  consumed samples: 1488  total_loss: 8.432  time: 0.2056(77.81)  data_time: 0.0035  lr: 9.20e-05  
[01/19 12:01:05] lb.utils.events INFO:  eta: 0:03:07  iteration: 94/1000  consumed samples: 1504  total_loss: 8.428  time: 0.2056(77.82)  data_time: 0.0035  lr: 9.30e-05  
[01/19 12:01:05] lb.utils.events INFO:  eta: 0:03:06  iteration: 95/1000  consumed samples: 1520  total_loss: 8.424  time: 0.2056(77.84)  data_time: 0.0035  lr: 9.40e-05  
[01/19 12:01:05] lb.utils.events INFO:  eta: 0:03:06  iteration: 96/1000  consumed samples: 1536  total_loss: 8.419  time: 0.2053(77.92)  data_time: 0.0034  lr: 9.50e-05  
[01/19 12:01:06] lb.utils.events INFO:  eta: 0:03:06  iteration: 97/1000  consumed samples: 1552  total_loss: 8.413  time: 0.2051(78.00)  data_time: 0.0035  lr: 9.60e-05  
[01/19 12:01:06] lb.utils.events INFO:  eta: 0:03:06  iteration: 98/1000  consumed samples: 1568  total_loss: 8.41  time: 0.2052(77.98)  data_time: 0.0035  lr: 9.70e-05  
[01/19 12:01:06] lb.utils.events INFO:  eta: 0:03:05  iteration: 99/1000  consumed samples: 1584  total_loss: 8.407  time: 0.2052(77.99)  data_time: 0.0035  lr: 9.80e-05  
[01/19 12:01:06] lb.utils.events INFO:  eta: 0:03:05  iteration: 100/1000  consumed samples: 1600  total_loss: 8.394  time: 0.2052(77.97)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:06] lb.utils.events INFO:  eta: 0:03:05  iteration: 101/1000  consumed samples: 1616  total_loss: 8.382  time: 0.2052(77.97)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:07] lb.utils.events INFO:  eta: 0:03:05  iteration: 102/1000  consumed samples: 1632  total_loss: 8.382  time: 0.2052(77.96)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:07] lb.utils.events INFO:  eta: 0:03:04  iteration: 103/1000  consumed samples: 1648  total_loss: 8.381  time: 0.2052(77.96)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:07] lb.utils.events INFO:  eta: 0:03:04  iteration: 104/1000  consumed samples: 1664  total_loss: 8.373  time: 0.2052(77.98)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:07] lb.utils.events INFO:  eta: 0:03:04  iteration: 105/1000  consumed samples: 1680  total_loss: 8.364  time: 0.2052(77.96)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:08] lb.utils.events INFO:  eta: 0:03:04  iteration: 106/1000  consumed samples: 1696  total_loss: 8.352  time: 0.2053(77.94)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:08] lb.utils.events INFO:  eta: 0:03:04  iteration: 107/1000  consumed samples: 1712  total_loss: 8.34  time: 0.2053(77.95)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:08] lb.utils.events INFO:  eta: 0:03:04  iteration: 108/1000  consumed samples: 1728  total_loss: 8.34  time: 0.2053(77.93)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:08] lb.utils.events INFO:  eta: 0:03:03  iteration: 109/1000  consumed samples: 1744  total_loss: 8.34  time: 0.2053(77.94)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:08] lb.utils.events INFO:  eta: 0:03:03  iteration: 110/1000  consumed samples: 1760  total_loss: 8.338  time: 0.2053(77.92)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:09] lb.utils.events INFO:  eta: 0:03:03  iteration: 111/1000  consumed samples: 1776  total_loss: 8.337  time: 0.2053(77.92)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:09] lb.utils.events INFO:  eta: 0:03:03  iteration: 112/1000  consumed samples: 1792  total_loss: 8.336  time: 0.2053(77.94)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:09] lb.utils.events INFO:  eta: 0:03:02  iteration: 113/1000  consumed samples: 1808  total_loss: 8.334  time: 0.2053(77.94)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:09] lb.utils.events INFO:  eta: 0:03:02  iteration: 114/1000  consumed samples: 1824  total_loss: 8.333  time: 0.2054(77.89)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:10] lb.utils.events INFO:  eta: 0:03:02  iteration: 115/1000  consumed samples: 1840  total_loss: 8.331  time: 0.2054(77.90)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:10] lb.utils.events INFO:  eta: 0:03:02  iteration: 116/1000  consumed samples: 1856  total_loss: 8.331  time: 0.2054(77.88)  data_time: 0.0043  lr: 9.90e-05  
[01/19 12:01:10] lb.utils.events INFO:  eta: 0:03:02  iteration: 117/1000  consumed samples: 1872  total_loss: 8.331  time: 0.2055(77.86)  data_time: 0.0043  lr: 9.90e-05  
[01/19 12:01:10] lb.utils.events INFO:  eta: 0:03:01  iteration: 118/1000  consumed samples: 1888  total_loss: 8.329  time: 0.2055(77.87)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:10] lb.utils.events INFO:  eta: 0:03:01  iteration: 119/1000  consumed samples: 1904  total_loss: 8.327  time: 0.2055(77.85)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:11] lb.utils.events INFO:  eta: 0:03:01  iteration: 120/1000  consumed samples: 1920  total_loss: 8.325  time: 0.2055(77.86)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:11] lb.utils.events INFO:  eta: 0:03:01  iteration: 121/1000  consumed samples: 1936  total_loss: 8.323  time: 0.2056(77.84)  data_time: 0.0043  lr: 9.90e-05  
[01/19 12:01:11] lb.utils.events INFO:  eta: 0:03:00  iteration: 122/1000  consumed samples: 1952  total_loss: 8.316  time: 0.2055(77.84)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:11] lb.utils.events INFO:  eta: 0:03:00  iteration: 123/1000  consumed samples: 1968  total_loss: 8.309  time: 0.2056(77.83)  data_time: 0.0043  lr: 9.90e-05  
[01/19 12:01:12] lb.utils.events INFO:  eta: 0:03:00  iteration: 124/1000  consumed samples: 1984  total_loss: 8.305  time: 0.2056(77.83)  data_time: 0.0043  lr: 9.90e-05  
[01/19 12:01:12] lb.utils.events INFO:  eta: 0:03:00  iteration: 125/1000  consumed samples: 2000  total_loss: 8.302  time: 0.2056(77.82)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:12] lb.utils.events INFO:  eta: 0:03:00  iteration: 126/1000  consumed samples: 2016  total_loss: 8.299  time: 0.2056(77.82)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:12] lb.utils.events INFO:  eta: 0:03:00  iteration: 127/1000  consumed samples: 2032  total_loss: 8.296  time: 0.2057(77.80)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:12] lb.utils.events INFO:  eta: 0:02:59  iteration: 128/1000  consumed samples: 2048  total_loss: 8.295  time: 0.2056(77.80)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:13] lb.utils.events INFO:  eta: 0:02:59  iteration: 129/1000  consumed samples: 2064  total_loss: 8.295  time: 0.2057(77.79)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:13] lb.utils.events INFO:  eta: 0:02:59  iteration: 130/1000  consumed samples: 2080  total_loss: 8.294  time: 0.2057(77.77)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:13] lb.utils.events INFO:  eta: 0:02:59  iteration: 131/1000  consumed samples: 2096  total_loss: 8.293  time: 0.2058(77.75)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:13] lb.utils.events INFO:  eta: 0:02:59  iteration: 132/1000  consumed samples: 2112  total_loss: 8.289  time: 0.2058(77.76)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:14] lb.utils.events INFO:  eta: 0:02:59  iteration: 133/1000  consumed samples: 2128  total_loss: 8.285  time: 0.2058(77.74)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:14] lb.utils.events INFO:  eta: 0:02:58  iteration: 134/1000  consumed samples: 2144  total_loss: 8.277  time: 0.2059(77.72)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:14] lb.utils.events INFO:  eta: 0:02:58  iteration: 135/1000  consumed samples: 2160  total_loss: 8.268  time: 0.2059(77.71)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:14] lb.utils.events INFO:  eta: 0:02:58  iteration: 136/1000  consumed samples: 2176  total_loss: 8.268  time: 0.2059(77.69)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:14] lb.utils.events INFO:  eta: 0:02:58  iteration: 137/1000  consumed samples: 2192  total_loss: 8.268  time: 0.2060(77.68)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:15] lb.utils.events INFO:  eta: 0:02:58  iteration: 138/1000  consumed samples: 2208  total_loss: 8.262  time: 0.2060(77.68)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:15] lb.utils.events INFO:  eta: 0:02:57  iteration: 139/1000  consumed samples: 2224  total_loss: 8.256  time: 0.2060(77.66)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:15] lb.utils.events INFO:  eta: 0:02:57  iteration: 140/1000  consumed samples: 2240  total_loss: 8.252  time: 0.2060(77.67)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:15] lb.utils.events INFO:  eta: 0:02:57  iteration: 141/1000  consumed samples: 2256  total_loss: 8.248  time: 0.2060(77.65)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:16] lb.utils.events INFO:  eta: 0:02:57  iteration: 142/1000  consumed samples: 2272  total_loss: 8.247  time: 0.2060(77.66)  data_time: 0.0043  lr: 9.90e-05  
[01/19 12:01:16] lb.utils.events INFO:  eta: 0:02:57  iteration: 143/1000  consumed samples: 2288  total_loss: 8.247  time: 0.2061(77.62)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:16] lb.utils.events INFO:  eta: 0:02:56  iteration: 144/1000  consumed samples: 2304  total_loss: 8.246  time: 0.2061(77.62)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:16] lb.utils.events INFO:  eta: 0:02:56  iteration: 145/1000  consumed samples: 2320  total_loss: 8.246  time: 0.2062(77.61)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:17] lb.utils.events INFO:  eta: 0:02:56  iteration: 146/1000  consumed samples: 2336  total_loss: 8.24  time: 0.2061(77.61)  data_time: 0.0044  lr: 9.90e-05  
[01/19 12:01:17] lb.utils.events INFO:  eta: 0:02:56  iteration: 147/1000  consumed samples: 2352  total_loss: 8.235  time: 0.2062(77.58)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:17] lb.utils.events INFO:  eta: 0:02:56  iteration: 148/1000  consumed samples: 2368  total_loss: 8.231  time: 0.2062(77.58)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:17] lb.utils.events INFO:  eta: 0:02:55  iteration: 149/1000  consumed samples: 2384  total_loss: 8.227  time: 0.2063(77.57)  data_time: 0.0045  lr: 9.90e-05  
[01/19 12:01:17] lb.utils.events INFO:  eta: 0:02:55  iteration: 150/1000  consumed samples: 2400  total_loss: 8.225  time: 0.2063(77.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:18] lb.utils.events INFO:  eta: 0:02:55  iteration: 151/1000  consumed samples: 2416  total_loss: 8.223  time: 0.2063(77.55)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:18] lb.utils.events INFO:  eta: 0:02:55  iteration: 152/1000  consumed samples: 2432  total_loss: 8.223  time: 0.2064(77.53)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:18] lb.utils.events INFO:  eta: 0:02:55  iteration: 153/1000  consumed samples: 2448  total_loss: 8.223  time: 0.2064(77.54)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:18] lb.utils.events INFO:  eta: 0:02:54  iteration: 154/1000  consumed samples: 2464  total_loss: 8.222  time: 0.2064(77.52)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:19] lb.utils.events INFO:  eta: 0:02:54  iteration: 155/1000  consumed samples: 2480  total_loss: 8.222  time: 0.2064(77.51)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:19] lb.utils.events INFO:  eta: 0:02:54  iteration: 156/1000  consumed samples: 2496  total_loss: 8.219  time: 0.2064(77.51)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:19] lb.utils.events INFO:  eta: 0:02:54  iteration: 157/1000  consumed samples: 2512  total_loss: 8.216  time: 0.2065(77.48)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:19] lb.utils.events INFO:  eta: 0:02:54  iteration: 158/1000  consumed samples: 2528  total_loss: 8.213  time: 0.2065(77.48)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:19] lb.utils.events INFO:  eta: 0:02:53  iteration: 159/1000  consumed samples: 2544  total_loss: 8.209  time: 0.2065(77.47)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:20] lb.utils.events INFO:  eta: 0:02:53  iteration: 160/1000  consumed samples: 2560  total_loss: 8.201  time: 0.2066(77.45)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:20] lb.utils.events INFO:  eta: 0:02:53  iteration: 161/1000  consumed samples: 2576  total_loss: 8.192  time: 0.2067(77.42)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:20] lb.utils.events INFO:  eta: 0:02:53  iteration: 162/1000  consumed samples: 2592  total_loss: 8.186  time: 0.2067(77.41)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:20] lb.utils.events INFO:  eta: 0:02:53  iteration: 163/1000  consumed samples: 2608  total_loss: 8.18  time: 0.2067(77.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:21] lb.utils.events INFO:  eta: 0:02:52  iteration: 164/1000  consumed samples: 2624  total_loss: 8.177  time: 0.2067(77.40)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:21] lb.utils.events INFO:  eta: 0:02:52  iteration: 165/1000  consumed samples: 2640  total_loss: 8.173  time: 0.2067(77.39)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:21] lb.utils.events INFO:  eta: 0:02:52  iteration: 166/1000  consumed samples: 2656  total_loss: 8.171  time: 0.2067(77.39)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:21] lb.utils.events INFO:  eta: 0:02:52  iteration: 167/1000  consumed samples: 2672  total_loss: 8.17  time: 0.2068(77.38)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:21] lb.utils.events INFO:  eta: 0:02:52  iteration: 168/1000  consumed samples: 2688  total_loss: 8.17  time: 0.2068(77.38)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:22] lb.utils.events INFO:  eta: 0:02:51  iteration: 169/1000  consumed samples: 2704  total_loss: 8.169  time: 0.2068(77.37)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:22] lb.utils.events INFO:  eta: 0:02:51  iteration: 170/1000  consumed samples: 2720  total_loss: 8.162  time: 0.2068(77.37)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:22] lb.utils.events INFO:  eta: 0:02:51  iteration: 171/1000  consumed samples: 2736  total_loss: 8.155  time: 0.2068(77.36)  data_time: 0.0054  lr: 9.90e-05  
[01/19 12:01:22] lb.utils.events INFO:  eta: 0:02:51  iteration: 172/1000  consumed samples: 2752  total_loss: 8.153  time: 0.2068(77.36)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:23] lb.utils.events INFO:  eta: 0:02:51  iteration: 173/1000  consumed samples: 2768  total_loss: 8.151  time: 0.2068(77.35)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:23] lb.utils.events INFO:  eta: 0:02:50  iteration: 174/1000  consumed samples: 2784  total_loss: 8.149  time: 0.2069(77.33)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:23] lb.utils.events INFO:  eta: 0:02:50  iteration: 175/1000  consumed samples: 2800  total_loss: 8.147  time: 0.2069(77.32)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:23] lb.utils.events INFO:  eta: 0:02:50  iteration: 176/1000  consumed samples: 2816  total_loss: 8.146  time: 0.2069(77.33)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:24] lb.utils.events INFO:  eta: 0:02:50  iteration: 177/1000  consumed samples: 2832  total_loss: 8.146  time: 0.2070(77.31)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:24] lb.utils.events INFO:  eta: 0:02:50  iteration: 178/1000  consumed samples: 2848  total_loss: 8.145  time: 0.2069(77.32)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:24] lb.utils.events INFO:  eta: 0:02:50  iteration: 179/1000  consumed samples: 2864  total_loss: 8.143  time: 0.2070(77.30)  data_time: 0.0052  lr: 9.90e-05  
[01/19 12:01:24] lb.utils.events INFO:  eta: 0:02:49  iteration: 180/1000  consumed samples: 2880  total_loss: 8.141  time: 0.2070(77.30)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:24] lb.utils.events INFO:  eta: 0:02:49  iteration: 181/1000  consumed samples: 2896  total_loss: 8.139  time: 0.2070(77.29)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:25] lb.utils.events INFO:  eta: 0:02:49  iteration: 182/1000  consumed samples: 2912  total_loss: 8.138  time: 0.2070(77.28)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:25] lb.utils.events INFO:  eta: 0:02:49  iteration: 183/1000  consumed samples: 2928  total_loss: 8.138  time: 0.2071(77.27)  data_time: 0.0053  lr: 9.90e-05  
[01/19 12:01:25] lb.utils.events INFO:  eta: 0:02:48  iteration: 184/1000  consumed samples: 2944  total_loss: 8.137  time: 0.2071(77.27)  data_time: 0.0052  lr: 9.90e-05  
[01/19 12:01:25] lb.utils.events INFO:  eta: 0:02:48  iteration: 185/1000  consumed samples: 2960  total_loss: 8.137  time: 0.2071(77.26)  data_time: 0.0052  lr: 9.90e-05  
[01/19 12:01:26] lb.utils.events INFO:  eta: 0:02:48  iteration: 186/1000  consumed samples: 2976  total_loss: 8.136  time: 0.2071(77.26)  data_time: 0.0052  lr: 9.90e-05  
[01/19 12:01:26] lb.utils.events INFO:  eta: 0:02:48  iteration: 187/1000  consumed samples: 2992  total_loss: 8.135  time: 0.2071(77.25)  data_time: 0.0051  lr: 9.90e-05  
[01/19 12:01:26] lb.utils.events INFO:  eta: 0:02:48  iteration: 188/1000  consumed samples: 3008  total_loss: 8.134  time: 0.2071(77.25)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:26] lb.utils.events INFO:  eta: 0:02:47  iteration: 189/1000  consumed samples: 3024  total_loss: 8.133  time: 0.2072(77.24)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:26] lb.utils.events INFO:  eta: 0:02:47  iteration: 190/1000  consumed samples: 3040  total_loss: 8.124  time: 0.2072(77.24)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:27] lb.utils.events INFO:  eta: 0:02:47  iteration: 191/1000  consumed samples: 3056  total_loss: 8.116  time: 0.2072(77.22)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:27] lb.utils.events INFO:  eta: 0:02:47  iteration: 192/1000  consumed samples: 3072  total_loss: 8.116  time: 0.2072(77.23)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:27] lb.utils.events INFO:  eta: 0:02:47  iteration: 193/1000  consumed samples: 3088  total_loss: 8.115  time: 0.2072(77.21)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:27] lb.utils.events INFO:  eta: 0:02:46  iteration: 194/1000  consumed samples: 3104  total_loss: 8.114  time: 0.2072(77.22)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:28] lb.utils.events INFO:  eta: 0:02:46  iteration: 195/1000  consumed samples: 3120  total_loss: 8.112  time: 0.2072(77.20)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:28] lb.utils.events INFO:  eta: 0:02:46  iteration: 196/1000  consumed samples: 3136  total_loss: 8.112  time: 0.2072(77.21)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:28] lb.utils.events INFO:  eta: 0:02:46  iteration: 197/1000  consumed samples: 3152  total_loss: 8.112  time: 0.2073(77.18)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:28] lb.utils.events INFO:  eta: 0:02:46  iteration: 198/1000  consumed samples: 3168  total_loss: 8.111  time: 0.2073(77.17)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:28] lb.utils.events INFO:  eta: 0:02:45  iteration: 199/1000  consumed samples: 3184  total_loss: 8.11  time: 0.2074(77.14)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:29] lb.utils.events INFO:  eta: 0:02:45  iteration: 200/1000  consumed samples: 3200  total_loss: 8.109  time: 0.2074(77.13)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:29] lb.utils.events INFO:  eta: 0:02:45  iteration: 201/1000  consumed samples: 3216  total_loss: 8.109  time: 0.2075(77.12)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:29] lb.utils.events INFO:  eta: 0:02:45  iteration: 202/1000  consumed samples: 3232  total_loss: 8.108  time: 0.2075(77.12)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:29] lb.utils.events INFO:  eta: 0:02:45  iteration: 203/1000  consumed samples: 3248  total_loss: 8.106  time: 0.2075(77.11)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:30] lb.utils.events INFO:  eta: 0:02:44  iteration: 204/1000  consumed samples: 3264  total_loss: 8.103  time: 0.2075(77.11)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:30] lb.utils.events INFO:  eta: 0:02:44  iteration: 205/1000  consumed samples: 3280  total_loss: 8.101  time: 0.2076(77.09)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:30] lb.utils.events INFO:  eta: 0:02:44  iteration: 206/1000  consumed samples: 3296  total_loss: 8.099  time: 0.2076(77.09)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:30] lb.utils.events INFO:  eta: 0:02:44  iteration: 207/1000  consumed samples: 3312  total_loss: 8.095  time: 0.2076(77.08)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:31] lb.utils.events INFO:  eta: 0:02:44  iteration: 208/1000  consumed samples: 3328  total_loss: 8.093  time: 0.2076(77.08)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:31] lb.utils.events INFO:  eta: 0:02:43  iteration: 209/1000  consumed samples: 3344  total_loss: 8.091  time: 0.2076(77.07)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:31] lb.utils.events INFO:  eta: 0:02:43  iteration: 210/1000  consumed samples: 3360  total_loss: 8.088  time: 0.2076(77.06)  data_time: 0.0048  lr: 9.90e-05  
[01/19 12:01:31] lb.utils.events INFO:  eta: 0:02:43  iteration: 211/1000  consumed samples: 3376  total_loss: 8.086  time: 0.2077(77.05)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:31] lb.utils.events INFO:  eta: 0:02:43  iteration: 212/1000  consumed samples: 3392  total_loss: 8.084  time: 0.2076(77.07)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:32] lb.utils.events INFO:  eta: 0:02:43  iteration: 213/1000  consumed samples: 3408  total_loss: 8.081  time: 0.2076(77.08)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:32] lb.utils.events INFO:  eta: 0:02:42  iteration: 214/1000  consumed samples: 3424  total_loss: 8.073  time: 0.2076(77.08)  data_time: 0.0047  lr: 9.90e-05  
[01/19 12:01:32] lb.utils.events INFO:  eta: 0:02:42  iteration: 215/1000  consumed samples: 3440  total_loss: 8.067  time: 0.2076(77.06)  data_time: 0.0046  lr: 9.90e-05  
[01/19 12:01:32] lb.utils.events INFO:  eta: 0:02:42  iteration: 216/1000  consumed samples: 3456  total_loss: 8.061  time: 0.2076(77.06)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:33] lb.utils.events INFO:  eta: 0:02:42  iteration: 217/1000  consumed samples: 3472  total_loss: 8.054  time: 0.2077(77.05)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:33] lb.utils.events INFO:  eta: 0:02:42  iteration: 218/1000  consumed samples: 3488  total_loss: 8.05  time: 0.2076(77.05)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:33] lb.utils.events INFO:  eta: 0:02:41  iteration: 219/1000  consumed samples: 3504  total_loss: 8.043  time: 0.2077(77.03)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:33] lb.utils.events INFO:  eta: 0:02:41  iteration: 220/1000  consumed samples: 3520  total_loss: 8.039  time: 0.2078(77.01)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:33] lb.utils.events INFO:  eta: 0:02:41  iteration: 221/1000  consumed samples: 3536  total_loss: 8.039  time: 0.2078(77.00)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:34] lb.utils.events INFO:  eta: 0:02:41  iteration: 222/1000  consumed samples: 3552  total_loss: 8.037  time: 0.2078(77.01)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:34] lb.utils.events INFO:  eta: 0:02:41  iteration: 223/1000  consumed samples: 3568  total_loss: 8.036  time: 0.2078(77.00)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:34] lb.utils.events INFO:  eta: 0:02:40  iteration: 224/1000  consumed samples: 3584  total_loss: 8.034  time: 0.2078(77.00)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:34] lb.utils.events INFO:  eta: 0:02:40  iteration: 225/1000  consumed samples: 3600  total_loss: 8.029  time: 0.2078(76.98)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:35] lb.utils.events INFO:  eta: 0:02:40  iteration: 226/1000  consumed samples: 3616  total_loss: 8.026  time: 0.2078(76.98)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:35] lb.utils.events INFO:  eta: 0:02:40  iteration: 227/1000  consumed samples: 3632  total_loss: 8.026  time: 0.2079(76.96)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:35] lb.utils.events INFO:  eta: 0:02:40  iteration: 228/1000  consumed samples: 3648  total_loss: 8.024  time: 0.2079(76.95)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:35] lb.utils.events INFO:  eta: 0:02:39  iteration: 229/1000  consumed samples: 3664  total_loss: 8.024  time: 0.2080(76.94)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:36] lb.utils.events INFO:  eta: 0:02:39  iteration: 230/1000  consumed samples: 3680  total_loss: 8.02  time: 0.2080(76.94)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:36] lb.utils.events INFO:  eta: 0:02:39  iteration: 231/1000  consumed samples: 3696  total_loss: 8.014  time: 0.2080(76.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:36] lb.utils.events INFO:  eta: 0:02:39  iteration: 232/1000  consumed samples: 3712  total_loss: 8.01  time: 0.2080(76.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:36] lb.utils.events INFO:  eta: 0:02:39  iteration: 233/1000  consumed samples: 3728  total_loss: 8.008  time: 0.2080(76.91)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:36] lb.utils.events INFO:  eta: 0:02:38  iteration: 234/1000  consumed samples: 3744  total_loss: 8.005  time: 0.2080(76.91)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:37] lb.utils.events INFO:  eta: 0:02:38  iteration: 235/1000  consumed samples: 3760  total_loss: 8  time: 0.2081(76.90)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:37] lb.utils.events INFO:  eta: 0:02:38  iteration: 236/1000  consumed samples: 3776  total_loss: 7.999  time: 0.2081(76.90)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:37] lb.utils.events INFO:  eta: 0:02:38  iteration: 237/1000  consumed samples: 3792  total_loss: 7.999  time: 0.2081(76.89)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:37] lb.utils.events INFO:  eta: 0:02:38  iteration: 238/1000  consumed samples: 3808  total_loss: 7.996  time: 0.2081(76.88)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:38] lb.utils.events INFO:  eta: 0:02:38  iteration: 239/1000  consumed samples: 3824  total_loss: 7.996  time: 0.2081(76.87)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:38] lb.utils.events INFO:  eta: 0:02:37  iteration: 240/1000  consumed samples: 3840  total_loss: 7.996  time: 0.2081(76.87)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:38] lb.utils.events INFO:  eta: 0:02:37  iteration: 241/1000  consumed samples: 3856  total_loss: 7.992  time: 0.2082(76.85)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:38] lb.utils.events INFO:  eta: 0:02:37  iteration: 242/1000  consumed samples: 3872  total_loss: 7.99  time: 0.2082(76.84)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:39] lb.utils.events INFO:  eta: 0:02:37  iteration: 243/1000  consumed samples: 3888  total_loss: 7.99  time: 0.2082(76.83)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:39] lb.utils.events INFO:  eta: 0:02:37  iteration: 244/1000  consumed samples: 3904  total_loss: 7.99  time: 0.2082(76.83)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:39] lb.utils.events INFO:  eta: 0:02:37  iteration: 245/1000  consumed samples: 3920  total_loss: 7.988  time: 0.2083(76.82)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:39] lb.utils.events INFO:  eta: 0:02:36  iteration: 246/1000  consumed samples: 3936  total_loss: 7.988  time: 0.2083(76.82)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:39] lb.utils.events INFO:  eta: 0:02:36  iteration: 247/1000  consumed samples: 3952  total_loss: 7.988  time: 0.2083(76.81)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:40] lb.utils.events INFO:  eta: 0:02:36  iteration: 248/1000  consumed samples: 3968  total_loss: 7.985  time: 0.2083(76.81)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:40] lb.utils.events INFO:  eta: 0:02:36  iteration: 249/1000  consumed samples: 3984  total_loss: 7.983  time: 0.2083(76.80)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:40] lb.utils.events INFO:  eta: 0:02:36  iteration: 250/1000  consumed samples: 4000  total_loss: 7.983  time: 0.2083(76.80)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:40] lb.utils.events INFO:  eta: 0:02:35  iteration: 251/1000  consumed samples: 4016  total_loss: 7.982  time: 0.2083(76.80)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:41] lb.utils.events INFO:  eta: 0:02:35  iteration: 252/1000  consumed samples: 4032  total_loss: 7.982  time: 0.2083(76.80)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:41] lb.utils.events INFO:  eta: 0:02:35  iteration: 253/1000  consumed samples: 4048  total_loss: 7.981  time: 0.2084(76.79)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:41] lb.utils.events INFO:  eta: 0:02:35  iteration: 254/1000  consumed samples: 4064  total_loss: 7.979  time: 0.2084(76.79)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:41] lb.utils.events INFO:  eta: 0:02:34  iteration: 255/1000  consumed samples: 4080  total_loss: 7.978  time: 0.2084(76.78)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:41] lb.utils.events INFO:  eta: 0:02:34  iteration: 256/1000  consumed samples: 4096  total_loss: 7.978  time: 0.2084(76.78)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:42] lb.utils.events INFO:  eta: 0:02:34  iteration: 257/1000  consumed samples: 4112  total_loss: 7.978  time: 0.2084(76.77)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:42] lb.utils.events INFO:  eta: 0:02:34  iteration: 258/1000  consumed samples: 4128  total_loss: 7.975  time: 0.2084(76.77)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:42] lb.utils.events INFO:  eta: 0:02:34  iteration: 259/1000  consumed samples: 4144  total_loss: 7.972  time: 0.2084(76.76)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:42] lb.utils.events INFO:  eta: 0:02:33  iteration: 260/1000  consumed samples: 4160  total_loss: 7.972  time: 0.2084(76.77)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:43] lb.utils.events INFO:  eta: 0:02:33  iteration: 261/1000  consumed samples: 4176  total_loss: 7.972  time: 0.2084(76.76)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:43] lb.utils.events INFO:  eta: 0:02:33  iteration: 262/1000  consumed samples: 4192  total_loss: 7.971  time: 0.2084(76.76)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:43] lb.utils.events INFO:  eta: 0:02:33  iteration: 263/1000  consumed samples: 4208  total_loss: 7.969  time: 0.2085(76.75)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:43] lb.utils.events INFO:  eta: 0:02:33  iteration: 264/1000  consumed samples: 4224  total_loss: 7.967  time: 0.2085(76.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:44] lb.utils.events INFO:  eta: 0:02:32  iteration: 265/1000  consumed samples: 4240  total_loss: 7.966  time: 0.2085(76.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:44] lb.utils.events INFO:  eta: 0:02:32  iteration: 266/1000  consumed samples: 4256  total_loss: 7.965  time: 0.2085(76.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:44] lb.utils.events INFO:  eta: 0:02:32  iteration: 267/1000  consumed samples: 4272  total_loss: 7.965  time: 0.2085(76.74)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:44] lb.utils.events INFO:  eta: 0:02:32  iteration: 268/1000  consumed samples: 4288  total_loss: 7.964  time: 0.2085(76.74)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:44] lb.utils.events INFO:  eta: 0:02:32  iteration: 269/1000  consumed samples: 4304  total_loss: 7.962  time: 0.2085(76.73)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:45] lb.utils.events INFO:  eta: 0:02:31  iteration: 270/1000  consumed samples: 4320  total_loss: 7.961  time: 0.2085(76.73)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:45] lb.utils.events INFO:  eta: 0:02:31  iteration: 271/1000  consumed samples: 4336  total_loss: 7.961  time: 0.2085(76.73)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:45] lb.utils.events INFO:  eta: 0:02:31  iteration: 272/1000  consumed samples: 4352  total_loss: 7.96  time: 0.2086(76.71)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:45] lb.utils.events INFO:  eta: 0:02:31  iteration: 273/1000  consumed samples: 4368  total_loss: 7.959  time: 0.2086(76.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:46] lb.utils.events INFO:  eta: 0:02:31  iteration: 274/1000  consumed samples: 4384  total_loss: 7.958  time: 0.2086(76.69)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:46] lb.utils.events INFO:  eta: 0:02:30  iteration: 275/1000  consumed samples: 4400  total_loss: 7.956  time: 0.2086(76.69)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:46] lb.utils.events INFO:  eta: 0:02:30  iteration: 276/1000  consumed samples: 4416  total_loss: 7.956  time: 0.2087(76.68)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:46] lb.utils.events INFO:  eta: 0:02:30  iteration: 277/1000  consumed samples: 4432  total_loss: 7.956  time: 0.2087(76.67)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:46] lb.utils.events INFO:  eta: 0:02:30  iteration: 278/1000  consumed samples: 4448  total_loss: 7.953  time: 0.2087(76.67)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:47] lb.utils.events INFO:  eta: 0:02:30  iteration: 279/1000  consumed samples: 4464  total_loss: 7.952  time: 0.2087(76.66)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:47] lb.utils.events INFO:  eta: 0:02:29  iteration: 280/1000  consumed samples: 4480  total_loss: 7.952  time: 0.2087(76.66)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:47] lb.utils.events INFO:  eta: 0:02:29  iteration: 281/1000  consumed samples: 4496  total_loss: 7.952  time: 0.2087(76.65)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:47] lb.utils.events INFO:  eta: 0:02:29  iteration: 282/1000  consumed samples: 4512  total_loss: 7.951  time: 0.2087(76.65)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:48] lb.utils.events INFO:  eta: 0:02:29  iteration: 283/1000  consumed samples: 4528  total_loss: 7.949  time: 0.2088(76.64)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:48] lb.utils.events INFO:  eta: 0:02:29  iteration: 284/1000  consumed samples: 4544  total_loss: 7.945  time: 0.2088(76.64)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:48] lb.utils.events INFO:  eta: 0:02:29  iteration: 285/1000  consumed samples: 4560  total_loss: 7.941  time: 0.2088(76.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:48] lb.utils.events INFO:  eta: 0:02:28  iteration: 286/1000  consumed samples: 4576  total_loss: 7.94  time: 0.2089(76.61)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:49] lb.utils.events INFO:  eta: 0:02:28  iteration: 287/1000  consumed samples: 4592  total_loss: 7.937  time: 0.2089(76.59)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:49] lb.utils.events INFO:  eta: 0:02:28  iteration: 288/1000  consumed samples: 4608  total_loss: 7.932  time: 0.2090(76.57)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:49] lb.utils.events INFO:  eta: 0:02:28  iteration: 289/1000  consumed samples: 4624  total_loss: 7.93  time: 0.2090(76.57)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:49] lb.utils.events INFO:  eta: 0:02:28  iteration: 290/1000  consumed samples: 4640  total_loss: 7.93  time: 0.2090(76.55)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:49] lb.utils.events INFO:  eta: 0:02:27  iteration: 291/1000  consumed samples: 4656  total_loss: 7.929  time: 0.2090(76.55)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:50] lb.utils.events INFO:  eta: 0:02:27  iteration: 292/1000  consumed samples: 4672  total_loss: 7.927  time: 0.2090(76.54)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:50] lb.utils.events INFO:  eta: 0:02:27  iteration: 293/1000  consumed samples: 4688  total_loss: 7.925  time: 0.2090(76.54)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:01:50] lb.utils.events INFO:  eta: 0:02:27  iteration: 294/1000  consumed samples: 4704  total_loss: 7.924  time: 0.2091(76.52)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:01:50] lb.utils.events INFO:  eta: 0:02:27  iteration: 295/1000  consumed samples: 4720  total_loss: 7.924  time: 0.2091(76.51)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:01:51] lb.utils.events INFO:  eta: 0:02:26  iteration: 296/1000  consumed samples: 4736  total_loss: 7.922  time: 0.2091(76.51)  data_time: 0.0029  lr: 9.90e-05  
[01/19 12:01:51] lb.utils.events INFO:  eta: 0:02:26  iteration: 297/1000  consumed samples: 4752  total_loss: 7.921  time: 0.2091(76.51)  data_time: 0.0030  lr: 9.90e-05  
[01/19 12:01:51] lb.utils.events INFO:  eta: 0:02:26  iteration: 298/1000  consumed samples: 4768  total_loss: 7.921  time: 0.2092(76.49)  data_time: 0.0029  lr: 9.90e-05  
[01/19 12:01:51] lb.utils.events INFO:  eta: 0:02:26  iteration: 299/1000  consumed samples: 4784  total_loss: 7.918  time: 0.2092(76.49)  data_time: 0.0029  lr: 9.90e-05  
[01/19 12:01:52] lb.utils.events INFO:  eta: 0:02:26  iteration: 300/1000  consumed samples: 4800  total_loss: 7.918  time: 0.2092(76.49)  data_time: 0.0029  lr: 9.90e-05  
[01/19 12:01:52] lb.utils.events INFO:  eta: 0:02:25  iteration: 301/1000  consumed samples: 4816  total_loss: 7.918  time: 0.2092(76.48)  data_time: 0.0030  lr: 9.90e-05  
[01/19 12:01:52] lb.utils.events INFO:  eta: 0:02:25  iteration: 302/1000  consumed samples: 4832  total_loss: 7.915  time: 0.2092(76.47)  data_time: 0.0030  lr: 9.90e-05  
[01/19 12:01:52] lb.utils.events INFO:  eta: 0:02:25  iteration: 303/1000  consumed samples: 4848  total_loss: 7.915  time: 0.2092(76.47)  data_time: 0.0031  lr: 9.90e-05  
[01/19 12:01:52] lb.utils.events INFO:  eta: 0:02:25  iteration: 304/1000  consumed samples: 4864  total_loss: 7.914  time: 0.2093(76.46)  data_time: 0.0031  lr: 9.90e-05  
[01/19 12:01:53] lb.utils.events INFO:  eta: 0:02:24  iteration: 305/1000  consumed samples: 4880  total_loss: 7.915  time: 0.2093(76.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:01:53] lb.utils.events INFO:  eta: 0:02:24  iteration: 306/1000  consumed samples: 4896  total_loss: 7.915  time: 0.2093(76.45)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:01:53] lb.utils.events INFO:  eta: 0:02:24  iteration: 307/1000  consumed samples: 4912  total_loss: 7.914  time: 0.2093(76.44)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:01:53] lb.utils.events INFO:  eta: 0:02:24  iteration: 308/1000  consumed samples: 4928  total_loss: 7.914  time: 0.2094(76.42)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:01:54] lb.utils.events INFO:  eta: 0:02:24  iteration: 309/1000  consumed samples: 4944  total_loss: 7.914  time: 0.2094(76.42)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:54] lb.utils.events INFO:  eta: 0:02:24  iteration: 310/1000  consumed samples: 4960  total_loss: 7.914  time: 0.2094(76.41)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:54] lb.utils.events INFO:  eta: 0:02:23  iteration: 311/1000  consumed samples: 4976  total_loss: 7.912  time: 0.2094(76.41)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:01:54] lb.utils.events INFO:  eta: 0:02:23  iteration: 312/1000  consumed samples: 4992  total_loss: 7.91  time: 0.2094(76.41)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:01:54] lb.utils.events INFO:  eta: 0:02:23  iteration: 313/1000  consumed samples: 5008  total_loss: 7.908  time: 0.2094(76.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:01:55] lb.utils.events INFO:  eta: 0:02:23  iteration: 314/1000  consumed samples: 5024  total_loss: 7.905  time: 0.2094(76.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:55] lb.utils.events INFO:  eta: 0:02:22  iteration: 315/1000  consumed samples: 5040  total_loss: 7.904  time: 0.2094(76.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:01:55] lb.utils.events INFO:  eta: 0:02:22  iteration: 316/1000  consumed samples: 5056  total_loss: 7.903  time: 0.2094(76.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:01:55] lb.utils.events INFO:  eta: 0:02:22  iteration: 317/1000  consumed samples: 5072  total_loss: 7.901  time: 0.2094(76.41)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:56] lb.utils.events INFO:  eta: 0:02:22  iteration: 318/1000  consumed samples: 5088  total_loss: 7.901  time: 0.2094(76.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:56] lb.utils.events INFO:  eta: 0:02:22  iteration: 319/1000  consumed samples: 5104  total_loss: 7.901  time: 0.2094(76.40)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:56] lb.utils.events INFO:  eta: 0:02:21  iteration: 320/1000  consumed samples: 5120  total_loss: 7.899  time: 0.2095(76.39)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:56] lb.utils.events INFO:  eta: 0:02:21  iteration: 321/1000  consumed samples: 5136  total_loss: 7.898  time: 0.2095(76.39)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:57] lb.utils.events INFO:  eta: 0:02:21  iteration: 322/1000  consumed samples: 5152  total_loss: 7.898  time: 0.2095(76.37)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:57] lb.utils.events INFO:  eta: 0:02:21  iteration: 323/1000  consumed samples: 5168  total_loss: 7.898  time: 0.2095(76.37)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:57] lb.utils.events INFO:  eta: 0:02:21  iteration: 324/1000  consumed samples: 5184  total_loss: 7.898  time: 0.2095(76.37)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:01:57] lb.utils.events INFO:  eta: 0:02:20  iteration: 325/1000  consumed samples: 5200  total_loss: 7.897  time: 0.2095(76.37)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:57] lb.utils.events INFO:  eta: 0:02:20  iteration: 326/1000  consumed samples: 5216  total_loss: 7.895  time: 0.2095(76.36)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:58] lb.utils.events INFO:  eta: 0:02:20  iteration: 327/1000  consumed samples: 5232  total_loss: 7.895  time: 0.2095(76.36)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:58] lb.utils.events INFO:  eta: 0:02:20  iteration: 328/1000  consumed samples: 5248  total_loss: 7.895  time: 0.2096(76.35)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:58] lb.utils.events INFO:  eta: 0:02:20  iteration: 329/1000  consumed samples: 5264  total_loss: 7.894  time: 0.2096(76.35)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:58] lb.utils.events INFO:  eta: 0:02:19  iteration: 330/1000  consumed samples: 5280  total_loss: 7.893  time: 0.2096(76.35)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:01:59] lb.utils.events INFO:  eta: 0:02:19  iteration: 331/1000  consumed samples: 5296  total_loss: 7.892  time: 0.2096(76.35)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:59] lb.utils.events INFO:  eta: 0:02:19  iteration: 332/1000  consumed samples: 5312  total_loss: 7.892  time: 0.2096(76.33)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:59] lb.utils.events INFO:  eta: 0:02:19  iteration: 333/1000  consumed samples: 5328  total_loss: 7.892  time: 0.2096(76.33)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:01:59] lb.utils.events INFO:  eta: 0:02:19  iteration: 334/1000  consumed samples: 5344  total_loss: 7.892  time: 0.2096(76.33)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:00] lb.utils.events INFO:  eta: 0:02:18  iteration: 335/1000  consumed samples: 5360  total_loss: 7.891  time: 0.2096(76.33)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:00] lb.utils.events INFO:  eta: 0:02:18  iteration: 336/1000  consumed samples: 5376  total_loss: 7.89  time: 0.2097(76.31)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:00] lb.utils.events INFO:  eta: 0:02:18  iteration: 337/1000  consumed samples: 5392  total_loss: 7.89  time: 0.2097(76.32)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:00] lb.utils.events INFO:  eta: 0:02:18  iteration: 338/1000  consumed samples: 5408  total_loss: 7.89  time: 0.2097(76.30)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:00] lb.utils.events INFO:  eta: 0:02:18  iteration: 339/1000  consumed samples: 5424  total_loss: 7.89  time: 0.2097(76.30)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:01] lb.utils.events INFO:  eta: 0:02:17  iteration: 340/1000  consumed samples: 5440  total_loss: 7.89  time: 0.2097(76.29)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:01] lb.utils.events INFO:  eta: 0:02:17  iteration: 341/1000  consumed samples: 5456  total_loss: 7.89  time: 0.2097(76.29)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:01] lb.utils.events INFO:  eta: 0:02:17  iteration: 342/1000  consumed samples: 5472  total_loss: 7.89  time: 0.2098(76.28)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:01] lb.utils.events INFO:  eta: 0:02:17  iteration: 343/1000  consumed samples: 5488  total_loss: 7.889  time: 0.2098(76.27)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:02] lb.utils.events INFO:  eta: 0:02:17  iteration: 344/1000  consumed samples: 5504  total_loss: 7.889  time: 0.2098(76.27)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:02] lb.utils.events INFO:  eta: 0:02:16  iteration: 345/1000  consumed samples: 5520  total_loss: 7.889  time: 0.2098(76.27)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:02] lb.utils.events INFO:  eta: 0:02:16  iteration: 346/1000  consumed samples: 5536  total_loss: 7.888  time: 0.2098(76.26)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:02] lb.utils.events INFO:  eta: 0:02:16  iteration: 347/1000  consumed samples: 5552  total_loss: 7.885  time: 0.2098(76.27)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:03] lb.utils.events INFO:  eta: 0:02:16  iteration: 348/1000  consumed samples: 5568  total_loss: 7.881  time: 0.2098(76.25)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:03] lb.utils.events INFO:  eta: 0:02:16  iteration: 349/1000  consumed samples: 5584  total_loss: 7.881  time: 0.2098(76.25)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:03] lb.utils.events INFO:  eta: 0:02:15  iteration: 350/1000  consumed samples: 5600  total_loss: 7.881  time: 0.2098(76.25)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:03] lb.utils.events INFO:  eta: 0:02:15  iteration: 351/1000  consumed samples: 5616  total_loss: 7.875  time: 0.2099(76.24)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:03] lb.utils.events INFO:  eta: 0:02:15  iteration: 352/1000  consumed samples: 5632  total_loss: 7.875  time: 0.2099(76.24)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:04] lb.utils.events INFO:  eta: 0:02:15  iteration: 353/1000  consumed samples: 5648  total_loss: 7.875  time: 0.2099(76.24)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:04] lb.utils.events INFO:  eta: 0:02:15  iteration: 354/1000  consumed samples: 5664  total_loss: 7.875  time: 0.2099(76.22)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:04] lb.utils.events INFO:  eta: 0:02:14  iteration: 355/1000  consumed samples: 5680  total_loss: 7.872  time: 0.2099(76.22)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:04] lb.utils.events INFO:  eta: 0:02:14  iteration: 356/1000  consumed samples: 5696  total_loss: 7.871  time: 0.2099(76.22)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:05] lb.utils.events INFO:  eta: 0:02:14  iteration: 357/1000  consumed samples: 5712  total_loss: 7.868  time: 0.2099(76.21)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:05] lb.utils.events INFO:  eta: 0:02:14  iteration: 358/1000  consumed samples: 5728  total_loss: 7.865  time: 0.2100(76.20)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:05] lb.utils.events INFO:  eta: 0:02:14  iteration: 359/1000  consumed samples: 5744  total_loss: 7.862  time: 0.2100(76.20)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:05] lb.utils.events INFO:  eta: 0:02:13  iteration: 360/1000  consumed samples: 5760  total_loss: 7.859  time: 0.2100(76.18)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:05] lb.utils.events INFO:  eta: 0:02:13  iteration: 361/1000  consumed samples: 5776  total_loss: 7.857  time: 0.2100(76.18)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:06] lb.utils.events INFO:  eta: 0:02:13  iteration: 362/1000  consumed samples: 5792  total_loss: 7.855  time: 0.2100(76.19)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:06] lb.utils.events INFO:  eta: 0:02:13  iteration: 363/1000  consumed samples: 5808  total_loss: 7.854  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:06] lb.utils.events INFO:  eta: 0:02:13  iteration: 364/1000  consumed samples: 5824  total_loss: 7.854  time: 0.2100(76.18)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:06] lb.utils.events INFO:  eta: 0:02:12  iteration: 365/1000  consumed samples: 5840  total_loss: 7.854  time: 0.2100(76.17)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:07] lb.utils.events INFO:  eta: 0:02:12  iteration: 366/1000  consumed samples: 5856  total_loss: 7.853  time: 0.2101(76.17)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:07] lb.utils.events INFO:  eta: 0:02:12  iteration: 367/1000  consumed samples: 5872  total_loss: 7.851  time: 0.2101(76.17)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:07] lb.utils.events INFO:  eta: 0:02:12  iteration: 368/1000  consumed samples: 5888  total_loss: 7.85  time: 0.2101(76.16)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:07] lb.utils.events INFO:  eta: 0:02:12  iteration: 369/1000  consumed samples: 5904  total_loss: 7.849  time: 0.2101(76.16)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:08] lb.utils.events INFO:  eta: 0:02:11  iteration: 370/1000  consumed samples: 5920  total_loss: 7.846  time: 0.2101(76.15)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:08] lb.utils.events INFO:  eta: 0:02:11  iteration: 371/1000  consumed samples: 5936  total_loss: 7.843  time: 0.2101(76.16)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:08] lb.utils.events INFO:  eta: 0:02:11  iteration: 372/1000  consumed samples: 5952  total_loss: 7.842  time: 0.2101(76.15)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:08] lb.utils.events INFO:  eta: 0:02:11  iteration: 373/1000  consumed samples: 5968  total_loss: 7.839  time: 0.2101(76.15)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:08] lb.utils.events INFO:  eta: 0:02:10  iteration: 374/1000  consumed samples: 5984  total_loss: 7.836  time: 0.2101(76.15)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:09] lb.utils.events INFO:  eta: 0:02:10  iteration: 375/1000  consumed samples: 6000  total_loss: 7.839  time: 0.2101(76.15)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:09] lb.utils.events INFO:  eta: 0:02:10  iteration: 376/1000  consumed samples: 6016  total_loss: 7.836  time: 0.2101(76.14)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:09] lb.utils.events INFO:  eta: 0:02:10  iteration: 377/1000  consumed samples: 6032  total_loss: 7.839  time: 0.2101(76.14)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:09] lb.utils.events INFO:  eta: 0:02:10  iteration: 378/1000  consumed samples: 6048  total_loss: 7.836  time: 0.2101(76.14)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:10] lb.utils.events INFO:  eta: 0:02:09  iteration: 379/1000  consumed samples: 6064  total_loss: 7.836  time: 0.2101(76.14)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:10] lb.utils.events INFO:  eta: 0:02:09  iteration: 380/1000  consumed samples: 6080  total_loss: 7.836  time: 0.2102(76.13)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:10] lb.utils.events INFO:  eta: 0:02:09  iteration: 381/1000  consumed samples: 6096  total_loss: 7.836  time: 0.2102(76.14)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:10] lb.utils.events INFO:  eta: 0:02:09  iteration: 382/1000  consumed samples: 6112  total_loss: 7.836  time: 0.2102(76.13)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:11] lb.utils.events INFO:  eta: 0:02:09  iteration: 383/1000  consumed samples: 6128  total_loss: 7.835  time: 0.2102(76.13)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:11] lb.utils.events INFO:  eta: 0:02:08  iteration: 384/1000  consumed samples: 6144  total_loss: 7.835  time: 0.2102(76.13)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:11] lb.utils.events INFO:  eta: 0:02:08  iteration: 385/1000  consumed samples: 6160  total_loss: 7.834  time: 0.2102(76.13)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:11] lb.utils.events INFO:  eta: 0:02:08  iteration: 386/1000  consumed samples: 6176  total_loss: 7.833  time: 0.2102(76.12)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:11] lb.utils.events INFO:  eta: 0:02:08  iteration: 387/1000  consumed samples: 6192  total_loss: 7.833  time: 0.2102(76.12)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:12] lb.utils.events INFO:  eta: 0:02:08  iteration: 388/1000  consumed samples: 6208  total_loss: 7.833  time: 0.2102(76.12)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:12] lb.utils.events INFO:  eta: 0:02:07  iteration: 389/1000  consumed samples: 6224  total_loss: 7.832  time: 0.2102(76.12)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:12] lb.utils.events INFO:  eta: 0:02:07  iteration: 390/1000  consumed samples: 6240  total_loss: 7.831  time: 0.2102(76.12)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:12] lb.utils.events INFO:  eta: 0:02:07  iteration: 391/1000  consumed samples: 6256  total_loss: 7.83  time: 0.2102(76.11)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:13] lb.utils.events INFO:  eta: 0:02:07  iteration: 392/1000  consumed samples: 6272  total_loss: 7.83  time: 0.2102(76.11)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:13] lb.utils.events INFO:  eta: 0:02:07  iteration: 393/1000  consumed samples: 6288  total_loss: 7.829  time: 0.2102(76.11)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:13] lb.utils.events INFO:  eta: 0:02:06  iteration: 394/1000  consumed samples: 6304  total_loss: 7.829  time: 0.2102(76.10)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:13] lb.utils.events INFO:  eta: 0:02:06  iteration: 395/1000  consumed samples: 6320  total_loss: 7.829  time: 0.2103(76.09)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:13] lb.utils.events INFO:  eta: 0:02:06  iteration: 396/1000  consumed samples: 6336  total_loss: 7.827  time: 0.2103(76.09)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:14] lb.utils.events INFO:  eta: 0:02:06  iteration: 397/1000  consumed samples: 6352  total_loss: 7.825  time: 0.2103(76.09)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:14] lb.utils.events INFO:  eta: 0:02:05  iteration: 398/1000  consumed samples: 6368  total_loss: 7.823  time: 0.2103(76.09)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:14] lb.utils.events INFO:  eta: 0:02:05  iteration: 399/1000  consumed samples: 6384  total_loss: 7.82  time: 0.2103(76.08)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:14] lb.utils.events INFO:  eta: 0:02:05  iteration: 400/1000  consumed samples: 6400  total_loss: 7.82  time: 0.2103(76.08)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:02:15] lb.utils.events INFO:  eta: 0:02:05  iteration: 401/1000  consumed samples: 6416  total_loss: 7.82  time: 0.2103(76.06)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:15] lb.utils.events INFO:  eta: 0:02:05  iteration: 402/1000  consumed samples: 6432  total_loss: 7.82  time: 0.2104(76.06)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:02:15] lb.utils.events INFO:  eta: 0:02:04  iteration: 403/1000  consumed samples: 6448  total_loss: 7.817  time: 0.2103(76.06)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:15] lb.utils.events INFO:  eta: 0:02:04  iteration: 404/1000  consumed samples: 6464  total_loss: 7.817  time: 0.2103(76.07)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:16] lb.utils.events INFO:  eta: 0:02:04  iteration: 405/1000  consumed samples: 6480  total_loss: 7.817  time: 0.2104(76.06)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:16] lb.utils.events INFO:  eta: 0:02:04  iteration: 406/1000  consumed samples: 6496  total_loss: 7.816  time: 0.2104(76.05)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:16] lb.utils.events INFO:  eta: 0:02:04  iteration: 407/1000  consumed samples: 6512  total_loss: 7.816  time: 0.2104(76.05)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:16] lb.utils.events INFO:  eta: 0:02:03  iteration: 408/1000  consumed samples: 6528  total_loss: 7.816  time: 0.2104(76.05)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:16] lb.utils.events INFO:  eta: 0:02:03  iteration: 409/1000  consumed samples: 6544  total_loss: 7.816  time: 0.2104(76.05)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:17] lb.utils.events INFO:  eta: 0:02:03  iteration: 410/1000  consumed samples: 6560  total_loss: 7.816  time: 0.2104(76.05)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:17] lb.utils.events INFO:  eta: 0:02:03  iteration: 411/1000  consumed samples: 6576  total_loss: 7.816  time: 0.2104(76.05)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:17] lb.utils.events INFO:  eta: 0:02:03  iteration: 412/1000  consumed samples: 6592  total_loss: 7.815  time: 0.2104(76.05)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:17] lb.utils.events INFO:  eta: 0:02:02  iteration: 413/1000  consumed samples: 6608  total_loss: 7.814  time: 0.2104(76.05)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:18] lb.utils.events INFO:  eta: 0:02:02  iteration: 414/1000  consumed samples: 6624  total_loss: 7.814  time: 0.2104(76.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:18] lb.utils.events INFO:  eta: 0:02:02  iteration: 415/1000  consumed samples: 6640  total_loss: 7.813  time: 0.2104(76.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:18] lb.utils.events INFO:  eta: 0:02:02  iteration: 416/1000  consumed samples: 6656  total_loss: 7.813  time: 0.2104(76.05)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:18] lb.utils.events INFO:  eta: 0:02:02  iteration: 417/1000  consumed samples: 6672  total_loss: 7.812  time: 0.2104(76.05)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:19] lb.utils.events INFO:  eta: 0:02:01  iteration: 418/1000  consumed samples: 6688  total_loss: 7.812  time: 0.2104(76.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:19] lb.utils.events INFO:  eta: 0:02:01  iteration: 419/1000  consumed samples: 6704  total_loss: 7.81  time: 0.2104(76.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:19] lb.utils.events INFO:  eta: 0:02:01  iteration: 420/1000  consumed samples: 6720  total_loss: 7.808  time: 0.2104(76.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:19] lb.utils.events INFO:  eta: 0:02:01  iteration: 421/1000  consumed samples: 6736  total_loss: 7.808  time: 0.2104(76.04)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:19] lb.utils.events INFO:  eta: 0:02:00  iteration: 422/1000  consumed samples: 6752  total_loss: 7.807  time: 0.2104(76.04)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:20] lb.utils.events INFO:  eta: 0:02:00  iteration: 423/1000  consumed samples: 6768  total_loss: 7.803  time: 0.2104(76.03)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:20] lb.utils.events INFO:  eta: 0:02:00  iteration: 424/1000  consumed samples: 6784  total_loss: 7.797  time: 0.2104(76.03)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:20] lb.utils.events INFO:  eta: 0:02:00  iteration: 425/1000  consumed samples: 6800  total_loss: 7.797  time: 0.2104(76.03)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:20] lb.utils.events INFO:  eta: 0:02:00  iteration: 426/1000  consumed samples: 6816  total_loss: 7.803  time: 0.2105(76.03)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:21] lb.utils.events INFO:  eta: 0:01:59  iteration: 427/1000  consumed samples: 6832  total_loss: 7.797  time: 0.2105(76.02)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:21] lb.utils.events INFO:  eta: 0:01:59  iteration: 428/1000  consumed samples: 6848  total_loss: 7.794  time: 0.2105(76.02)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:21] lb.utils.events INFO:  eta: 0:01:59  iteration: 429/1000  consumed samples: 6864  total_loss: 7.794  time: 0.2105(76.01)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:21] lb.utils.events INFO:  eta: 0:01:59  iteration: 430/1000  consumed samples: 6880  total_loss: 7.794  time: 0.2105(76.01)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:21] lb.utils.events INFO:  eta: 0:01:59  iteration: 431/1000  consumed samples: 6896  total_loss: 7.794  time: 0.2105(76.01)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:22] lb.utils.events INFO:  eta: 0:01:58  iteration: 432/1000  consumed samples: 6912  total_loss: 7.794  time: 0.2105(76.01)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:22] lb.utils.events INFO:  eta: 0:01:58  iteration: 433/1000  consumed samples: 6928  total_loss: 7.794  time: 0.2105(76.01)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:22] lb.utils.events INFO:  eta: 0:01:58  iteration: 434/1000  consumed samples: 6944  total_loss: 7.793  time: 0.2105(76.00)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:22] lb.utils.events INFO:  eta: 0:01:58  iteration: 435/1000  consumed samples: 6960  total_loss: 7.793  time: 0.2105(76.00)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:23] lb.utils.events INFO:  eta: 0:01:58  iteration: 436/1000  consumed samples: 6976  total_loss: 7.793  time: 0.2105(76.00)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:23] lb.utils.events INFO:  eta: 0:01:57  iteration: 437/1000  consumed samples: 6992  total_loss: 7.792  time: 0.2105(76.00)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:23] lb.utils.events INFO:  eta: 0:01:57  iteration: 438/1000  consumed samples: 7008  total_loss: 7.791  time: 0.2105(75.99)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:23] lb.utils.events INFO:  eta: 0:01:57  iteration: 439/1000  consumed samples: 7024  total_loss: 7.789  time: 0.2105(75.99)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:24] lb.utils.events INFO:  eta: 0:01:57  iteration: 440/1000  consumed samples: 7040  total_loss: 7.789  time: 0.2106(75.99)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:24] lb.utils.events INFO:  eta: 0:01:57  iteration: 441/1000  consumed samples: 7056  total_loss: 7.789  time: 0.2106(75.99)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:24] lb.utils.events INFO:  eta: 0:01:56  iteration: 442/1000  consumed samples: 7072  total_loss: 7.789  time: 0.2106(75.99)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:24] lb.utils.events INFO:  eta: 0:01:56  iteration: 443/1000  consumed samples: 7088  total_loss: 7.787  time: 0.2106(75.99)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:24] lb.utils.events INFO:  eta: 0:01:56  iteration: 444/1000  consumed samples: 7104  total_loss: 7.786  time: 0.2106(75.99)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:25] lb.utils.events INFO:  eta: 0:01:56  iteration: 445/1000  consumed samples: 7120  total_loss: 7.785  time: 0.2106(75.99)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:25] lb.utils.events INFO:  eta: 0:01:56  iteration: 446/1000  consumed samples: 7136  total_loss: 7.784  time: 0.2106(75.98)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:25] lb.utils.events INFO:  eta: 0:01:55  iteration: 447/1000  consumed samples: 7152  total_loss: 7.783  time: 0.2106(75.98)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:25] lb.utils.events INFO:  eta: 0:01:55  iteration: 448/1000  consumed samples: 7168  total_loss: 7.781  time: 0.2106(75.98)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:26] lb.utils.events INFO:  eta: 0:01:55  iteration: 449/1000  consumed samples: 7184  total_loss: 7.779  time: 0.2106(75.98)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:26] lb.utils.events INFO:  eta: 0:01:55  iteration: 450/1000  consumed samples: 7200  total_loss: 7.779  time: 0.2106(75.97)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:26] lb.utils.events INFO:  eta: 0:01:54  iteration: 451/1000  consumed samples: 7216  total_loss: 7.779  time: 0.2106(75.97)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:26] lb.utils.events INFO:  eta: 0:01:54  iteration: 452/1000  consumed samples: 7232  total_loss: 7.778  time: 0.2106(75.96)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:27] lb.utils.events INFO:  eta: 0:01:54  iteration: 453/1000  consumed samples: 7248  total_loss: 7.777  time: 0.2106(75.96)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:27] lb.utils.events INFO:  eta: 0:01:54  iteration: 454/1000  consumed samples: 7264  total_loss: 7.776  time: 0.2106(75.96)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:27] lb.utils.events INFO:  eta: 0:01:54  iteration: 455/1000  consumed samples: 7280  total_loss: 7.776  time: 0.2106(75.96)  data_time: 0.0042  lr: 9.90e-05  
[01/19 12:02:27] lb.utils.events INFO:  eta: 0:01:53  iteration: 456/1000  consumed samples: 7296  total_loss: 7.775  time: 0.2106(75.96)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:27] lb.utils.events INFO:  eta: 0:01:53  iteration: 457/1000  consumed samples: 7312  total_loss: 7.774  time: 0.2106(75.96)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:28] lb.utils.events INFO:  eta: 0:01:53  iteration: 458/1000  consumed samples: 7328  total_loss: 7.773  time: 0.2107(75.95)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:28] lb.utils.events INFO:  eta: 0:01:53  iteration: 459/1000  consumed samples: 7344  total_loss: 7.771  time: 0.2107(75.95)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:28] lb.utils.events INFO:  eta: 0:01:53  iteration: 460/1000  consumed samples: 7360  total_loss: 7.77  time: 0.2107(75.95)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:28] lb.utils.events INFO:  eta: 0:01:52  iteration: 461/1000  consumed samples: 7376  total_loss: 7.77  time: 0.2107(75.95)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:29] lb.utils.events INFO:  eta: 0:01:52  iteration: 462/1000  consumed samples: 7392  total_loss: 7.77  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:29] lb.utils.events INFO:  eta: 0:01:52  iteration: 463/1000  consumed samples: 7408  total_loss: 7.77  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:29] lb.utils.events INFO:  eta: 0:01:52  iteration: 464/1000  consumed samples: 7424  total_loss: 7.768  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:29] lb.utils.events INFO:  eta: 0:01:52  iteration: 465/1000  consumed samples: 7440  total_loss: 7.768  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:30] lb.utils.events INFO:  eta: 0:01:51  iteration: 466/1000  consumed samples: 7456  total_loss: 7.768  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:30] lb.utils.events INFO:  eta: 0:01:51  iteration: 467/1000  consumed samples: 7472  total_loss: 7.765  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:30] lb.utils.events INFO:  eta: 0:01:51  iteration: 468/1000  consumed samples: 7488  total_loss: 7.764  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:30] lb.utils.events INFO:  eta: 0:01:51  iteration: 469/1000  consumed samples: 7504  total_loss: 7.764  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:30] lb.utils.events INFO:  eta: 0:01:51  iteration: 470/1000  consumed samples: 7520  total_loss: 7.764  time: 0.2107(75.93)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:31] lb.utils.events INFO:  eta: 0:01:50  iteration: 471/1000  consumed samples: 7536  total_loss: 7.763  time: 0.2107(75.94)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:31] lb.utils.events INFO:  eta: 0:01:50  iteration: 472/1000  consumed samples: 7552  total_loss: 7.762  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:31] lb.utils.events INFO:  eta: 0:01:50  iteration: 473/1000  consumed samples: 7568  total_loss: 7.762  time: 0.2107(75.94)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:31] lb.utils.events INFO:  eta: 0:01:50  iteration: 474/1000  consumed samples: 7584  total_loss: 7.761  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:32] lb.utils.events INFO:  eta: 0:01:50  iteration: 475/1000  consumed samples: 7600  total_loss: 7.76  time: 0.2107(75.94)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:32] lb.utils.events INFO:  eta: 0:01:49  iteration: 476/1000  consumed samples: 7616  total_loss: 7.76  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:32] lb.utils.events INFO:  eta: 0:01:49  iteration: 477/1000  consumed samples: 7632  total_loss: 7.76  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:32] lb.utils.events INFO:  eta: 0:01:49  iteration: 478/1000  consumed samples: 7648  total_loss: 7.759  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:32] lb.utils.events INFO:  eta: 0:01:49  iteration: 479/1000  consumed samples: 7664  total_loss: 7.759  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:33] lb.utils.events INFO:  eta: 0:01:48  iteration: 480/1000  consumed samples: 7680  total_loss: 7.758  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:33] lb.utils.events INFO:  eta: 0:01:48  iteration: 481/1000  consumed samples: 7696  total_loss: 7.756  time: 0.2107(75.93)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:33] lb.utils.events INFO:  eta: 0:01:48  iteration: 482/1000  consumed samples: 7712  total_loss: 7.756  time: 0.2107(75.92)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:33] lb.utils.events INFO:  eta: 0:01:48  iteration: 483/1000  consumed samples: 7728  total_loss: 7.755  time: 0.2107(75.92)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:34] lb.utils.events INFO:  eta: 0:01:48  iteration: 484/1000  consumed samples: 7744  total_loss: 7.754  time: 0.2107(75.92)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:34] lb.utils.events INFO:  eta: 0:01:47  iteration: 485/1000  consumed samples: 7760  total_loss: 7.754  time: 0.2108(75.92)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:34] lb.utils.events INFO:  eta: 0:01:47  iteration: 486/1000  consumed samples: 7776  total_loss: 7.754  time: 0.2108(75.91)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:34] lb.utils.events INFO:  eta: 0:01:47  iteration: 487/1000  consumed samples: 7792  total_loss: 7.752  time: 0.2108(75.91)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:35] lb.utils.events INFO:  eta: 0:01:47  iteration: 488/1000  consumed samples: 7808  total_loss: 7.752  time: 0.2108(75.91)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:35] lb.utils.events INFO:  eta: 0:01:47  iteration: 489/1000  consumed samples: 7824  total_loss: 7.752  time: 0.2108(75.91)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:35] lb.utils.events INFO:  eta: 0:01:46  iteration: 490/1000  consumed samples: 7840  total_loss: 7.752  time: 0.2108(75.91)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:35] lb.utils.events INFO:  eta: 0:01:46  iteration: 491/1000  consumed samples: 7856  total_loss: 7.752  time: 0.2108(75.90)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:35] lb.utils.events INFO:  eta: 0:01:46  iteration: 492/1000  consumed samples: 7872  total_loss: 7.75  time: 0.2108(75.90)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:36] lb.utils.events INFO:  eta: 0:01:46  iteration: 493/1000  consumed samples: 7888  total_loss: 7.749  time: 0.2108(75.90)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:36] lb.utils.events INFO:  eta: 0:01:46  iteration: 494/1000  consumed samples: 7904  total_loss: 7.748  time: 0.2108(75.89)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:36] lb.utils.events INFO:  eta: 0:01:45  iteration: 495/1000  consumed samples: 7920  total_loss: 7.747  time: 0.2108(75.89)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:36] lb.utils.events INFO:  eta: 0:01:45  iteration: 496/1000  consumed samples: 7936  total_loss: 7.745  time: 0.2108(75.88)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:37] lb.utils.events INFO:  eta: 0:01:45  iteration: 497/1000  consumed samples: 7952  total_loss: 7.745  time: 0.2109(75.88)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:37] lb.utils.events INFO:  eta: 0:01:45  iteration: 498/1000  consumed samples: 7968  total_loss: 7.745  time: 0.2109(75.88)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:37] lb.utils.events INFO:  eta: 0:01:45  iteration: 499/1000  consumed samples: 7984  total_loss: 7.744  time: 0.2109(75.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:37] lb.utils.events INFO:  eta: 0:01:44  iteration: 500/1000  consumed samples: 8000  total_loss: 7.743  time: 0.2109(75.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:38] lb.utils.events INFO:  eta: 0:01:44  iteration: 501/1000  consumed samples: 8016  total_loss: 7.737  time: 0.2109(75.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:38] lb.utils.events INFO:  eta: 0:01:44  iteration: 502/1000  consumed samples: 8032  total_loss: 7.737  time: 0.2109(75.87)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:38] lb.utils.events INFO:  eta: 0:01:44  iteration: 503/1000  consumed samples: 8048  total_loss: 7.73  time: 0.2109(75.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:38] lb.utils.events INFO:  eta: 0:01:43  iteration: 504/1000  consumed samples: 8064  total_loss: 7.728  time: 0.2109(75.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:38] lb.utils.events INFO:  eta: 0:01:43  iteration: 505/1000  consumed samples: 8080  total_loss: 7.727  time: 0.2109(75.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:39] lb.utils.events INFO:  eta: 0:01:43  iteration: 506/1000  consumed samples: 8096  total_loss: 7.727  time: 0.2109(75.86)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:39] lb.utils.events INFO:  eta: 0:01:43  iteration: 507/1000  consumed samples: 8112  total_loss: 7.725  time: 0.2109(75.87)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:39] lb.utils.events INFO:  eta: 0:01:43  iteration: 508/1000  consumed samples: 8128  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:39] lb.utils.events INFO:  eta: 0:01:42  iteration: 509/1000  consumed samples: 8144  total_loss: 7.722  time: 0.2109(75.87)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:40] lb.utils.events INFO:  eta: 0:01:42  iteration: 510/1000  consumed samples: 8160  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:40] lb.utils.events INFO:  eta: 0:01:42  iteration: 511/1000  consumed samples: 8176  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:40] lb.utils.events INFO:  eta: 0:01:42  iteration: 512/1000  consumed samples: 8192  total_loss: 7.722  time: 0.2109(75.85)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:40] lb.utils.events INFO:  eta: 0:01:42  iteration: 513/1000  consumed samples: 8208  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:40] lb.utils.events INFO:  eta: 0:01:41  iteration: 514/1000  consumed samples: 8224  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 515/1000  consumed samples: 8240  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 516/1000  consumed samples: 8256  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 517/1000  consumed samples: 8272  total_loss: 7.722  time: 0.2109(75.86)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 518/1000  consumed samples: 8288  total_loss: 7.722  time: 0.2109(75.85)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:42] lb.utils.events INFO:  eta: 0:01:40  iteration: 519/1000  consumed samples: 8304  total_loss: 7.721  time: 0.2109(75.85)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:42] lb.utils.events INFO:  eta: 0:01:40  iteration: 520/1000  consumed samples: 8320  total_loss: 7.721  time: 0.2109(75.85)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:42] lb.utils.events INFO:  eta: 0:01:40  iteration: 521/1000  consumed samples: 8336  total_loss: 7.722  time: 0.2109(75.85)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:42] lb.utils.events INFO:  eta: 0:01:40  iteration: 522/1000  consumed samples: 8352  total_loss: 7.721  time: 0.2110(75.84)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 523/1000  consumed samples: 8368  total_loss: 7.719  time: 0.2110(75.84)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:43] lb.utils.events INFO:  eta: 0:01:39  iteration: 524/1000  consumed samples: 8384  total_loss: 7.718  time: 0.2110(75.84)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:43] lb.utils.events INFO:  eta: 0:01:39  iteration: 525/1000  consumed samples: 8400  total_loss: 7.717  time: 0.2110(75.84)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:43] lb.utils.events INFO:  eta: 0:01:39  iteration: 526/1000  consumed samples: 8416  total_loss: 7.717  time: 0.2110(75.84)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:43] lb.utils.events INFO:  eta: 0:01:39  iteration: 527/1000  consumed samples: 8432  total_loss: 7.718  time: 0.2110(75.84)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:02:44] lb.utils.events INFO:  eta: 0:01:38  iteration: 528/1000  consumed samples: 8448  total_loss: 7.717  time: 0.2110(75.84)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:44] lb.utils.events INFO:  eta: 0:01:38  iteration: 529/1000  consumed samples: 8464  total_loss: 7.716  time: 0.2110(75.84)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:44] lb.utils.events INFO:  eta: 0:01:38  iteration: 530/1000  consumed samples: 8480  total_loss: 7.715  time: 0.2110(75.83)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:44] lb.utils.events INFO:  eta: 0:01:38  iteration: 531/1000  consumed samples: 8496  total_loss: 7.715  time: 0.2110(75.83)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 532/1000  consumed samples: 8512  total_loss: 7.715  time: 0.2110(75.83)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:45] lb.utils.events INFO:  eta: 0:01:37  iteration: 533/1000  consumed samples: 8528  total_loss: 7.715  time: 0.2110(75.83)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:45] lb.utils.events INFO:  eta: 0:01:37  iteration: 534/1000  consumed samples: 8544  total_loss: 7.713  time: 0.2110(75.83)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:45] lb.utils.events INFO:  eta: 0:01:37  iteration: 535/1000  consumed samples: 8560  total_loss: 7.713  time: 0.2110(75.82)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 536/1000  consumed samples: 8576  total_loss: 7.713  time: 0.2110(75.82)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 537/1000  consumed samples: 8592  total_loss: 7.713  time: 0.2110(75.82)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:46] lb.utils.events INFO:  eta: 0:01:36  iteration: 538/1000  consumed samples: 8608  total_loss: 7.715  time: 0.2110(75.82)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:02:46] lb.utils.events INFO:  eta: 0:01:36  iteration: 539/1000  consumed samples: 8624  total_loss: 7.713  time: 0.2110(75.82)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:46] lb.utils.events INFO:  eta: 0:01:36  iteration: 540/1000  consumed samples: 8640  total_loss: 7.712  time: 0.2110(75.82)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:47] lb.utils.events INFO:  eta: 0:01:36  iteration: 541/1000  consumed samples: 8656  total_loss: 7.712  time: 0.2110(75.82)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:47] lb.utils.events INFO:  eta: 0:01:36  iteration: 542/1000  consumed samples: 8672  total_loss: 7.709  time: 0.2110(75.82)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:47] lb.utils.events INFO:  eta: 0:01:35  iteration: 543/1000  consumed samples: 8688  total_loss: 7.704  time: 0.2110(75.81)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:47] lb.utils.events INFO:  eta: 0:01:35  iteration: 544/1000  consumed samples: 8704  total_loss: 7.704  time: 0.2110(75.82)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 545/1000  consumed samples: 8720  total_loss: 7.701  time: 0.2110(75.81)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:02:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 546/1000  consumed samples: 8736  total_loss: 7.701  time: 0.2111(75.81)  data_time: 0.0081  lr: 9.90e-05  
[01/19 12:02:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 547/1000  consumed samples: 8752  total_loss: 7.701  time: 0.2111(75.81)  data_time: 0.0081  lr: 9.90e-05  
[01/19 12:02:48] lb.utils.events INFO:  eta: 0:01:34  iteration: 548/1000  consumed samples: 8768  total_loss: 7.7  time: 0.2111(75.80)  data_time: 0.0080  lr: 9.90e-05  
[01/19 12:02:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 549/1000  consumed samples: 8784  total_loss: 7.7  time: 0.2111(75.80)  data_time: 0.0081  lr: 9.90e-05  
[01/19 12:02:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 550/1000  consumed samples: 8800  total_loss: 7.7  time: 0.2111(75.80)  data_time: 0.0080  lr: 9.90e-05  
[01/19 12:02:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 551/1000  consumed samples: 8816  total_loss: 7.697  time: 0.2111(75.80)  data_time: 0.0080  lr: 9.90e-05  
[01/19 12:02:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 552/1000  consumed samples: 8832  total_loss: 7.697  time: 0.2111(75.79)  data_time: 0.0079  lr: 9.90e-05  
[01/19 12:02:49] lb.utils.events INFO:  eta: 0:01:33  iteration: 553/1000  consumed samples: 8848  total_loss: 7.695  time: 0.2111(75.79)  data_time: 0.0078  lr: 9.90e-05  
[01/19 12:02:50] lb.utils.events INFO:  eta: 0:01:33  iteration: 554/1000  consumed samples: 8864  total_loss: 7.694  time: 0.2111(75.79)  data_time: 0.0078  lr: 9.90e-05  
[01/19 12:02:50] lb.utils.events INFO:  eta: 0:01:33  iteration: 555/1000  consumed samples: 8880  total_loss: 7.694  time: 0.2111(75.79)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:50] lb.utils.events INFO:  eta: 0:01:33  iteration: 556/1000  consumed samples: 8896  total_loss: 7.694  time: 0.2111(75.78)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:50] lb.utils.events INFO:  eta: 0:01:32  iteration: 557/1000  consumed samples: 8912  total_loss: 7.693  time: 0.2111(75.79)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 558/1000  consumed samples: 8928  total_loss: 7.693  time: 0.2111(75.78)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 559/1000  consumed samples: 8944  total_loss: 7.691  time: 0.2111(75.78)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 560/1000  consumed samples: 8960  total_loss: 7.691  time: 0.2111(75.78)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 561/1000  consumed samples: 8976  total_loss: 7.688  time: 0.2111(75.78)  data_time: 0.0076  lr: 9.90e-05  
[01/19 12:02:51] lb.utils.events INFO:  eta: 0:01:31  iteration: 562/1000  consumed samples: 8992  total_loss: 7.688  time: 0.2111(75.78)  data_time: 0.0077  lr: 9.90e-05  
[01/19 12:02:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 563/1000  consumed samples: 9008  total_loss: 7.685  time: 0.2111(75.78)  data_time: 0.0076  lr: 9.90e-05  
[01/19 12:02:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 564/1000  consumed samples: 9024  total_loss: 7.688  time: 0.2111(75.78)  data_time: 0.0076  lr: 9.90e-05  
[01/19 12:02:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 565/1000  consumed samples: 9040  total_loss: 7.685  time: 0.2111(75.78)  data_time: 0.0076  lr: 9.90e-05  
[01/19 12:02:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 566/1000  consumed samples: 9056  total_loss: 7.679  time: 0.2112(75.78)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 567/1000  consumed samples: 9072  total_loss: 7.679  time: 0.2112(75.78)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 568/1000  consumed samples: 9088  total_loss: 7.673  time: 0.2112(75.77)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 569/1000  consumed samples: 9104  total_loss: 7.673  time: 0.2112(75.77)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 570/1000  consumed samples: 9120  total_loss: 7.673  time: 0.2112(75.77)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:02:54] lb.utils.events INFO:  eta: 0:01:30  iteration: 571/1000  consumed samples: 9136  total_loss: 7.673  time: 0.2112(75.77)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:54] lb.utils.events INFO:  eta: 0:01:29  iteration: 572/1000  consumed samples: 9152  total_loss: 7.673  time: 0.2112(75.76)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:02:54] lb.utils.events INFO:  eta: 0:01:29  iteration: 573/1000  consumed samples: 9168  total_loss: 7.679  time: 0.2112(75.76)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:54] lb.utils.events INFO:  eta: 0:01:29  iteration: 574/1000  consumed samples: 9184  total_loss: 7.679  time: 0.2112(75.76)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:54] lb.utils.events INFO:  eta: 0:01:29  iteration: 575/1000  consumed samples: 9200  total_loss: 7.673  time: 0.2112(75.76)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:55] lb.utils.events INFO:  eta: 0:01:29  iteration: 576/1000  consumed samples: 9216  total_loss: 7.67  time: 0.2112(75.76)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:55] lb.utils.events INFO:  eta: 0:01:28  iteration: 577/1000  consumed samples: 9232  total_loss: 7.67  time: 0.2112(75.76)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:55] lb.utils.events INFO:  eta: 0:01:28  iteration: 578/1000  consumed samples: 9248  total_loss: 7.67  time: 0.2112(75.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:55] lb.utils.events INFO:  eta: 0:01:28  iteration: 579/1000  consumed samples: 9264  total_loss: 7.669  time: 0.2112(75.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:56] lb.utils.events INFO:  eta: 0:01:28  iteration: 580/1000  consumed samples: 9280  total_loss: 7.667  time: 0.2112(75.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:56] lb.utils.events INFO:  eta: 0:01:27  iteration: 581/1000  consumed samples: 9296  total_loss: 7.664  time: 0.2112(75.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:56] lb.utils.events INFO:  eta: 0:01:27  iteration: 582/1000  consumed samples: 9312  total_loss: 7.664  time: 0.2112(75.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:56] lb.utils.events INFO:  eta: 0:01:27  iteration: 583/1000  consumed samples: 9328  total_loss: 7.664  time: 0.2112(75.75)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:57] lb.utils.events INFO:  eta: 0:01:27  iteration: 584/1000  consumed samples: 9344  total_loss: 7.663  time: 0.2112(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:57] lb.utils.events INFO:  eta: 0:01:27  iteration: 585/1000  consumed samples: 9360  total_loss: 7.663  time: 0.2112(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:57] lb.utils.events INFO:  eta: 0:01:26  iteration: 586/1000  consumed samples: 9376  total_loss: 7.663  time: 0.2112(75.74)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:57] lb.utils.events INFO:  eta: 0:01:26  iteration: 587/1000  consumed samples: 9392  total_loss: 7.661  time: 0.2112(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:57] lb.utils.events INFO:  eta: 0:01:26  iteration: 588/1000  consumed samples: 9408  total_loss: 7.663  time: 0.2112(75.74)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:58] lb.utils.events INFO:  eta: 0:01:26  iteration: 589/1000  consumed samples: 9424  total_loss: 7.663  time: 0.2112(75.74)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:02:58] lb.utils.events INFO:  eta: 0:01:26  iteration: 590/1000  consumed samples: 9440  total_loss: 7.661  time: 0.2113(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:58] lb.utils.events INFO:  eta: 0:01:25  iteration: 591/1000  consumed samples: 9456  total_loss: 7.661  time: 0.2113(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:58] lb.utils.events INFO:  eta: 0:01:25  iteration: 592/1000  consumed samples: 9472  total_loss: 7.659  time: 0.2112(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 593/1000  consumed samples: 9488  total_loss: 7.659  time: 0.2113(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 594/1000  consumed samples: 9504  total_loss: 7.659  time: 0.2113(75.74)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 595/1000  consumed samples: 9520  total_loss: 7.659  time: 0.2113(75.73)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:02:59] lb.utils.events INFO:  eta: 0:01:24  iteration: 596/1000  consumed samples: 9536  total_loss: 7.659  time: 0.2113(75.73)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 597/1000  consumed samples: 9552  total_loss: 7.659  time: 0.2113(75.73)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 598/1000  consumed samples: 9568  total_loss: 7.659  time: 0.2113(75.73)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 599/1000  consumed samples: 9584  total_loss: 7.656  time: 0.2113(75.73)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:00] lb.utils.events INFO:  eta: 0:01:23  iteration: 600/1000  consumed samples: 9600  total_loss: 7.654  time: 0.2113(75.72)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:00] lb.utils.events INFO:  eta: 0:01:23  iteration: 601/1000  consumed samples: 9616  total_loss: 7.654  time: 0.2113(75.72)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:01] lb.utils.events INFO:  eta: 0:01:23  iteration: 602/1000  consumed samples: 9632  total_loss: 7.653  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:01] lb.utils.events INFO:  eta: 0:01:23  iteration: 603/1000  consumed samples: 9648  total_loss: 7.654  time: 0.2113(75.72)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:01] lb.utils.events INFO:  eta: 0:01:23  iteration: 604/1000  consumed samples: 9664  total_loss: 7.653  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:01] lb.utils.events INFO:  eta: 0:01:22  iteration: 605/1000  consumed samples: 9680  total_loss: 7.652  time: 0.2113(75.72)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 606/1000  consumed samples: 9696  total_loss: 7.649  time: 0.2113(75.72)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 607/1000  consumed samples: 9712  total_loss: 7.649  time: 0.2113(75.72)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 608/1000  consumed samples: 9728  total_loss: 7.649  time: 0.2113(75.71)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 609/1000  consumed samples: 9744  total_loss: 7.648  time: 0.2113(75.72)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:02] lb.utils.events INFO:  eta: 0:01:21  iteration: 610/1000  consumed samples: 9760  total_loss: 7.647  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 611/1000  consumed samples: 9776  total_loss: 7.646  time: 0.2113(75.72)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 612/1000  consumed samples: 9792  total_loss: 7.647  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 613/1000  consumed samples: 9808  total_loss: 7.648  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 614/1000  consumed samples: 9824  total_loss: 7.647  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 615/1000  consumed samples: 9840  total_loss: 7.646  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 616/1000  consumed samples: 9856  total_loss: 7.646  time: 0.2113(75.71)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 617/1000  consumed samples: 9872  total_loss: 7.645  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 618/1000  consumed samples: 9888  total_loss: 7.645  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 619/1000  consumed samples: 9904  total_loss: 7.645  time: 0.2113(75.71)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 620/1000  consumed samples: 9920  total_loss: 7.645  time: 0.2113(75.70)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 621/1000  consumed samples: 9936  total_loss: 7.644  time: 0.2113(75.71)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 622/1000  consumed samples: 9952  total_loss: 7.645  time: 0.2114(75.70)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 623/1000  consumed samples: 9968  total_loss: 7.644  time: 0.2113(75.70)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:06] lb.utils.events INFO:  eta: 0:01:18  iteration: 624/1000  consumed samples: 9984  total_loss: 7.643  time: 0.2114(75.70)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:06] lb.utils.events INFO:  eta: 0:01:18  iteration: 625/1000  consumed samples: 10000  total_loss: 7.643  time: 0.2114(75.70)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:06] lb.utils.events INFO:  eta: 0:01:18  iteration: 626/1000  consumed samples: 10016  total_loss: 7.639  time: 0.2114(75.70)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:06] lb.utils.events INFO:  eta: 0:01:18  iteration: 627/1000  consumed samples: 10032  total_loss: 7.639  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:07] lb.utils.events INFO:  eta: 0:01:18  iteration: 628/1000  consumed samples: 10048  total_loss: 7.639  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:07] lb.utils.events INFO:  eta: 0:01:17  iteration: 629/1000  consumed samples: 10064  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:07] lb.utils.events INFO:  eta: 0:01:17  iteration: 630/1000  consumed samples: 10080  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:07] lb.utils.events INFO:  eta: 0:01:17  iteration: 631/1000  consumed samples: 10096  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:08] lb.utils.events INFO:  eta: 0:01:17  iteration: 632/1000  consumed samples: 10112  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:08] lb.utils.events INFO:  eta: 0:01:17  iteration: 633/1000  consumed samples: 10128  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:08] lb.utils.events INFO:  eta: 0:01:16  iteration: 634/1000  consumed samples: 10144  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:08] lb.utils.events INFO:  eta: 0:01:16  iteration: 635/1000  consumed samples: 10160  total_loss: 7.636  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:08] lb.utils.events INFO:  eta: 0:01:16  iteration: 636/1000  consumed samples: 10176  total_loss: 7.636  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:09] lb.utils.events INFO:  eta: 0:01:16  iteration: 637/1000  consumed samples: 10192  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:09] lb.utils.events INFO:  eta: 0:01:16  iteration: 638/1000  consumed samples: 10208  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:09] lb.utils.events INFO:  eta: 0:01:15  iteration: 639/1000  consumed samples: 10224  total_loss: 7.637  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:09] lb.utils.events INFO:  eta: 0:01:15  iteration: 640/1000  consumed samples: 10240  total_loss: 7.636  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:10] lb.utils.events INFO:  eta: 0:01:15  iteration: 641/1000  consumed samples: 10256  total_loss: 7.636  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:10] lb.utils.events INFO:  eta: 0:01:15  iteration: 642/1000  consumed samples: 10272  total_loss: 7.635  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:10] lb.utils.events INFO:  eta: 0:01:14  iteration: 643/1000  consumed samples: 10288  total_loss: 7.634  time: 0.2114(75.69)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:10] lb.utils.events INFO:  eta: 0:01:14  iteration: 644/1000  consumed samples: 10304  total_loss: 7.63  time: 0.2114(75.68)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 645/1000  consumed samples: 10320  total_loss: 7.63  time: 0.2114(75.68)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 646/1000  consumed samples: 10336  total_loss: 7.626  time: 0.2114(75.68)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 647/1000  consumed samples: 10352  total_loss: 7.625  time: 0.2114(75.68)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:11] lb.utils.events INFO:  eta: 0:01:13  iteration: 648/1000  consumed samples: 10368  total_loss: 7.624  time: 0.2114(75.67)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:11] lb.utils.events INFO:  eta: 0:01:13  iteration: 649/1000  consumed samples: 10384  total_loss: 7.624  time: 0.2114(75.67)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 650/1000  consumed samples: 10400  total_loss: 7.624  time: 0.2114(75.67)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 651/1000  consumed samples: 10416  total_loss: 7.622  time: 0.2114(75.67)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 652/1000  consumed samples: 10432  total_loss: 7.619  time: 0.2115(75.66)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:12] lb.utils.events INFO:  eta: 0:01:12  iteration: 653/1000  consumed samples: 10448  total_loss: 7.619  time: 0.2115(75.66)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 654/1000  consumed samples: 10464  total_loss: 7.619  time: 0.2115(75.66)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 655/1000  consumed samples: 10480  total_loss: 7.618  time: 0.2115(75.66)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 656/1000  consumed samples: 10496  total_loss: 7.619  time: 0.2115(75.65)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:03:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 657/1000  consumed samples: 10512  total_loss: 7.619  time: 0.2115(75.66)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 658/1000  consumed samples: 10528  total_loss: 7.618  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 659/1000  consumed samples: 10544  total_loss: 7.617  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 660/1000  consumed samples: 10560  total_loss: 7.617  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 661/1000  consumed samples: 10576  total_loss: 7.614  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 662/1000  consumed samples: 10592  total_loss: 7.614  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 663/1000  consumed samples: 10608  total_loss: 7.614  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 664/1000  consumed samples: 10624  total_loss: 7.614  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 665/1000  consumed samples: 10640  total_loss: 7.617  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 666/1000  consumed samples: 10656  total_loss: 7.614  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 667/1000  consumed samples: 10672  total_loss: 7.614  time: 0.2115(75.65)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 668/1000  consumed samples: 10688  total_loss: 7.611  time: 0.2115(75.65)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 669/1000  consumed samples: 10704  total_loss: 7.611  time: 0.2115(75.64)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 670/1000  consumed samples: 10720  total_loss: 7.611  time: 0.2115(75.65)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 671/1000  consumed samples: 10736  total_loss: 7.61  time: 0.2115(75.64)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 672/1000  consumed samples: 10752  total_loss: 7.609  time: 0.2115(75.64)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 673/1000  consumed samples: 10768  total_loss: 7.604  time: 0.2115(75.64)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 674/1000  consumed samples: 10784  total_loss: 7.609  time: 0.2115(75.63)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 675/1000  consumed samples: 10800  total_loss: 7.609  time: 0.2116(75.63)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:18] lb.utils.events INFO:  eta: 0:01:08  iteration: 676/1000  consumed samples: 10816  total_loss: 7.604  time: 0.2115(75.63)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:18] lb.utils.events INFO:  eta: 0:01:07  iteration: 677/1000  consumed samples: 10832  total_loss: 7.604  time: 0.2116(75.63)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:18] lb.utils.events INFO:  eta: 0:01:07  iteration: 678/1000  consumed samples: 10848  total_loss: 7.604  time: 0.2116(75.63)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:18] lb.utils.events INFO:  eta: 0:01:07  iteration: 679/1000  consumed samples: 10864  total_loss: 7.599  time: 0.2115(75.63)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:19] lb.utils.events INFO:  eta: 0:01:07  iteration: 680/1000  consumed samples: 10880  total_loss: 7.599  time: 0.2116(75.63)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:19] lb.utils.events INFO:  eta: 0:01:07  iteration: 681/1000  consumed samples: 10896  total_loss: 7.599  time: 0.2116(75.63)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 682/1000  consumed samples: 10912  total_loss: 7.596  time: 0.2116(75.63)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 683/1000  consumed samples: 10928  total_loss: 7.596  time: 0.2116(75.63)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 684/1000  consumed samples: 10944  total_loss: 7.596  time: 0.2116(75.63)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:20] lb.utils.events INFO:  eta: 0:01:06  iteration: 685/1000  consumed samples: 10960  total_loss: 7.599  time: 0.2116(75.63)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:20] lb.utils.events INFO:  eta: 0:01:05  iteration: 686/1000  consumed samples: 10976  total_loss: 7.596  time: 0.2116(75.63)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:20] lb.utils.events INFO:  eta: 0:01:05  iteration: 687/1000  consumed samples: 10992  total_loss: 7.596  time: 0.2116(75.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:20] lb.utils.events INFO:  eta: 0:01:05  iteration: 688/1000  consumed samples: 11008  total_loss: 7.594  time: 0.2116(75.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:21] lb.utils.events INFO:  eta: 0:01:05  iteration: 689/1000  consumed samples: 11024  total_loss: 7.596  time: 0.2116(75.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:21] lb.utils.events INFO:  eta: 0:01:05  iteration: 690/1000  consumed samples: 11040  total_loss: 7.594  time: 0.2116(75.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:21] lb.utils.events INFO:  eta: 0:01:04  iteration: 691/1000  consumed samples: 11056  total_loss: 7.593  time: 0.2116(75.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:21] lb.utils.events INFO:  eta: 0:01:04  iteration: 692/1000  consumed samples: 11072  total_loss: 7.591  time: 0.2116(75.62)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:22] lb.utils.events INFO:  eta: 0:01:04  iteration: 693/1000  consumed samples: 11088  total_loss: 7.591  time: 0.2116(75.62)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:22] lb.utils.events INFO:  eta: 0:01:04  iteration: 694/1000  consumed samples: 11104  total_loss: 7.589  time: 0.2116(75.62)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:22] lb.utils.events INFO:  eta: 0:01:04  iteration: 695/1000  consumed samples: 11120  total_loss: 7.589  time: 0.2116(75.61)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:22] lb.utils.events INFO:  eta: 0:01:03  iteration: 696/1000  consumed samples: 11136  total_loss: 7.589  time: 0.2116(75.62)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:22] lb.utils.events INFO:  eta: 0:01:03  iteration: 697/1000  consumed samples: 11152  total_loss: 7.587  time: 0.2116(75.61)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:23] lb.utils.events INFO:  eta: 0:01:03  iteration: 698/1000  consumed samples: 11168  total_loss: 7.587  time: 0.2116(75.61)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:23] lb.utils.events INFO:  eta: 0:01:03  iteration: 699/1000  consumed samples: 11184  total_loss: 7.587  time: 0.2116(75.61)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:23] lb.utils.events INFO:  eta: 0:01:03  iteration: 700/1000  consumed samples: 11200  total_loss: 7.586  time: 0.2116(75.61)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:23] lb.utils.events INFO:  eta: 0:01:02  iteration: 701/1000  consumed samples: 11216  total_loss: 7.584  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:24] lb.utils.events INFO:  eta: 0:01:02  iteration: 702/1000  consumed samples: 11232  total_loss: 7.581  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:24] lb.utils.events INFO:  eta: 0:01:02  iteration: 703/1000  consumed samples: 11248  total_loss: 7.578  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:24] lb.utils.events INFO:  eta: 0:01:02  iteration: 704/1000  consumed samples: 11264  total_loss: 7.578  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:24] lb.utils.events INFO:  eta: 0:01:01  iteration: 705/1000  consumed samples: 11280  total_loss: 7.577  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 706/1000  consumed samples: 11296  total_loss: 7.576  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 707/1000  consumed samples: 11312  total_loss: 7.576  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 708/1000  consumed samples: 11328  total_loss: 7.576  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 709/1000  consumed samples: 11344  total_loss: 7.576  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:25] lb.utils.events INFO:  eta: 0:01:00  iteration: 710/1000  consumed samples: 11360  total_loss: 7.576  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 711/1000  consumed samples: 11376  total_loss: 7.576  time: 0.2116(75.60)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 712/1000  consumed samples: 11392  total_loss: 7.576  time: 0.2116(75.61)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 713/1000  consumed samples: 11408  total_loss: 7.576  time: 0.2116(75.60)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 714/1000  consumed samples: 11424  total_loss: 7.576  time: 0.2116(75.60)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 715/1000  consumed samples: 11440  total_loss: 7.575  time: 0.2116(75.60)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 716/1000  consumed samples: 11456  total_loss: 7.575  time: 0.2116(75.60)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 717/1000  consumed samples: 11472  total_loss: 7.575  time: 0.2116(75.60)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 718/1000  consumed samples: 11488  total_loss: 7.573  time: 0.2116(75.60)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 719/1000  consumed samples: 11504  total_loss: 7.573  time: 0.2116(75.60)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 720/1000  consumed samples: 11520  total_loss: 7.573  time: 0.2117(75.59)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 721/1000  consumed samples: 11536  total_loss: 7.573  time: 0.2117(75.59)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 722/1000  consumed samples: 11552  total_loss: 7.571  time: 0.2117(75.59)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 723/1000  consumed samples: 11568  total_loss: 7.573  time: 0.2117(75.59)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 724/1000  consumed samples: 11584  total_loss: 7.573  time: 0.2117(75.59)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 725/1000  consumed samples: 11600  total_loss: 7.571  time: 0.2117(75.59)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 726/1000  consumed samples: 11616  total_loss: 7.57  time: 0.2117(75.59)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 727/1000  consumed samples: 11632  total_loss: 7.57  time: 0.2117(75.58)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:30] lb.utils.events INFO:  eta: 0:00:57  iteration: 728/1000  consumed samples: 11648  total_loss: 7.57  time: 0.2117(75.59)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 729/1000  consumed samples: 11664  total_loss: 7.57  time: 0.2117(75.58)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 730/1000  consumed samples: 11680  total_loss: 7.569  time: 0.2117(75.58)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 731/1000  consumed samples: 11696  total_loss: 7.569  time: 0.2117(75.58)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 732/1000  consumed samples: 11712  total_loss: 7.567  time: 0.2117(75.58)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:31] lb.utils.events INFO:  eta: 0:00:56  iteration: 733/1000  consumed samples: 11728  total_loss: 7.566  time: 0.2117(75.58)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 734/1000  consumed samples: 11744  total_loss: 7.565  time: 0.2117(75.57)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:03:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 735/1000  consumed samples: 11760  total_loss: 7.565  time: 0.2117(75.57)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 736/1000  consumed samples: 11776  total_loss: 7.565  time: 0.2117(75.57)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:32] lb.utils.events INFO:  eta: 0:00:55  iteration: 737/1000  consumed samples: 11792  total_loss: 7.565  time: 0.2117(75.57)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:32] lb.utils.events INFO:  eta: 0:00:55  iteration: 738/1000  consumed samples: 11808  total_loss: 7.565  time: 0.2117(75.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 739/1000  consumed samples: 11824  total_loss: 7.565  time: 0.2117(75.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 740/1000  consumed samples: 11840  total_loss: 7.564  time: 0.2118(75.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:33] lb.utils.events INFO:  eta: 0:00:54  iteration: 741/1000  consumed samples: 11856  total_loss: 7.564  time: 0.2118(75.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:33] lb.utils.events INFO:  eta: 0:00:54  iteration: 742/1000  consumed samples: 11872  total_loss: 7.564  time: 0.2118(75.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:33] lb.utils.events INFO:  eta: 0:00:54  iteration: 743/1000  consumed samples: 11888  total_loss: 7.564  time: 0.2118(75.56)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 744/1000  consumed samples: 11904  total_loss: 7.564  time: 0.2118(75.55)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 745/1000  consumed samples: 11920  total_loss: 7.564  time: 0.2118(75.55)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:34] lb.utils.events INFO:  eta: 0:00:53  iteration: 746/1000  consumed samples: 11936  total_loss: 7.564  time: 0.2118(75.55)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:34] lb.utils.events INFO:  eta: 0:00:53  iteration: 747/1000  consumed samples: 11952  total_loss: 7.565  time: 0.2118(75.55)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:34] lb.utils.events INFO:  eta: 0:00:52  iteration: 748/1000  consumed samples: 11968  total_loss: 7.565  time: 0.2118(75.55)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:34] lb.utils.events INFO:  eta: 0:00:52  iteration: 749/1000  consumed samples: 11984  total_loss: 7.565  time: 0.2118(75.55)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 750/1000  consumed samples: 12000  total_loss: 7.564  time: 0.2118(75.54)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 751/1000  consumed samples: 12016  total_loss: 7.564  time: 0.2118(75.54)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 752/1000  consumed samples: 12032  total_loss: 7.564  time: 0.2118(75.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:35] lb.utils.events INFO:  eta: 0:00:51  iteration: 753/1000  consumed samples: 12048  total_loss: 7.562  time: 0.2118(75.53)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:03:36] lb.utils.events INFO:  eta: 0:00:51  iteration: 754/1000  consumed samples: 12064  total_loss: 7.564  time: 0.2118(75.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:36] lb.utils.events INFO:  eta: 0:00:51  iteration: 755/1000  consumed samples: 12080  total_loss: 7.562  time: 0.2118(75.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:36] lb.utils.events INFO:  eta: 0:00:51  iteration: 756/1000  consumed samples: 12096  total_loss: 7.561  time: 0.2118(75.54)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:36] lb.utils.events INFO:  eta: 0:00:51  iteration: 757/1000  consumed samples: 12112  total_loss: 7.56  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:36] lb.utils.events INFO:  eta: 0:00:50  iteration: 758/1000  consumed samples: 12128  total_loss: 7.559  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:37] lb.utils.events INFO:  eta: 0:00:50  iteration: 759/1000  consumed samples: 12144  total_loss: 7.56  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:37] lb.utils.events INFO:  eta: 0:00:50  iteration: 760/1000  consumed samples: 12160  total_loss: 7.559  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:37] lb.utils.events INFO:  eta: 0:00:50  iteration: 761/1000  consumed samples: 12176  total_loss: 7.559  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:37] lb.utils.events INFO:  eta: 0:00:50  iteration: 762/1000  consumed samples: 12192  total_loss: 7.557  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:38] lb.utils.events INFO:  eta: 0:00:49  iteration: 763/1000  consumed samples: 12208  total_loss: 7.555  time: 0.2118(75.53)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:38] lb.utils.events INFO:  eta: 0:00:49  iteration: 764/1000  consumed samples: 12224  total_loss: 7.555  time: 0.2118(75.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:38] lb.utils.events INFO:  eta: 0:00:49  iteration: 765/1000  consumed samples: 12240  total_loss: 7.555  time: 0.2118(75.53)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:38] lb.utils.events INFO:  eta: 0:00:49  iteration: 766/1000  consumed samples: 12256  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:38] lb.utils.events INFO:  eta: 0:00:48  iteration: 767/1000  consumed samples: 12272  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:39] lb.utils.events INFO:  eta: 0:00:48  iteration: 768/1000  consumed samples: 12288  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:39] lb.utils.events INFO:  eta: 0:00:48  iteration: 769/1000  consumed samples: 12304  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:39] lb.utils.events INFO:  eta: 0:00:48  iteration: 770/1000  consumed samples: 12320  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:39] lb.utils.events INFO:  eta: 0:00:48  iteration: 771/1000  consumed samples: 12336  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:40] lb.utils.events INFO:  eta: 0:00:47  iteration: 772/1000  consumed samples: 12352  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:40] lb.utils.events INFO:  eta: 0:00:47  iteration: 773/1000  consumed samples: 12368  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:40] lb.utils.events INFO:  eta: 0:00:47  iteration: 774/1000  consumed samples: 12384  total_loss: 7.555  time: 0.2119(75.52)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:40] lb.utils.events INFO:  eta: 0:00:47  iteration: 775/1000  consumed samples: 12400  total_loss: 7.554  time: 0.2119(75.52)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:41] lb.utils.events INFO:  eta: 0:00:47  iteration: 776/1000  consumed samples: 12416  total_loss: 7.554  time: 0.2119(75.52)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:41] lb.utils.events INFO:  eta: 0:00:46  iteration: 777/1000  consumed samples: 12432  total_loss: 7.554  time: 0.2119(75.52)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:41] lb.utils.events INFO:  eta: 0:00:46  iteration: 778/1000  consumed samples: 12448  total_loss: 7.554  time: 0.2119(75.51)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:03:41] lb.utils.events INFO:  eta: 0:00:46  iteration: 779/1000  consumed samples: 12464  total_loss: 7.554  time: 0.2119(75.51)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:03:41] lb.utils.events INFO:  eta: 0:00:46  iteration: 780/1000  consumed samples: 12480  total_loss: 7.554  time: 0.2119(75.51)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:42] lb.utils.events INFO:  eta: 0:00:46  iteration: 781/1000  consumed samples: 12496  total_loss: 7.553  time: 0.2119(75.51)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:42] lb.utils.events INFO:  eta: 0:00:45  iteration: 782/1000  consumed samples: 12512  total_loss: 7.551  time: 0.2119(75.51)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:42] lb.utils.events INFO:  eta: 0:00:45  iteration: 783/1000  consumed samples: 12528  total_loss: 7.553  time: 0.2119(75.51)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:42] lb.utils.events INFO:  eta: 0:00:45  iteration: 784/1000  consumed samples: 12544  total_loss: 7.554  time: 0.2119(75.50)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:43] lb.utils.events INFO:  eta: 0:00:45  iteration: 785/1000  consumed samples: 12560  total_loss: 7.554  time: 0.2119(75.51)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:43] lb.utils.events INFO:  eta: 0:00:44  iteration: 786/1000  consumed samples: 12576  total_loss: 7.553  time: 0.2119(75.50)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:43] lb.utils.events INFO:  eta: 0:00:44  iteration: 787/1000  consumed samples: 12592  total_loss: 7.554  time: 0.2119(75.50)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:43] lb.utils.events INFO:  eta: 0:00:44  iteration: 788/1000  consumed samples: 12608  total_loss: 7.554  time: 0.2119(75.50)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:44] lb.utils.events INFO:  eta: 0:00:44  iteration: 789/1000  consumed samples: 12624  total_loss: 7.554  time: 0.2119(75.50)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:44] lb.utils.events INFO:  eta: 0:00:44  iteration: 790/1000  consumed samples: 12640  total_loss: 7.553  time: 0.2119(75.49)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:44] lb.utils.events INFO:  eta: 0:00:43  iteration: 791/1000  consumed samples: 12656  total_loss: 7.551  time: 0.2119(75.49)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:44] lb.utils.events INFO:  eta: 0:00:43  iteration: 792/1000  consumed samples: 12672  total_loss: 7.551  time: 0.2119(75.49)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:44] lb.utils.events INFO:  eta: 0:00:43  iteration: 793/1000  consumed samples: 12688  total_loss: 7.549  time: 0.2119(75.49)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:45] lb.utils.events INFO:  eta: 0:00:43  iteration: 794/1000  consumed samples: 12704  total_loss: 7.549  time: 0.2119(75.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:45] lb.utils.events INFO:  eta: 0:00:43  iteration: 795/1000  consumed samples: 12720  total_loss: 7.551  time: 0.2119(75.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:45] lb.utils.events INFO:  eta: 0:00:42  iteration: 796/1000  consumed samples: 12736  total_loss: 7.551  time: 0.2119(75.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:45] lb.utils.events INFO:  eta: 0:00:42  iteration: 797/1000  consumed samples: 12752  total_loss: 7.551  time: 0.2119(75.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:46] lb.utils.events INFO:  eta: 0:00:42  iteration: 798/1000  consumed samples: 12768  total_loss: 7.549  time: 0.2120(75.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:46] lb.utils.events INFO:  eta: 0:00:42  iteration: 799/1000  consumed samples: 12784  total_loss: 7.549  time: 0.2120(75.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:46] lb.utils.events INFO:  eta: 0:00:42  iteration: 800/1000  consumed samples: 12800  total_loss: 7.549  time: 0.2120(75.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:46] lb.utils.events INFO:  eta: 0:00:41  iteration: 801/1000  consumed samples: 12816  total_loss: 7.549  time: 0.2120(75.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:47] lb.utils.events INFO:  eta: 0:00:41  iteration: 802/1000  consumed samples: 12832  total_loss: 7.549  time: 0.2120(75.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:47] lb.utils.events INFO:  eta: 0:00:41  iteration: 803/1000  consumed samples: 12848  total_loss: 7.549  time: 0.2120(75.48)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:47] lb.utils.events INFO:  eta: 0:00:41  iteration: 804/1000  consumed samples: 12864  total_loss: 7.548  time: 0.2120(75.48)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:47] lb.utils.events INFO:  eta: 0:00:41  iteration: 805/1000  consumed samples: 12880  total_loss: 7.548  time: 0.2120(75.48)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:47] lb.utils.events INFO:  eta: 0:00:40  iteration: 806/1000  consumed samples: 12896  total_loss: 7.548  time: 0.2120(75.48)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:48] lb.utils.events INFO:  eta: 0:00:40  iteration: 807/1000  consumed samples: 12912  total_loss: 7.548  time: 0.2120(75.48)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:48] lb.utils.events INFO:  eta: 0:00:40  iteration: 808/1000  consumed samples: 12928  total_loss: 7.548  time: 0.2120(75.47)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:48] lb.utils.events INFO:  eta: 0:00:40  iteration: 809/1000  consumed samples: 12944  total_loss: 7.548  time: 0.2120(75.47)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:48] lb.utils.events INFO:  eta: 0:00:39  iteration: 810/1000  consumed samples: 12960  total_loss: 7.548  time: 0.2120(75.47)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:49] lb.utils.events INFO:  eta: 0:00:39  iteration: 811/1000  consumed samples: 12976  total_loss: 7.548  time: 0.2120(75.47)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:49] lb.utils.events INFO:  eta: 0:00:39  iteration: 812/1000  consumed samples: 12992  total_loss: 7.547  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:49] lb.utils.events INFO:  eta: 0:00:39  iteration: 813/1000  consumed samples: 13008  total_loss: 7.547  time: 0.2120(75.46)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:49] lb.utils.events INFO:  eta: 0:00:39  iteration: 814/1000  consumed samples: 13024  total_loss: 7.547  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:50] lb.utils.events INFO:  eta: 0:00:38  iteration: 815/1000  consumed samples: 13040  total_loss: 7.545  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:50] lb.utils.events INFO:  eta: 0:00:38  iteration: 816/1000  consumed samples: 13056  total_loss: 7.545  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:50] lb.utils.events INFO:  eta: 0:00:38  iteration: 817/1000  consumed samples: 13072  total_loss: 7.545  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:50] lb.utils.events INFO:  eta: 0:00:38  iteration: 818/1000  consumed samples: 13088  total_loss: 7.545  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:50] lb.utils.events INFO:  eta: 0:00:38  iteration: 819/1000  consumed samples: 13104  total_loss: 7.544  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:51] lb.utils.events INFO:  eta: 0:00:37  iteration: 820/1000  consumed samples: 13120  total_loss: 7.544  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:51] lb.utils.events INFO:  eta: 0:00:37  iteration: 821/1000  consumed samples: 13136  total_loss: 7.544  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:51] lb.utils.events INFO:  eta: 0:00:37  iteration: 822/1000  consumed samples: 13152  total_loss: 7.544  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:51] lb.utils.events INFO:  eta: 0:00:37  iteration: 823/1000  consumed samples: 13168  total_loss: 7.544  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:52] lb.utils.events INFO:  eta: 0:00:37  iteration: 824/1000  consumed samples: 13184  total_loss: 7.545  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:52] lb.utils.events INFO:  eta: 0:00:36  iteration: 825/1000  consumed samples: 13200  total_loss: 7.544  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:52] lb.utils.events INFO:  eta: 0:00:36  iteration: 826/1000  consumed samples: 13216  total_loss: 7.542  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:52] lb.utils.events INFO:  eta: 0:00:36  iteration: 827/1000  consumed samples: 13232  total_loss: 7.542  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:53] lb.utils.events INFO:  eta: 0:00:36  iteration: 828/1000  consumed samples: 13248  total_loss: 7.54  time: 0.2120(75.47)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:53] lb.utils.events INFO:  eta: 0:00:35  iteration: 829/1000  consumed samples: 13264  total_loss: 7.54  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:53] lb.utils.events INFO:  eta: 0:00:35  iteration: 830/1000  consumed samples: 13280  total_loss: 7.538  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:53] lb.utils.events INFO:  eta: 0:00:35  iteration: 831/1000  consumed samples: 13296  total_loss: 7.536  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:53] lb.utils.events INFO:  eta: 0:00:35  iteration: 832/1000  consumed samples: 13312  total_loss: 7.535  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:54] lb.utils.events INFO:  eta: 0:00:35  iteration: 833/1000  consumed samples: 13328  total_loss: 7.535  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:54] lb.utils.events INFO:  eta: 0:00:34  iteration: 834/1000  consumed samples: 13344  total_loss: 7.534  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:54] lb.utils.events INFO:  eta: 0:00:34  iteration: 835/1000  consumed samples: 13360  total_loss: 7.534  time: 0.2120(75.47)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:54] lb.utils.events INFO:  eta: 0:00:34  iteration: 836/1000  consumed samples: 13376  total_loss: 7.535  time: 0.2120(75.47)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:55] lb.utils.events INFO:  eta: 0:00:34  iteration: 837/1000  consumed samples: 13392  total_loss: 7.535  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:55] lb.utils.events INFO:  eta: 0:00:34  iteration: 838/1000  consumed samples: 13408  total_loss: 7.535  time: 0.2120(75.47)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:55] lb.utils.events INFO:  eta: 0:00:33  iteration: 839/1000  consumed samples: 13424  total_loss: 7.536  time: 0.2120(75.47)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:03:55] lb.utils.events INFO:  eta: 0:00:33  iteration: 840/1000  consumed samples: 13440  total_loss: 7.535  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:55] lb.utils.events INFO:  eta: 0:00:33  iteration: 841/1000  consumed samples: 13456  total_loss: 7.534  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:56] lb.utils.events INFO:  eta: 0:00:33  iteration: 842/1000  consumed samples: 13472  total_loss: 7.534  time: 0.2120(75.46)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:56] lb.utils.events INFO:  eta: 0:00:32  iteration: 843/1000  consumed samples: 13488  total_loss: 7.534  time: 0.2120(75.47)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:56] lb.utils.events INFO:  eta: 0:00:32  iteration: 844/1000  consumed samples: 13504  total_loss: 7.534  time: 0.2120(75.46)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:03:56] lb.utils.events INFO:  eta: 0:00:32  iteration: 845/1000  consumed samples: 13520  total_loss: 7.532  time: 0.2120(75.46)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:57] lb.utils.events INFO:  eta: 0:00:32  iteration: 846/1000  consumed samples: 13536  total_loss: 7.53  time: 0.2120(75.46)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:03:57] lb.utils.events INFO:  eta: 0:00:32  iteration: 847/1000  consumed samples: 13552  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:57] lb.utils.events INFO:  eta: 0:00:31  iteration: 848/1000  consumed samples: 13568  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:03:57] lb.utils.events INFO:  eta: 0:00:31  iteration: 849/1000  consumed samples: 13584  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:03:58] lb.utils.events INFO:  eta: 0:00:31  iteration: 850/1000  consumed samples: 13600  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:58] lb.utils.events INFO:  eta: 0:00:31  iteration: 851/1000  consumed samples: 13616  total_loss: 7.53  time: 0.2120(75.46)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:03:58] lb.utils.events INFO:  eta: 0:00:31  iteration: 852/1000  consumed samples: 13632  total_loss: 7.53  time: 0.2120(75.46)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:03:58] lb.utils.events INFO:  eta: 0:00:30  iteration: 853/1000  consumed samples: 13648  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:03:58] lb.utils.events INFO:  eta: 0:00:30  iteration: 854/1000  consumed samples: 13664  total_loss: 7.528  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:03:59] lb.utils.events INFO:  eta: 0:00:30  iteration: 855/1000  consumed samples: 13680  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:03:59] lb.utils.events INFO:  eta: 0:00:30  iteration: 856/1000  consumed samples: 13696  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:03:59] lb.utils.events INFO:  eta: 0:00:30  iteration: 857/1000  consumed samples: 13712  total_loss: 7.528  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:03:59] lb.utils.events INFO:  eta: 0:00:29  iteration: 858/1000  consumed samples: 13728  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:04:00] lb.utils.events INFO:  eta: 0:00:29  iteration: 859/1000  consumed samples: 13744  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:04:00] lb.utils.events INFO:  eta: 0:00:29  iteration: 860/1000  consumed samples: 13760  total_loss: 7.529  time: 0.2120(75.46)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:04:00] lb.utils.events INFO:  eta: 0:00:29  iteration: 861/1000  consumed samples: 13776  total_loss: 7.528  time: 0.2120(75.46)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:04:00] lb.utils.events INFO:  eta: 0:00:28  iteration: 862/1000  consumed samples: 13792  total_loss: 7.528  time: 0.2121(75.45)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:04:01] lb.utils.events INFO:  eta: 0:00:28  iteration: 863/1000  consumed samples: 13808  total_loss: 7.528  time: 0.2121(75.45)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:04:01] lb.utils.events INFO:  eta: 0:00:28  iteration: 864/1000  consumed samples: 13824  total_loss: 7.528  time: 0.2121(75.45)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:04:01] lb.utils.events INFO:  eta: 0:00:28  iteration: 865/1000  consumed samples: 13840  total_loss: 7.528  time: 0.2121(75.45)  data_time: 0.0032  lr: 9.90e-05  
[01/19 12:04:01] lb.utils.events INFO:  eta: 0:00:28  iteration: 866/1000  consumed samples: 13856  total_loss: 7.528  time: 0.2121(75.45)  data_time: 0.0033  lr: 9.90e-05  
[01/19 12:04:01] lb.utils.events INFO:  eta: 0:00:27  iteration: 867/1000  consumed samples: 13872  total_loss: 7.526  time: 0.2121(75.45)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:04:02] lb.utils.events INFO:  eta: 0:00:27  iteration: 868/1000  consumed samples: 13888  total_loss: 7.528  time: 0.2121(75.45)  data_time: 0.0034  lr: 9.90e-05  
[01/19 12:04:02] lb.utils.events INFO:  eta: 0:00:27  iteration: 869/1000  consumed samples: 13904  total_loss: 7.526  time: 0.2121(75.45)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:04:02] lb.utils.events INFO:  eta: 0:00:27  iteration: 870/1000  consumed samples: 13920  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0035  lr: 9.90e-05  
[01/19 12:04:02] lb.utils.events INFO:  eta: 0:00:27  iteration: 871/1000  consumed samples: 13936  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:04:03] lb.utils.events INFO:  eta: 0:00:26  iteration: 872/1000  consumed samples: 13952  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:03] lb.utils.events INFO:  eta: 0:00:26  iteration: 873/1000  consumed samples: 13968  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:03] lb.utils.events INFO:  eta: 0:00:26  iteration: 874/1000  consumed samples: 13984  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:03] lb.utils.events INFO:  eta: 0:00:26  iteration: 875/1000  consumed samples: 14000  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:04] lb.utils.events INFO:  eta: 0:00:26  iteration: 876/1000  consumed samples: 14016  total_loss: 7.523  time: 0.2121(75.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:04] lb.utils.events INFO:  eta: 0:00:25  iteration: 877/1000  consumed samples: 14032  total_loss: 7.521  time: 0.2121(75.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:04] lb.utils.events INFO:  eta: 0:00:25  iteration: 878/1000  consumed samples: 14048  total_loss: 7.52  time: 0.2121(75.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:04] lb.utils.events INFO:  eta: 0:00:25  iteration: 879/1000  consumed samples: 14064  total_loss: 7.518  time: 0.2121(75.45)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:04] lb.utils.events INFO:  eta: 0:00:25  iteration: 880/1000  consumed samples: 14080  total_loss: 7.518  time: 0.2121(75.45)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:05] lb.utils.events INFO:  eta: 0:00:24  iteration: 881/1000  consumed samples: 14096  total_loss: 7.516  time: 0.2121(75.44)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:05] lb.utils.events INFO:  eta: 0:00:24  iteration: 882/1000  consumed samples: 14112  total_loss: 7.516  time: 0.2121(75.44)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:05] lb.utils.events INFO:  eta: 0:00:24  iteration: 883/1000  consumed samples: 14128  total_loss: 7.514  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:05] lb.utils.events INFO:  eta: 0:00:24  iteration: 884/1000  consumed samples: 14144  total_loss: 7.512  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:06] lb.utils.events INFO:  eta: 0:00:24  iteration: 885/1000  consumed samples: 14160  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:06] lb.utils.events INFO:  eta: 0:00:23  iteration: 886/1000  consumed samples: 14176  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:06] lb.utils.events INFO:  eta: 0:00:23  iteration: 887/1000  consumed samples: 14192  total_loss: 7.512  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:06] lb.utils.events INFO:  eta: 0:00:23  iteration: 888/1000  consumed samples: 14208  total_loss: 7.512  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:06] lb.utils.events INFO:  eta: 0:00:23  iteration: 889/1000  consumed samples: 14224  total_loss: 7.512  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:07] lb.utils.events INFO:  eta: 0:00:23  iteration: 890/1000  consumed samples: 14240  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:07] lb.utils.events INFO:  eta: 0:00:22  iteration: 891/1000  consumed samples: 14256  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:07] lb.utils.events INFO:  eta: 0:00:22  iteration: 892/1000  consumed samples: 14272  total_loss: 7.507  time: 0.2121(75.44)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:07] lb.utils.events INFO:  eta: 0:00:22  iteration: 893/1000  consumed samples: 14288  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:08] lb.utils.events INFO:  eta: 0:00:22  iteration: 894/1000  consumed samples: 14304  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:08] lb.utils.events INFO:  eta: 0:00:22  iteration: 895/1000  consumed samples: 14320  total_loss: 7.512  time: 0.2121(75.44)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:08] lb.utils.events INFO:  eta: 0:00:21  iteration: 896/1000  consumed samples: 14336  total_loss: 7.509  time: 0.2121(75.44)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:08] lb.utils.events INFO:  eta: 0:00:21  iteration: 897/1000  consumed samples: 14352  total_loss: 7.509  time: 0.2121(75.43)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:09] lb.utils.events INFO:  eta: 0:00:21  iteration: 898/1000  consumed samples: 14368  total_loss: 7.509  time: 0.2121(75.43)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:09] lb.utils.events INFO:  eta: 0:00:21  iteration: 899/1000  consumed samples: 14384  total_loss: 7.509  time: 0.2121(75.43)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:09] lb.utils.events INFO:  eta: 0:00:20  iteration: 900/1000  consumed samples: 14400  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:09] lb.utils.events INFO:  eta: 0:00:20  iteration: 901/1000  consumed samples: 14416  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:09] lb.utils.events INFO:  eta: 0:00:20  iteration: 902/1000  consumed samples: 14432  total_loss: 7.505  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:10] lb.utils.events INFO:  eta: 0:00:20  iteration: 903/1000  consumed samples: 14448  total_loss: 7.505  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:10] lb.utils.events INFO:  eta: 0:00:20  iteration: 904/1000  consumed samples: 14464  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:10] lb.utils.events INFO:  eta: 0:00:19  iteration: 905/1000  consumed samples: 14480  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:10] lb.utils.events INFO:  eta: 0:00:19  iteration: 906/1000  consumed samples: 14496  total_loss: 7.509  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:11] lb.utils.events INFO:  eta: 0:00:19  iteration: 907/1000  consumed samples: 14512  total_loss: 7.509  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:11] lb.utils.events INFO:  eta: 0:00:19  iteration: 908/1000  consumed samples: 14528  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:11] lb.utils.events INFO:  eta: 0:00:19  iteration: 909/1000  consumed samples: 14544  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:11] lb.utils.events INFO:  eta: 0:00:18  iteration: 910/1000  consumed samples: 14560  total_loss: 7.505  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:12] lb.utils.events INFO:  eta: 0:00:18  iteration: 911/1000  consumed samples: 14576  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:12] lb.utils.events INFO:  eta: 0:00:18  iteration: 912/1000  consumed samples: 14592  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:12] lb.utils.events INFO:  eta: 0:00:18  iteration: 913/1000  consumed samples: 14608  total_loss: 7.507  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:12] lb.utils.events INFO:  eta: 0:00:18  iteration: 914/1000  consumed samples: 14624  total_loss: 7.505  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:12] lb.utils.events INFO:  eta: 0:00:17  iteration: 915/1000  consumed samples: 14640  total_loss: 7.502  time: 0.2121(75.43)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:04:13] lb.utils.events INFO:  eta: 0:00:17  iteration: 916/1000  consumed samples: 14656  total_loss: 7.5  time: 0.2121(75.43)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:04:13] lb.utils.events INFO:  eta: 0:00:17  iteration: 917/1000  consumed samples: 14672  total_loss: 7.502  time: 0.2121(75.43)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:04:13] lb.utils.events INFO:  eta: 0:00:17  iteration: 918/1000  consumed samples: 14688  total_loss: 7.502  time: 0.2121(75.43)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:04:13] lb.utils.events INFO:  eta: 0:00:16  iteration: 919/1000  consumed samples: 14704  total_loss: 7.505  time: 0.2121(75.43)  data_time: 0.0041  lr: 9.90e-05  
[01/19 12:04:14] lb.utils.events INFO:  eta: 0:00:16  iteration: 920/1000  consumed samples: 14720  total_loss: 7.502  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:14] lb.utils.events INFO:  eta: 0:00:16  iteration: 921/1000  consumed samples: 14736  total_loss: 7.5  time: 0.2121(75.43)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:14] lb.utils.events INFO:  eta: 0:00:16  iteration: 922/1000  consumed samples: 14752  total_loss: 7.5  time: 0.2121(75.42)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:14] lb.utils.events INFO:  eta: 0:00:16  iteration: 923/1000  consumed samples: 14768  total_loss: 7.499  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:15] lb.utils.events INFO:  eta: 0:00:15  iteration: 924/1000  consumed samples: 14784  total_loss: 7.499  time: 0.2121(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:15] lb.utils.events INFO:  eta: 0:00:15  iteration: 925/1000  consumed samples: 14800  total_loss: 7.497  time: 0.2121(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:15] lb.utils.events INFO:  eta: 0:00:15  iteration: 926/1000  consumed samples: 14816  total_loss: 7.496  time: 0.2121(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:15] lb.utils.events INFO:  eta: 0:00:15  iteration: 927/1000  consumed samples: 14832  total_loss: 7.493  time: 0.2121(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:15] lb.utils.events INFO:  eta: 0:00:15  iteration: 928/1000  consumed samples: 14848  total_loss: 7.493  time: 0.2122(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:16] lb.utils.events INFO:  eta: 0:00:14  iteration: 929/1000  consumed samples: 14864  total_loss: 7.493  time: 0.2121(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:16] lb.utils.events INFO:  eta: 0:00:14  iteration: 930/1000  consumed samples: 14880  total_loss: 7.491  time: 0.2122(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:16] lb.utils.events INFO:  eta: 0:00:14  iteration: 931/1000  consumed samples: 14896  total_loss: 7.491  time: 0.2122(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:16] lb.utils.events INFO:  eta: 0:00:14  iteration: 932/1000  consumed samples: 14912  total_loss: 7.49  time: 0.2121(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:17] lb.utils.events INFO:  eta: 0:00:13  iteration: 933/1000  consumed samples: 14928  total_loss: 7.49  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:17] lb.utils.events INFO:  eta: 0:00:13  iteration: 934/1000  consumed samples: 14944  total_loss: 7.488  time: 0.2121(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:17] lb.utils.events INFO:  eta: 0:00:13  iteration: 935/1000  consumed samples: 14960  total_loss: 7.488  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:17] lb.utils.events INFO:  eta: 0:00:13  iteration: 936/1000  consumed samples: 14976  total_loss: 7.486  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:18] lb.utils.events INFO:  eta: 0:00:13  iteration: 937/1000  consumed samples: 14992  total_loss: 7.486  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:18] lb.utils.events INFO:  eta: 0:00:12  iteration: 938/1000  consumed samples: 15008  total_loss: 7.486  time: 0.2122(75.42)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:18] lb.utils.events INFO:  eta: 0:00:12  iteration: 939/1000  consumed samples: 15024  total_loss: 7.484  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:18] lb.utils.events INFO:  eta: 0:00:12  iteration: 940/1000  consumed samples: 15040  total_loss: 7.484  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:18] lb.utils.events INFO:  eta: 0:00:12  iteration: 941/1000  consumed samples: 15056  total_loss: 7.484  time: 0.2122(75.42)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:19] lb.utils.events INFO:  eta: 0:00:12  iteration: 942/1000  consumed samples: 15072  total_loss: 7.479  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:19] lb.utils.events INFO:  eta: 0:00:11  iteration: 943/1000  consumed samples: 15088  total_loss: 7.479  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:19] lb.utils.events INFO:  eta: 0:00:11  iteration: 944/1000  consumed samples: 15104  total_loss: 7.475  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:19] lb.utils.events INFO:  eta: 0:00:11  iteration: 945/1000  consumed samples: 15120  total_loss: 7.473  time: 0.2122(75.41)  data_time: 0.0036  lr: 9.90e-05  
[01/19 12:04:20] lb.utils.events INFO:  eta: 0:00:11  iteration: 946/1000  consumed samples: 15136  total_loss: 7.471  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:20] lb.utils.events INFO:  eta: 0:00:11  iteration: 947/1000  consumed samples: 15152  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:20] lb.utils.events INFO:  eta: 0:00:10  iteration: 948/1000  consumed samples: 15168  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:20] lb.utils.events INFO:  eta: 0:00:10  iteration: 949/1000  consumed samples: 15184  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:20] lb.utils.events INFO:  eta: 0:00:10  iteration: 950/1000  consumed samples: 15200  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:21] lb.utils.events INFO:  eta: 0:00:10  iteration: 951/1000  consumed samples: 15216  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:21] lb.utils.events INFO:  eta: 0:00:09  iteration: 952/1000  consumed samples: 15232  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:21] lb.utils.events INFO:  eta: 0:00:09  iteration: 953/1000  consumed samples: 15248  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:21] lb.utils.events INFO:  eta: 0:00:09  iteration: 954/1000  consumed samples: 15264  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:22] lb.utils.events INFO:  eta: 0:00:09  iteration: 955/1000  consumed samples: 15280  total_loss: 7.471  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:22] lb.utils.events INFO:  eta: 0:00:09  iteration: 956/1000  consumed samples: 15296  total_loss: 7.473  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:22] lb.utils.events INFO:  eta: 0:00:08  iteration: 957/1000  consumed samples: 15312  total_loss: 7.473  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:22] lb.utils.events INFO:  eta: 0:00:08  iteration: 958/1000  consumed samples: 15328  total_loss: 7.471  time: 0.2122(75.41)  data_time: 0.0037  lr: 9.90e-05  
[01/19 12:04:23] lb.utils.events INFO:  eta: 0:00:08  iteration: 959/1000  consumed samples: 15344  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:23] lb.utils.events INFO:  eta: 0:00:08  iteration: 960/1000  consumed samples: 15360  total_loss: 7.471  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:23] lb.utils.events INFO:  eta: 0:00:08  iteration: 961/1000  consumed samples: 15376  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:23] lb.utils.events INFO:  eta: 0:00:07  iteration: 962/1000  consumed samples: 15392  total_loss: 7.47  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:23] lb.utils.events INFO:  eta: 0:00:07  iteration: 963/1000  consumed samples: 15408  total_loss: 7.47  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:24] lb.utils.events INFO:  eta: 0:00:07  iteration: 964/1000  consumed samples: 15424  total_loss: 7.471  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:24] lb.utils.events INFO:  eta: 0:00:07  iteration: 965/1000  consumed samples: 15440  total_loss: 7.473  time: 0.2122(75.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:24] lb.utils.events INFO:  eta: 0:00:07  iteration: 966/1000  consumed samples: 15456  total_loss: 7.471  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:24] lb.utils.events INFO:  eta: 0:00:06  iteration: 967/1000  consumed samples: 15472  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:25] lb.utils.events INFO:  eta: 0:00:06  iteration: 968/1000  consumed samples: 15488  total_loss: 7.47  time: 0.2122(75.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:25] lb.utils.events INFO:  eta: 0:00:06  iteration: 969/1000  consumed samples: 15504  total_loss: 7.468  time: 0.2122(75.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:25] lb.utils.events INFO:  eta: 0:00:06  iteration: 970/1000  consumed samples: 15520  total_loss: 7.465  time: 0.2122(75.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:25] lb.utils.events INFO:  eta: 0:00:05  iteration: 971/1000  consumed samples: 15536  total_loss: 7.468  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 972/1000  consumed samples: 15552  total_loss: 7.468  time: 0.2122(75.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 973/1000  consumed samples: 15568  total_loss: 7.468  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 974/1000  consumed samples: 15584  total_loss: 7.468  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 975/1000  consumed samples: 15600  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:26] lb.utils.events INFO:  eta: 0:00:04  iteration: 976/1000  consumed samples: 15616  total_loss: 7.468  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:27] lb.utils.events INFO:  eta: 0:00:04  iteration: 977/1000  consumed samples: 15632  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:27] lb.utils.events INFO:  eta: 0:00:04  iteration: 978/1000  consumed samples: 15648  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:27] lb.utils.events INFO:  eta: 0:00:04  iteration: 979/1000  consumed samples: 15664  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:27] lb.utils.events INFO:  eta: 0:00:04  iteration: 980/1000  consumed samples: 15680  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:28] lb.utils.events INFO:  eta: 0:00:03  iteration: 981/1000  consumed samples: 15696  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:28] lb.utils.events INFO:  eta: 0:00:03  iteration: 982/1000  consumed samples: 15712  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:28] lb.utils.events INFO:  eta: 0:00:03  iteration: 983/1000  consumed samples: 15728  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:28] lb.utils.events INFO:  eta: 0:00:03  iteration: 984/1000  consumed samples: 15744  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:29] lb.utils.events INFO:  eta: 0:00:02  iteration: 985/1000  consumed samples: 15760  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:29] lb.utils.events INFO:  eta: 0:00:02  iteration: 986/1000  consumed samples: 15776  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:29] lb.utils.events INFO:  eta: 0:00:02  iteration: 987/1000  consumed samples: 15792  total_loss: 7.465  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:29] lb.utils.events INFO:  eta: 0:00:02  iteration: 988/1000  consumed samples: 15808  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:29] lb.utils.events INFO:  eta: 0:00:02  iteration: 989/1000  consumed samples: 15824  total_loss: 7.462  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:30] lb.utils.events INFO:  eta: 0:00:01  iteration: 990/1000  consumed samples: 15840  total_loss: 7.462  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:30] lb.utils.events INFO:  eta: 0:00:01  iteration: 991/1000  consumed samples: 15856  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:30] lb.utils.events INFO:  eta: 0:00:01  iteration: 992/1000  consumed samples: 15872  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0038  lr: 9.90e-05  
[01/19 12:04:30] lb.utils.events INFO:  eta: 0:00:01  iteration: 993/1000  consumed samples: 15888  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:31] lb.utils.events INFO:  eta: 0:00:01  iteration: 994/1000  consumed samples: 15904  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0039  lr: 9.90e-05  
[01/19 12:04:31] lb.utils.events INFO:  eta: 0:00:00  iteration: 995/1000  consumed samples: 15920  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:31] lb.utils.events INFO:  eta: 0:00:00  iteration: 996/1000  consumed samples: 15936  total_loss: 7.463  time: 0.2122(75.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:31] lb.utils.events INFO:  eta: 0:00:00  iteration: 997/1000  consumed samples: 15952  total_loss: 7.462  time: 0.2122(75.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:32] lb.utils.events INFO:  eta: 0:00:00  iteration: 998/1000  consumed samples: 15968  total_loss: 7.461  time: 0.2122(75.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:32] lb.utils.events INFO:  eta: 0:00:00  iteration: 999/1000  consumed samples: 15984  total_loss: 7.46  time: 0.2122(75.40)  data_time: 0.0040  lr: 9.90e-05  
[01/19 12:04:32] lb.utils.checkpoint INFO: Saving checkpoint to ./demo_output/test_config/model_final
[01/19 12:04:33] lb.trainer.hooks INFO: Overall training speed: 998 iterations in 0:03:31 (0.2122 s / it)
[01/19 12:04:33] lb.trainer.hooks INFO: Total training time: 0:03:48 (0:00:16 on hooks)
[01/19 14:42:18] libai INFO: Rank of current process: 0. World size: 1
[01/19 14:42:18] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 14:42:18] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 14:42:18] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 14:42:18] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 14:42:19] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 14:42:23] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 14:42:23] libai INFO: Loadding megatron weight
[01/19 14:42:23] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 14:42:24] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 14:42:24] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 14:42:24] lb.data.build INFO:     train:      16000
[01/19 14:42:24] lb.data.build INFO:     validation: 160000
[01/19 14:42:24] lb.data.build INFO:     test:       160000
[01/19 14:42:24] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 14:42:24] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     reading document index...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 14:42:24] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 14:42:24] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.941342 seconds
[01/19 14:42:24] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 14:42:24] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 14:42:24] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 14:42:24] lb.data.dataset_utils INFO:  > dataset split:
[01/19 14:42:24] lb.data.dataset_utils INFO:     train:
[01/19 14:42:24] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 14:42:24] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 14:42:24] lb.data.dataset_utils INFO:     validation:
[01/19 14:42:24] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 14:42:24] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 14:42:24] lb.data.dataset_utils INFO:     test:
[01/19 14:42:24] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 14:42:24] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 14:42:25] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 14:42:25] lb.data.dataset_utils INFO:     loaded indexed file in 0.028 seconds
[01/19 14:42:25] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 14:42:25] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 14:42:25] lb.data.dataset_utils INFO:     loaded indexed file in 0.008 seconds
[01/19 14:42:25] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 14:42:25] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 14:42:25] lb.data.dataset_utils INFO:     loaded indexed file in 0.009 seconds
[01/19 14:42:25] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 14:42:25] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 14:42:26] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 14:42:36] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.0683  lr: 0.00e+00  
[01/19 14:42:37] lb.utils.events INFO:  eta: 0:04:05  iteration: 2/1000  consumed samples: 32  total_loss: 8.65  data_time: 0.0406  lr: 1.00e-06  
[01/19 14:42:37] lb.utils.events INFO:  eta: 0:03:28  iteration: 3/1000  consumed samples: 48  total_loss: 8.663  time: 0.2090(76.56)  data_time: 0.0304  lr: 2.00e-06  
[01/19 14:42:37] lb.utils.events INFO:  eta: 0:03:30  iteration: 4/1000  consumed samples: 64  total_loss: 8.65  time: 0.2111(75.80)  data_time: 0.0268  lr: 3.00e-06  
[01/19 14:42:37] lb.utils.events INFO:  eta: 0:03:27  iteration: 5/1000  consumed samples: 80  total_loss: 8.663  time: 0.2074(77.14)  data_time: 0.0295  lr: 4.00e-06  
[01/19 14:42:38] lb.utils.events INFO:  eta: 0:03:29  iteration: 6/1000  consumed samples: 96  total_loss: 8.68  time: 0.2087(76.65)  data_time: 0.0286  lr: 5.00e-06  
[01/19 14:42:38] lb.utils.events INFO:  eta: 0:03:27  iteration: 7/1000  consumed samples: 112  total_loss: 8.663  time: 0.2084(76.77)  data_time: 0.0255  lr: 6.00e-06  
[01/19 14:42:38] lb.utils.events INFO:  eta: 0:03:28  iteration: 8/1000  consumed samples: 128  total_loss: 8.658  time: 0.2094(76.42)  data_time: 0.0226  lr: 7.00e-06  
[01/19 14:42:38] lb.utils.events INFO:  eta: 0:03:26  iteration: 9/1000  consumed samples: 144  total_loss: 8.652  time: 0.2090(76.57)  data_time: 0.0204  lr: 8.00e-06  
[01/19 14:42:38] lb.utils.events INFO:  eta: 0:03:25  iteration: 10/1000  consumed samples: 160  total_loss: 8.657  time: 0.2084(76.79)  data_time: 0.0194  lr: 9.00e-06  
[01/19 14:42:39] lb.utils.events INFO:  eta: 0:03:24  iteration: 11/1000  consumed samples: 176  total_loss: 8.662  time: 0.2078(76.98)  data_time: 0.0232  lr: 1.00e-05  
[01/19 14:42:39] lb.utils.events INFO:  eta: 0:03:25  iteration: 12/1000  consumed samples: 192  total_loss: 8.657  time: 0.2080(76.91)  data_time: 0.0218  lr: 1.10e-05  
[01/19 14:42:39] lb.utils.events INFO:  eta: 0:03:26  iteration: 13/1000  consumed samples: 208  total_loss: 8.652  time: 0.2084(76.76)  data_time: 0.0206  lr: 1.20e-05  
[01/19 14:42:39] lb.utils.events INFO:  eta: 0:03:26  iteration: 14/1000  consumed samples: 224  total_loss: 8.657  time: 0.2089(76.61)  data_time: 0.0194  lr: 1.30e-05  
[01/19 14:42:40] lb.utils.events INFO:  eta: 0:03:25  iteration: 15/1000  consumed samples: 240  total_loss: 8.662  time: 0.2088(76.64)  data_time: 0.0186  lr: 1.40e-05  
[01/19 14:42:40] lb.utils.events INFO:  eta: 0:03:25  iteration: 16/1000  consumed samples: 256  total_loss: 8.657  time: 0.2091(76.51)  data_time: 0.0178  lr: 1.50e-05  
[01/19 14:42:40] lb.utils.events INFO:  eta: 0:03:25  iteration: 17/1000  consumed samples: 272  total_loss: 8.652  time: 0.2091(76.52)  data_time: 0.0175  lr: 1.60e-05  
[01/19 14:42:40] lb.utils.events INFO:  eta: 0:03:25  iteration: 18/1000  consumed samples: 288  total_loss: 8.646  time: 0.2092(76.49)  data_time: 0.0173  lr: 1.70e-05  
[01/19 14:42:40] lb.utils.events INFO:  eta: 0:03:24  iteration: 19/1000  consumed samples: 304  total_loss: 8.652  time: 0.2089(76.60)  data_time: 0.0166  lr: 1.80e-05  
[01/19 14:42:41] lb.utils.events INFO:  eta: 0:03:24  iteration: 20/1000  consumed samples: 320  total_loss: 8.657  time: 0.2090(76.55)  data_time: 0.0180  lr: 1.90e-05  
[01/19 14:42:41] lb.utils.events INFO:  eta: 0:03:24  iteration: 21/1000  consumed samples: 336  total_loss: 8.652  time: 0.2088(76.64)  data_time: 0.0149  lr: 2.00e-05  
[01/19 14:42:41] lb.utils.events INFO:  eta: 0:03:24  iteration: 22/1000  consumed samples: 352  total_loss: 8.646  time: 0.2091(76.53)  data_time: 0.0160  lr: 2.10e-05  
[01/19 14:42:41] lb.utils.events INFO:  eta: 0:03:23  iteration: 23/1000  consumed samples: 368  total_loss: 8.64  time: 0.2088(76.62)  data_time: 0.0156  lr: 2.20e-05  
[01/19 14:42:42] lb.utils.events INFO:  eta: 0:03:23  iteration: 24/1000  consumed samples: 384  total_loss: 8.646  time: 0.2076(77.05)  data_time: 0.0150  lr: 2.30e-05  
[01/19 14:42:42] lb.utils.events INFO:  eta: 0:03:23  iteration: 25/1000  consumed samples: 400  total_loss: 8.652  time: 0.2079(76.95)  data_time: 0.0131  lr: 2.40e-05  
[01/19 14:42:42] lb.utils.events INFO:  eta: 0:03:23  iteration: 26/1000  consumed samples: 416  total_loss: 8.646  time: 0.2079(76.95)  data_time: 0.0122  lr: 2.50e-05  
[01/19 14:42:42] lb.utils.events INFO:  eta: 0:03:23  iteration: 27/1000  consumed samples: 432  total_loss: 8.64  time: 0.2082(76.85)  data_time: 0.0122  lr: 2.60e-05  
[01/19 14:42:42] lb.utils.events INFO:  eta: 0:03:23  iteration: 28/1000  consumed samples: 448  total_loss: 8.638  time: 0.2084(76.76)  data_time: 0.0125  lr: 2.70e-05  
[01/19 14:42:43] lb.utils.events INFO:  eta: 0:03:23  iteration: 29/1000  consumed samples: 464  total_loss: 8.637  time: 0.2086(76.70)  data_time: 0.0128  lr: 2.80e-05  
[01/19 14:42:43] lb.utils.events INFO:  eta: 0:03:23  iteration: 30/1000  consumed samples: 480  total_loss: 8.634  time: 0.2087(76.66)  data_time: 0.0128  lr: 2.90e-05  
[01/19 14:42:43] lb.utils.events INFO:  eta: 0:03:23  iteration: 31/1000  consumed samples: 496  total_loss: 8.632  time: 0.2087(76.66)  data_time: 0.0101  lr: 3.00e-05  
[01/19 14:42:43] lb.utils.events INFO:  eta: 0:03:22  iteration: 32/1000  consumed samples: 512  total_loss: 8.631  time: 0.2089(76.59)  data_time: 0.0101  lr: 3.10e-05  
[01/19 14:42:43] lb.utils.events INFO:  eta: 0:03:22  iteration: 33/1000  consumed samples: 528  total_loss: 8.631  time: 0.2089(76.60)  data_time: 0.0100  lr: 3.20e-05  
[01/19 14:42:44] lb.utils.events INFO:  eta: 0:03:22  iteration: 34/1000  consumed samples: 544  total_loss: 8.628  time: 0.2089(76.60)  data_time: 0.0101  lr: 3.30e-05  
[01/19 14:42:44] lb.utils.events INFO:  eta: 0:03:22  iteration: 35/1000  consumed samples: 560  total_loss: 8.625  time: 0.2091(76.53)  data_time: 0.0107  lr: 3.40e-05  
[01/19 14:42:44] lb.utils.events INFO:  eta: 0:03:22  iteration: 36/1000  consumed samples: 576  total_loss: 8.624  time: 0.2093(76.46)  data_time: 0.0105  lr: 3.50e-05  
[01/19 14:42:44] lb.utils.events INFO:  eta: 0:03:21  iteration: 37/1000  consumed samples: 592  total_loss: 8.623  time: 0.2092(76.47)  data_time: 0.0109  lr: 3.60e-05  
[01/19 14:42:45] lb.utils.events INFO:  eta: 0:03:21  iteration: 38/1000  consumed samples: 608  total_loss: 8.621  time: 0.2094(76.41)  data_time: 0.0103  lr: 3.70e-05  
[01/19 14:42:45] lb.utils.events INFO:  eta: 0:03:21  iteration: 39/1000  consumed samples: 624  total_loss: 8.618  time: 0.2096(76.34)  data_time: 0.0102  lr: 3.80e-05  
[01/19 14:42:45] lb.utils.events INFO:  eta: 0:03:21  iteration: 40/1000  consumed samples: 640  total_loss: 8.615  time: 0.2097(76.30)  data_time: 0.0082  lr: 3.90e-05  
[01/19 14:42:45] lb.utils.events INFO:  eta: 0:03:21  iteration: 41/1000  consumed samples: 656  total_loss: 8.612  time: 0.2097(76.31)  data_time: 0.0083  lr: 4.00e-05  
[01/19 14:42:45] lb.utils.events INFO:  eta: 0:03:21  iteration: 42/1000  consumed samples: 672  total_loss: 8.611  time: 0.2098(76.26)  data_time: 0.0124  lr: 4.10e-05  
[01/19 14:42:46] lb.utils.events INFO:  eta: 0:03:20  iteration: 43/1000  consumed samples: 688  total_loss: 8.61  time: 0.2096(76.35)  data_time: 0.0124  lr: 4.20e-05  
[01/19 14:42:46] lb.utils.events INFO:  eta: 0:03:21  iteration: 44/1000  consumed samples: 704  total_loss: 8.609  time: 0.2097(76.30)  data_time: 0.0123  lr: 4.30e-05  
[01/19 14:42:46] lb.utils.events INFO:  eta: 0:03:20  iteration: 45/1000  consumed samples: 720  total_loss: 8.607  time: 0.2096(76.32)  data_time: 0.0125  lr: 4.40e-05  
[01/19 14:42:46] lb.utils.events INFO:  eta: 0:03:20  iteration: 46/1000  consumed samples: 736  total_loss: 8.605  time: 0.2097(76.31)  data_time: 0.0129  lr: 4.50e-05  
[01/19 14:42:47] lb.utils.events INFO:  eta: 0:03:20  iteration: 47/1000  consumed samples: 752  total_loss: 8.603  time: 0.2097(76.32)  data_time: 0.0138  lr: 4.60e-05  
[01/19 14:42:47] lb.utils.events INFO:  eta: 0:03:20  iteration: 48/1000  consumed samples: 768  total_loss: 8.602  time: 0.2105(76.02)  data_time: 0.0212  lr: 4.70e-05  
[01/19 14:42:47] lb.utils.events INFO:  eta: 0:03:19  iteration: 49/1000  consumed samples: 784  total_loss: 8.602  time: 0.2100(76.21)  data_time: 0.0209  lr: 4.80e-05  
[01/19 14:42:47] lb.utils.events INFO:  eta: 0:03:19  iteration: 50/1000  consumed samples: 800  total_loss: 8.593  time: 0.2100(76.19)  data_time: 0.0205  lr: 4.90e-05  
[01/19 14:42:48] lb.utils.events INFO:  eta: 0:03:19  iteration: 51/1000  consumed samples: 816  total_loss: 8.585  time: 0.2099(76.23)  data_time: 0.0203  lr: 5.00e-05  
[01/19 14:42:48] lb.utils.events INFO:  eta: 0:03:18  iteration: 52/1000  consumed samples: 832  total_loss: 8.579  time: 0.2097(76.28)  data_time: 0.0204  lr: 5.10e-05  
[01/19 14:42:48] lb.utils.events INFO:  eta: 0:03:18  iteration: 53/1000  consumed samples: 848  total_loss: 8.572  time: 0.2097(76.30)  data_time: 0.0225  lr: 5.20e-05  
[01/19 14:42:48] lb.utils.events INFO:  eta: 0:03:18  iteration: 54/1000  consumed samples: 864  total_loss: 8.571  time: 0.2098(76.26)  data_time: 0.0243  lr: 5.30e-05  
[01/19 14:42:48] lb.utils.events INFO:  eta: 0:03:17  iteration: 55/1000  consumed samples: 880  total_loss: 8.571  time: 0.2097(76.31)  data_time: 0.0247  lr: 5.40e-05  
[01/19 14:42:49] lb.utils.events INFO:  eta: 0:03:17  iteration: 56/1000  consumed samples: 896  total_loss: 8.569  time: 0.2097(76.29)  data_time: 0.0247  lr: 5.50e-05  
[01/19 14:42:49] lb.utils.events INFO:  eta: 0:03:17  iteration: 57/1000  consumed samples: 912  total_loss: 8.568  time: 0.2097(76.30)  data_time: 0.0240  lr: 5.60e-05  
[01/19 14:42:49] lb.utils.events INFO:  eta: 0:03:17  iteration: 58/1000  consumed samples: 928  total_loss: 8.559  time: 0.2097(76.31)  data_time: 0.0240  lr: 5.70e-05  
[01/19 14:42:49] lb.utils.events INFO:  eta: 0:03:16  iteration: 59/1000  consumed samples: 944  total_loss: 8.55  time: 0.2096(76.32)  data_time: 0.0250  lr: 5.80e-05  
[01/19 14:42:50] lb.utils.events INFO:  eta: 0:03:16  iteration: 60/1000  consumed samples: 960  total_loss: 8.55  time: 0.2098(76.28)  data_time: 0.0251  lr: 5.90e-05  
[01/19 14:42:50] lb.utils.events INFO:  eta: 0:03:16  iteration: 61/1000  consumed samples: 976  total_loss: 8.55  time: 0.2098(76.28)  data_time: 0.0266  lr: 6.00e-05  
[01/19 14:42:50] lb.utils.events INFO:  eta: 0:03:16  iteration: 62/1000  consumed samples: 992  total_loss: 8.541  time: 0.2098(76.26)  data_time: 0.0209  lr: 6.10e-05  
[01/19 14:42:50] lb.utils.events INFO:  eta: 0:03:16  iteration: 63/1000  consumed samples: 1008  total_loss: 8.533  time: 0.2098(76.27)  data_time: 0.0209  lr: 6.20e-05  
[01/19 14:42:50] lb.utils.events INFO:  eta: 0:03:15  iteration: 64/1000  consumed samples: 1024  total_loss: 8.533  time: 0.2097(76.28)  data_time: 0.0210  lr: 6.30e-05  
[01/19 14:42:51] lb.utils.events INFO:  eta: 0:03:15  iteration: 65/1000  consumed samples: 1040  total_loss: 8.533  time: 0.2097(76.28)  data_time: 0.0217  lr: 6.40e-05  
[01/19 14:42:51] lb.utils.events INFO:  eta: 0:03:15  iteration: 66/1000  consumed samples: 1056  total_loss: 8.529  time: 0.2097(76.29)  data_time: 0.0212  lr: 6.50e-05  
[01/19 14:42:51] lb.utils.events INFO:  eta: 0:03:14  iteration: 67/1000  consumed samples: 1072  total_loss: 8.526  time: 0.2097(76.31)  data_time: 0.0201  lr: 6.60e-05  
[01/19 14:42:51] lb.utils.events INFO:  eta: 0:03:14  iteration: 68/1000  consumed samples: 1088  total_loss: 8.523  time: 0.2097(76.30)  data_time: 0.0126  lr: 6.70e-05  
[01/19 14:42:52] lb.utils.events INFO:  eta: 0:03:14  iteration: 69/1000  consumed samples: 1104  total_loss: 8.519  time: 0.2096(76.32)  data_time: 0.0132  lr: 6.80e-05  
[01/19 14:42:52] lb.utils.events INFO:  eta: 0:03:14  iteration: 70/1000  consumed samples: 1120  total_loss: 8.517  time: 0.2096(76.33)  data_time: 0.0145  lr: 6.90e-05  
[01/19 14:42:52] lb.utils.events INFO:  eta: 0:03:13  iteration: 71/1000  consumed samples: 1136  total_loss: 8.515  time: 0.2096(76.35)  data_time: 0.0146  lr: 7.00e-05  
[01/19 14:42:52] lb.utils.events INFO:  eta: 0:03:13  iteration: 72/1000  consumed samples: 1152  total_loss: 8.514  time: 0.2095(76.36)  data_time: 0.0143  lr: 7.10e-05  
[01/19 14:42:52] lb.utils.events INFO:  eta: 0:03:13  iteration: 73/1000  consumed samples: 1168  total_loss: 8.513  time: 0.2095(76.36)  data_time: 0.0130  lr: 7.20e-05  
[01/19 14:42:53] lb.utils.events INFO:  eta: 0:03:13  iteration: 74/1000  consumed samples: 1184  total_loss: 8.511  time: 0.2095(76.37)  data_time: 0.0114  lr: 7.30e-05  
[01/19 14:42:53] lb.utils.events INFO:  eta: 0:03:13  iteration: 75/1000  consumed samples: 1200  total_loss: 8.509  time: 0.2095(76.36)  data_time: 0.0102  lr: 7.40e-05  
[01/19 14:42:53] lb.utils.events INFO:  eta: 0:03:12  iteration: 76/1000  consumed samples: 1216  total_loss: 8.505  time: 0.2096(76.33)  data_time: 0.0105  lr: 7.50e-05  
[01/19 14:42:53] lb.utils.events INFO:  eta: 0:03:12  iteration: 77/1000  consumed samples: 1232  total_loss: 8.502  time: 0.2096(76.33)  data_time: 0.0112  lr: 7.60e-05  
[01/19 14:42:54] lb.utils.events INFO:  eta: 0:03:12  iteration: 78/1000  consumed samples: 1248  total_loss: 8.488  time: 0.2097(76.29)  data_time: 0.0112  lr: 7.70e-05  
[01/19 14:42:54] lb.utils.events INFO:  eta: 0:03:12  iteration: 79/1000  consumed samples: 1264  total_loss: 8.474  time: 0.2097(76.31)  data_time: 0.0108  lr: 7.80e-05  
[01/19 14:42:54] lb.utils.events INFO:  eta: 0:03:12  iteration: 80/1000  consumed samples: 1280  total_loss: 8.466  time: 0.2098(76.28)  data_time: 0.0107  lr: 7.90e-05  
[01/19 14:42:54] lb.utils.events INFO:  eta: 0:03:12  iteration: 81/1000  consumed samples: 1296  total_loss: 8.458  time: 0.2098(76.28)  data_time: 0.0090  lr: 8.00e-05  
[01/19 14:42:54] lb.utils.events INFO:  eta: 0:03:11  iteration: 82/1000  consumed samples: 1312  total_loss: 8.455  time: 0.2097(76.29)  data_time: 0.0090  lr: 8.10e-05  
[01/19 14:42:55] lb.utils.events INFO:  eta: 0:03:11  iteration: 83/1000  consumed samples: 1328  total_loss: 8.451  time: 0.2097(76.30)  data_time: 0.0100  lr: 8.20e-05  
[01/19 14:42:55] lb.utils.events INFO:  eta: 0:03:11  iteration: 84/1000  consumed samples: 1344  total_loss: 8.45  time: 0.2097(76.31)  data_time: 0.0098  lr: 8.30e-05  
[01/19 14:42:55] lb.utils.events INFO:  eta: 0:03:11  iteration: 85/1000  consumed samples: 1360  total_loss: 8.448  time: 0.2096(76.32)  data_time: 0.0094  lr: 8.40e-05  
[01/19 14:42:55] lb.utils.events INFO:  eta: 0:03:10  iteration: 86/1000  consumed samples: 1376  total_loss: 8.445  time: 0.2097(76.31)  data_time: 0.0097  lr: 8.50e-05  
[01/19 14:42:56] lb.utils.events INFO:  eta: 0:03:10  iteration: 87/1000  consumed samples: 1392  total_loss: 8.441  time: 0.2097(76.31)  data_time: 0.0101  lr: 8.60e-05  
[01/19 14:42:56] lb.utils.events INFO:  eta: 0:03:10  iteration: 88/1000  consumed samples: 1408  total_loss: 8.441  time: 0.2097(76.29)  data_time: 0.0099  lr: 8.70e-05  
[01/19 14:42:56] lb.utils.events INFO:  eta: 0:03:10  iteration: 89/1000  consumed samples: 1424  total_loss: 8.44  time: 0.2098(76.27)  data_time: 0.0095  lr: 8.80e-05  
[01/19 14:42:56] lb.utils.events INFO:  eta: 0:03:10  iteration: 90/1000  consumed samples: 1440  total_loss: 8.439  time: 0.2098(76.25)  data_time: 0.0085  lr: 8.90e-05  
[01/19 14:42:56] lb.utils.events INFO:  eta: 0:03:09  iteration: 91/1000  consumed samples: 1456  total_loss: 8.437  time: 0.2098(76.26)  data_time: 0.0095  lr: 9.00e-05  
[01/19 14:42:57] lb.utils.events INFO:  eta: 0:03:09  iteration: 92/1000  consumed samples: 1472  total_loss: 8.434  time: 0.2099(76.24)  data_time: 0.0098  lr: 9.10e-05  
[01/19 14:42:57] lb.utils.events INFO:  eta: 0:03:09  iteration: 93/1000  consumed samples: 1488  total_loss: 8.432  time: 0.2099(76.24)  data_time: 0.0091  lr: 9.20e-05  
[01/19 14:42:57] lb.utils.events INFO:  eta: 0:03:09  iteration: 94/1000  consumed samples: 1504  total_loss: 8.428  time: 0.2099(76.21)  data_time: 0.0087  lr: 9.30e-05  
[01/19 14:42:57] lb.utils.events INFO:  eta: 0:03:09  iteration: 95/1000  consumed samples: 1520  total_loss: 8.424  time: 0.2100(76.19)  data_time: 0.0087  lr: 9.40e-05  
[01/19 14:42:58] lb.utils.events INFO:  eta: 0:03:09  iteration: 96/1000  consumed samples: 1536  total_loss: 8.419  time: 0.2100(76.19)  data_time: 0.0085  lr: 9.50e-05  
[01/19 14:42:58] lb.utils.events INFO:  eta: 0:03:09  iteration: 97/1000  consumed samples: 1552  total_loss: 8.413  time: 0.2101(76.16)  data_time: 0.0093  lr: 9.60e-05  
[01/19 14:42:58] lb.utils.events INFO:  eta: 0:03:09  iteration: 98/1000  consumed samples: 1568  total_loss: 8.41  time: 0.2101(76.16)  data_time: 0.0093  lr: 9.70e-05  
[01/19 14:42:58] lb.utils.events INFO:  eta: 0:03:08  iteration: 99/1000  consumed samples: 1584  total_loss: 8.407  time: 0.2102(76.13)  data_time: 0.0087  lr: 9.80e-05  
[01/19 14:42:58] lb.utils.events INFO:  eta: 0:03:08  iteration: 100/1000  consumed samples: 1600  total_loss: 8.394  time: 0.2102(76.14)  data_time: 0.0087  lr: 9.90e-05  
[01/19 14:42:59] lb.utils.events INFO:  eta: 0:03:08  iteration: 101/1000  consumed samples: 1616  total_loss: 8.382  time: 0.2102(76.11)  data_time: 0.0088  lr: 9.90e-05  
[01/19 14:42:59] lb.utils.events INFO:  eta: 0:03:08  iteration: 102/1000  consumed samples: 1632  total_loss: 8.382  time: 0.2102(76.11)  data_time: 0.0088  lr: 9.90e-05  
[01/19 14:42:59] lb.utils.events INFO:  eta: 0:03:08  iteration: 103/1000  consumed samples: 1648  total_loss: 8.381  time: 0.2103(76.08)  data_time: 0.0087  lr: 9.90e-05  
[01/19 14:42:59] lb.utils.events INFO:  eta: 0:03:07  iteration: 104/1000  consumed samples: 1664  total_loss: 8.373  time: 0.2097(76.32)  data_time: 0.0092  lr: 9.90e-05  
[01/19 14:43:00] lb.utils.events INFO:  eta: 0:03:07  iteration: 105/1000  consumed samples: 1680  total_loss: 8.364  time: 0.2097(76.29)  data_time: 0.0105  lr: 9.90e-05  
[01/19 14:43:00] lb.utils.events INFO:  eta: 0:03:07  iteration: 106/1000  consumed samples: 1696  total_loss: 8.352  time: 0.2097(76.30)  data_time: 0.0106  lr: 9.90e-05  
[01/19 14:43:00] lb.utils.events INFO:  eta: 0:03:07  iteration: 107/1000  consumed samples: 1712  total_loss: 8.34  time: 0.2098(76.27)  data_time: 0.0101  lr: 9.90e-05  
[01/19 14:43:00] lb.utils.events INFO:  eta: 0:03:07  iteration: 108/1000  consumed samples: 1728  total_loss: 8.34  time: 0.2098(76.26)  data_time: 0.0101  lr: 9.90e-05  
[01/19 14:43:00] lb.utils.events INFO:  eta: 0:03:06  iteration: 109/1000  consumed samples: 1744  total_loss: 8.34  time: 0.2099(76.24)  data_time: 0.0103  lr: 9.90e-05  
[01/19 14:43:01] lb.utils.events INFO:  eta: 0:03:06  iteration: 110/1000  consumed samples: 1760  total_loss: 8.338  time: 0.2099(76.24)  data_time: 0.0108  lr: 9.90e-05  
[01/19 14:43:01] lb.utils.events INFO:  eta: 0:03:06  iteration: 111/1000  consumed samples: 1776  total_loss: 8.337  time: 0.2099(76.23)  data_time: 0.0106  lr: 9.90e-05  
[01/19 14:43:01] lb.utils.events INFO:  eta: 0:03:06  iteration: 112/1000  consumed samples: 1792  total_loss: 8.336  time: 0.2099(76.22)  data_time: 0.0109  lr: 9.90e-05  
[01/19 14:43:01] lb.utils.events INFO:  eta: 0:03:06  iteration: 113/1000  consumed samples: 1808  total_loss: 8.334  time: 0.2099(76.21)  data_time: 0.0144  lr: 9.90e-05  
[01/19 14:43:02] lb.utils.events INFO:  eta: 0:03:05  iteration: 114/1000  consumed samples: 1824  total_loss: 8.333  time: 0.2099(76.22)  data_time: 0.0145  lr: 9.90e-05  
[01/19 14:43:02] lb.utils.events INFO:  eta: 0:03:05  iteration: 115/1000  consumed samples: 1840  total_loss: 8.331  time: 0.2100(76.20)  data_time: 0.0145  lr: 9.90e-05  
[01/19 14:43:02] lb.utils.events INFO:  eta: 0:03:05  iteration: 116/1000  consumed samples: 1856  total_loss: 8.331  time: 0.2100(76.20)  data_time: 0.0146  lr: 9.90e-05  
[01/19 14:43:02] lb.utils.events INFO:  eta: 0:03:05  iteration: 117/1000  consumed samples: 1872  total_loss: 8.331  time: 0.2100(76.18)  data_time: 0.0147  lr: 9.90e-05  
[01/19 14:43:02] lb.utils.events INFO:  eta: 0:03:05  iteration: 118/1000  consumed samples: 1888  total_loss: 8.329  time: 0.2100(76.18)  data_time: 0.0147  lr: 9.90e-05  
[01/19 14:43:03] lb.utils.events INFO:  eta: 0:03:04  iteration: 119/1000  consumed samples: 1904  total_loss: 8.327  time: 0.2101(76.16)  data_time: 0.0147  lr: 9.90e-05  
[01/19 14:43:03] lb.utils.events INFO:  eta: 0:03:04  iteration: 120/1000  consumed samples: 1920  total_loss: 8.325  time: 0.2101(76.16)  data_time: 0.0186  lr: 9.90e-05  
[01/19 14:43:03] lb.utils.events INFO:  eta: 0:03:04  iteration: 121/1000  consumed samples: 1936  total_loss: 8.323  time: 0.2101(76.15)  data_time: 0.0185  lr: 9.90e-05  
[01/19 14:43:03] lb.utils.events INFO:  eta: 0:03:04  iteration: 122/1000  consumed samples: 1952  total_loss: 8.316  time: 0.2099(76.24)  data_time: 0.0191  lr: 9.90e-05  
[01/19 14:43:04] lb.utils.events INFO:  eta: 0:03:04  iteration: 123/1000  consumed samples: 1968  total_loss: 8.309  time: 0.2099(76.21)  data_time: 0.0181  lr: 9.90e-05  
[01/19 14:43:04] lb.utils.events INFO:  eta: 0:03:03  iteration: 124/1000  consumed samples: 1984  total_loss: 8.305  time: 0.2099(76.22)  data_time: 0.0179  lr: 9.90e-05  
[01/19 14:43:04] lb.utils.events INFO:  eta: 0:03:03  iteration: 125/1000  consumed samples: 2000  total_loss: 8.302  time: 0.2100(76.19)  data_time: 0.0161  lr: 9.90e-05  
[01/19 14:43:04] lb.utils.events INFO:  eta: 0:03:03  iteration: 126/1000  consumed samples: 2016  total_loss: 8.299  time: 0.2100(76.19)  data_time: 0.0162  lr: 9.90e-05  
[01/19 14:43:04] lb.utils.events INFO:  eta: 0:03:03  iteration: 127/1000  consumed samples: 2032  total_loss: 8.296  time: 0.2101(76.16)  data_time: 0.0176  lr: 9.90e-05  
[01/19 14:43:05] lb.utils.events INFO:  eta: 0:03:02  iteration: 128/1000  consumed samples: 2048  total_loss: 8.295  time: 0.2101(76.16)  data_time: 0.0176  lr: 9.90e-05  
[01/19 14:43:05] lb.utils.events INFO:  eta: 0:03:02  iteration: 129/1000  consumed samples: 2064  total_loss: 8.295  time: 0.2101(76.14)  data_time: 0.0173  lr: 9.90e-05  
[01/19 14:43:05] lb.utils.events INFO:  eta: 0:03:02  iteration: 130/1000  consumed samples: 2080  total_loss: 8.294  time: 0.2101(76.14)  data_time: 0.0185  lr: 9.90e-05  
[01/19 14:43:05] lb.utils.events INFO:  eta: 0:03:02  iteration: 131/1000  consumed samples: 2096  total_loss: 8.293  time: 0.2102(76.12)  data_time: 0.0175  lr: 9.90e-05  
[01/19 14:43:06] lb.utils.events INFO:  eta: 0:03:02  iteration: 132/1000  consumed samples: 2112  total_loss: 8.289  time: 0.2101(76.14)  data_time: 0.0169  lr: 9.90e-05  
[01/19 14:43:06] lb.utils.events INFO:  eta: 0:03:02  iteration: 133/1000  consumed samples: 2128  total_loss: 8.285  time: 0.2102(76.14)  data_time: 0.0139  lr: 9.90e-05  
[01/19 14:43:06] lb.utils.events INFO:  eta: 0:03:01  iteration: 134/1000  consumed samples: 2144  total_loss: 8.277  time: 0.2105(76.02)  data_time: 0.0147  lr: 9.90e-05  
[01/19 14:43:06] lb.utils.events INFO:  eta: 0:03:01  iteration: 135/1000  consumed samples: 2160  total_loss: 8.268  time: 0.2102(76.11)  data_time: 0.0147  lr: 9.90e-05  
[01/19 14:43:06] lb.utils.events INFO:  eta: 0:03:01  iteration: 136/1000  consumed samples: 2176  total_loss: 8.268  time: 0.2102(76.13)  data_time: 0.0148  lr: 9.90e-05  
[01/19 14:43:07] lb.utils.events INFO:  eta: 0:03:01  iteration: 137/1000  consumed samples: 2192  total_loss: 8.268  time: 0.2102(76.10)  data_time: 0.0172  lr: 9.90e-05  
[01/19 14:43:07] lb.utils.events INFO:  eta: 0:03:01  iteration: 138/1000  consumed samples: 2208  total_loss: 8.262  time: 0.2103(76.10)  data_time: 0.0172  lr: 9.90e-05  
[01/19 14:43:07] lb.utils.events INFO:  eta: 0:03:00  iteration: 139/1000  consumed samples: 2224  total_loss: 8.256  time: 0.2103(76.07)  data_time: 0.0174  lr: 9.90e-05  
[01/19 14:43:07] lb.utils.events INFO:  eta: 0:03:00  iteration: 140/1000  consumed samples: 2240  total_loss: 8.252  time: 0.2104(76.06)  data_time: 0.0135  lr: 9.90e-05  
[01/19 14:43:08] lb.utils.events INFO:  eta: 0:03:00  iteration: 141/1000  consumed samples: 2256  total_loss: 8.248  time: 0.2104(76.04)  data_time: 0.0143  lr: 9.90e-05  
[01/19 14:43:08] lb.utils.events INFO:  eta: 0:03:00  iteration: 142/1000  consumed samples: 2272  total_loss: 8.247  time: 0.2105(76.01)  data_time: 0.0141  lr: 9.90e-05  
[01/19 14:43:08] lb.utils.events INFO:  eta: 0:03:00  iteration: 143/1000  consumed samples: 2288  total_loss: 8.247  time: 0.2103(76.06)  data_time: 0.0147  lr: 9.90e-05  
[01/19 14:43:08] lb.utils.events INFO:  eta: 0:02:59  iteration: 144/1000  consumed samples: 2304  total_loss: 8.246  time: 0.2104(76.05)  data_time: 0.0149  lr: 9.90e-05  
[01/19 14:43:09] lb.utils.events INFO:  eta: 0:02:59  iteration: 145/1000  consumed samples: 2320  total_loss: 8.246  time: 0.2103(76.07)  data_time: 0.0153  lr: 9.90e-05  
[01/19 14:43:09] lb.utils.events INFO:  eta: 0:02:59  iteration: 146/1000  consumed samples: 2336  total_loss: 8.24  time: 0.2103(76.08)  data_time: 0.0148  lr: 9.90e-05  
[01/19 14:43:09] lb.utils.events INFO:  eta: 0:02:59  iteration: 147/1000  consumed samples: 2352  total_loss: 8.235  time: 0.2103(76.08)  data_time: 0.0137  lr: 9.90e-05  
[01/19 14:43:09] lb.utils.events INFO:  eta: 0:02:58  iteration: 148/1000  consumed samples: 2368  total_loss: 8.231  time: 0.2104(76.06)  data_time: 0.0140  lr: 9.90e-05  
[01/19 14:43:09] lb.utils.events INFO:  eta: 0:02:58  iteration: 149/1000  consumed samples: 2384  total_loss: 8.227  time: 0.2103(76.06)  data_time: 0.0143  lr: 9.90e-05  
[01/19 14:43:10] lb.utils.events INFO:  eta: 0:02:58  iteration: 150/1000  consumed samples: 2400  total_loss: 8.225  time: 0.2104(76.06)  data_time: 0.0123  lr: 9.90e-05  
[01/19 14:43:10] lb.utils.events INFO:  eta: 0:02:58  iteration: 151/1000  consumed samples: 2416  total_loss: 8.223  time: 0.2104(76.04)  data_time: 0.0124  lr: 9.90e-05  
[01/19 14:43:10] lb.utils.events INFO:  eta: 0:02:58  iteration: 152/1000  consumed samples: 2432  total_loss: 8.223  time: 0.2104(76.04)  data_time: 0.0150  lr: 9.90e-05  
[01/19 14:43:10] lb.utils.events INFO:  eta: 0:02:57  iteration: 153/1000  consumed samples: 2448  total_loss: 8.223  time: 0.2105(76.02)  data_time: 0.0144  lr: 9.90e-05  
[01/19 14:43:11] lb.utils.events INFO:  eta: 0:02:57  iteration: 154/1000  consumed samples: 2464  total_loss: 8.222  time: 0.2105(76.03)  data_time: 0.0136  lr: 9.90e-05  
[01/19 14:43:11] lb.utils.events INFO:  eta: 0:02:57  iteration: 155/1000  consumed samples: 2480  total_loss: 8.222  time: 0.2105(76.02)  data_time: 0.0137  lr: 9.90e-05  
[01/19 14:43:11] lb.utils.events INFO:  eta: 0:02:57  iteration: 156/1000  consumed samples: 2496  total_loss: 8.219  time: 0.2105(76.02)  data_time: 0.0139  lr: 9.90e-05  
[01/19 14:43:11] lb.utils.events INFO:  eta: 0:02:57  iteration: 157/1000  consumed samples: 2512  total_loss: 8.216  time: 0.2105(76.00)  data_time: 0.0102  lr: 9.90e-05  
[01/19 14:43:11] lb.utils.events INFO:  eta: 0:02:57  iteration: 158/1000  consumed samples: 2528  total_loss: 8.213  time: 0.2105(75.99)  data_time: 0.0102  lr: 9.90e-05  
[01/19 14:43:12] lb.utils.events INFO:  eta: 0:02:57  iteration: 159/1000  consumed samples: 2544  total_loss: 8.209  time: 0.2106(75.98)  data_time: 0.0100  lr: 9.90e-05  
[01/19 14:43:12] lb.utils.events INFO:  eta: 0:02:56  iteration: 160/1000  consumed samples: 2560  total_loss: 8.201  time: 0.2106(75.97)  data_time: 0.0100  lr: 9.90e-05  
[01/19 14:43:12] lb.utils.events INFO:  eta: 0:02:56  iteration: 161/1000  consumed samples: 2576  total_loss: 8.192  time: 0.2106(75.98)  data_time: 0.0096  lr: 9.90e-05  
[01/19 14:43:12] lb.utils.events INFO:  eta: 0:02:56  iteration: 162/1000  consumed samples: 2592  total_loss: 8.186  time: 0.2106(75.97)  data_time: 0.0097  lr: 9.90e-05  
[01/19 14:43:13] lb.utils.events INFO:  eta: 0:02:56  iteration: 163/1000  consumed samples: 2608  total_loss: 8.18  time: 0.2106(75.98)  data_time: 0.0095  lr: 9.90e-05  
[01/19 14:43:13] lb.utils.events INFO:  eta: 0:02:55  iteration: 164/1000  consumed samples: 2624  total_loss: 8.177  time: 0.2106(75.99)  data_time: 0.0105  lr: 9.90e-05  
[01/19 14:43:13] lb.utils.events INFO:  eta: 0:02:55  iteration: 165/1000  consumed samples: 2640  total_loss: 8.173  time: 0.2106(75.97)  data_time: 0.0103  lr: 9.90e-05  
[01/19 14:43:13] lb.utils.events INFO:  eta: 0:02:55  iteration: 166/1000  consumed samples: 2656  total_loss: 8.171  time: 0.2106(75.97)  data_time: 0.0103  lr: 9.90e-05  
[01/19 14:43:13] lb.utils.events INFO:  eta: 0:02:55  iteration: 167/1000  consumed samples: 2672  total_loss: 8.17  time: 0.2106(75.99)  data_time: 0.0116  lr: 9.90e-05  
[01/19 14:43:14] lb.utils.events INFO:  eta: 0:02:54  iteration: 168/1000  consumed samples: 2688  total_loss: 8.17  time: 0.2105(76.00)  data_time: 0.0113  lr: 9.90e-05  
[01/19 14:43:14] lb.utils.events INFO:  eta: 0:02:54  iteration: 169/1000  consumed samples: 2704  total_loss: 8.169  time: 0.2105(76.00)  data_time: 0.0110  lr: 9.90e-05  
[01/19 14:43:14] lb.utils.events INFO:  eta: 0:02:54  iteration: 170/1000  consumed samples: 2720  total_loss: 8.162  time: 0.2105(76.00)  data_time: 0.0110  lr: 9.90e-05  
[01/19 14:43:14] lb.utils.events INFO:  eta: 0:02:54  iteration: 171/1000  consumed samples: 2736  total_loss: 8.155  time: 0.2106(75.98)  data_time: 0.0111  lr: 9.90e-05  
[01/19 14:43:15] lb.utils.events INFO:  eta: 0:02:54  iteration: 172/1000  consumed samples: 2752  total_loss: 8.153  time: 0.2106(75.98)  data_time: 0.0088  lr: 9.90e-05  
[01/19 14:43:15] lb.utils.events INFO:  eta: 0:02:54  iteration: 173/1000  consumed samples: 2768  total_loss: 8.151  time: 0.2106(75.96)  data_time: 0.0092  lr: 9.90e-05  
[01/19 14:43:15] lb.utils.events INFO:  eta: 0:02:54  iteration: 174/1000  consumed samples: 2784  total_loss: 8.149  time: 0.2107(75.96)  data_time: 0.0092  lr: 9.90e-05  
[01/19 14:43:15] lb.utils.events INFO:  eta: 0:02:53  iteration: 175/1000  consumed samples: 2800  total_loss: 8.147  time: 0.2107(75.94)  data_time: 0.0094  lr: 9.90e-05  
[01/19 14:43:15] lb.utils.events INFO:  eta: 0:02:53  iteration: 176/1000  consumed samples: 2816  total_loss: 8.146  time: 0.2107(75.93)  data_time: 0.0113  lr: 9.90e-05  
[01/19 14:43:16] lb.utils.events INFO:  eta: 0:02:53  iteration: 177/1000  consumed samples: 2832  total_loss: 8.146  time: 0.2108(75.92)  data_time: 0.0108  lr: 9.90e-05  
[01/19 14:43:16] lb.utils.events INFO:  eta: 0:02:53  iteration: 178/1000  consumed samples: 2848  total_loss: 8.145  time: 0.2108(75.92)  data_time: 0.0108  lr: 9.90e-05  
[01/19 14:43:16] lb.utils.events INFO:  eta: 0:02:53  iteration: 179/1000  consumed samples: 2864  total_loss: 8.143  time: 0.2108(75.90)  data_time: 0.0108  lr: 9.90e-05  
[01/19 14:43:16] lb.utils.events INFO:  eta: 0:02:53  iteration: 180/1000  consumed samples: 2880  total_loss: 8.141  time: 0.2108(75.89)  data_time: 0.0110  lr: 9.90e-05  
[01/19 14:43:17] lb.utils.events INFO:  eta: 0:02:52  iteration: 181/1000  consumed samples: 2896  total_loss: 8.139  time: 0.2109(75.87)  data_time: 0.0107  lr: 9.90e-05  
[01/19 14:43:17] lb.utils.events INFO:  eta: 0:02:52  iteration: 182/1000  consumed samples: 2912  total_loss: 8.138  time: 0.2109(75.87)  data_time: 0.0100  lr: 9.90e-05  
[01/19 14:43:17] lb.utils.events INFO:  eta: 0:02:52  iteration: 183/1000  consumed samples: 2928  total_loss: 8.138  time: 0.2109(75.86)  data_time: 0.0101  lr: 9.90e-05  
[01/19 14:43:17] lb.utils.events INFO:  eta: 0:02:52  iteration: 184/1000  consumed samples: 2944  total_loss: 8.137  time: 0.2109(75.85)  data_time: 0.0108  lr: 9.90e-05  
[01/19 14:43:18] lb.utils.events INFO:  eta: 0:02:52  iteration: 185/1000  consumed samples: 2960  total_loss: 8.137  time: 0.2110(75.84)  data_time: 0.0105  lr: 9.90e-05  
[01/19 14:43:18] lb.utils.events INFO:  eta: 0:02:51  iteration: 186/1000  consumed samples: 2976  total_loss: 8.136  time: 0.2110(75.84)  data_time: 0.0107  lr: 9.90e-05  
[01/19 14:43:18] lb.utils.events INFO:  eta: 0:02:51  iteration: 187/1000  consumed samples: 2992  total_loss: 8.135  time: 0.2110(75.82)  data_time: 0.0093  lr: 9.90e-05  
[01/19 14:43:18] lb.utils.events INFO:  eta: 0:02:51  iteration: 188/1000  consumed samples: 3008  total_loss: 8.134  time: 0.2110(75.81)  data_time: 0.0096  lr: 9.90e-05  
[01/19 14:43:18] lb.utils.events INFO:  eta: 0:02:51  iteration: 189/1000  consumed samples: 3024  total_loss: 8.133  time: 0.2111(75.80)  data_time: 0.0096  lr: 9.90e-05  
[01/19 14:43:19] lb.utils.events INFO:  eta: 0:02:51  iteration: 190/1000  consumed samples: 3040  total_loss: 8.124  time: 0.2111(75.79)  data_time: 0.0097  lr: 9.90e-05  
[01/19 14:43:19] lb.utils.events INFO:  eta: 0:02:51  iteration: 191/1000  consumed samples: 3056  total_loss: 8.116  time: 0.2112(75.77)  data_time: 0.0096  lr: 9.90e-05  
[01/19 14:43:19] lb.utils.events INFO:  eta: 0:02:50  iteration: 192/1000  consumed samples: 3072  total_loss: 8.116  time: 0.2112(75.77)  data_time: 0.0095  lr: 9.90e-05  
[01/19 14:43:19] lb.utils.events INFO:  eta: 0:02:50  iteration: 193/1000  consumed samples: 3088  total_loss: 8.115  time: 0.2112(75.75)  data_time: 0.0092  lr: 9.90e-05  
[01/19 14:43:20] lb.utils.events INFO:  eta: 0:02:50  iteration: 194/1000  consumed samples: 3104  total_loss: 8.114  time: 0.2112(75.74)  data_time: 0.0094  lr: 9.90e-05  
[01/19 14:43:20] lb.utils.events INFO:  eta: 0:02:50  iteration: 195/1000  consumed samples: 3120  total_loss: 8.112  time: 0.2112(75.74)  data_time: 0.0093  lr: 9.90e-05  
[01/19 14:43:20] lb.utils.events INFO:  eta: 0:02:50  iteration: 196/1000  consumed samples: 3136  total_loss: 8.112  time: 0.2113(75.74)  data_time: 0.0070  lr: 9.90e-05  
[01/19 14:43:20] lb.utils.events INFO:  eta: 0:02:49  iteration: 197/1000  consumed samples: 3152  total_loss: 8.112  time: 0.2113(75.72)  data_time: 0.0070  lr: 9.90e-05  
[01/19 14:43:20] lb.utils.events INFO:  eta: 0:02:49  iteration: 198/1000  consumed samples: 3168  total_loss: 8.111  time: 0.2113(75.71)  data_time: 0.0071  lr: 9.90e-05  
[01/19 14:43:21] lb.utils.events INFO:  eta: 0:02:49  iteration: 199/1000  consumed samples: 3184  total_loss: 8.11  time: 0.2114(75.70)  data_time: 0.0072  lr: 9.90e-05  
[01/19 14:43:21] lb.utils.events INFO:  eta: 0:02:49  iteration: 200/1000  consumed samples: 3200  total_loss: 8.109  time: 0.2114(75.69)  data_time: 0.0071  lr: 9.90e-05  
[01/19 14:43:21] lb.utils.events INFO:  eta: 0:02:49  iteration: 201/1000  consumed samples: 3216  total_loss: 8.109  time: 0.2114(75.68)  data_time: 0.0071  lr: 9.90e-05  
[01/19 14:43:21] lb.utils.events INFO:  eta: 0:02:48  iteration: 202/1000  consumed samples: 3232  total_loss: 8.108  time: 0.2114(75.69)  data_time: 0.0071  lr: 9.90e-05  
[01/19 14:43:22] lb.utils.events INFO:  eta: 0:02:48  iteration: 203/1000  consumed samples: 3248  total_loss: 8.106  time: 0.2115(75.67)  data_time: 0.0067  lr: 9.90e-05  
[01/19 14:43:22] lb.utils.events INFO:  eta: 0:02:48  iteration: 204/1000  consumed samples: 3264  total_loss: 8.103  time: 0.2115(75.65)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:43:22] lb.utils.events INFO:  eta: 0:02:48  iteration: 205/1000  consumed samples: 3280  total_loss: 8.101  time: 0.2115(75.64)  data_time: 0.0049  lr: 9.90e-05  
[01/19 14:43:22] lb.utils.events INFO:  eta: 0:02:48  iteration: 206/1000  consumed samples: 3296  total_loss: 8.099  time: 0.2116(75.63)  data_time: 0.0049  lr: 9.90e-05  
[01/19 14:43:23] lb.utils.events INFO:  eta: 0:02:47  iteration: 207/1000  consumed samples: 3312  total_loss: 8.095  time: 0.2116(75.62)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:23] lb.utils.events INFO:  eta: 0:02:47  iteration: 208/1000  consumed samples: 3328  total_loss: 8.093  time: 0.2116(75.60)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:23] lb.utils.events INFO:  eta: 0:02:47  iteration: 209/1000  consumed samples: 3344  total_loss: 8.091  time: 0.2116(75.60)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:23] lb.utils.events INFO:  eta: 0:02:47  iteration: 210/1000  consumed samples: 3360  total_loss: 8.088  time: 0.2117(75.58)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:23] lb.utils.events INFO:  eta: 0:02:47  iteration: 211/1000  consumed samples: 3376  total_loss: 8.086  time: 0.2117(75.58)  data_time: 0.0046  lr: 9.90e-05  
[01/19 14:43:24] lb.utils.events INFO:  eta: 0:02:47  iteration: 212/1000  consumed samples: 3392  total_loss: 8.084  time: 0.2117(75.56)  data_time: 0.0046  lr: 9.90e-05  
[01/19 14:43:24] lb.utils.events INFO:  eta: 0:02:46  iteration: 213/1000  consumed samples: 3408  total_loss: 8.081  time: 0.2118(75.56)  data_time: 0.0046  lr: 9.90e-05  
[01/19 14:43:24] lb.utils.events INFO:  eta: 0:02:46  iteration: 214/1000  consumed samples: 3424  total_loss: 8.073  time: 0.2118(75.54)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:24] lb.utils.events INFO:  eta: 0:02:46  iteration: 215/1000  consumed samples: 3440  total_loss: 8.067  time: 0.2119(75.52)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:25] lb.utils.events INFO:  eta: 0:02:46  iteration: 216/1000  consumed samples: 3456  total_loss: 8.061  time: 0.2118(75.53)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:43:25] lb.utils.events INFO:  eta: 0:02:46  iteration: 217/1000  consumed samples: 3472  total_loss: 8.054  time: 0.2119(75.51)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:25] lb.utils.events INFO:  eta: 0:02:45  iteration: 218/1000  consumed samples: 3488  total_loss: 8.05  time: 0.2119(75.49)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:43:25] lb.utils.events INFO:  eta: 0:02:45  iteration: 219/1000  consumed samples: 3504  total_loss: 8.043  time: 0.2120(75.48)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:43:25] lb.utils.events INFO:  eta: 0:02:45  iteration: 220/1000  consumed samples: 3520  total_loss: 8.039  time: 0.2120(75.46)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:43:26] lb.utils.events INFO:  eta: 0:02:45  iteration: 221/1000  consumed samples: 3536  total_loss: 8.039  time: 0.2121(75.44)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:43:26] lb.utils.events INFO:  eta: 0:02:45  iteration: 222/1000  consumed samples: 3552  total_loss: 8.037  time: 0.2121(75.45)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:43:26] lb.utils.events INFO:  eta: 0:02:44  iteration: 223/1000  consumed samples: 3568  total_loss: 8.036  time: 0.2121(75.43)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:43:26] lb.utils.events INFO:  eta: 0:02:44  iteration: 224/1000  consumed samples: 3584  total_loss: 8.034  time: 0.2122(75.41)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:43:27] lb.utils.events INFO:  eta: 0:02:44  iteration: 225/1000  consumed samples: 3600  total_loss: 8.029  time: 0.2122(75.41)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:43:27] lb.utils.events INFO:  eta: 0:02:44  iteration: 226/1000  consumed samples: 3616  total_loss: 8.026  time: 0.2122(75.39)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:43:27] lb.utils.events INFO:  eta: 0:02:44  iteration: 227/1000  consumed samples: 3632  total_loss: 8.026  time: 0.2122(75.40)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:43:27] lb.utils.events INFO:  eta: 0:02:43  iteration: 228/1000  consumed samples: 3648  total_loss: 8.024  time: 0.2123(75.37)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:43:28] lb.utils.events INFO:  eta: 0:02:43  iteration: 229/1000  consumed samples: 3664  total_loss: 8.024  time: 0.2123(75.36)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:43:28] lb.utils.events INFO:  eta: 0:02:43  iteration: 230/1000  consumed samples: 3680  total_loss: 8.02  time: 0.2123(75.35)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:43:28] lb.utils.events INFO:  eta: 0:02:43  iteration: 231/1000  consumed samples: 3696  total_loss: 8.014  time: 0.2124(75.33)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:43:28] lb.utils.events INFO:  eta: 0:02:43  iteration: 232/1000  consumed samples: 3712  total_loss: 8.01  time: 0.2124(75.33)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:28] lb.utils.events INFO:  eta: 0:02:43  iteration: 233/1000  consumed samples: 3728  total_loss: 8.008  time: 0.2125(75.31)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:29] lb.utils.events INFO:  eta: 0:02:43  iteration: 234/1000  consumed samples: 3744  total_loss: 8.005  time: 0.2125(75.30)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:29] lb.utils.events INFO:  eta: 0:02:42  iteration: 235/1000  consumed samples: 3760  total_loss: 8  time: 0.2125(75.29)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:29] lb.utils.events INFO:  eta: 0:02:42  iteration: 236/1000  consumed samples: 3776  total_loss: 7.999  time: 0.2125(75.28)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:29] lb.utils.events INFO:  eta: 0:02:42  iteration: 237/1000  consumed samples: 3792  total_loss: 7.999  time: 0.2126(75.26)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:30] lb.utils.events INFO:  eta: 0:02:42  iteration: 238/1000  consumed samples: 3808  total_loss: 7.996  time: 0.2126(75.25)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:30] lb.utils.events INFO:  eta: 0:02:42  iteration: 239/1000  consumed samples: 3824  total_loss: 7.996  time: 0.2127(75.23)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:30] lb.utils.events INFO:  eta: 0:02:41  iteration: 240/1000  consumed samples: 3840  total_loss: 7.996  time: 0.2127(75.22)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:30] lb.utils.events INFO:  eta: 0:02:41  iteration: 241/1000  consumed samples: 3856  total_loss: 7.992  time: 0.2128(75.20)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:31] lb.utils.events INFO:  eta: 0:02:41  iteration: 242/1000  consumed samples: 3872  total_loss: 7.99  time: 0.2128(75.19)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:31] lb.utils.events INFO:  eta: 0:02:41  iteration: 243/1000  consumed samples: 3888  total_loss: 7.99  time: 0.2128(75.18)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:31] lb.utils.events INFO:  eta: 0:02:41  iteration: 244/1000  consumed samples: 3904  total_loss: 7.99  time: 0.2128(75.19)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:31] lb.utils.events INFO:  eta: 0:02:40  iteration: 245/1000  consumed samples: 3920  total_loss: 7.988  time: 0.2128(75.17)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:43:31] lb.utils.events INFO:  eta: 0:02:40  iteration: 246/1000  consumed samples: 3936  total_loss: 7.988  time: 0.2129(75.17)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:43:32] lb.utils.events INFO:  eta: 0:02:40  iteration: 247/1000  consumed samples: 3952  total_loss: 7.988  time: 0.2129(75.15)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:43:32] lb.utils.events INFO:  eta: 0:02:40  iteration: 248/1000  consumed samples: 3968  total_loss: 7.985  time: 0.2129(75.15)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:43:32] lb.utils.events INFO:  eta: 0:02:39  iteration: 249/1000  consumed samples: 3984  total_loss: 7.983  time: 0.2130(75.13)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:43:32] lb.utils.events INFO:  eta: 0:02:39  iteration: 250/1000  consumed samples: 4000  total_loss: 7.983  time: 0.2130(75.13)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:43:33] lb.utils.events INFO:  eta: 0:02:39  iteration: 251/1000  consumed samples: 4016  total_loss: 7.982  time: 0.2130(75.11)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:43:33] lb.utils.events INFO:  eta: 0:02:39  iteration: 252/1000  consumed samples: 4032  total_loss: 7.982  time: 0.2130(75.12)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:33] lb.utils.events INFO:  eta: 0:02:39  iteration: 253/1000  consumed samples: 4048  total_loss: 7.981  time: 0.2130(75.11)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:33] lb.utils.events INFO:  eta: 0:02:38  iteration: 254/1000  consumed samples: 4064  total_loss: 7.979  time: 0.2130(75.10)  data_time: 0.0043  lr: 9.90e-05  
[01/19 14:43:33] lb.utils.events INFO:  eta: 0:02:38  iteration: 255/1000  consumed samples: 4080  total_loss: 7.978  time: 0.2130(75.10)  data_time: 0.0044  lr: 9.90e-05  
[01/19 14:43:34] lb.utils.events INFO:  eta: 0:02:38  iteration: 256/1000  consumed samples: 4096  total_loss: 7.978  time: 0.2131(75.09)  data_time: 0.0046  lr: 9.90e-05  
[01/19 14:43:34] lb.utils.events INFO:  eta: 0:02:38  iteration: 257/1000  consumed samples: 4112  total_loss: 7.978  time: 0.2131(75.08)  data_time: 0.0046  lr: 9.90e-05  
[01/19 14:43:34] lb.utils.events INFO:  eta: 0:02:38  iteration: 258/1000  consumed samples: 4128  total_loss: 7.975  time: 0.2131(75.08)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:43:34] lb.utils.events INFO:  eta: 0:02:37  iteration: 259/1000  consumed samples: 4144  total_loss: 7.972  time: 0.2131(75.07)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:43:35] lb.utils.events INFO:  eta: 0:02:37  iteration: 260/1000  consumed samples: 4160  total_loss: 7.972  time: 0.2131(75.07)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:35] lb.utils.events INFO:  eta: 0:02:37  iteration: 261/1000  consumed samples: 4176  total_loss: 7.972  time: 0.2132(75.06)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:35] lb.utils.events INFO:  eta: 0:02:37  iteration: 262/1000  consumed samples: 4192  total_loss: 7.971  time: 0.2132(75.05)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:43:35] lb.utils.events INFO:  eta: 0:02:37  iteration: 263/1000  consumed samples: 4208  total_loss: 7.969  time: 0.2132(75.04)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:43:36] lb.utils.events INFO:  eta: 0:02:37  iteration: 264/1000  consumed samples: 4224  total_loss: 7.967  time: 0.2132(75.04)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:36] lb.utils.events INFO:  eta: 0:02:36  iteration: 265/1000  consumed samples: 4240  total_loss: 7.966  time: 0.2133(75.03)  data_time: 0.0043  lr: 9.90e-05  
[01/19 14:43:36] lb.utils.events INFO:  eta: 0:02:36  iteration: 266/1000  consumed samples: 4256  total_loss: 7.965  time: 0.2133(75.02)  data_time: 0.0044  lr: 9.90e-05  
[01/19 14:43:36] lb.utils.events INFO:  eta: 0:02:36  iteration: 267/1000  consumed samples: 4272  total_loss: 7.965  time: 0.2133(75.01)  data_time: 0.0044  lr: 9.90e-05  
[01/19 14:43:36] lb.utils.events INFO:  eta: 0:02:36  iteration: 268/1000  consumed samples: 4288  total_loss: 7.964  time: 0.2133(75.00)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:37] lb.utils.events INFO:  eta: 0:02:36  iteration: 269/1000  consumed samples: 4304  total_loss: 7.962  time: 0.2134(74.99)  data_time: 0.0053  lr: 9.90e-05  
[01/19 14:43:37] lb.utils.events INFO:  eta: 0:02:36  iteration: 270/1000  consumed samples: 4320  total_loss: 7.961  time: 0.2134(74.99)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:43:37] lb.utils.events INFO:  eta: 0:02:35  iteration: 271/1000  consumed samples: 4336  total_loss: 7.961  time: 0.2134(74.97)  data_time: 0.0051  lr: 9.90e-05  
[01/19 14:43:37] lb.utils.events INFO:  eta: 0:02:35  iteration: 272/1000  consumed samples: 4352  total_loss: 7.96  time: 0.2134(74.97)  data_time: 0.0051  lr: 9.90e-05  
[01/19 14:43:38] lb.utils.events INFO:  eta: 0:02:35  iteration: 273/1000  consumed samples: 4368  total_loss: 7.959  time: 0.2135(74.96)  data_time: 0.0050  lr: 9.90e-05  
[01/19 14:43:38] lb.utils.events INFO:  eta: 0:02:35  iteration: 274/1000  consumed samples: 4384  total_loss: 7.958  time: 0.2135(74.95)  data_time: 0.0049  lr: 9.90e-05  
[01/19 14:43:38] lb.utils.events INFO:  eta: 0:02:35  iteration: 275/1000  consumed samples: 4400  total_loss: 7.956  time: 0.2135(74.94)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:43:38] lb.utils.events INFO:  eta: 0:02:34  iteration: 276/1000  consumed samples: 4416  total_loss: 7.956  time: 0.2135(74.94)  data_time: 0.0047  lr: 9.90e-05  
[01/19 14:43:39] lb.utils.events INFO:  eta: 0:02:34  iteration: 277/1000  consumed samples: 4432  total_loss: 7.956  time: 0.2136(74.92)  data_time: 0.0046  lr: 9.90e-05  
[01/19 14:43:39] lb.utils.events INFO:  eta: 0:02:34  iteration: 278/1000  consumed samples: 4448  total_loss: 7.953  time: 0.2136(74.92)  data_time: 0.0045  lr: 9.90e-05  
[01/19 14:43:39] lb.utils.events INFO:  eta: 0:02:34  iteration: 279/1000  consumed samples: 4464  total_loss: 7.952  time: 0.2136(74.91)  data_time: 0.0044  lr: 9.90e-05  
[01/19 14:43:39] lb.utils.events INFO:  eta: 0:02:34  iteration: 280/1000  consumed samples: 4480  total_loss: 7.952  time: 0.2136(74.90)  data_time: 0.0044  lr: 9.90e-05  
[01/19 14:43:39] lb.utils.events INFO:  eta: 0:02:33  iteration: 281/1000  consumed samples: 4496  total_loss: 7.952  time: 0.2136(74.90)  data_time: 0.0043  lr: 9.90e-05  
[01/19 14:43:40] lb.utils.events INFO:  eta: 0:02:33  iteration: 282/1000  consumed samples: 4512  total_loss: 7.951  time: 0.2137(74.89)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:40] lb.utils.events INFO:  eta: 0:02:33  iteration: 283/1000  consumed samples: 4528  total_loss: 7.949  time: 0.2137(74.87)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:43:40] lb.utils.events INFO:  eta: 0:02:33  iteration: 284/1000  consumed samples: 4544  total_loss: 7.945  time: 0.2137(74.87)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:43:40] lb.utils.events INFO:  eta: 0:02:33  iteration: 285/1000  consumed samples: 4560  total_loss: 7.941  time: 0.2137(74.87)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:43:41] lb.utils.events INFO:  eta: 0:02:32  iteration: 286/1000  consumed samples: 4576  total_loss: 7.94  time: 0.2137(74.86)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:43:41] lb.utils.events INFO:  eta: 0:02:32  iteration: 287/1000  consumed samples: 4592  total_loss: 7.937  time: 0.2138(74.85)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:43:41] lb.utils.events INFO:  eta: 0:02:32  iteration: 288/1000  consumed samples: 4608  total_loss: 7.932  time: 0.2138(74.85)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:41] lb.utils.events INFO:  eta: 0:02:32  iteration: 289/1000  consumed samples: 4624  total_loss: 7.93  time: 0.2138(74.85)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:41] lb.utils.events INFO:  eta: 0:02:32  iteration: 290/1000  consumed samples: 4640  total_loss: 7.93  time: 0.2138(74.84)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:42] lb.utils.events INFO:  eta: 0:02:31  iteration: 291/1000  consumed samples: 4656  total_loss: 7.929  time: 0.2138(74.84)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:42] lb.utils.events INFO:  eta: 0:02:31  iteration: 292/1000  consumed samples: 4672  total_loss: 7.927  time: 0.2138(74.83)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:42] lb.utils.events INFO:  eta: 0:02:31  iteration: 293/1000  consumed samples: 4688  total_loss: 7.925  time: 0.2138(74.83)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:42] lb.utils.events INFO:  eta: 0:02:31  iteration: 294/1000  consumed samples: 4704  total_loss: 7.924  time: 0.2138(74.82)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:43] lb.utils.events INFO:  eta: 0:02:31  iteration: 295/1000  consumed samples: 4720  total_loss: 7.924  time: 0.2139(74.82)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:43] lb.utils.events INFO:  eta: 0:02:30  iteration: 296/1000  consumed samples: 4736  total_loss: 7.922  time: 0.2139(74.81)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:43] lb.utils.events INFO:  eta: 0:02:30  iteration: 297/1000  consumed samples: 4752  total_loss: 7.921  time: 0.2139(74.80)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:43] lb.utils.events INFO:  eta: 0:02:30  iteration: 298/1000  consumed samples: 4768  total_loss: 7.921  time: 0.2139(74.80)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:44] lb.utils.events INFO:  eta: 0:02:30  iteration: 299/1000  consumed samples: 4784  total_loss: 7.918  time: 0.2139(74.79)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:44] lb.utils.events INFO:  eta: 0:02:30  iteration: 300/1000  consumed samples: 4800  total_loss: 7.918  time: 0.2140(74.78)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:44] lb.utils.events INFO:  eta: 0:02:29  iteration: 301/1000  consumed samples: 4816  total_loss: 7.918  time: 0.2140(74.77)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:44] lb.utils.events INFO:  eta: 0:02:29  iteration: 302/1000  consumed samples: 4832  total_loss: 7.915  time: 0.2140(74.76)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:44] lb.utils.events INFO:  eta: 0:02:29  iteration: 303/1000  consumed samples: 4848  total_loss: 7.915  time: 0.2140(74.76)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:43:45] lb.utils.events INFO:  eta: 0:02:29  iteration: 304/1000  consumed samples: 4864  total_loss: 7.914  time: 0.2140(74.75)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:43:45] lb.utils.events INFO:  eta: 0:02:29  iteration: 305/1000  consumed samples: 4880  total_loss: 7.915  time: 0.2140(74.75)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:43:45] lb.utils.events INFO:  eta: 0:02:28  iteration: 306/1000  consumed samples: 4896  total_loss: 7.915  time: 0.2141(74.74)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:43:45] lb.utils.events INFO:  eta: 0:02:28  iteration: 307/1000  consumed samples: 4912  total_loss: 7.914  time: 0.2141(74.74)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:43:46] lb.utils.events INFO:  eta: 0:02:28  iteration: 308/1000  consumed samples: 4928  total_loss: 7.914  time: 0.2141(74.73)  data_time: 0.0051  lr: 9.90e-05  
[01/19 14:43:46] lb.utils.events INFO:  eta: 0:02:28  iteration: 309/1000  consumed samples: 4944  total_loss: 7.914  time: 0.2141(74.73)  data_time: 0.0051  lr: 9.90e-05  
[01/19 14:43:46] lb.utils.events INFO:  eta: 0:02:28  iteration: 310/1000  consumed samples: 4960  total_loss: 7.914  time: 0.2141(74.72)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:43:46] lb.utils.events INFO:  eta: 0:02:27  iteration: 311/1000  consumed samples: 4976  total_loss: 7.912  time: 0.2141(74.72)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:43:47] lb.utils.events INFO:  eta: 0:02:27  iteration: 312/1000  consumed samples: 4992  total_loss: 7.91  time: 0.2142(74.71)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:47] lb.utils.events INFO:  eta: 0:02:27  iteration: 313/1000  consumed samples: 5008  total_loss: 7.908  time: 0.2142(74.70)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:47] lb.utils.events INFO:  eta: 0:02:27  iteration: 314/1000  consumed samples: 5024  total_loss: 7.905  time: 0.2142(74.69)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:43:47] lb.utils.events INFO:  eta: 0:02:27  iteration: 315/1000  consumed samples: 5040  total_loss: 7.904  time: 0.2142(74.69)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:43:47] lb.utils.events INFO:  eta: 0:02:26  iteration: 316/1000  consumed samples: 5056  total_loss: 7.903  time: 0.2143(74.68)  data_time: 0.0056  lr: 9.90e-05  
[01/19 14:43:48] lb.utils.events INFO:  eta: 0:02:26  iteration: 317/1000  consumed samples: 5072  total_loss: 7.901  time: 0.2143(74.68)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:43:48] lb.utils.events INFO:  eta: 0:02:26  iteration: 318/1000  consumed samples: 5088  total_loss: 7.901  time: 0.2143(74.67)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:48] lb.utils.events INFO:  eta: 0:02:26  iteration: 319/1000  consumed samples: 5104  total_loss: 7.901  time: 0.2143(74.66)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:48] lb.utils.events INFO:  eta: 0:02:26  iteration: 320/1000  consumed samples: 5120  total_loss: 7.899  time: 0.2143(74.65)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:43:49] lb.utils.events INFO:  eta: 0:02:25  iteration: 321/1000  consumed samples: 5136  total_loss: 7.898  time: 0.2143(74.65)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:49] lb.utils.events INFO:  eta: 0:02:25  iteration: 322/1000  consumed samples: 5152  total_loss: 7.898  time: 0.2144(74.64)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:43:49] lb.utils.events INFO:  eta: 0:02:25  iteration: 323/1000  consumed samples: 5168  total_loss: 7.898  time: 0.2144(74.64)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:49] lb.utils.events INFO:  eta: 0:02:25  iteration: 324/1000  consumed samples: 5184  total_loss: 7.898  time: 0.2144(74.63)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:43:49] lb.utils.events INFO:  eta: 0:02:25  iteration: 325/1000  consumed samples: 5200  total_loss: 7.897  time: 0.2144(74.63)  data_time: 0.0053  lr: 9.90e-05  
[01/19 14:43:50] lb.utils.events INFO:  eta: 0:02:24  iteration: 326/1000  consumed samples: 5216  total_loss: 7.895  time: 0.2144(74.61)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:43:50] lb.utils.events INFO:  eta: 0:02:24  iteration: 327/1000  consumed samples: 5232  total_loss: 7.895  time: 0.2144(74.61)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:43:50] lb.utils.events INFO:  eta: 0:02:24  iteration: 328/1000  consumed samples: 5248  total_loss: 7.895  time: 0.2145(74.60)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:43:50] lb.utils.events INFO:  eta: 0:02:24  iteration: 329/1000  consumed samples: 5264  total_loss: 7.894  time: 0.2145(74.60)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:43:51] lb.utils.events INFO:  eta: 0:02:24  iteration: 330/1000  consumed samples: 5280  total_loss: 7.893  time: 0.2145(74.59)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:43:51] lb.utils.events INFO:  eta: 0:02:23  iteration: 331/1000  consumed samples: 5296  total_loss: 7.892  time: 0.2145(74.59)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:43:51] lb.utils.events INFO:  eta: 0:02:23  iteration: 332/1000  consumed samples: 5312  total_loss: 7.892  time: 0.2145(74.58)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:43:51] lb.utils.events INFO:  eta: 0:02:23  iteration: 333/1000  consumed samples: 5328  total_loss: 7.892  time: 0.2146(74.57)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:52] lb.utils.events INFO:  eta: 0:02:23  iteration: 334/1000  consumed samples: 5344  total_loss: 7.892  time: 0.2146(74.56)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:52] lb.utils.events INFO:  eta: 0:02:22  iteration: 335/1000  consumed samples: 5360  total_loss: 7.891  time: 0.2146(74.56)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:52] lb.utils.events INFO:  eta: 0:02:22  iteration: 336/1000  consumed samples: 5376  total_loss: 7.89  time: 0.2146(74.56)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:52] lb.utils.events INFO:  eta: 0:02:22  iteration: 337/1000  consumed samples: 5392  total_loss: 7.89  time: 0.2146(74.56)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:52] lb.utils.events INFO:  eta: 0:02:22  iteration: 338/1000  consumed samples: 5408  total_loss: 7.89  time: 0.2146(74.55)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:53] lb.utils.events INFO:  eta: 0:02:22  iteration: 339/1000  consumed samples: 5424  total_loss: 7.89  time: 0.2146(74.55)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:53] lb.utils.events INFO:  eta: 0:02:21  iteration: 340/1000  consumed samples: 5440  total_loss: 7.89  time: 0.2147(74.54)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:43:53] lb.utils.events INFO:  eta: 0:02:21  iteration: 341/1000  consumed samples: 5456  total_loss: 7.89  time: 0.2147(74.53)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:53] lb.utils.events INFO:  eta: 0:02:21  iteration: 342/1000  consumed samples: 5472  total_loss: 7.89  time: 0.2147(74.52)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:54] lb.utils.events INFO:  eta: 0:02:21  iteration: 343/1000  consumed samples: 5488  total_loss: 7.889  time: 0.2147(74.52)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:54] lb.utils.events INFO:  eta: 0:02:21  iteration: 344/1000  consumed samples: 5504  total_loss: 7.889  time: 0.2147(74.52)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:54] lb.utils.events INFO:  eta: 0:02:20  iteration: 345/1000  consumed samples: 5520  total_loss: 7.889  time: 0.2147(74.52)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:54] lb.utils.events INFO:  eta: 0:02:20  iteration: 346/1000  consumed samples: 5536  total_loss: 7.888  time: 0.2147(74.52)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:55] lb.utils.events INFO:  eta: 0:02:20  iteration: 347/1000  consumed samples: 5552  total_loss: 7.885  time: 0.2147(74.51)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:55] lb.utils.events INFO:  eta: 0:02:20  iteration: 348/1000  consumed samples: 5568  total_loss: 7.881  time: 0.2147(74.51)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:43:55] lb.utils.events INFO:  eta: 0:02:20  iteration: 349/1000  consumed samples: 5584  total_loss: 7.881  time: 0.2147(74.51)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:55] lb.utils.events INFO:  eta: 0:02:19  iteration: 350/1000  consumed samples: 5600  total_loss: 7.881  time: 0.2148(74.50)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:55] lb.utils.events INFO:  eta: 0:02:19  iteration: 351/1000  consumed samples: 5616  total_loss: 7.875  time: 0.2148(74.50)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:56] lb.utils.events INFO:  eta: 0:02:19  iteration: 352/1000  consumed samples: 5632  total_loss: 7.875  time: 0.2148(74.49)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:56] lb.utils.events INFO:  eta: 0:02:19  iteration: 353/1000  consumed samples: 5648  total_loss: 7.875  time: 0.2148(74.49)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:56] lb.utils.events INFO:  eta: 0:02:19  iteration: 354/1000  consumed samples: 5664  total_loss: 7.875  time: 0.2148(74.48)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:56] lb.utils.events INFO:  eta: 0:02:18  iteration: 355/1000  consumed samples: 5680  total_loss: 7.872  time: 0.2148(74.48)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:57] lb.utils.events INFO:  eta: 0:02:18  iteration: 356/1000  consumed samples: 5696  total_loss: 7.871  time: 0.2149(74.47)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:57] lb.utils.events INFO:  eta: 0:02:18  iteration: 357/1000  consumed samples: 5712  total_loss: 7.868  time: 0.2149(74.47)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:57] lb.utils.events INFO:  eta: 0:02:18  iteration: 358/1000  consumed samples: 5728  total_loss: 7.865  time: 0.2149(74.47)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:57] lb.utils.events INFO:  eta: 0:02:18  iteration: 359/1000  consumed samples: 5744  total_loss: 7.862  time: 0.2149(74.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:58] lb.utils.events INFO:  eta: 0:02:17  iteration: 360/1000  consumed samples: 5760  total_loss: 7.859  time: 0.2149(74.46)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:43:58] lb.utils.events INFO:  eta: 0:02:17  iteration: 361/1000  consumed samples: 5776  total_loss: 7.857  time: 0.2149(74.46)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:43:58] lb.utils.events INFO:  eta: 0:02:17  iteration: 362/1000  consumed samples: 5792  total_loss: 7.855  time: 0.2149(74.45)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:58] lb.utils.events INFO:  eta: 0:02:17  iteration: 363/1000  consumed samples: 5808  total_loss: 7.854  time: 0.2149(74.45)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:58] lb.utils.events INFO:  eta: 0:02:17  iteration: 364/1000  consumed samples: 5824  total_loss: 7.854  time: 0.2149(74.44)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:59] lb.utils.events INFO:  eta: 0:02:16  iteration: 365/1000  consumed samples: 5840  total_loss: 7.854  time: 0.2149(74.44)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:59] lb.utils.events INFO:  eta: 0:02:16  iteration: 366/1000  consumed samples: 5856  total_loss: 7.853  time: 0.2149(74.44)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:59] lb.utils.events INFO:  eta: 0:02:16  iteration: 367/1000  consumed samples: 5872  total_loss: 7.851  time: 0.2149(74.44)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:43:59] lb.utils.events INFO:  eta: 0:02:16  iteration: 368/1000  consumed samples: 5888  total_loss: 7.85  time: 0.2150(74.43)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:00] lb.utils.events INFO:  eta: 0:02:15  iteration: 369/1000  consumed samples: 5904  total_loss: 7.849  time: 0.2150(74.43)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:00] lb.utils.events INFO:  eta: 0:02:15  iteration: 370/1000  consumed samples: 5920  total_loss: 7.846  time: 0.2150(74.42)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:00] lb.utils.events INFO:  eta: 0:02:15  iteration: 371/1000  consumed samples: 5936  total_loss: 7.843  time: 0.2150(74.42)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:00] lb.utils.events INFO:  eta: 0:02:15  iteration: 372/1000  consumed samples: 5952  total_loss: 7.842  time: 0.2150(74.41)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:00] lb.utils.events INFO:  eta: 0:02:15  iteration: 373/1000  consumed samples: 5968  total_loss: 7.839  time: 0.2150(74.41)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:01] lb.utils.events INFO:  eta: 0:02:14  iteration: 374/1000  consumed samples: 5984  total_loss: 7.836  time: 0.2150(74.41)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:01] lb.utils.events INFO:  eta: 0:02:14  iteration: 375/1000  consumed samples: 6000  total_loss: 7.839  time: 0.2150(74.41)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:01] lb.utils.events INFO:  eta: 0:02:14  iteration: 376/1000  consumed samples: 6016  total_loss: 7.836  time: 0.2150(74.41)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:01] lb.utils.events INFO:  eta: 0:02:14  iteration: 377/1000  consumed samples: 6032  total_loss: 7.839  time: 0.2150(74.40)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:02] lb.utils.events INFO:  eta: 0:02:14  iteration: 378/1000  consumed samples: 6048  total_loss: 7.836  time: 0.2151(74.40)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:02] lb.utils.events INFO:  eta: 0:02:13  iteration: 379/1000  consumed samples: 6064  total_loss: 7.836  time: 0.2151(74.39)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:02] lb.utils.events INFO:  eta: 0:02:13  iteration: 380/1000  consumed samples: 6080  total_loss: 7.836  time: 0.2151(74.39)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:02] lb.utils.events INFO:  eta: 0:02:13  iteration: 381/1000  consumed samples: 6096  total_loss: 7.836  time: 0.2151(74.38)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:03] lb.utils.events INFO:  eta: 0:02:13  iteration: 382/1000  consumed samples: 6112  total_loss: 7.836  time: 0.2151(74.40)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:03] lb.utils.events INFO:  eta: 0:02:12  iteration: 383/1000  consumed samples: 6128  total_loss: 7.835  time: 0.2151(74.39)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:03] lb.utils.events INFO:  eta: 0:02:12  iteration: 384/1000  consumed samples: 6144  total_loss: 7.835  time: 0.2151(74.39)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:03] lb.utils.events INFO:  eta: 0:02:12  iteration: 385/1000  consumed samples: 6160  total_loss: 7.834  time: 0.2151(74.38)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:03] lb.utils.events INFO:  eta: 0:02:12  iteration: 386/1000  consumed samples: 6176  total_loss: 7.833  time: 0.2151(74.37)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:04] lb.utils.events INFO:  eta: 0:02:12  iteration: 387/1000  consumed samples: 6192  total_loss: 7.833  time: 0.2152(74.37)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:04] lb.utils.events INFO:  eta: 0:02:11  iteration: 388/1000  consumed samples: 6208  total_loss: 7.833  time: 0.2152(74.36)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:04] lb.utils.events INFO:  eta: 0:02:11  iteration: 389/1000  consumed samples: 6224  total_loss: 7.832  time: 0.2152(74.35)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:04] lb.utils.events INFO:  eta: 0:02:11  iteration: 390/1000  consumed samples: 6240  total_loss: 7.831  time: 0.2152(74.35)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:05] lb.utils.events INFO:  eta: 0:02:11  iteration: 391/1000  consumed samples: 6256  total_loss: 7.83  time: 0.2152(74.35)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:05] lb.utils.events INFO:  eta: 0:02:11  iteration: 392/1000  consumed samples: 6272  total_loss: 7.83  time: 0.2152(74.35)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:05] lb.utils.events INFO:  eta: 0:02:10  iteration: 393/1000  consumed samples: 6288  total_loss: 7.829  time: 0.2152(74.34)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:05] lb.utils.events INFO:  eta: 0:02:10  iteration: 394/1000  consumed samples: 6304  total_loss: 7.829  time: 0.2152(74.34)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:06] lb.utils.events INFO:  eta: 0:02:10  iteration: 395/1000  consumed samples: 6320  total_loss: 7.829  time: 0.2153(74.33)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:06] lb.utils.events INFO:  eta: 0:02:10  iteration: 396/1000  consumed samples: 6336  total_loss: 7.827  time: 0.2153(74.33)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:06] lb.utils.events INFO:  eta: 0:02:10  iteration: 397/1000  consumed samples: 6352  total_loss: 7.825  time: 0.2153(74.32)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:06] lb.utils.events INFO:  eta: 0:02:09  iteration: 398/1000  consumed samples: 6368  total_loss: 7.823  time: 0.2153(74.32)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:06] lb.utils.events INFO:  eta: 0:02:09  iteration: 399/1000  consumed samples: 6384  total_loss: 7.82  time: 0.2153(74.31)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:07] lb.utils.events INFO:  eta: 0:02:09  iteration: 400/1000  consumed samples: 6400  total_loss: 7.82  time: 0.2153(74.31)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:07] lb.utils.events INFO:  eta: 0:02:09  iteration: 401/1000  consumed samples: 6416  total_loss: 7.82  time: 0.2153(74.30)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:07] lb.utils.events INFO:  eta: 0:02:09  iteration: 402/1000  consumed samples: 6432  total_loss: 7.82  time: 0.2153(74.30)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:07] lb.utils.events INFO:  eta: 0:02:08  iteration: 403/1000  consumed samples: 6448  total_loss: 7.817  time: 0.2154(74.30)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:08] lb.utils.events INFO:  eta: 0:02:08  iteration: 404/1000  consumed samples: 6464  total_loss: 7.817  time: 0.2154(74.29)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:08] lb.utils.events INFO:  eta: 0:02:08  iteration: 405/1000  consumed samples: 6480  total_loss: 7.817  time: 0.2154(74.29)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:08] lb.utils.events INFO:  eta: 0:02:08  iteration: 406/1000  consumed samples: 6496  total_loss: 7.816  time: 0.2154(74.29)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:08] lb.utils.events INFO:  eta: 0:02:08  iteration: 407/1000  consumed samples: 6512  total_loss: 7.816  time: 0.2154(74.28)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:08] lb.utils.events INFO:  eta: 0:02:07  iteration: 408/1000  consumed samples: 6528  total_loss: 7.816  time: 0.2154(74.28)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:09] lb.utils.events INFO:  eta: 0:02:07  iteration: 409/1000  consumed samples: 6544  total_loss: 7.816  time: 0.2154(74.27)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:09] lb.utils.events INFO:  eta: 0:02:07  iteration: 410/1000  consumed samples: 6560  total_loss: 7.816  time: 0.2154(74.27)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:44:09] lb.utils.events INFO:  eta: 0:02:07  iteration: 411/1000  consumed samples: 6576  total_loss: 7.816  time: 0.2154(74.26)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:09] lb.utils.events INFO:  eta: 0:02:07  iteration: 412/1000  consumed samples: 6592  total_loss: 7.815  time: 0.2154(74.26)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:10] lb.utils.events INFO:  eta: 0:02:06  iteration: 413/1000  consumed samples: 6608  total_loss: 7.814  time: 0.2155(74.26)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:10] lb.utils.events INFO:  eta: 0:02:06  iteration: 414/1000  consumed samples: 6624  total_loss: 7.814  time: 0.2155(74.26)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:10] lb.utils.events INFO:  eta: 0:02:06  iteration: 415/1000  consumed samples: 6640  total_loss: 7.813  time: 0.2155(74.25)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:10] lb.utils.events INFO:  eta: 0:02:06  iteration: 416/1000  consumed samples: 6656  total_loss: 7.813  time: 0.2155(74.24)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:11] lb.utils.events INFO:  eta: 0:02:05  iteration: 417/1000  consumed samples: 6672  total_loss: 7.812  time: 0.2155(74.24)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:11] lb.utils.events INFO:  eta: 0:02:05  iteration: 418/1000  consumed samples: 6688  total_loss: 7.812  time: 0.2155(74.24)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:11] lb.utils.events INFO:  eta: 0:02:05  iteration: 419/1000  consumed samples: 6704  total_loss: 7.81  time: 0.2155(74.24)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:11] lb.utils.events INFO:  eta: 0:02:05  iteration: 420/1000  consumed samples: 6720  total_loss: 7.808  time: 0.2155(74.24)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:11] lb.utils.events INFO:  eta: 0:02:05  iteration: 421/1000  consumed samples: 6736  total_loss: 7.808  time: 0.2155(74.23)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:12] lb.utils.events INFO:  eta: 0:02:04  iteration: 422/1000  consumed samples: 6752  total_loss: 7.807  time: 0.2156(74.23)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:12] lb.utils.events INFO:  eta: 0:02:04  iteration: 423/1000  consumed samples: 6768  total_loss: 7.803  time: 0.2156(74.22)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:12] lb.utils.events INFO:  eta: 0:02:04  iteration: 424/1000  consumed samples: 6784  total_loss: 7.797  time: 0.2156(74.22)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:44:12] lb.utils.events INFO:  eta: 0:02:04  iteration: 425/1000  consumed samples: 6800  total_loss: 7.797  time: 0.2156(74.22)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:13] lb.utils.events INFO:  eta: 0:02:04  iteration: 426/1000  consumed samples: 6816  total_loss: 7.803  time: 0.2156(74.22)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:44:13] lb.utils.events INFO:  eta: 0:02:03  iteration: 427/1000  consumed samples: 6832  total_loss: 7.797  time: 0.2156(74.21)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:44:13] lb.utils.events INFO:  eta: 0:02:03  iteration: 428/1000  consumed samples: 6848  total_loss: 7.794  time: 0.2156(74.21)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:44:13] lb.utils.events INFO:  eta: 0:02:03  iteration: 429/1000  consumed samples: 6864  total_loss: 7.794  time: 0.2156(74.20)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:44:14] lb.utils.events INFO:  eta: 0:02:03  iteration: 430/1000  consumed samples: 6880  total_loss: 7.794  time: 0.2156(74.20)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:14] lb.utils.events INFO:  eta: 0:02:02  iteration: 431/1000  consumed samples: 6896  total_loss: 7.794  time: 0.2156(74.21)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:14] lb.utils.events INFO:  eta: 0:02:02  iteration: 432/1000  consumed samples: 6912  total_loss: 7.794  time: 0.2156(74.21)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:14] lb.utils.events INFO:  eta: 0:02:02  iteration: 433/1000  consumed samples: 6928  total_loss: 7.794  time: 0.2156(74.20)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:14] lb.utils.events INFO:  eta: 0:02:02  iteration: 434/1000  consumed samples: 6944  total_loss: 7.793  time: 0.2156(74.19)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:15] lb.utils.events INFO:  eta: 0:02:02  iteration: 435/1000  consumed samples: 6960  total_loss: 7.793  time: 0.2157(74.19)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:15] lb.utils.events INFO:  eta: 0:02:01  iteration: 436/1000  consumed samples: 6976  total_loss: 7.793  time: 0.2157(74.19)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:15] lb.utils.events INFO:  eta: 0:02:01  iteration: 437/1000  consumed samples: 6992  total_loss: 7.792  time: 0.2157(74.19)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:44:15] lb.utils.events INFO:  eta: 0:02:01  iteration: 438/1000  consumed samples: 7008  total_loss: 7.791  time: 0.2157(74.19)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:16] lb.utils.events INFO:  eta: 0:02:01  iteration: 439/1000  consumed samples: 7024  total_loss: 7.789  time: 0.2157(74.18)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:16] lb.utils.events INFO:  eta: 0:02:01  iteration: 440/1000  consumed samples: 7040  total_loss: 7.789  time: 0.2157(74.18)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:16] lb.utils.events INFO:  eta: 0:02:00  iteration: 441/1000  consumed samples: 7056  total_loss: 7.789  time: 0.2157(74.17)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:16] lb.utils.events INFO:  eta: 0:02:00  iteration: 442/1000  consumed samples: 7072  total_loss: 7.789  time: 0.2157(74.17)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:17] lb.utils.events INFO:  eta: 0:02:00  iteration: 443/1000  consumed samples: 7088  total_loss: 7.787  time: 0.2157(74.17)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:17] lb.utils.events INFO:  eta: 0:02:00  iteration: 444/1000  consumed samples: 7104  total_loss: 7.786  time: 0.2157(74.17)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:17] lb.utils.events INFO:  eta: 0:02:00  iteration: 445/1000  consumed samples: 7120  total_loss: 7.785  time: 0.2157(74.16)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:17] lb.utils.events INFO:  eta: 0:01:59  iteration: 446/1000  consumed samples: 7136  total_loss: 7.784  time: 0.2157(74.16)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:17] lb.utils.events INFO:  eta: 0:01:59  iteration: 447/1000  consumed samples: 7152  total_loss: 7.783  time: 0.2157(74.16)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:18] lb.utils.events INFO:  eta: 0:01:59  iteration: 448/1000  consumed samples: 7168  total_loss: 7.781  time: 0.2157(74.16)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:18] lb.utils.events INFO:  eta: 0:01:59  iteration: 449/1000  consumed samples: 7184  total_loss: 7.779  time: 0.2157(74.16)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:18] lb.utils.events INFO:  eta: 0:01:58  iteration: 450/1000  consumed samples: 7200  total_loss: 7.779  time: 0.2158(74.16)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:18] lb.utils.events INFO:  eta: 0:01:58  iteration: 451/1000  consumed samples: 7216  total_loss: 7.779  time: 0.2158(74.15)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:19] lb.utils.events INFO:  eta: 0:01:58  iteration: 452/1000  consumed samples: 7232  total_loss: 7.778  time: 0.2158(74.15)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:19] lb.utils.events INFO:  eta: 0:01:58  iteration: 453/1000  consumed samples: 7248  total_loss: 7.777  time: 0.2158(74.15)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:19] lb.utils.events INFO:  eta: 0:01:58  iteration: 454/1000  consumed samples: 7264  total_loss: 7.776  time: 0.2158(74.16)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:19] lb.utils.events INFO:  eta: 0:01:57  iteration: 455/1000  consumed samples: 7280  total_loss: 7.776  time: 0.2158(74.15)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:19] lb.utils.events INFO:  eta: 0:01:57  iteration: 456/1000  consumed samples: 7296  total_loss: 7.775  time: 0.2157(74.16)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:20] lb.utils.events INFO:  eta: 0:01:57  iteration: 457/1000  consumed samples: 7312  total_loss: 7.774  time: 0.2158(74.16)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:20] lb.utils.events INFO:  eta: 0:01:57  iteration: 458/1000  consumed samples: 7328  total_loss: 7.773  time: 0.2158(74.16)  data_time: 0.0048  lr: 9.90e-05  
[01/19 14:44:20] lb.utils.events INFO:  eta: 0:01:56  iteration: 459/1000  consumed samples: 7344  total_loss: 7.771  time: 0.2158(74.16)  data_time: 0.0049  lr: 9.90e-05  
[01/19 14:44:20] lb.utils.events INFO:  eta: 0:01:56  iteration: 460/1000  consumed samples: 7360  total_loss: 7.77  time: 0.2158(74.16)  data_time: 0.0049  lr: 9.90e-05  
[01/19 14:44:21] lb.utils.events INFO:  eta: 0:01:56  iteration: 461/1000  consumed samples: 7376  total_loss: 7.77  time: 0.2158(74.16)  data_time: 0.0050  lr: 9.90e-05  
[01/19 14:44:21] lb.utils.events INFO:  eta: 0:01:56  iteration: 462/1000  consumed samples: 7392  total_loss: 7.77  time: 0.2158(74.15)  data_time: 0.0051  lr: 9.90e-05  
[01/19 14:44:21] lb.utils.events INFO:  eta: 0:01:56  iteration: 463/1000  consumed samples: 7408  total_loss: 7.77  time: 0.2158(74.15)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:44:21] lb.utils.events INFO:  eta: 0:01:55  iteration: 464/1000  consumed samples: 7424  total_loss: 7.768  time: 0.2158(74.14)  data_time: 0.0052  lr: 9.90e-05  
[01/19 14:44:22] lb.utils.events INFO:  eta: 0:01:55  iteration: 465/1000  consumed samples: 7440  total_loss: 7.768  time: 0.2158(74.14)  data_time: 0.0053  lr: 9.90e-05  
[01/19 14:44:22] lb.utils.events INFO:  eta: 0:01:55  iteration: 466/1000  consumed samples: 7456  total_loss: 7.768  time: 0.2158(74.14)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:22] lb.utils.events INFO:  eta: 0:01:55  iteration: 467/1000  consumed samples: 7472  total_loss: 7.765  time: 0.2158(74.14)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:44:22] lb.utils.events INFO:  eta: 0:01:55  iteration: 468/1000  consumed samples: 7488  total_loss: 7.764  time: 0.2158(74.14)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:44:22] lb.utils.events INFO:  eta: 0:01:54  iteration: 469/1000  consumed samples: 7504  total_loss: 7.764  time: 0.2158(74.14)  data_time: 0.0055  lr: 9.90e-05  
[01/19 14:44:23] lb.utils.events INFO:  eta: 0:01:54  iteration: 470/1000  consumed samples: 7520  total_loss: 7.764  time: 0.2158(74.13)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:23] lb.utils.events INFO:  eta: 0:01:54  iteration: 471/1000  consumed samples: 7536  total_loss: 7.763  time: 0.2158(74.13)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:23] lb.utils.events INFO:  eta: 0:01:54  iteration: 472/1000  consumed samples: 7552  total_loss: 7.762  time: 0.2158(74.13)  data_time: 0.0053  lr: 9.90e-05  
[01/19 14:44:23] lb.utils.events INFO:  eta: 0:01:53  iteration: 473/1000  consumed samples: 7568  total_loss: 7.762  time: 0.2158(74.14)  data_time: 0.0053  lr: 9.90e-05  
[01/19 14:44:24] lb.utils.events INFO:  eta: 0:01:53  iteration: 474/1000  consumed samples: 7584  total_loss: 7.761  time: 0.2158(74.14)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:24] lb.utils.events INFO:  eta: 0:01:53  iteration: 475/1000  consumed samples: 7600  total_loss: 7.76  time: 0.2158(74.13)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:24] lb.utils.events INFO:  eta: 0:01:53  iteration: 476/1000  consumed samples: 7616  total_loss: 7.76  time: 0.2158(74.13)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:24] lb.utils.events INFO:  eta: 0:01:53  iteration: 477/1000  consumed samples: 7632  total_loss: 7.76  time: 0.2158(74.13)  data_time: 0.0054  lr: 9.90e-05  
[01/19 14:44:25] lb.utils.events INFO:  eta: 0:01:52  iteration: 478/1000  consumed samples: 7648  total_loss: 7.759  time: 0.2158(74.13)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:25] lb.utils.events INFO:  eta: 0:01:52  iteration: 479/1000  consumed samples: 7664  total_loss: 7.759  time: 0.2158(74.13)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:44:25] lb.utils.events INFO:  eta: 0:01:52  iteration: 480/1000  consumed samples: 7680  total_loss: 7.758  time: 0.2159(74.12)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:25] lb.utils.events INFO:  eta: 0:01:52  iteration: 481/1000  consumed samples: 7696  total_loss: 7.756  time: 0.2158(74.13)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:25] lb.utils.events INFO:  eta: 0:01:52  iteration: 482/1000  consumed samples: 7712  total_loss: 7.756  time: 0.2159(74.12)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:26] lb.utils.events INFO:  eta: 0:01:51  iteration: 483/1000  consumed samples: 7728  total_loss: 7.755  time: 0.2159(74.12)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:26] lb.utils.events INFO:  eta: 0:01:51  iteration: 484/1000  consumed samples: 7744  total_loss: 7.754  time: 0.2159(74.12)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:44:26] lb.utils.events INFO:  eta: 0:01:51  iteration: 485/1000  consumed samples: 7760  total_loss: 7.754  time: 0.2158(74.13)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:44:26] lb.utils.events INFO:  eta: 0:01:51  iteration: 486/1000  consumed samples: 7776  total_loss: 7.754  time: 0.2159(74.12)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:27] lb.utils.events INFO:  eta: 0:01:50  iteration: 487/1000  consumed samples: 7792  total_loss: 7.752  time: 0.2159(74.13)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:27] lb.utils.events INFO:  eta: 0:01:50  iteration: 488/1000  consumed samples: 7808  total_loss: 7.752  time: 0.2158(74.13)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:27] lb.utils.events INFO:  eta: 0:01:50  iteration: 489/1000  consumed samples: 7824  total_loss: 7.752  time: 0.2158(74.13)  data_time: 0.0042  lr: 9.90e-05  
[01/19 14:44:27] lb.utils.events INFO:  eta: 0:01:50  iteration: 490/1000  consumed samples: 7840  total_loss: 7.752  time: 0.2159(74.12)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:44:28] lb.utils.events INFO:  eta: 0:01:50  iteration: 491/1000  consumed samples: 7856  total_loss: 7.752  time: 0.2159(74.12)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:28] lb.utils.events INFO:  eta: 0:01:49  iteration: 492/1000  consumed samples: 7872  total_loss: 7.75  time: 0.2159(74.12)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:28] lb.utils.events INFO:  eta: 0:01:49  iteration: 493/1000  consumed samples: 7888  total_loss: 7.749  time: 0.2159(74.12)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:44:28] lb.utils.events INFO:  eta: 0:01:49  iteration: 494/1000  consumed samples: 7904  total_loss: 7.748  time: 0.2159(74.12)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:44:28] lb.utils.events INFO:  eta: 0:01:49  iteration: 495/1000  consumed samples: 7920  total_loss: 7.747  time: 0.2159(74.12)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:44:29] lb.utils.events INFO:  eta: 0:01:48  iteration: 496/1000  consumed samples: 7936  total_loss: 7.745  time: 0.2159(74.11)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:29] lb.utils.events INFO:  eta: 0:01:48  iteration: 497/1000  consumed samples: 7952  total_loss: 7.745  time: 0.2159(74.11)  data_time: 0.0041  lr: 9.90e-05  
[01/19 14:44:29] lb.utils.events INFO:  eta: 0:01:48  iteration: 498/1000  consumed samples: 7968  total_loss: 7.745  time: 0.2159(74.11)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:29] lb.utils.events INFO:  eta: 0:01:48  iteration: 499/1000  consumed samples: 7984  total_loss: 7.744  time: 0.2159(74.11)  data_time: 0.0040  lr: 9.90e-05  
[01/19 14:44:30] lb.utils.events INFO:  eta: 0:01:48  iteration: 500/1000  consumed samples: 8000  total_loss: 7.743  time: 0.2159(74.10)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:30] lb.utils.events INFO:  eta: 0:01:47  iteration: 501/1000  consumed samples: 8016  total_loss: 7.737  time: 0.2159(74.10)  data_time: 0.0039  lr: 9.90e-05  
[01/19 14:44:30] lb.utils.events INFO:  eta: 0:01:47  iteration: 502/1000  consumed samples: 8032  total_loss: 7.737  time: 0.2159(74.10)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:44:30] lb.utils.events INFO:  eta: 0:01:47  iteration: 503/1000  consumed samples: 8048  total_loss: 7.73  time: 0.2159(74.10)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:44:30] lb.utils.events INFO:  eta: 0:01:47  iteration: 504/1000  consumed samples: 8064  total_loss: 7.728  time: 0.2160(74.09)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:44:31] lb.utils.events INFO:  eta: 0:01:47  iteration: 505/1000  consumed samples: 8080  total_loss: 7.727  time: 0.2160(74.09)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:44:31] lb.utils.events INFO:  eta: 0:01:46  iteration: 506/1000  consumed samples: 8096  total_loss: 7.727  time: 0.2160(74.08)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:44:31] lb.utils.events INFO:  eta: 0:01:46  iteration: 507/1000  consumed samples: 8112  total_loss: 7.725  time: 0.2160(74.08)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:44:31] lb.utils.events INFO:  eta: 0:01:46  iteration: 508/1000  consumed samples: 8128  total_loss: 7.722  time: 0.2160(74.08)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:32] lb.utils.events INFO:  eta: 0:01:46  iteration: 509/1000  consumed samples: 8144  total_loss: 7.722  time: 0.2160(74.08)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:32] lb.utils.events INFO:  eta: 0:01:45  iteration: 510/1000  consumed samples: 8160  total_loss: 7.722  time: 0.2160(74.08)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:32] lb.utils.events INFO:  eta: 0:01:45  iteration: 511/1000  consumed samples: 8176  total_loss: 7.722  time: 0.2160(74.08)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:32] lb.utils.events INFO:  eta: 0:01:45  iteration: 512/1000  consumed samples: 8192  total_loss: 7.722  time: 0.2160(74.07)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:33] lb.utils.events INFO:  eta: 0:01:45  iteration: 513/1000  consumed samples: 8208  total_loss: 7.722  time: 0.2160(74.07)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:33] lb.utils.events INFO:  eta: 0:01:45  iteration: 514/1000  consumed samples: 8224  total_loss: 7.722  time: 0.2160(74.07)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:33] lb.utils.events INFO:  eta: 0:01:44  iteration: 515/1000  consumed samples: 8240  total_loss: 7.722  time: 0.2160(74.06)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:33] lb.utils.events INFO:  eta: 0:01:44  iteration: 516/1000  consumed samples: 8256  total_loss: 7.722  time: 0.2160(74.06)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:33] lb.utils.events INFO:  eta: 0:01:44  iteration: 517/1000  consumed samples: 8272  total_loss: 7.722  time: 0.2160(74.06)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:34] lb.utils.events INFO:  eta: 0:01:44  iteration: 518/1000  consumed samples: 8288  total_loss: 7.722  time: 0.2160(74.06)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:34] lb.utils.events INFO:  eta: 0:01:44  iteration: 519/1000  consumed samples: 8304  total_loss: 7.721  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:34] lb.utils.events INFO:  eta: 0:01:43  iteration: 520/1000  consumed samples: 8320  total_loss: 7.721  time: 0.2161(74.05)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:34] lb.utils.events INFO:  eta: 0:01:43  iteration: 521/1000  consumed samples: 8336  total_loss: 7.722  time: 0.2161(74.05)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:35] lb.utils.events INFO:  eta: 0:01:43  iteration: 522/1000  consumed samples: 8352  total_loss: 7.721  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:35] lb.utils.events INFO:  eta: 0:01:43  iteration: 523/1000  consumed samples: 8368  total_loss: 7.719  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:35] lb.utils.events INFO:  eta: 0:01:42  iteration: 524/1000  consumed samples: 8384  total_loss: 7.718  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:35] lb.utils.events INFO:  eta: 0:01:42  iteration: 525/1000  consumed samples: 8400  total_loss: 7.717  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:36] lb.utils.events INFO:  eta: 0:01:42  iteration: 526/1000  consumed samples: 8416  total_loss: 7.717  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:36] lb.utils.events INFO:  eta: 0:01:42  iteration: 527/1000  consumed samples: 8432  total_loss: 7.718  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:36] lb.utils.events INFO:  eta: 0:01:42  iteration: 528/1000  consumed samples: 8448  total_loss: 7.717  time: 0.2161(74.04)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:36] lb.utils.events INFO:  eta: 0:01:41  iteration: 529/1000  consumed samples: 8464  total_loss: 7.716  time: 0.2161(74.04)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:36] lb.utils.events INFO:  eta: 0:01:41  iteration: 530/1000  consumed samples: 8480  total_loss: 7.715  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:37] lb.utils.events INFO:  eta: 0:01:41  iteration: 531/1000  consumed samples: 8496  total_loss: 7.715  time: 0.2161(74.05)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:37] lb.utils.events INFO:  eta: 0:01:41  iteration: 532/1000  consumed samples: 8512  total_loss: 7.715  time: 0.2161(74.04)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:37] lb.utils.events INFO:  eta: 0:01:41  iteration: 533/1000  consumed samples: 8528  total_loss: 7.715  time: 0.2161(74.04)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:37] lb.utils.events INFO:  eta: 0:01:40  iteration: 534/1000  consumed samples: 8544  total_loss: 7.713  time: 0.2161(74.04)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:38] lb.utils.events INFO:  eta: 0:01:40  iteration: 535/1000  consumed samples: 8560  total_loss: 7.713  time: 0.2161(74.03)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:38] lb.utils.events INFO:  eta: 0:01:40  iteration: 536/1000  consumed samples: 8576  total_loss: 7.713  time: 0.2161(74.03)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:38] lb.utils.events INFO:  eta: 0:01:40  iteration: 537/1000  consumed samples: 8592  total_loss: 7.713  time: 0.2161(74.03)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:38] lb.utils.events INFO:  eta: 0:01:39  iteration: 538/1000  consumed samples: 8608  total_loss: 7.715  time: 0.2161(74.03)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:39] lb.utils.events INFO:  eta: 0:01:39  iteration: 539/1000  consumed samples: 8624  total_loss: 7.713  time: 0.2162(74.02)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:39] lb.utils.events INFO:  eta: 0:01:39  iteration: 540/1000  consumed samples: 8640  total_loss: 7.712  time: 0.2162(74.02)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:39] lb.utils.events INFO:  eta: 0:01:39  iteration: 541/1000  consumed samples: 8656  total_loss: 7.712  time: 0.2162(74.02)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:39] lb.utils.events INFO:  eta: 0:01:39  iteration: 542/1000  consumed samples: 8672  total_loss: 7.709  time: 0.2162(74.02)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:39] lb.utils.events INFO:  eta: 0:01:38  iteration: 543/1000  consumed samples: 8688  total_loss: 7.704  time: 0.2162(74.01)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:40] lb.utils.events INFO:  eta: 0:01:38  iteration: 544/1000  consumed samples: 8704  total_loss: 7.704  time: 0.2162(74.01)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:40] lb.utils.events INFO:  eta: 0:01:38  iteration: 545/1000  consumed samples: 8720  total_loss: 7.701  time: 0.2162(74.00)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:40] lb.utils.events INFO:  eta: 0:01:38  iteration: 546/1000  consumed samples: 8736  total_loss: 7.701  time: 0.2162(74.00)  data_time: 0.0057  lr: 9.90e-05  
[01/19 14:44:40] lb.utils.events INFO:  eta: 0:01:37  iteration: 547/1000  consumed samples: 8752  total_loss: 7.701  time: 0.2162(74.00)  data_time: 0.0057  lr: 9.90e-05  
[01/19 14:44:41] lb.utils.events INFO:  eta: 0:01:37  iteration: 548/1000  consumed samples: 8768  total_loss: 7.7  time: 0.2162(74.00)  data_time: 0.0057  lr: 9.90e-05  
[01/19 14:44:41] lb.utils.events INFO:  eta: 0:01:37  iteration: 549/1000  consumed samples: 8784  total_loss: 7.7  time: 0.2162(74.01)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:41] lb.utils.events INFO:  eta: 0:01:37  iteration: 550/1000  consumed samples: 8800  total_loss: 7.7  time: 0.2162(74.01)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:41] lb.utils.events INFO:  eta: 0:01:37  iteration: 551/1000  consumed samples: 8816  total_loss: 7.697  time: 0.2162(74.00)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:41] lb.utils.events INFO:  eta: 0:01:36  iteration: 552/1000  consumed samples: 8832  total_loss: 7.697  time: 0.2162(74.00)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:42] lb.utils.events INFO:  eta: 0:01:36  iteration: 553/1000  consumed samples: 8848  total_loss: 7.695  time: 0.2162(74.00)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:42] lb.utils.events INFO:  eta: 0:01:36  iteration: 554/1000  consumed samples: 8864  total_loss: 7.694  time: 0.2162(74.00)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:42] lb.utils.events INFO:  eta: 0:01:36  iteration: 555/1000  consumed samples: 8880  total_loss: 7.694  time: 0.2162(73.99)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:42] lb.utils.events INFO:  eta: 0:01:36  iteration: 556/1000  consumed samples: 8896  total_loss: 7.694  time: 0.2162(73.99)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:43] lb.utils.events INFO:  eta: 0:01:35  iteration: 557/1000  consumed samples: 8912  total_loss: 7.693  time: 0.2162(73.99)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:43] lb.utils.events INFO:  eta: 0:01:35  iteration: 558/1000  consumed samples: 8928  total_loss: 7.693  time: 0.2162(73.99)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:43] lb.utils.events INFO:  eta: 0:01:35  iteration: 559/1000  consumed samples: 8944  total_loss: 7.691  time: 0.2162(73.99)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:43] lb.utils.events INFO:  eta: 0:01:35  iteration: 560/1000  consumed samples: 8960  total_loss: 7.691  time: 0.2162(73.99)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:44] lb.utils.events INFO:  eta: 0:01:34  iteration: 561/1000  consumed samples: 8976  total_loss: 7.688  time: 0.2163(73.98)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:44] lb.utils.events INFO:  eta: 0:01:34  iteration: 562/1000  consumed samples: 8992  total_loss: 7.688  time: 0.2163(73.98)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:44] lb.utils.events INFO:  eta: 0:01:34  iteration: 563/1000  consumed samples: 9008  total_loss: 7.685  time: 0.2163(73.98)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:44] lb.utils.events INFO:  eta: 0:01:34  iteration: 564/1000  consumed samples: 9024  total_loss: 7.688  time: 0.2163(73.98)  data_time: 0.0058  lr: 9.90e-05  
[01/19 14:44:44] lb.utils.events INFO:  eta: 0:01:34  iteration: 565/1000  consumed samples: 9040  total_loss: 7.685  time: 0.2163(73.98)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:45] lb.utils.events INFO:  eta: 0:01:33  iteration: 566/1000  consumed samples: 9056  total_loss: 7.679  time: 0.2163(73.98)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:45] lb.utils.events INFO:  eta: 0:01:33  iteration: 567/1000  consumed samples: 9072  total_loss: 7.679  time: 0.2163(73.98)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:45] lb.utils.events INFO:  eta: 0:01:33  iteration: 568/1000  consumed samples: 9088  total_loss: 7.673  time: 0.2163(73.98)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:45] lb.utils.events INFO:  eta: 0:01:33  iteration: 569/1000  consumed samples: 9104  total_loss: 7.673  time: 0.2163(73.97)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:46] lb.utils.events INFO:  eta: 0:01:33  iteration: 570/1000  consumed samples: 9120  total_loss: 7.673  time: 0.2163(73.98)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:46] lb.utils.events INFO:  eta: 0:01:32  iteration: 571/1000  consumed samples: 9136  total_loss: 7.673  time: 0.2163(73.97)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:46] lb.utils.events INFO:  eta: 0:01:32  iteration: 572/1000  consumed samples: 9152  total_loss: 7.673  time: 0.2163(73.97)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:46] lb.utils.events INFO:  eta: 0:01:32  iteration: 573/1000  consumed samples: 9168  total_loss: 7.679  time: 0.2163(73.97)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:47] lb.utils.events INFO:  eta: 0:01:32  iteration: 574/1000  consumed samples: 9184  total_loss: 7.679  time: 0.2163(73.97)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:47] lb.utils.events INFO:  eta: 0:01:31  iteration: 575/1000  consumed samples: 9200  total_loss: 7.673  time: 0.2163(73.96)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:47] lb.utils.events INFO:  eta: 0:01:31  iteration: 576/1000  consumed samples: 9216  total_loss: 7.67  time: 0.2163(73.96)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:47] lb.utils.events INFO:  eta: 0:01:31  iteration: 577/1000  consumed samples: 9232  total_loss: 7.67  time: 0.2163(73.96)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:47] lb.utils.events INFO:  eta: 0:01:31  iteration: 578/1000  consumed samples: 9248  total_loss: 7.67  time: 0.2163(73.96)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:48] lb.utils.events INFO:  eta: 0:01:31  iteration: 579/1000  consumed samples: 9264  total_loss: 7.669  time: 0.2164(73.95)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:48] lb.utils.events INFO:  eta: 0:01:30  iteration: 580/1000  consumed samples: 9280  total_loss: 7.667  time: 0.2164(73.95)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:48] lb.utils.events INFO:  eta: 0:01:30  iteration: 581/1000  consumed samples: 9296  total_loss: 7.664  time: 0.2164(73.95)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:44:48] lb.utils.events INFO:  eta: 0:01:30  iteration: 582/1000  consumed samples: 9312  total_loss: 7.664  time: 0.2164(73.95)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:49] lb.utils.events INFO:  eta: 0:01:30  iteration: 583/1000  consumed samples: 9328  total_loss: 7.664  time: 0.2164(73.94)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:49] lb.utils.events INFO:  eta: 0:01:30  iteration: 584/1000  consumed samples: 9344  total_loss: 7.663  time: 0.2164(73.94)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:49] lb.utils.events INFO:  eta: 0:01:29  iteration: 585/1000  consumed samples: 9360  total_loss: 7.663  time: 0.2164(73.94)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:49] lb.utils.events INFO:  eta: 0:01:29  iteration: 586/1000  consumed samples: 9376  total_loss: 7.663  time: 0.2164(73.94)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:50] lb.utils.events INFO:  eta: 0:01:29  iteration: 587/1000  consumed samples: 9392  total_loss: 7.661  time: 0.2164(73.93)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:50] lb.utils.events INFO:  eta: 0:01:29  iteration: 588/1000  consumed samples: 9408  total_loss: 7.663  time: 0.2164(73.93)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:44:50] lb.utils.events INFO:  eta: 0:01:28  iteration: 589/1000  consumed samples: 9424  total_loss: 7.663  time: 0.2164(73.93)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:50] lb.utils.events INFO:  eta: 0:01:28  iteration: 590/1000  consumed samples: 9440  total_loss: 7.661  time: 0.2164(73.93)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:50] lb.utils.events INFO:  eta: 0:01:28  iteration: 591/1000  consumed samples: 9456  total_loss: 7.661  time: 0.2164(73.92)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:51] lb.utils.events INFO:  eta: 0:01:28  iteration: 592/1000  consumed samples: 9472  total_loss: 7.659  time: 0.2165(73.92)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:51] lb.utils.events INFO:  eta: 0:01:28  iteration: 593/1000  consumed samples: 9488  total_loss: 7.659  time: 0.2165(73.92)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:51] lb.utils.events INFO:  eta: 0:01:27  iteration: 594/1000  consumed samples: 9504  total_loss: 7.659  time: 0.2165(73.92)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:51] lb.utils.events INFO:  eta: 0:01:27  iteration: 595/1000  consumed samples: 9520  total_loss: 7.659  time: 0.2165(73.91)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:52] lb.utils.events INFO:  eta: 0:01:27  iteration: 596/1000  consumed samples: 9536  total_loss: 7.659  time: 0.2165(73.91)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:52] lb.utils.events INFO:  eta: 0:01:27  iteration: 597/1000  consumed samples: 9552  total_loss: 7.659  time: 0.2165(73.91)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:52] lb.utils.events INFO:  eta: 0:01:26  iteration: 598/1000  consumed samples: 9568  total_loss: 7.659  time: 0.2165(73.91)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:52] lb.utils.events INFO:  eta: 0:01:26  iteration: 599/1000  consumed samples: 9584  total_loss: 7.656  time: 0.2165(73.90)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:52] lb.utils.events INFO:  eta: 0:01:26  iteration: 600/1000  consumed samples: 9600  total_loss: 7.654  time: 0.2165(73.90)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:53] lb.utils.events INFO:  eta: 0:01:26  iteration: 601/1000  consumed samples: 9616  total_loss: 7.654  time: 0.2165(73.90)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:53] lb.utils.events INFO:  eta: 0:01:26  iteration: 602/1000  consumed samples: 9632  total_loss: 7.653  time: 0.2165(73.90)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:53] lb.utils.events INFO:  eta: 0:01:25  iteration: 603/1000  consumed samples: 9648  total_loss: 7.654  time: 0.2165(73.89)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:53] lb.utils.events INFO:  eta: 0:01:25  iteration: 604/1000  consumed samples: 9664  total_loss: 7.653  time: 0.2165(73.89)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:54] lb.utils.events INFO:  eta: 0:01:25  iteration: 605/1000  consumed samples: 9680  total_loss: 7.652  time: 0.2165(73.89)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:44:54] lb.utils.events INFO:  eta: 0:01:25  iteration: 606/1000  consumed samples: 9696  total_loss: 7.649  time: 0.2165(73.89)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:44:54] lb.utils.events INFO:  eta: 0:01:25  iteration: 607/1000  consumed samples: 9712  total_loss: 7.649  time: 0.2166(73.88)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:54] lb.utils.events INFO:  eta: 0:01:24  iteration: 608/1000  consumed samples: 9728  total_loss: 7.649  time: 0.2166(73.88)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:55] lb.utils.events INFO:  eta: 0:01:24  iteration: 609/1000  consumed samples: 9744  total_loss: 7.648  time: 0.2166(73.88)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:55] lb.utils.events INFO:  eta: 0:01:24  iteration: 610/1000  consumed samples: 9760  total_loss: 7.647  time: 0.2166(73.88)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:44:55] lb.utils.events INFO:  eta: 0:01:24  iteration: 611/1000  consumed samples: 9776  total_loss: 7.646  time: 0.2166(73.88)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:55] lb.utils.events INFO:  eta: 0:01:23  iteration: 612/1000  consumed samples: 9792  total_loss: 7.647  time: 0.2166(73.88)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:44:55] lb.utils.events INFO:  eta: 0:01:23  iteration: 613/1000  consumed samples: 9808  total_loss: 7.648  time: 0.2166(73.88)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:56] lb.utils.events INFO:  eta: 0:01:23  iteration: 614/1000  consumed samples: 9824  total_loss: 7.647  time: 0.2166(73.88)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:56] lb.utils.events INFO:  eta: 0:01:23  iteration: 615/1000  consumed samples: 9840  total_loss: 7.646  time: 0.2166(73.88)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:56] lb.utils.events INFO:  eta: 0:01:23  iteration: 616/1000  consumed samples: 9856  total_loss: 7.646  time: 0.2166(73.87)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:44:56] lb.utils.events INFO:  eta: 0:01:22  iteration: 617/1000  consumed samples: 9872  total_loss: 7.645  time: 0.2166(73.87)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:57] lb.utils.events INFO:  eta: 0:01:22  iteration: 618/1000  consumed samples: 9888  total_loss: 7.645  time: 0.2166(73.87)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:57] lb.utils.events INFO:  eta: 0:01:22  iteration: 619/1000  consumed samples: 9904  total_loss: 7.645  time: 0.2166(73.88)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:57] lb.utils.events INFO:  eta: 0:01:22  iteration: 620/1000  consumed samples: 9920  total_loss: 7.645  time: 0.2166(73.88)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:44:57] lb.utils.events INFO:  eta: 0:01:22  iteration: 621/1000  consumed samples: 9936  total_loss: 7.644  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:58] lb.utils.events INFO:  eta: 0:01:21  iteration: 622/1000  consumed samples: 9952  total_loss: 7.645  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:58] lb.utils.events INFO:  eta: 0:01:21  iteration: 623/1000  consumed samples: 9968  total_loss: 7.644  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:58] lb.utils.events INFO:  eta: 0:01:21  iteration: 624/1000  consumed samples: 9984  total_loss: 7.643  time: 0.2166(73.87)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:58] lb.utils.events INFO:  eta: 0:01:21  iteration: 625/1000  consumed samples: 10000  total_loss: 7.643  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:44:58] lb.utils.events INFO:  eta: 0:01:20  iteration: 626/1000  consumed samples: 10016  total_loss: 7.639  time: 0.2166(73.88)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:59] lb.utils.events INFO:  eta: 0:01:20  iteration: 627/1000  consumed samples: 10032  total_loss: 7.639  time: 0.2166(73.87)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:59] lb.utils.events INFO:  eta: 0:01:20  iteration: 628/1000  consumed samples: 10048  total_loss: 7.639  time: 0.2166(73.87)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:44:59] lb.utils.events INFO:  eta: 0:01:20  iteration: 629/1000  consumed samples: 10064  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:44:59] lb.utils.events INFO:  eta: 0:01:20  iteration: 630/1000  consumed samples: 10080  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:00] lb.utils.events INFO:  eta: 0:01:19  iteration: 631/1000  consumed samples: 10096  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:00] lb.utils.events INFO:  eta: 0:01:19  iteration: 632/1000  consumed samples: 10112  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:45:00] lb.utils.events INFO:  eta: 0:01:19  iteration: 633/1000  consumed samples: 10128  total_loss: 7.637  time: 0.2166(73.88)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:45:00] lb.utils.events INFO:  eta: 0:01:19  iteration: 634/1000  consumed samples: 10144  total_loss: 7.637  time: 0.2166(73.88)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 635/1000  consumed samples: 10160  total_loss: 7.636  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 636/1000  consumed samples: 10176  total_loss: 7.636  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 637/1000  consumed samples: 10192  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 638/1000  consumed samples: 10208  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 639/1000  consumed samples: 10224  total_loss: 7.637  time: 0.2166(73.87)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:02] lb.utils.events INFO:  eta: 0:01:17  iteration: 640/1000  consumed samples: 10240  total_loss: 7.636  time: 0.2166(73.87)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:45:02] lb.utils.events INFO:  eta: 0:01:17  iteration: 641/1000  consumed samples: 10256  total_loss: 7.636  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:02] lb.utils.events INFO:  eta: 0:01:17  iteration: 642/1000  consumed samples: 10272  total_loss: 7.635  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:02] lb.utils.events INFO:  eta: 0:01:17  iteration: 643/1000  consumed samples: 10288  total_loss: 7.634  time: 0.2166(73.86)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:03] lb.utils.events INFO:  eta: 0:01:17  iteration: 644/1000  consumed samples: 10304  total_loss: 7.63  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:03] lb.utils.events INFO:  eta: 0:01:16  iteration: 645/1000  consumed samples: 10320  total_loss: 7.63  time: 0.2166(73.87)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:03] lb.utils.events INFO:  eta: 0:01:16  iteration: 646/1000  consumed samples: 10336  total_loss: 7.626  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:03] lb.utils.events INFO:  eta: 0:01:16  iteration: 647/1000  consumed samples: 10352  total_loss: 7.625  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:03] lb.utils.events INFO:  eta: 0:01:16  iteration: 648/1000  consumed samples: 10368  total_loss: 7.624  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:04] lb.utils.events INFO:  eta: 0:01:15  iteration: 649/1000  consumed samples: 10384  total_loss: 7.624  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:04] lb.utils.events INFO:  eta: 0:01:15  iteration: 650/1000  consumed samples: 10400  total_loss: 7.624  time: 0.2166(73.88)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:04] lb.utils.events INFO:  eta: 0:01:15  iteration: 651/1000  consumed samples: 10416  total_loss: 7.622  time: 0.2166(73.88)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:04] lb.utils.events INFO:  eta: 0:01:15  iteration: 652/1000  consumed samples: 10432  total_loss: 7.619  time: 0.2166(73.87)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:05] lb.utils.events INFO:  eta: 0:01:15  iteration: 653/1000  consumed samples: 10448  total_loss: 7.619  time: 0.2166(73.87)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:45:05] lb.utils.events INFO:  eta: 0:01:14  iteration: 654/1000  consumed samples: 10464  total_loss: 7.619  time: 0.2166(73.87)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:45:05] lb.utils.events INFO:  eta: 0:01:14  iteration: 655/1000  consumed samples: 10480  total_loss: 7.618  time: 0.2166(73.87)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:45:05] lb.utils.events INFO:  eta: 0:01:14  iteration: 656/1000  consumed samples: 10496  total_loss: 7.619  time: 0.2166(73.86)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:45:06] lb.utils.events INFO:  eta: 0:01:14  iteration: 657/1000  consumed samples: 10512  total_loss: 7.619  time: 0.2166(73.86)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:45:06] lb.utils.events INFO:  eta: 0:01:14  iteration: 658/1000  consumed samples: 10528  total_loss: 7.618  time: 0.2166(73.86)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:06] lb.utils.events INFO:  eta: 0:01:13  iteration: 659/1000  consumed samples: 10544  total_loss: 7.617  time: 0.2166(73.86)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:06] lb.utils.events INFO:  eta: 0:01:13  iteration: 660/1000  consumed samples: 10560  total_loss: 7.617  time: 0.2166(73.86)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:06] lb.utils.events INFO:  eta: 0:01:13  iteration: 661/1000  consumed samples: 10576  total_loss: 7.614  time: 0.2166(73.86)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:07] lb.utils.events INFO:  eta: 0:01:13  iteration: 662/1000  consumed samples: 10592  total_loss: 7.614  time: 0.2166(73.86)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:07] lb.utils.events INFO:  eta: 0:01:12  iteration: 663/1000  consumed samples: 10608  total_loss: 7.614  time: 0.2166(73.86)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:07] lb.utils.events INFO:  eta: 0:01:12  iteration: 664/1000  consumed samples: 10624  total_loss: 7.614  time: 0.2166(73.85)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:07] lb.utils.events INFO:  eta: 0:01:12  iteration: 665/1000  consumed samples: 10640  total_loss: 7.617  time: 0.2166(73.85)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:08] lb.utils.events INFO:  eta: 0:01:12  iteration: 666/1000  consumed samples: 10656  total_loss: 7.614  time: 0.2167(73.85)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:08] lb.utils.events INFO:  eta: 0:01:12  iteration: 667/1000  consumed samples: 10672  total_loss: 7.614  time: 0.2167(73.85)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:08] lb.utils.events INFO:  eta: 0:01:11  iteration: 668/1000  consumed samples: 10688  total_loss: 7.611  time: 0.2167(73.84)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:08] lb.utils.events INFO:  eta: 0:01:11  iteration: 669/1000  consumed samples: 10704  total_loss: 7.611  time: 0.2167(73.84)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:09] lb.utils.events INFO:  eta: 0:01:11  iteration: 670/1000  consumed samples: 10720  total_loss: 7.611  time: 0.2167(73.84)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:09] lb.utils.events INFO:  eta: 0:01:11  iteration: 671/1000  consumed samples: 10736  total_loss: 7.61  time: 0.2167(73.84)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:09] lb.utils.events INFO:  eta: 0:01:10  iteration: 672/1000  consumed samples: 10752  total_loss: 7.609  time: 0.2167(73.83)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:09] lb.utils.events INFO:  eta: 0:01:10  iteration: 673/1000  consumed samples: 10768  total_loss: 7.604  time: 0.2167(73.83)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:09] lb.utils.events INFO:  eta: 0:01:10  iteration: 674/1000  consumed samples: 10784  total_loss: 7.609  time: 0.2167(73.83)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:10] lb.utils.events INFO:  eta: 0:01:10  iteration: 675/1000  consumed samples: 10800  total_loss: 7.609  time: 0.2167(73.83)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:10] lb.utils.events INFO:  eta: 0:01:10  iteration: 676/1000  consumed samples: 10816  total_loss: 7.604  time: 0.2167(73.83)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:10] lb.utils.events INFO:  eta: 0:01:09  iteration: 677/1000  consumed samples: 10832  total_loss: 7.604  time: 0.2167(73.83)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:10] lb.utils.events INFO:  eta: 0:01:09  iteration: 678/1000  consumed samples: 10848  total_loss: 7.604  time: 0.2167(73.82)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:11] lb.utils.events INFO:  eta: 0:01:09  iteration: 679/1000  consumed samples: 10864  total_loss: 7.599  time: 0.2167(73.82)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:11] lb.utils.events INFO:  eta: 0:01:09  iteration: 680/1000  consumed samples: 10880  total_loss: 7.599  time: 0.2167(73.82)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:11] lb.utils.events INFO:  eta: 0:01:09  iteration: 681/1000  consumed samples: 10896  total_loss: 7.599  time: 0.2167(73.82)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:11] lb.utils.events INFO:  eta: 0:01:08  iteration: 682/1000  consumed samples: 10912  total_loss: 7.596  time: 0.2168(73.81)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:12] lb.utils.events INFO:  eta: 0:01:08  iteration: 683/1000  consumed samples: 10928  total_loss: 7.596  time: 0.2168(73.81)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:12] lb.utils.events INFO:  eta: 0:01:08  iteration: 684/1000  consumed samples: 10944  total_loss: 7.596  time: 0.2168(73.82)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:12] lb.utils.events INFO:  eta: 0:01:08  iteration: 685/1000  consumed samples: 10960  total_loss: 7.599  time: 0.2168(73.81)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:12] lb.utils.events INFO:  eta: 0:01:07  iteration: 686/1000  consumed samples: 10976  total_loss: 7.596  time: 0.2168(73.81)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:12] lb.utils.events INFO:  eta: 0:01:07  iteration: 687/1000  consumed samples: 10992  total_loss: 7.596  time: 0.2168(73.81)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:13] lb.utils.events INFO:  eta: 0:01:07  iteration: 688/1000  consumed samples: 11008  total_loss: 7.594  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:13] lb.utils.events INFO:  eta: 0:01:07  iteration: 689/1000  consumed samples: 11024  total_loss: 7.596  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:13] lb.utils.events INFO:  eta: 0:01:07  iteration: 690/1000  consumed samples: 11040  total_loss: 7.594  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:13] lb.utils.events INFO:  eta: 0:01:06  iteration: 691/1000  consumed samples: 11056  total_loss: 7.593  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 692/1000  consumed samples: 11072  total_loss: 7.591  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 693/1000  consumed samples: 11088  total_loss: 7.591  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 694/1000  consumed samples: 11104  total_loss: 7.589  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:14] lb.utils.events INFO:  eta: 0:01:05  iteration: 695/1000  consumed samples: 11120  total_loss: 7.589  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 696/1000  consumed samples: 11136  total_loss: 7.589  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 697/1000  consumed samples: 11152  total_loss: 7.587  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 698/1000  consumed samples: 11168  total_loss: 7.587  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 699/1000  consumed samples: 11184  total_loss: 7.587  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:15] lb.utils.events INFO:  eta: 0:01:04  iteration: 700/1000  consumed samples: 11200  total_loss: 7.586  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:16] lb.utils.events INFO:  eta: 0:01:04  iteration: 701/1000  consumed samples: 11216  total_loss: 7.584  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:16] lb.utils.events INFO:  eta: 0:01:04  iteration: 702/1000  consumed samples: 11232  total_loss: 7.581  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:16] lb.utils.events INFO:  eta: 0:01:04  iteration: 703/1000  consumed samples: 11248  total_loss: 7.578  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:16] lb.utils.events INFO:  eta: 0:01:04  iteration: 704/1000  consumed samples: 11264  total_loss: 7.578  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:17] lb.utils.events INFO:  eta: 0:01:03  iteration: 705/1000  consumed samples: 11280  total_loss: 7.577  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:17] lb.utils.events INFO:  eta: 0:01:03  iteration: 706/1000  consumed samples: 11296  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:17] lb.utils.events INFO:  eta: 0:01:03  iteration: 707/1000  consumed samples: 11312  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:17] lb.utils.events INFO:  eta: 0:01:03  iteration: 708/1000  consumed samples: 11328  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:17] lb.utils.events INFO:  eta: 0:01:02  iteration: 709/1000  consumed samples: 11344  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:18] lb.utils.events INFO:  eta: 0:01:02  iteration: 710/1000  consumed samples: 11360  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:18] lb.utils.events INFO:  eta: 0:01:02  iteration: 711/1000  consumed samples: 11376  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:18] lb.utils.events INFO:  eta: 0:01:02  iteration: 712/1000  consumed samples: 11392  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:18] lb.utils.events INFO:  eta: 0:01:02  iteration: 713/1000  consumed samples: 11408  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:19] lb.utils.events INFO:  eta: 0:01:01  iteration: 714/1000  consumed samples: 11424  total_loss: 7.576  time: 0.2168(73.80)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:19] lb.utils.events INFO:  eta: 0:01:01  iteration: 715/1000  consumed samples: 11440  total_loss: 7.575  time: 0.2168(73.79)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:19] lb.utils.events INFO:  eta: 0:01:01  iteration: 716/1000  consumed samples: 11456  total_loss: 7.575  time: 0.2168(73.79)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:19] lb.utils.events INFO:  eta: 0:01:01  iteration: 717/1000  consumed samples: 11472  total_loss: 7.575  time: 0.2168(73.79)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:20] lb.utils.events INFO:  eta: 0:01:01  iteration: 718/1000  consumed samples: 11488  total_loss: 7.573  time: 0.2168(73.79)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:20] lb.utils.events INFO:  eta: 0:01:00  iteration: 719/1000  consumed samples: 11504  total_loss: 7.573  time: 0.2168(73.79)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:20] lb.utils.events INFO:  eta: 0:01:00  iteration: 720/1000  consumed samples: 11520  total_loss: 7.573  time: 0.2168(73.79)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:45:20] lb.utils.events INFO:  eta: 0:01:00  iteration: 721/1000  consumed samples: 11536  total_loss: 7.573  time: 0.2168(73.79)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:45:20] lb.utils.events INFO:  eta: 0:01:00  iteration: 722/1000  consumed samples: 11552  total_loss: 7.571  time: 0.2168(73.79)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:45:21] lb.utils.events INFO:  eta: 0:00:59  iteration: 723/1000  consumed samples: 11568  total_loss: 7.573  time: 0.2168(73.78)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:45:21] lb.utils.events INFO:  eta: 0:00:59  iteration: 724/1000  consumed samples: 11584  total_loss: 7.573  time: 0.2168(73.78)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:21] lb.utils.events INFO:  eta: 0:00:59  iteration: 725/1000  consumed samples: 11600  total_loss: 7.571  time: 0.2169(73.78)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:21] lb.utils.events INFO:  eta: 0:00:59  iteration: 726/1000  consumed samples: 11616  total_loss: 7.57  time: 0.2169(73.78)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:45:22] lb.utils.events INFO:  eta: 0:00:59  iteration: 727/1000  consumed samples: 11632  total_loss: 7.57  time: 0.2169(73.78)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:22] lb.utils.events INFO:  eta: 0:00:58  iteration: 728/1000  consumed samples: 11648  total_loss: 7.57  time: 0.2169(73.78)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:22] lb.utils.events INFO:  eta: 0:00:58  iteration: 729/1000  consumed samples: 11664  total_loss: 7.57  time: 0.2169(73.78)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:22] lb.utils.events INFO:  eta: 0:00:58  iteration: 730/1000  consumed samples: 11680  total_loss: 7.569  time: 0.2169(73.78)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:45:23] lb.utils.events INFO:  eta: 0:00:58  iteration: 731/1000  consumed samples: 11696  total_loss: 7.569  time: 0.2169(73.77)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:45:23] lb.utils.events INFO:  eta: 0:00:57  iteration: 732/1000  consumed samples: 11712  total_loss: 7.567  time: 0.2169(73.77)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:45:23] lb.utils.events INFO:  eta: 0:00:57  iteration: 733/1000  consumed samples: 11728  total_loss: 7.566  time: 0.2169(73.77)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:23] lb.utils.events INFO:  eta: 0:00:57  iteration: 734/1000  consumed samples: 11744  total_loss: 7.565  time: 0.2169(73.77)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:23] lb.utils.events INFO:  eta: 0:00:57  iteration: 735/1000  consumed samples: 11760  total_loss: 7.565  time: 0.2169(73.76)  data_time: 0.0038  lr: 9.90e-05  
[01/19 14:45:24] lb.utils.events INFO:  eta: 0:00:57  iteration: 736/1000  consumed samples: 11776  total_loss: 7.565  time: 0.2169(73.76)  data_time: 0.0037  lr: 9.90e-05  
[01/19 14:45:24] lb.utils.events INFO:  eta: 0:00:56  iteration: 737/1000  consumed samples: 11792  total_loss: 7.565  time: 0.2169(73.76)  data_time: 0.0036  lr: 9.90e-05  
[01/19 14:45:24] lb.utils.events INFO:  eta: 0:00:56  iteration: 738/1000  consumed samples: 11808  total_loss: 7.565  time: 0.2169(73.76)  data_time: 0.0035  lr: 9.90e-05  
[01/19 14:45:24] lb.utils.events INFO:  eta: 0:00:56  iteration: 739/1000  consumed samples: 11824  total_loss: 7.565  time: 0.2169(73.76)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:45:25] lb.utils.events INFO:  eta: 0:00:56  iteration: 740/1000  consumed samples: 11840  total_loss: 7.564  time: 0.2169(73.76)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:45:25] lb.utils.events INFO:  eta: 0:00:56  iteration: 741/1000  consumed samples: 11856  total_loss: 7.564  time: 0.2169(73.75)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:45:25] lb.utils.events INFO:  eta: 0:00:55  iteration: 742/1000  consumed samples: 11872  total_loss: 7.564  time: 0.2169(73.75)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:45:25] lb.utils.events INFO:  eta: 0:00:55  iteration: 743/1000  consumed samples: 11888  total_loss: 7.564  time: 0.2169(73.75)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:45:26] lb.utils.events INFO:  eta: 0:00:55  iteration: 744/1000  consumed samples: 11904  total_loss: 7.564  time: 0.2169(73.75)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:26] lb.utils.events INFO:  eta: 0:00:55  iteration: 745/1000  consumed samples: 11920  total_loss: 7.564  time: 0.2170(73.75)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:26] lb.utils.events INFO:  eta: 0:00:54  iteration: 746/1000  consumed samples: 11936  total_loss: 7.564  time: 0.2170(73.75)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:26] lb.utils.events INFO:  eta: 0:00:54  iteration: 747/1000  consumed samples: 11952  total_loss: 7.565  time: 0.2170(73.74)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:26] lb.utils.events INFO:  eta: 0:00:54  iteration: 748/1000  consumed samples: 11968  total_loss: 7.565  time: 0.2170(73.74)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:27] lb.utils.events INFO:  eta: 0:00:54  iteration: 749/1000  consumed samples: 11984  total_loss: 7.565  time: 0.2170(73.74)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:27] lb.utils.events INFO:  eta: 0:00:54  iteration: 750/1000  consumed samples: 12000  total_loss: 7.564  time: 0.2170(73.74)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:27] lb.utils.events INFO:  eta: 0:00:53  iteration: 751/1000  consumed samples: 12016  total_loss: 7.564  time: 0.2170(73.74)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:27] lb.utils.events INFO:  eta: 0:00:53  iteration: 752/1000  consumed samples: 12032  total_loss: 7.564  time: 0.2170(73.74)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:28] lb.utils.events INFO:  eta: 0:00:53  iteration: 753/1000  consumed samples: 12048  total_loss: 7.562  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:28] lb.utils.events INFO:  eta: 0:00:53  iteration: 754/1000  consumed samples: 12064  total_loss: 7.564  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:28] lb.utils.events INFO:  eta: 0:00:52  iteration: 755/1000  consumed samples: 12080  total_loss: 7.562  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:28] lb.utils.events INFO:  eta: 0:00:52  iteration: 756/1000  consumed samples: 12096  total_loss: 7.561  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:29] lb.utils.events INFO:  eta: 0:00:52  iteration: 757/1000  consumed samples: 12112  total_loss: 7.56  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:29] lb.utils.events INFO:  eta: 0:00:52  iteration: 758/1000  consumed samples: 12128  total_loss: 7.559  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:29] lb.utils.events INFO:  eta: 0:00:52  iteration: 759/1000  consumed samples: 12144  total_loss: 7.56  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:29] lb.utils.events INFO:  eta: 0:00:51  iteration: 760/1000  consumed samples: 12160  total_loss: 7.559  time: 0.2170(73.73)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:29] lb.utils.events INFO:  eta: 0:00:51  iteration: 761/1000  consumed samples: 12176  total_loss: 7.559  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:30] lb.utils.events INFO:  eta: 0:00:51  iteration: 762/1000  consumed samples: 12192  total_loss: 7.557  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:30] lb.utils.events INFO:  eta: 0:00:51  iteration: 763/1000  consumed samples: 12208  total_loss: 7.555  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:30] lb.utils.events INFO:  eta: 0:00:51  iteration: 764/1000  consumed samples: 12224  total_loss: 7.555  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:30] lb.utils.events INFO:  eta: 0:00:50  iteration: 765/1000  consumed samples: 12240  total_loss: 7.555  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:31] lb.utils.events INFO:  eta: 0:00:50  iteration: 766/1000  consumed samples: 12256  total_loss: 7.555  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:31] lb.utils.events INFO:  eta: 0:00:50  iteration: 767/1000  consumed samples: 12272  total_loss: 7.555  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:31] lb.utils.events INFO:  eta: 0:00:50  iteration: 768/1000  consumed samples: 12288  total_loss: 7.555  time: 0.2170(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:31] lb.utils.events INFO:  eta: 0:00:49  iteration: 769/1000  consumed samples: 12304  total_loss: 7.555  time: 0.2171(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:31] lb.utils.events INFO:  eta: 0:00:49  iteration: 770/1000  consumed samples: 12320  total_loss: 7.555  time: 0.2171(73.72)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:32] lb.utils.events INFO:  eta: 0:00:49  iteration: 771/1000  consumed samples: 12336  total_loss: 7.555  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:32] lb.utils.events INFO:  eta: 0:00:49  iteration: 772/1000  consumed samples: 12352  total_loss: 7.555  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:32] lb.utils.events INFO:  eta: 0:00:49  iteration: 773/1000  consumed samples: 12368  total_loss: 7.555  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:32] lb.utils.events INFO:  eta: 0:00:48  iteration: 774/1000  consumed samples: 12384  total_loss: 7.555  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:33] lb.utils.events INFO:  eta: 0:00:48  iteration: 775/1000  consumed samples: 12400  total_loss: 7.554  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:33] lb.utils.events INFO:  eta: 0:00:48  iteration: 776/1000  consumed samples: 12416  total_loss: 7.554  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:33] lb.utils.events INFO:  eta: 0:00:48  iteration: 777/1000  consumed samples: 12432  total_loss: 7.554  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:33] lb.utils.events INFO:  eta: 0:00:47  iteration: 778/1000  consumed samples: 12448  total_loss: 7.554  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:34] lb.utils.events INFO:  eta: 0:00:47  iteration: 779/1000  consumed samples: 12464  total_loss: 7.554  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:34] lb.utils.events INFO:  eta: 0:00:47  iteration: 780/1000  consumed samples: 12480  total_loss: 7.554  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:34] lb.utils.events INFO:  eta: 0:00:47  iteration: 781/1000  consumed samples: 12496  total_loss: 7.553  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:34] lb.utils.events INFO:  eta: 0:00:47  iteration: 782/1000  consumed samples: 12512  total_loss: 7.551  time: 0.2171(73.71)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:34] lb.utils.events INFO:  eta: 0:00:46  iteration: 783/1000  consumed samples: 12528  total_loss: 7.553  time: 0.2171(73.70)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:35] lb.utils.events INFO:  eta: 0:00:46  iteration: 784/1000  consumed samples: 12544  total_loss: 7.554  time: 0.2171(73.70)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:35] lb.utils.events INFO:  eta: 0:00:46  iteration: 785/1000  consumed samples: 12560  total_loss: 7.554  time: 0.2171(73.70)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:35] lb.utils.events INFO:  eta: 0:00:46  iteration: 786/1000  consumed samples: 12576  total_loss: 7.553  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:35] lb.utils.events INFO:  eta: 0:00:46  iteration: 787/1000  consumed samples: 12592  total_loss: 7.554  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:36] lb.utils.events INFO:  eta: 0:00:45  iteration: 788/1000  consumed samples: 12608  total_loss: 7.554  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:36] lb.utils.events INFO:  eta: 0:00:45  iteration: 789/1000  consumed samples: 12624  total_loss: 7.554  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:36] lb.utils.events INFO:  eta: 0:00:45  iteration: 790/1000  consumed samples: 12640  total_loss: 7.553  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:36] lb.utils.events INFO:  eta: 0:00:45  iteration: 791/1000  consumed samples: 12656  total_loss: 7.551  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:37] lb.utils.events INFO:  eta: 0:00:44  iteration: 792/1000  consumed samples: 12672  total_loss: 7.551  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:37] lb.utils.events INFO:  eta: 0:00:44  iteration: 793/1000  consumed samples: 12688  total_loss: 7.549  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:37] lb.utils.events INFO:  eta: 0:00:44  iteration: 794/1000  consumed samples: 12704  total_loss: 7.549  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:37] lb.utils.events INFO:  eta: 0:00:44  iteration: 795/1000  consumed samples: 12720  total_loss: 7.551  time: 0.2171(73.69)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:37] lb.utils.events INFO:  eta: 0:00:44  iteration: 796/1000  consumed samples: 12736  total_loss: 7.551  time: 0.2171(73.68)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:38] lb.utils.events INFO:  eta: 0:00:43  iteration: 797/1000  consumed samples: 12752  total_loss: 7.551  time: 0.2172(73.68)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:38] lb.utils.events INFO:  eta: 0:00:43  iteration: 798/1000  consumed samples: 12768  total_loss: 7.549  time: 0.2171(73.68)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:38] lb.utils.events INFO:  eta: 0:00:43  iteration: 799/1000  consumed samples: 12784  total_loss: 7.549  time: 0.2172(73.68)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:38] lb.utils.events INFO:  eta: 0:00:43  iteration: 800/1000  consumed samples: 12800  total_loss: 7.549  time: 0.2171(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:39] lb.utils.events INFO:  eta: 0:00:43  iteration: 801/1000  consumed samples: 12816  total_loss: 7.549  time: 0.2172(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:39] lb.utils.events INFO:  eta: 0:00:42  iteration: 802/1000  consumed samples: 12832  total_loss: 7.549  time: 0.2172(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:39] lb.utils.events INFO:  eta: 0:00:42  iteration: 803/1000  consumed samples: 12848  total_loss: 7.549  time: 0.2172(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:39] lb.utils.events INFO:  eta: 0:00:42  iteration: 804/1000  consumed samples: 12864  total_loss: 7.548  time: 0.2172(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:40] lb.utils.events INFO:  eta: 0:00:42  iteration: 805/1000  consumed samples: 12880  total_loss: 7.548  time: 0.2172(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:40] lb.utils.events INFO:  eta: 0:00:41  iteration: 806/1000  consumed samples: 12896  total_loss: 7.548  time: 0.2172(73.68)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:40] lb.utils.events INFO:  eta: 0:00:41  iteration: 807/1000  consumed samples: 12912  total_loss: 7.548  time: 0.2172(73.68)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:40] lb.utils.events INFO:  eta: 0:00:41  iteration: 808/1000  consumed samples: 12928  total_loss: 7.548  time: 0.2172(73.68)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:40] lb.utils.events INFO:  eta: 0:00:41  iteration: 809/1000  consumed samples: 12944  total_loss: 7.548  time: 0.2172(73.67)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:41] lb.utils.events INFO:  eta: 0:00:41  iteration: 810/1000  consumed samples: 12960  total_loss: 7.548  time: 0.2172(73.67)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:41] lb.utils.events INFO:  eta: 0:00:40  iteration: 811/1000  consumed samples: 12976  total_loss: 7.548  time: 0.2172(73.67)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:41] lb.utils.events INFO:  eta: 0:00:40  iteration: 812/1000  consumed samples: 12992  total_loss: 7.547  time: 0.2172(73.67)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:41] lb.utils.events INFO:  eta: 0:00:40  iteration: 813/1000  consumed samples: 13008  total_loss: 7.547  time: 0.2172(73.67)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:42] lb.utils.events INFO:  eta: 0:00:40  iteration: 814/1000  consumed samples: 13024  total_loss: 7.547  time: 0.2172(73.67)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:42] lb.utils.events INFO:  eta: 0:00:39  iteration: 815/1000  consumed samples: 13040  total_loss: 7.545  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:42] lb.utils.events INFO:  eta: 0:00:39  iteration: 816/1000  consumed samples: 13056  total_loss: 7.545  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:42] lb.utils.events INFO:  eta: 0:00:39  iteration: 817/1000  consumed samples: 13072  total_loss: 7.545  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:43] lb.utils.events INFO:  eta: 0:00:39  iteration: 818/1000  consumed samples: 13088  total_loss: 7.545  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:43] lb.utils.events INFO:  eta: 0:00:39  iteration: 819/1000  consumed samples: 13104  total_loss: 7.544  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:43] lb.utils.events INFO:  eta: 0:00:38  iteration: 820/1000  consumed samples: 13120  total_loss: 7.544  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:43] lb.utils.events INFO:  eta: 0:00:38  iteration: 821/1000  consumed samples: 13136  total_loss: 7.544  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:43] lb.utils.events INFO:  eta: 0:00:38  iteration: 822/1000  consumed samples: 13152  total_loss: 7.544  time: 0.2172(73.66)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:44] lb.utils.events INFO:  eta: 0:00:38  iteration: 823/1000  consumed samples: 13168  total_loss: 7.544  time: 0.2172(73.65)  data_time: 0.0024  lr: 9.90e-05  
[01/19 14:45:44] lb.utils.events INFO:  eta: 0:00:38  iteration: 824/1000  consumed samples: 13184  total_loss: 7.545  time: 0.2172(73.65)  data_time: 0.0024  lr: 9.90e-05  
[01/19 14:45:44] lb.utils.events INFO:  eta: 0:00:37  iteration: 825/1000  consumed samples: 13200  total_loss: 7.544  time: 0.2172(73.65)  data_time: 0.0024  lr: 9.90e-05  
[01/19 14:45:44] lb.utils.events INFO:  eta: 0:00:37  iteration: 826/1000  consumed samples: 13216  total_loss: 7.542  time: 0.2172(73.65)  data_time: 0.0024  lr: 9.90e-05  
[01/19 14:45:45] lb.utils.events INFO:  eta: 0:00:37  iteration: 827/1000  consumed samples: 13232  total_loss: 7.542  time: 0.2172(73.65)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:45] lb.utils.events INFO:  eta: 0:00:37  iteration: 828/1000  consumed samples: 13248  total_loss: 7.54  time: 0.2172(73.65)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:45] lb.utils.events INFO:  eta: 0:00:36  iteration: 829/1000  consumed samples: 13264  total_loss: 7.54  time: 0.2172(73.65)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:45] lb.utils.events INFO:  eta: 0:00:36  iteration: 830/1000  consumed samples: 13280  total_loss: 7.538  time: 0.2173(73.65)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:46] lb.utils.events INFO:  eta: 0:00:36  iteration: 831/1000  consumed samples: 13296  total_loss: 7.536  time: 0.2173(73.65)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:46] lb.utils.events INFO:  eta: 0:00:36  iteration: 832/1000  consumed samples: 13312  total_loss: 7.535  time: 0.2173(73.64)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:46] lb.utils.events INFO:  eta: 0:00:36  iteration: 833/1000  consumed samples: 13328  total_loss: 7.535  time: 0.2173(73.64)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:46] lb.utils.events INFO:  eta: 0:00:35  iteration: 834/1000  consumed samples: 13344  total_loss: 7.534  time: 0.2173(73.64)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:46] lb.utils.events INFO:  eta: 0:00:35  iteration: 835/1000  consumed samples: 13360  total_loss: 7.534  time: 0.2173(73.64)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:45:47] lb.utils.events INFO:  eta: 0:00:35  iteration: 836/1000  consumed samples: 13376  total_loss: 7.535  time: 0.2173(73.64)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:47] lb.utils.events INFO:  eta: 0:00:35  iteration: 837/1000  consumed samples: 13392  total_loss: 7.535  time: 0.2173(73.64)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:47] lb.utils.events INFO:  eta: 0:00:34  iteration: 838/1000  consumed samples: 13408  total_loss: 7.535  time: 0.2173(73.63)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:47] lb.utils.events INFO:  eta: 0:00:34  iteration: 839/1000  consumed samples: 13424  total_loss: 7.536  time: 0.2173(73.63)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:48] lb.utils.events INFO:  eta: 0:00:34  iteration: 840/1000  consumed samples: 13440  total_loss: 7.535  time: 0.2173(73.63)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:48] lb.utils.events INFO:  eta: 0:00:34  iteration: 841/1000  consumed samples: 13456  total_loss: 7.534  time: 0.2173(73.63)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:48] lb.utils.events INFO:  eta: 0:00:34  iteration: 842/1000  consumed samples: 13472  total_loss: 7.534  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:48] lb.utils.events INFO:  eta: 0:00:33  iteration: 843/1000  consumed samples: 13488  total_loss: 7.534  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:49] lb.utils.events INFO:  eta: 0:00:33  iteration: 844/1000  consumed samples: 13504  total_loss: 7.534  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:49] lb.utils.events INFO:  eta: 0:00:33  iteration: 845/1000  consumed samples: 13520  total_loss: 7.532  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:49] lb.utils.events INFO:  eta: 0:00:33  iteration: 846/1000  consumed samples: 13536  total_loss: 7.53  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:49] lb.utils.events INFO:  eta: 0:00:33  iteration: 847/1000  consumed samples: 13552  total_loss: 7.529  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:49] lb.utils.events INFO:  eta: 0:00:32  iteration: 848/1000  consumed samples: 13568  total_loss: 7.529  time: 0.2173(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:50] lb.utils.events INFO:  eta: 0:00:32  iteration: 849/1000  consumed samples: 13584  total_loss: 7.529  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:50] lb.utils.events INFO:  eta: 0:00:32  iteration: 850/1000  consumed samples: 13600  total_loss: 7.529  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:50] lb.utils.events INFO:  eta: 0:00:32  iteration: 851/1000  consumed samples: 13616  total_loss: 7.53  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:50] lb.utils.events INFO:  eta: 0:00:31  iteration: 852/1000  consumed samples: 13632  total_loss: 7.53  time: 0.2173(73.62)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:51] lb.utils.events INFO:  eta: 0:00:31  iteration: 853/1000  consumed samples: 13648  total_loss: 7.529  time: 0.2174(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:51] lb.utils.events INFO:  eta: 0:00:31  iteration: 854/1000  consumed samples: 13664  total_loss: 7.528  time: 0.2174(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:51] lb.utils.events INFO:  eta: 0:00:31  iteration: 855/1000  consumed samples: 13680  total_loss: 7.529  time: 0.2174(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:51] lb.utils.events INFO:  eta: 0:00:31  iteration: 856/1000  consumed samples: 13696  total_loss: 7.529  time: 0.2174(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:51] lb.utils.events INFO:  eta: 0:00:30  iteration: 857/1000  consumed samples: 13712  total_loss: 7.528  time: 0.2174(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:52] lb.utils.events INFO:  eta: 0:00:30  iteration: 858/1000  consumed samples: 13728  total_loss: 7.529  time: 0.2174(73.61)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:52] lb.utils.events INFO:  eta: 0:00:30  iteration: 859/1000  consumed samples: 13744  total_loss: 7.529  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:52] lb.utils.events INFO:  eta: 0:00:30  iteration: 860/1000  consumed samples: 13760  total_loss: 7.529  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:52] lb.utils.events INFO:  eta: 0:00:29  iteration: 861/1000  consumed samples: 13776  total_loss: 7.528  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:53] lb.utils.events INFO:  eta: 0:00:29  iteration: 862/1000  consumed samples: 13792  total_loss: 7.528  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:53] lb.utils.events INFO:  eta: 0:00:29  iteration: 863/1000  consumed samples: 13808  total_loss: 7.528  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:53] lb.utils.events INFO:  eta: 0:00:29  iteration: 864/1000  consumed samples: 13824  total_loss: 7.528  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:53] lb.utils.events INFO:  eta: 0:00:29  iteration: 865/1000  consumed samples: 13840  total_loss: 7.528  time: 0.2174(73.60)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:54] lb.utils.events INFO:  eta: 0:00:28  iteration: 866/1000  consumed samples: 13856  total_loss: 7.528  time: 0.2174(73.59)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:54] lb.utils.events INFO:  eta: 0:00:28  iteration: 867/1000  consumed samples: 13872  total_loss: 7.526  time: 0.2174(73.59)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:54] lb.utils.events INFO:  eta: 0:00:28  iteration: 868/1000  consumed samples: 13888  total_loss: 7.528  time: 0.2174(73.59)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:54] lb.utils.events INFO:  eta: 0:00:28  iteration: 869/1000  consumed samples: 13904  total_loss: 7.526  time: 0.2174(73.59)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:54] lb.utils.events INFO:  eta: 0:00:28  iteration: 870/1000  consumed samples: 13920  total_loss: 7.523  time: 0.2174(73.59)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:45:55] lb.utils.events INFO:  eta: 0:00:27  iteration: 871/1000  consumed samples: 13936  total_loss: 7.523  time: 0.2174(73.60)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:45:55] lb.utils.events INFO:  eta: 0:00:27  iteration: 872/1000  consumed samples: 13952  total_loss: 7.523  time: 0.2174(73.60)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:55] lb.utils.events INFO:  eta: 0:00:27  iteration: 873/1000  consumed samples: 13968  total_loss: 7.523  time: 0.2174(73.59)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:55] lb.utils.events INFO:  eta: 0:00:27  iteration: 874/1000  consumed samples: 13984  total_loss: 7.523  time: 0.2174(73.59)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:56] lb.utils.events INFO:  eta: 0:00:26  iteration: 875/1000  consumed samples: 14000  total_loss: 7.523  time: 0.2174(73.59)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:56] lb.utils.events INFO:  eta: 0:00:26  iteration: 876/1000  consumed samples: 14016  total_loss: 7.523  time: 0.2174(73.59)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:56] lb.utils.events INFO:  eta: 0:00:26  iteration: 877/1000  consumed samples: 14032  total_loss: 7.521  time: 0.2174(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:56] lb.utils.events INFO:  eta: 0:00:26  iteration: 878/1000  consumed samples: 14048  total_loss: 7.52  time: 0.2174(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:57] lb.utils.events INFO:  eta: 0:00:26  iteration: 879/1000  consumed samples: 14064  total_loss: 7.518  time: 0.2174(73.59)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:57] lb.utils.events INFO:  eta: 0:00:25  iteration: 880/1000  consumed samples: 14080  total_loss: 7.518  time: 0.2174(73.59)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:57] lb.utils.events INFO:  eta: 0:00:25  iteration: 881/1000  consumed samples: 14096  total_loss: 7.516  time: 0.2174(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:57] lb.utils.events INFO:  eta: 0:00:25  iteration: 882/1000  consumed samples: 14112  total_loss: 7.516  time: 0.2174(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:57] lb.utils.events INFO:  eta: 0:00:25  iteration: 883/1000  consumed samples: 14128  total_loss: 7.514  time: 0.2174(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:58] lb.utils.events INFO:  eta: 0:00:25  iteration: 884/1000  consumed samples: 14144  total_loss: 7.512  time: 0.2174(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:58] lb.utils.events INFO:  eta: 0:00:24  iteration: 885/1000  consumed samples: 14160  total_loss: 7.509  time: 0.2175(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:58] lb.utils.events INFO:  eta: 0:00:24  iteration: 886/1000  consumed samples: 14176  total_loss: 7.509  time: 0.2175(73.58)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:45:58] lb.utils.events INFO:  eta: 0:00:24  iteration: 887/1000  consumed samples: 14192  total_loss: 7.512  time: 0.2175(73.57)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:59] lb.utils.events INFO:  eta: 0:00:24  iteration: 888/1000  consumed samples: 14208  total_loss: 7.512  time: 0.2175(73.57)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:59] lb.utils.events INFO:  eta: 0:00:23  iteration: 889/1000  consumed samples: 14224  total_loss: 7.512  time: 0.2175(73.57)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:59] lb.utils.events INFO:  eta: 0:00:23  iteration: 890/1000  consumed samples: 14240  total_loss: 7.509  time: 0.2175(73.57)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:45:59] lb.utils.events INFO:  eta: 0:00:23  iteration: 891/1000  consumed samples: 14256  total_loss: 7.509  time: 0.2175(73.56)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:00] lb.utils.events INFO:  eta: 0:00:23  iteration: 892/1000  consumed samples: 14272  total_loss: 7.507  time: 0.2175(73.56)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:00] lb.utils.events INFO:  eta: 0:00:23  iteration: 893/1000  consumed samples: 14288  total_loss: 7.509  time: 0.2175(73.56)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:00] lb.utils.events INFO:  eta: 0:00:22  iteration: 894/1000  consumed samples: 14304  total_loss: 7.509  time: 0.2175(73.56)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:00] lb.utils.events INFO:  eta: 0:00:22  iteration: 895/1000  consumed samples: 14320  total_loss: 7.512  time: 0.2175(73.55)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:01] lb.utils.events INFO:  eta: 0:00:22  iteration: 896/1000  consumed samples: 14336  total_loss: 7.509  time: 0.2175(73.55)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:01] lb.utils.events INFO:  eta: 0:00:22  iteration: 897/1000  consumed samples: 14352  total_loss: 7.509  time: 0.2175(73.55)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:01] lb.utils.events INFO:  eta: 0:00:21  iteration: 898/1000  consumed samples: 14368  total_loss: 7.509  time: 0.2175(73.55)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:01] lb.utils.events INFO:  eta: 0:00:21  iteration: 899/1000  consumed samples: 14384  total_loss: 7.509  time: 0.2176(73.54)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:01] lb.utils.events INFO:  eta: 0:00:21  iteration: 900/1000  consumed samples: 14400  total_loss: 7.507  time: 0.2176(73.54)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:02] lb.utils.events INFO:  eta: 0:00:21  iteration: 901/1000  consumed samples: 14416  total_loss: 7.507  time: 0.2176(73.54)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:02] lb.utils.events INFO:  eta: 0:00:21  iteration: 902/1000  consumed samples: 14432  total_loss: 7.505  time: 0.2176(73.54)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:02] lb.utils.events INFO:  eta: 0:00:20  iteration: 903/1000  consumed samples: 14448  total_loss: 7.505  time: 0.2176(73.54)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:02] lb.utils.events INFO:  eta: 0:00:20  iteration: 904/1000  consumed samples: 14464  total_loss: 7.507  time: 0.2176(73.53)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:03] lb.utils.events INFO:  eta: 0:00:20  iteration: 905/1000  consumed samples: 14480  total_loss: 7.507  time: 0.2176(73.53)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:03] lb.utils.events INFO:  eta: 0:00:20  iteration: 906/1000  consumed samples: 14496  total_loss: 7.509  time: 0.2176(73.53)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:03] lb.utils.events INFO:  eta: 0:00:20  iteration: 907/1000  consumed samples: 14512  total_loss: 7.509  time: 0.2176(73.53)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:03] lb.utils.events INFO:  eta: 0:00:19  iteration: 908/1000  consumed samples: 14528  total_loss: 7.507  time: 0.2176(73.53)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:04] lb.utils.events INFO:  eta: 0:00:19  iteration: 909/1000  consumed samples: 14544  total_loss: 7.507  time: 0.2176(73.52)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:04] lb.utils.events INFO:  eta: 0:00:19  iteration: 910/1000  consumed samples: 14560  total_loss: 7.505  time: 0.2176(73.52)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:04] lb.utils.events INFO:  eta: 0:00:19  iteration: 911/1000  consumed samples: 14576  total_loss: 7.507  time: 0.2176(73.52)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:04] lb.utils.events INFO:  eta: 0:00:18  iteration: 912/1000  consumed samples: 14592  total_loss: 7.507  time: 0.2176(73.52)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:04] lb.utils.events INFO:  eta: 0:00:18  iteration: 913/1000  consumed samples: 14608  total_loss: 7.507  time: 0.2176(73.51)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:05] lb.utils.events INFO:  eta: 0:00:18  iteration: 914/1000  consumed samples: 14624  total_loss: 7.505  time: 0.2176(73.52)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:05] lb.utils.events INFO:  eta: 0:00:18  iteration: 915/1000  consumed samples: 14640  total_loss: 7.502  time: 0.2176(73.51)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:46:05] lb.utils.events INFO:  eta: 0:00:18  iteration: 916/1000  consumed samples: 14656  total_loss: 7.5  time: 0.2177(73.51)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:46:05] lb.utils.events INFO:  eta: 0:00:17  iteration: 917/1000  consumed samples: 14672  total_loss: 7.502  time: 0.2177(73.51)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:46:06] lb.utils.events INFO:  eta: 0:00:17  iteration: 918/1000  consumed samples: 14688  total_loss: 7.502  time: 0.2177(73.51)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:46:06] lb.utils.events INFO:  eta: 0:00:17  iteration: 919/1000  consumed samples: 14704  total_loss: 7.505  time: 0.2177(73.51)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:46:06] lb.utils.events INFO:  eta: 0:00:17  iteration: 920/1000  consumed samples: 14720  total_loss: 7.502  time: 0.2177(73.50)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:46:06] lb.utils.events INFO:  eta: 0:00:16  iteration: 921/1000  consumed samples: 14736  total_loss: 7.5  time: 0.2177(73.50)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:07] lb.utils.events INFO:  eta: 0:00:16  iteration: 922/1000  consumed samples: 14752  total_loss: 7.5  time: 0.2177(73.50)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:07] lb.utils.events INFO:  eta: 0:00:16  iteration: 923/1000  consumed samples: 14768  total_loss: 7.499  time: 0.2177(73.50)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:07] lb.utils.events INFO:  eta: 0:00:16  iteration: 924/1000  consumed samples: 14784  total_loss: 7.499  time: 0.2177(73.50)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:07] lb.utils.events INFO:  eta: 0:00:16  iteration: 925/1000  consumed samples: 14800  total_loss: 7.497  time: 0.2177(73.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:07] lb.utils.events INFO:  eta: 0:00:15  iteration: 926/1000  consumed samples: 14816  total_loss: 7.496  time: 0.2177(73.49)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:08] lb.utils.events INFO:  eta: 0:00:15  iteration: 927/1000  consumed samples: 14832  total_loss: 7.493  time: 0.2177(73.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:46:08] lb.utils.events INFO:  eta: 0:00:15  iteration: 928/1000  consumed samples: 14848  total_loss: 7.493  time: 0.2177(73.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:46:08] lb.utils.events INFO:  eta: 0:00:15  iteration: 929/1000  consumed samples: 14864  total_loss: 7.493  time: 0.2177(73.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:46:08] lb.utils.events INFO:  eta: 0:00:15  iteration: 930/1000  consumed samples: 14880  total_loss: 7.491  time: 0.2177(73.49)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:46:09] lb.utils.events INFO:  eta: 0:00:14  iteration: 931/1000  consumed samples: 14896  total_loss: 7.491  time: 0.2177(73.48)  data_time: 0.0034  lr: 9.90e-05  
[01/19 14:46:09] lb.utils.events INFO:  eta: 0:00:14  iteration: 932/1000  consumed samples: 14912  total_loss: 7.49  time: 0.2177(73.48)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:09] lb.utils.events INFO:  eta: 0:00:14  iteration: 933/1000  consumed samples: 14928  total_loss: 7.49  time: 0.2177(73.48)  data_time: 0.0033  lr: 9.90e-05  
[01/19 14:46:09] lb.utils.events INFO:  eta: 0:00:14  iteration: 934/1000  consumed samples: 14944  total_loss: 7.488  time: 0.2177(73.48)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:46:10] lb.utils.events INFO:  eta: 0:00:13  iteration: 935/1000  consumed samples: 14960  total_loss: 7.488  time: 0.2178(73.48)  data_time: 0.0032  lr: 9.90e-05  
[01/19 14:46:10] lb.utils.events INFO:  eta: 0:00:13  iteration: 936/1000  consumed samples: 14976  total_loss: 7.486  time: 0.2178(73.47)  data_time: 0.0031  lr: 9.90e-05  
[01/19 14:46:10] lb.utils.events INFO:  eta: 0:00:13  iteration: 937/1000  consumed samples: 14992  total_loss: 7.486  time: 0.2178(73.47)  data_time: 0.0030  lr: 9.90e-05  
[01/19 14:46:10] lb.utils.events INFO:  eta: 0:00:13  iteration: 938/1000  consumed samples: 15008  total_loss: 7.486  time: 0.2178(73.47)  data_time: 0.0029  lr: 9.90e-05  
[01/19 14:46:11] lb.utils.events INFO:  eta: 0:00:13  iteration: 939/1000  consumed samples: 15024  total_loss: 7.484  time: 0.2178(73.47)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:11] lb.utils.events INFO:  eta: 0:00:12  iteration: 940/1000  consumed samples: 15040  total_loss: 7.484  time: 0.2178(73.46)  data_time: 0.0028  lr: 9.90e-05  
[01/19 14:46:11] lb.utils.events INFO:  eta: 0:00:12  iteration: 941/1000  consumed samples: 15056  total_loss: 7.484  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:11] lb.utils.events INFO:  eta: 0:00:12  iteration: 942/1000  consumed samples: 15072  total_loss: 7.479  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:11] lb.utils.events INFO:  eta: 0:00:12  iteration: 943/1000  consumed samples: 15088  total_loss: 7.479  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:12] lb.utils.events INFO:  eta: 0:00:11  iteration: 944/1000  consumed samples: 15104  total_loss: 7.475  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:12] lb.utils.events INFO:  eta: 0:00:11  iteration: 945/1000  consumed samples: 15120  total_loss: 7.473  time: 0.2178(73.47)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:12] lb.utils.events INFO:  eta: 0:00:11  iteration: 946/1000  consumed samples: 15136  total_loss: 7.471  time: 0.2178(73.47)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:12] lb.utils.events INFO:  eta: 0:00:11  iteration: 947/1000  consumed samples: 15152  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:13] lb.utils.events INFO:  eta: 0:00:11  iteration: 948/1000  consumed samples: 15168  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:13] lb.utils.events INFO:  eta: 0:00:10  iteration: 949/1000  consumed samples: 15184  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:13] lb.utils.events INFO:  eta: 0:00:10  iteration: 950/1000  consumed samples: 15200  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:13] lb.utils.events INFO:  eta: 0:00:10  iteration: 951/1000  consumed samples: 15216  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:14] lb.utils.events INFO:  eta: 0:00:10  iteration: 952/1000  consumed samples: 15232  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:14] lb.utils.events INFO:  eta: 0:00:10  iteration: 953/1000  consumed samples: 15248  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 954/1000  consumed samples: 15264  total_loss: 7.47  time: 0.2178(73.46)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 955/1000  consumed samples: 15280  total_loss: 7.471  time: 0.2178(73.45)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 956/1000  consumed samples: 15296  total_loss: 7.473  time: 0.2178(73.45)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:15] lb.utils.events INFO:  eta: 0:00:09  iteration: 957/1000  consumed samples: 15312  total_loss: 7.473  time: 0.2178(73.45)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:15] lb.utils.events INFO:  eta: 0:00:08  iteration: 958/1000  consumed samples: 15328  total_loss: 7.471  time: 0.2178(73.45)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:15] lb.utils.events INFO:  eta: 0:00:08  iteration: 959/1000  consumed samples: 15344  total_loss: 7.47  time: 0.2179(73.44)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:15] lb.utils.events INFO:  eta: 0:00:08  iteration: 960/1000  consumed samples: 15360  total_loss: 7.471  time: 0.2179(73.44)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:16] lb.utils.events INFO:  eta: 0:00:08  iteration: 961/1000  consumed samples: 15376  total_loss: 7.47  time: 0.2179(73.44)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:16] lb.utils.events INFO:  eta: 0:00:08  iteration: 962/1000  consumed samples: 15392  total_loss: 7.47  time: 0.2179(73.44)  data_time: 0.0027  lr: 9.90e-05  
[01/19 14:46:16] lb.utils.events INFO:  eta: 0:00:07  iteration: 963/1000  consumed samples: 15408  total_loss: 7.47  time: 0.2179(73.44)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:16] lb.utils.events INFO:  eta: 0:00:07  iteration: 964/1000  consumed samples: 15424  total_loss: 7.471  time: 0.2179(73.43)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:17] lb.utils.events INFO:  eta: 0:00:07  iteration: 965/1000  consumed samples: 15440  total_loss: 7.473  time: 0.2179(73.43)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:17] lb.utils.events INFO:  eta: 0:00:07  iteration: 966/1000  consumed samples: 15456  total_loss: 7.471  time: 0.2179(73.43)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:17] lb.utils.events INFO:  eta: 0:00:06  iteration: 967/1000  consumed samples: 15472  total_loss: 7.47  time: 0.2179(73.43)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:17] lb.utils.events INFO:  eta: 0:00:06  iteration: 968/1000  consumed samples: 15488  total_loss: 7.47  time: 0.2179(73.43)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:18] lb.utils.events INFO:  eta: 0:00:06  iteration: 969/1000  consumed samples: 15504  total_loss: 7.468  time: 0.2179(73.43)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:18] lb.utils.events INFO:  eta: 0:00:06  iteration: 970/1000  consumed samples: 15520  total_loss: 7.465  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:18] lb.utils.events INFO:  eta: 0:00:06  iteration: 971/1000  consumed samples: 15536  total_loss: 7.468  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:18] lb.utils.events INFO:  eta: 0:00:05  iteration: 972/1000  consumed samples: 15552  total_loss: 7.468  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:18] lb.utils.events INFO:  eta: 0:00:05  iteration: 973/1000  consumed samples: 15568  total_loss: 7.468  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:19] lb.utils.events INFO:  eta: 0:00:05  iteration: 974/1000  consumed samples: 15584  total_loss: 7.468  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:19] lb.utils.events INFO:  eta: 0:00:05  iteration: 975/1000  consumed samples: 15600  total_loss: 7.465  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:19] lb.utils.events INFO:  eta: 0:00:05  iteration: 976/1000  consumed samples: 15616  total_loss: 7.468  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:19] lb.utils.events INFO:  eta: 0:00:04  iteration: 977/1000  consumed samples: 15632  total_loss: 7.465  time: 0.2179(73.43)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:20] lb.utils.events INFO:  eta: 0:00:04  iteration: 978/1000  consumed samples: 15648  total_loss: 7.465  time: 0.2179(73.42)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:20] lb.utils.events INFO:  eta: 0:00:04  iteration: 979/1000  consumed samples: 15664  total_loss: 7.465  time: 0.2179(73.42)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:20] lb.utils.events INFO:  eta: 0:00:04  iteration: 980/1000  consumed samples: 15680  total_loss: 7.463  time: 0.2179(73.42)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:20] lb.utils.events INFO:  eta: 0:00:03  iteration: 981/1000  consumed samples: 15696  total_loss: 7.463  time: 0.2179(73.42)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:21] lb.utils.events INFO:  eta: 0:00:03  iteration: 982/1000  consumed samples: 15712  total_loss: 7.465  time: 0.2179(73.41)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:21] lb.utils.events INFO:  eta: 0:00:03  iteration: 983/1000  consumed samples: 15728  total_loss: 7.465  time: 0.2179(73.41)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:21] lb.utils.events INFO:  eta: 0:00:03  iteration: 984/1000  consumed samples: 15744  total_loss: 7.463  time: 0.2180(73.41)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:21] lb.utils.events INFO:  eta: 0:00:03  iteration: 985/1000  consumed samples: 15760  total_loss: 7.463  time: 0.2180(73.41)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:21] lb.utils.events INFO:  eta: 0:00:02  iteration: 986/1000  consumed samples: 15776  total_loss: 7.465  time: 0.2180(73.41)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:22] lb.utils.events INFO:  eta: 0:00:02  iteration: 987/1000  consumed samples: 15792  total_loss: 7.465  time: 0.2180(73.41)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:22] lb.utils.events INFO:  eta: 0:00:02  iteration: 988/1000  consumed samples: 15808  total_loss: 7.463  time: 0.2180(73.40)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:22] lb.utils.events INFO:  eta: 0:00:02  iteration: 989/1000  consumed samples: 15824  total_loss: 7.462  time: 0.2180(73.40)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:22] lb.utils.events INFO:  eta: 0:00:01  iteration: 990/1000  consumed samples: 15840  total_loss: 7.462  time: 0.2180(73.40)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:23] lb.utils.events INFO:  eta: 0:00:01  iteration: 991/1000  consumed samples: 15856  total_loss: 7.463  time: 0.2180(73.40)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:23] lb.utils.events INFO:  eta: 0:00:01  iteration: 992/1000  consumed samples: 15872  total_loss: 7.463  time: 0.2180(73.39)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:23] lb.utils.events INFO:  eta: 0:00:01  iteration: 993/1000  consumed samples: 15888  total_loss: 7.463  time: 0.2180(73.39)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:23] lb.utils.events INFO:  eta: 0:00:01  iteration: 994/1000  consumed samples: 15904  total_loss: 7.463  time: 0.2180(73.39)  data_time: 0.0025  lr: 9.90e-05  
[01/19 14:46:24] lb.utils.events INFO:  eta: 0:00:00  iteration: 995/1000  consumed samples: 15920  total_loss: 7.463  time: 0.2180(73.39)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:24] lb.utils.events INFO:  eta: 0:00:00  iteration: 996/1000  consumed samples: 15936  total_loss: 7.463  time: 0.2180(73.38)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:24] lb.utils.events INFO:  eta: 0:00:00  iteration: 997/1000  consumed samples: 15952  total_loss: 7.462  time: 0.2180(73.38)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:24] lb.utils.events INFO:  eta: 0:00:00  iteration: 998/1000  consumed samples: 15968  total_loss: 7.461  time: 0.2180(73.38)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:25] lb.utils.events INFO:  eta: 0:00:00  iteration: 999/1000  consumed samples: 15984  total_loss: 7.46  time: 0.2180(73.38)  data_time: 0.0026  lr: 9.90e-05  
[01/19 14:46:25] lb.utils.checkpoint INFO: Saving checkpoint to ./demo_output/test_config/model_final
[01/19 14:46:25] lb.trainer.hooks INFO: Overall training speed: 998 iterations in 0:03:37 (0.2181 s / it)
[01/19 14:46:25] lb.trainer.hooks INFO: Total training time: 0:03:48 (0:00:10 on hooks)
[01/19 14:48:15] libai INFO: Rank of current process: 0. World size: 1
[01/19 14:48:15] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 14:48:15] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 14:48:15] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 14:48:15] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 14:48:15] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 14:48:18] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 14:48:18] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 14:48:18] libai INFO: Loadding megatron weight
[01/19 14:48:18] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 14:48:18] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 14:48:18] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 14:48:18] lb.data.build INFO:     train:      16000
[01/19 14:48:18] lb.data.build INFO:     validation: 160000
[01/19 14:48:18] lb.data.build INFO:     test:       160000
[01/19 14:48:18] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 14:48:18] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     reading document index...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 14:48:18] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 14:48:18] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.113178 seconds
[01/19 14:48:18] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 14:48:18] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 14:48:18] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 14:48:18] lb.data.dataset_utils INFO:  > dataset split:
[01/19 14:48:18] lb.data.dataset_utils INFO:     train:
[01/19 14:48:18] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 14:48:18] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 14:48:18] lb.data.dataset_utils INFO:     validation:
[01/19 14:48:18] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 14:48:18] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 14:48:18] lb.data.dataset_utils INFO:     test:
[01/19 14:48:18] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 14:48:18] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 14:48:18] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 14:48:18] lb.data.dataset_utils INFO:     loaded indexed file in 0.006 seconds
[01/19 14:48:18] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 14:48:18] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 14:48:18] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 14:48:18] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 14:48:18] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 14:48:18] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 14:48:18] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 14:48:18] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 14:48:19] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 14:48:24] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.1984  lr: 1.00e-04  
[01/19 14:48:24] lb.utils.events INFO:  eta: 0:03:55  iteration: 2/1000  consumed samples: 32  total_loss: 8.659  data_time: 0.1011  lr: 1.00e-04  
[01/19 14:48:24] lb.utils.events INFO:  eta: 0:03:23  iteration: 3/1000  consumed samples: 48  total_loss: 8.681  time: 0.2047(78.18)  data_time: 0.0692  lr: 1.00e-04  
[01/19 14:48:25] lb.utils.events INFO:  eta: 0:03:29  iteration: 4/1000  consumed samples: 64  total_loss: 8.659  time: 0.2108(75.91)  data_time: 0.0525  lr: 1.00e-04  
[01/19 14:48:25] lb.utils.events INFO:  eta: 0:03:34  iteration: 5/1000  consumed samples: 80  total_loss: 8.654  time: 0.2124(75.33)  data_time: 0.0507  lr: 1.00e-04  
[01/19 14:48:25] lb.utils.events INFO:  eta: 0:03:34  iteration: 6/1000  consumed samples: 96  total_loss: 8.646  time: 0.2136(74.89)  data_time: 0.0426  lr: 1.00e-04  
[01/19 14:48:25] lb.utils.events INFO:  eta: 0:03:33  iteration: 7/1000  consumed samples: 112  total_loss: 8.637  time: 0.2131(75.08)  data_time: 0.0369  lr: 1.00e-04  
[01/19 14:48:25] lb.utils.events INFO:  eta: 0:03:34  iteration: 8/1000  consumed samples: 128  total_loss: 8.627  time: 0.2137(74.86)  data_time: 0.0326  lr: 1.00e-04  
[01/19 14:48:26] lb.utils.events INFO:  eta: 0:03:33  iteration: 9/1000  consumed samples: 144  total_loss: 8.617  time: 0.2132(75.04)  data_time: 0.0295  lr: 1.00e-04  
[01/19 14:48:26] lb.utils.events INFO:  eta: 0:03:33  iteration: 10/1000  consumed samples: 160  total_loss: 8.612  time: 0.2137(74.88)  data_time: 0.0268  lr: 1.00e-04  
[01/19 14:48:26] lb.utils.events INFO:  eta: 0:03:34  iteration: 11/1000  consumed samples: 176  total_loss: 8.607  time: 0.2140(74.75)  data_time: 0.0247  lr: 1.00e-04  
[01/19 14:48:26] lb.utils.events INFO:  eta: 0:03:34  iteration: 12/1000  consumed samples: 192  total_loss: 8.589  time: 0.2144(74.64)  data_time: 0.0229  lr: 1.00e-04  
[01/19 14:48:27] lb.utils.events INFO:  eta: 0:03:33  iteration: 13/1000  consumed samples: 208  total_loss: 8.571  time: 0.2146(74.55)  data_time: 0.0215  lr: 1.00e-04  
[01/19 14:48:27] lb.utils.events INFO:  eta: 0:03:33  iteration: 14/1000  consumed samples: 224  total_loss: 8.577  time: 0.2149(74.47)  data_time: 0.0201  lr: 1.00e-04  
[01/19 14:48:27] lb.utils.events INFO:  eta: 0:03:33  iteration: 15/1000  consumed samples: 240  total_loss: 8.571  time: 0.2146(74.57)  data_time: 0.0191  lr: 1.00e-04  
[01/19 14:48:27] lb.utils.events INFO:  eta: 0:03:33  iteration: 16/1000  consumed samples: 256  total_loss: 8.567  time: 0.2148(74.49)  data_time: 0.0181  lr: 1.00e-04  
[01/19 14:48:27] lb.utils.events INFO:  eta: 0:03:32  iteration: 17/1000  consumed samples: 272  total_loss: 8.562  time: 0.2150(74.42)  data_time: 0.0172  lr: 1.00e-04  
[01/19 14:48:28] lb.utils.events INFO:  eta: 0:03:32  iteration: 18/1000  consumed samples: 288  total_loss: 8.562  time: 0.2152(74.36)  data_time: 0.0164  lr: 1.00e-04  
[01/19 14:48:28] lb.utils.events INFO:  eta: 0:03:32  iteration: 19/1000  consumed samples: 304  total_loss: 8.561  time: 0.2154(74.29)  data_time: 0.0157  lr: 1.00e-04  
[01/19 14:48:28] lb.utils.events INFO:  eta: 0:03:32  iteration: 20/1000  consumed samples: 320  total_loss: 8.559  time: 0.2155(74.25)  data_time: 0.0151  lr: 1.00e-04  
[01/19 14:48:28] lb.utils.events INFO:  eta: 0:03:32  iteration: 21/1000  consumed samples: 336  total_loss: 8.558  time: 0.2152(74.34)  data_time: 0.0055  lr: 1.00e-04  
[01/19 14:48:29] lb.utils.events INFO:  eta: 0:03:32  iteration: 22/1000  consumed samples: 352  total_loss: 8.521  time: 0.2154(74.30)  data_time: 0.0055  lr: 1.00e-04  
[01/19 14:48:29] lb.utils.events INFO:  eta: 0:03:31  iteration: 23/1000  consumed samples: 368  total_loss: 8.483  time: 0.2152(74.36)  data_time: 0.0054  lr: 1.00e-04  
[01/19 14:48:29] lb.utils.events INFO:  eta: 0:03:31  iteration: 24/1000  consumed samples: 384  total_loss: 8.446  time: 0.2153(74.31)  data_time: 0.0055  lr: 1.00e-04  
[01/19 14:48:29] lb.utils.events INFO:  eta: 0:03:31  iteration: 25/1000  consumed samples: 400  total_loss: 8.409  time: 0.2151(74.38)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:29] lb.utils.events INFO:  eta: 0:03:31  iteration: 26/1000  consumed samples: 416  total_loss: 8.407  time: 0.2153(74.32)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:30] lb.utils.events INFO:  eta: 0:03:30  iteration: 27/1000  consumed samples: 432  total_loss: 8.405  time: 0.2150(74.40)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:30] lb.utils.events INFO:  eta: 0:03:30  iteration: 28/1000  consumed samples: 448  total_loss: 8.394  time: 0.2152(74.36)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:30] lb.utils.events INFO:  eta: 0:03:30  iteration: 29/1000  consumed samples: 464  total_loss: 8.384  time: 0.2153(74.32)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:30] lb.utils.events INFO:  eta: 0:03:30  iteration: 30/1000  consumed samples: 480  total_loss: 8.379  time: 0.2154(74.29)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:31] lb.utils.events INFO:  eta: 0:03:30  iteration: 31/1000  consumed samples: 496  total_loss: 8.374  time: 0.2152(74.34)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:31] lb.utils.events INFO:  eta: 0:03:30  iteration: 32/1000  consumed samples: 512  total_loss: 8.373  time: 0.2153(74.31)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:31] lb.utils.events INFO:  eta: 0:03:29  iteration: 33/1000  consumed samples: 528  total_loss: 8.372  time: 0.2154(74.29)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:31] lb.utils.events INFO:  eta: 0:03:29  iteration: 34/1000  consumed samples: 544  total_loss: 8.371  time: 0.2155(74.26)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:31] lb.utils.events INFO:  eta: 0:03:29  iteration: 35/1000  consumed samples: 560  total_loss: 8.37  time: 0.2153(74.30)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:32] lb.utils.events INFO:  eta: 0:03:29  iteration: 36/1000  consumed samples: 576  total_loss: 8.356  time: 0.2154(74.27)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:32] lb.utils.events INFO:  eta: 0:03:29  iteration: 37/1000  consumed samples: 592  total_loss: 8.343  time: 0.2153(74.32)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:32] lb.utils.events INFO:  eta: 0:03:28  iteration: 38/1000  consumed samples: 608  total_loss: 8.337  time: 0.2154(74.28)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:32] lb.utils.events INFO:  eta: 0:03:28  iteration: 39/1000  consumed samples: 624  total_loss: 8.331  time: 0.2155(74.26)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:33] lb.utils.events INFO:  eta: 0:03:28  iteration: 40/1000  consumed samples: 640  total_loss: 8.304  time: 0.2155(74.23)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:33] lb.utils.events INFO:  eta: 0:03:28  iteration: 41/1000  consumed samples: 656  total_loss: 8.277  time: 0.2156(74.20)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:33] lb.utils.events INFO:  eta: 0:03:28  iteration: 42/1000  consumed samples: 672  total_loss: 8.267  time: 0.2157(74.18)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:33] lb.utils.events INFO:  eta: 0:03:27  iteration: 43/1000  consumed samples: 688  total_loss: 8.257  time: 0.2156(74.20)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:33] lb.utils.events INFO:  eta: 0:03:27  iteration: 44/1000  consumed samples: 704  total_loss: 8.251  time: 0.2157(74.18)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:34] lb.utils.events INFO:  eta: 0:03:27  iteration: 45/1000  consumed samples: 720  total_loss: 8.246  time: 0.2156(74.21)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:34] lb.utils.events INFO:  eta: 0:03:27  iteration: 46/1000  consumed samples: 736  total_loss: 8.228  time: 0.2157(74.18)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:34] lb.utils.events INFO:  eta: 0:03:26  iteration: 47/1000  consumed samples: 752  total_loss: 8.21  time: 0.2154(74.29)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:34] lb.utils.events INFO:  eta: 0:03:26  iteration: 48/1000  consumed samples: 768  total_loss: 8.208  time: 0.2154(74.27)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:48:35] lb.utils.events INFO:  eta: 0:03:26  iteration: 49/1000  consumed samples: 784  total_loss: 8.206  time: 0.2155(74.24)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:48:35] lb.utils.events INFO:  eta: 0:03:26  iteration: 50/1000  consumed samples: 800  total_loss: 8.201  time: 0.2156(74.20)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:48:35] lb.utils.events INFO:  eta: 0:03:26  iteration: 51/1000  consumed samples: 816  total_loss: 8.197  time: 0.2155(74.26)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:48:35] lb.utils.events INFO:  eta: 0:03:25  iteration: 52/1000  consumed samples: 832  total_loss: 8.195  time: 0.2155(74.24)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:48:35] lb.utils.events INFO:  eta: 0:03:25  iteration: 53/1000  consumed samples: 848  total_loss: 8.194  time: 0.2155(74.26)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:48:36] lb.utils.events INFO:  eta: 0:03:25  iteration: 54/1000  consumed samples: 864  total_loss: 8.192  time: 0.2156(74.23)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:48:36] lb.utils.events INFO:  eta: 0:03:25  iteration: 55/1000  consumed samples: 880  total_loss: 8.191  time: 0.2156(74.21)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:48:36] lb.utils.events INFO:  eta: 0:03:25  iteration: 56/1000  consumed samples: 896  total_loss: 8.186  time: 0.2157(74.18)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:48:36] lb.utils.events INFO:  eta: 0:03:24  iteration: 57/1000  consumed samples: 912  total_loss: 8.182  time: 0.2157(74.19)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:48:37] lb.utils.events INFO:  eta: 0:03:24  iteration: 58/1000  consumed samples: 928  total_loss: 8.178  time: 0.2157(74.17)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:48:37] lb.utils.events INFO:  eta: 0:03:24  iteration: 59/1000  consumed samples: 944  total_loss: 8.175  time: 0.2157(74.19)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:48:37] lb.utils.events INFO:  eta: 0:03:24  iteration: 60/1000  consumed samples: 960  total_loss: 8.173  time: 0.2157(74.17)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:37] lb.utils.events INFO:  eta: 0:03:24  iteration: 61/1000  consumed samples: 976  total_loss: 8.171  time: 0.2157(74.18)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:37] lb.utils.events INFO:  eta: 0:03:23  iteration: 62/1000  consumed samples: 992  total_loss: 8.165  time: 0.2157(74.16)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:38] lb.utils.events INFO:  eta: 0:03:23  iteration: 63/1000  consumed samples: 1008  total_loss: 8.158  time: 0.2157(74.18)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:38] lb.utils.events INFO:  eta: 0:03:23  iteration: 64/1000  consumed samples: 1024  total_loss: 8.158  time: 0.2158(74.16)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:48:38] lb.utils.events INFO:  eta: 0:03:23  iteration: 65/1000  consumed samples: 1040  total_loss: 8.158  time: 0.2157(74.17)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:38] lb.utils.events INFO:  eta: 0:03:22  iteration: 66/1000  consumed samples: 1056  total_loss: 8.158  time: 0.2158(74.16)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:39] lb.utils.events INFO:  eta: 0:03:22  iteration: 67/1000  consumed samples: 1072  total_loss: 8.158  time: 0.2157(74.17)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:39] lb.utils.events INFO:  eta: 0:03:22  iteration: 68/1000  consumed samples: 1088  total_loss: 8.158  time: 0.2158(74.15)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:39] lb.utils.events INFO:  eta: 0:03:22  iteration: 69/1000  consumed samples: 1104  total_loss: 8.157  time: 0.2158(74.13)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:39] lb.utils.events INFO:  eta: 0:03:22  iteration: 70/1000  consumed samples: 1120  total_loss: 8.151  time: 0.2159(74.11)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:40] lb.utils.events INFO:  eta: 0:03:21  iteration: 71/1000  consumed samples: 1136  total_loss: 8.144  time: 0.2159(74.12)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:40] lb.utils.events INFO:  eta: 0:03:21  iteration: 72/1000  consumed samples: 1152  total_loss: 8.142  time: 0.2159(74.10)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:40] lb.utils.events INFO:  eta: 0:03:21  iteration: 73/1000  consumed samples: 1168  total_loss: 8.141  time: 0.2159(74.12)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:40] lb.utils.events INFO:  eta: 0:03:21  iteration: 74/1000  consumed samples: 1184  total_loss: 8.142  time: 0.2159(74.10)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:40] lb.utils.events INFO:  eta: 0:03:21  iteration: 75/1000  consumed samples: 1200  total_loss: 8.141  time: 0.2159(74.11)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:41] lb.utils.events INFO:  eta: 0:03:20  iteration: 76/1000  consumed samples: 1216  total_loss: 8.137  time: 0.2159(74.09)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:41] lb.utils.events INFO:  eta: 0:03:20  iteration: 77/1000  consumed samples: 1232  total_loss: 8.134  time: 0.2159(74.11)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:41] lb.utils.events INFO:  eta: 0:03:20  iteration: 78/1000  consumed samples: 1248  total_loss: 8.131  time: 0.2159(74.12)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:41] lb.utils.events INFO:  eta: 0:03:20  iteration: 79/1000  consumed samples: 1264  total_loss: 8.128  time: 0.2158(74.13)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:42] lb.utils.events INFO:  eta: 0:03:19  iteration: 80/1000  consumed samples: 1280  total_loss: 8.131  time: 0.2159(74.11)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:42] lb.utils.events INFO:  eta: 0:03:19  iteration: 81/1000  consumed samples: 1296  total_loss: 8.128  time: 0.2159(74.12)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:48:42] lb.utils.events INFO:  eta: 0:03:19  iteration: 82/1000  consumed samples: 1312  total_loss: 8.124  time: 0.2159(74.10)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:48:42] lb.utils.events INFO:  eta: 0:03:19  iteration: 83/1000  consumed samples: 1328  total_loss: 8.12  time: 0.2159(74.11)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:48:42] lb.utils.events INFO:  eta: 0:03:19  iteration: 84/1000  consumed samples: 1344  total_loss: 8.119  time: 0.2159(74.09)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:48:43] lb.utils.events INFO:  eta: 0:03:18  iteration: 85/1000  consumed samples: 1360  total_loss: 8.118  time: 0.2159(74.10)  data_time: 0.0028  lr: 1.00e-04  
[01/19 14:48:43] lb.utils.events INFO:  eta: 0:03:18  iteration: 86/1000  consumed samples: 1376  total_loss: 8.106  time: 0.2160(74.08)  data_time: 0.0028  lr: 1.00e-04  
[01/19 14:48:43] lb.utils.events INFO:  eta: 0:03:18  iteration: 87/1000  consumed samples: 1392  total_loss: 8.118  time: 0.2159(74.09)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:48:43] lb.utils.events INFO:  eta: 0:03:18  iteration: 88/1000  consumed samples: 1408  total_loss: 8.106  time: 0.2160(74.07)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:48:44] lb.utils.events INFO:  eta: 0:03:17  iteration: 89/1000  consumed samples: 1424  total_loss: 8.094  time: 0.2160(74.08)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:48:44] lb.utils.events INFO:  eta: 0:03:17  iteration: 90/1000  consumed samples: 1440  total_loss: 8.09  time: 0.2160(74.06)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:44] lb.utils.events INFO:  eta: 0:03:17  iteration: 91/1000  consumed samples: 1456  total_loss: 8.085  time: 0.2160(74.07)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:44] lb.utils.events INFO:  eta: 0:03:17  iteration: 92/1000  consumed samples: 1472  total_loss: 8.083  time: 0.2161(74.05)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:44] lb.utils.events INFO:  eta: 0:03:17  iteration: 93/1000  consumed samples: 1488  total_loss: 8.081  time: 0.2161(74.03)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:45] lb.utils.events INFO:  eta: 0:03:16  iteration: 94/1000  consumed samples: 1504  total_loss: 8.083  time: 0.2161(74.05)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:45] lb.utils.events INFO:  eta: 0:03:16  iteration: 95/1000  consumed samples: 1520  total_loss: 8.081  time: 0.2161(74.04)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:45] lb.utils.events INFO:  eta: 0:03:16  iteration: 96/1000  consumed samples: 1536  total_loss: 8.079  time: 0.2161(74.02)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:45] lb.utils.events INFO:  eta: 0:03:16  iteration: 97/1000  consumed samples: 1552  total_loss: 8.077  time: 0.2161(74.03)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:46] lb.utils.events INFO:  eta: 0:03:16  iteration: 98/1000  consumed samples: 1568  total_loss: 8.074  time: 0.2162(74.01)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:46] lb.utils.events INFO:  eta: 0:03:15  iteration: 99/1000  consumed samples: 1584  total_loss: 8.072  time: 0.2162(74.02)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:46] lb.utils.events INFO:  eta: 0:03:15  iteration: 100/1000  consumed samples: 1600  total_loss: 8.067  time: 0.2162(74.00)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:46] lb.utils.events INFO:  eta: 0:03:15  iteration: 101/1000  consumed samples: 1616  total_loss: 8.061  time: 0.2163(73.98)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:47] lb.utils.events INFO:  eta: 0:03:15  iteration: 102/1000  consumed samples: 1632  total_loss: 8.058  time: 0.2162(73.99)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:47] lb.utils.events INFO:  eta: 0:03:14  iteration: 103/1000  consumed samples: 1648  total_loss: 8.055  time: 0.2162(74.00)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:47] lb.utils.events INFO:  eta: 0:03:14  iteration: 104/1000  consumed samples: 1664  total_loss: 8.054  time: 0.2162(74.01)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:47] lb.utils.events INFO:  eta: 0:03:14  iteration: 105/1000  consumed samples: 1680  total_loss: 8.053  time: 0.2162(73.99)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:47] lb.utils.events INFO:  eta: 0:03:14  iteration: 106/1000  consumed samples: 1696  total_loss: 8.049  time: 0.2162(74.00)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:48] lb.utils.events INFO:  eta: 0:03:14  iteration: 107/1000  consumed samples: 1712  total_loss: 8.044  time: 0.2163(73.98)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:48] lb.utils.events INFO:  eta: 0:03:13  iteration: 108/1000  consumed samples: 1728  total_loss: 8.038  time: 0.2162(73.99)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:48] lb.utils.events INFO:  eta: 0:03:13  iteration: 109/1000  consumed samples: 1744  total_loss: 8.044  time: 0.2163(73.97)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:48] lb.utils.events INFO:  eta: 0:03:13  iteration: 110/1000  consumed samples: 1760  total_loss: 8.038  time: 0.2163(73.98)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:49] lb.utils.events INFO:  eta: 0:03:13  iteration: 111/1000  consumed samples: 1776  total_loss: 8.044  time: 0.2163(73.96)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:49] lb.utils.events INFO:  eta: 0:03:12  iteration: 112/1000  consumed samples: 1792  total_loss: 8.049  time: 0.2163(73.97)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:49] lb.utils.events INFO:  eta: 0:03:12  iteration: 113/1000  consumed samples: 1808  total_loss: 8.044  time: 0.2164(73.95)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:49] lb.utils.events INFO:  eta: 0:03:12  iteration: 114/1000  consumed samples: 1824  total_loss: 8.038  time: 0.2164(73.93)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:49] lb.utils.events INFO:  eta: 0:03:12  iteration: 115/1000  consumed samples: 1840  total_loss: 8.044  time: 0.2164(73.92)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:50] lb.utils.events INFO:  eta: 0:03:12  iteration: 116/1000  consumed samples: 1856  total_loss: 8.049  time: 0.2165(73.91)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:50] lb.utils.events INFO:  eta: 0:03:11  iteration: 117/1000  consumed samples: 1872  total_loss: 8.053  time: 0.2165(73.89)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:50] lb.utils.events INFO:  eta: 0:03:11  iteration: 118/1000  consumed samples: 1888  total_loss: 8.049  time: 0.2166(73.88)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:50] lb.utils.events INFO:  eta: 0:03:11  iteration: 119/1000  consumed samples: 1904  total_loss: 8.044  time: 0.2166(73.86)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:48:51] lb.utils.events INFO:  eta: 0:03:11  iteration: 120/1000  consumed samples: 1920  total_loss: 8.045  time: 0.2167(73.85)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:51] lb.utils.events INFO:  eta: 0:03:11  iteration: 121/1000  consumed samples: 1936  total_loss: 8.046  time: 0.2166(73.85)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:51] lb.utils.events INFO:  eta: 0:03:11  iteration: 122/1000  consumed samples: 1952  total_loss: 8.045  time: 0.2167(73.84)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:51] lb.utils.events INFO:  eta: 0:03:10  iteration: 123/1000  consumed samples: 1968  total_loss: 8.044  time: 0.2167(73.84)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:51] lb.utils.events INFO:  eta: 0:03:10  iteration: 124/1000  consumed samples: 1984  total_loss: 8.038  time: 0.2167(73.82)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:52] lb.utils.events INFO:  eta: 0:03:10  iteration: 125/1000  consumed samples: 2000  total_loss: 8.044  time: 0.2167(73.83)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:52] lb.utils.events INFO:  eta: 0:03:10  iteration: 126/1000  consumed samples: 2016  total_loss: 8.038  time: 0.2168(73.82)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:52] lb.utils.events INFO:  eta: 0:03:09  iteration: 127/1000  consumed samples: 2032  total_loss: 8.032  time: 0.2167(73.82)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:52] lb.utils.events INFO:  eta: 0:03:09  iteration: 128/1000  consumed samples: 2048  total_loss: 8.029  time: 0.2168(73.81)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:53] lb.utils.events INFO:  eta: 0:03:09  iteration: 129/1000  consumed samples: 2064  total_loss: 8.026  time: 0.2168(73.81)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:53] lb.utils.events INFO:  eta: 0:03:09  iteration: 130/1000  consumed samples: 2080  total_loss: 8.025  time: 0.2168(73.79)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:53] lb.utils.events INFO:  eta: 0:03:08  iteration: 131/1000  consumed samples: 2096  total_loss: 8.025  time: 0.2168(73.79)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:53] lb.utils.events INFO:  eta: 0:03:08  iteration: 132/1000  consumed samples: 2112  total_loss: 8.024  time: 0.2169(73.78)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:54] lb.utils.events INFO:  eta: 0:03:08  iteration: 133/1000  consumed samples: 2128  total_loss: 8.022  time: 0.2169(73.78)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:54] lb.utils.events INFO:  eta: 0:03:08  iteration: 134/1000  consumed samples: 2144  total_loss: 8.022  time: 0.2169(73.76)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:54] lb.utils.events INFO:  eta: 0:03:08  iteration: 135/1000  consumed samples: 2160  total_loss: 8.022  time: 0.2169(73.76)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:54] lb.utils.events INFO:  eta: 0:03:08  iteration: 136/1000  consumed samples: 2176  total_loss: 8.022  time: 0.2170(73.75)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:48:54] lb.utils.events INFO:  eta: 0:03:07  iteration: 137/1000  consumed samples: 2192  total_loss: 8.022  time: 0.2170(73.75)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:48:55] lb.utils.events INFO:  eta: 0:03:07  iteration: 138/1000  consumed samples: 2208  total_loss: 8.022  time: 0.2170(73.73)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:48:55] lb.utils.events INFO:  eta: 0:03:07  iteration: 139/1000  consumed samples: 2224  total_loss: 8.022  time: 0.2170(73.73)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:48:55] lb.utils.events INFO:  eta: 0:03:07  iteration: 140/1000  consumed samples: 2240  total_loss: 8.021  time: 0.2171(73.71)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:48:55] lb.utils.events INFO:  eta: 0:03:06  iteration: 141/1000  consumed samples: 2256  total_loss: 8.02  time: 0.2171(73.71)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:48:56] lb.utils.events INFO:  eta: 0:03:06  iteration: 142/1000  consumed samples: 2272  total_loss: 8.02  time: 0.2171(73.70)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:48:56] lb.utils.events INFO:  eta: 0:03:06  iteration: 143/1000  consumed samples: 2288  total_loss: 8.02  time: 0.2172(73.68)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:56] lb.utils.events INFO:  eta: 0:03:06  iteration: 144/1000  consumed samples: 2304  total_loss: 8.018  time: 0.2172(73.67)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:56] lb.utils.events INFO:  eta: 0:03:06  iteration: 145/1000  consumed samples: 2320  total_loss: 8.016  time: 0.2172(73.66)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:57] lb.utils.events INFO:  eta: 0:03:05  iteration: 146/1000  consumed samples: 2336  total_loss: 8.018  time: 0.2172(73.65)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:57] lb.utils.events INFO:  eta: 0:03:05  iteration: 147/1000  consumed samples: 2352  total_loss: 8.016  time: 0.2172(73.65)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:48:57] lb.utils.events INFO:  eta: 0:03:05  iteration: 148/1000  consumed samples: 2368  total_loss: 8.018  time: 0.2173(73.64)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:48:57] lb.utils.events INFO:  eta: 0:03:05  iteration: 149/1000  consumed samples: 2384  total_loss: 8.016  time: 0.2173(73.65)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:57] lb.utils.events INFO:  eta: 0:03:04  iteration: 150/1000  consumed samples: 2400  total_loss: 8.015  time: 0.2173(73.63)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:58] lb.utils.events INFO:  eta: 0:03:04  iteration: 151/1000  consumed samples: 2416  total_loss: 8.014  time: 0.2173(73.63)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:58] lb.utils.events INFO:  eta: 0:03:04  iteration: 152/1000  consumed samples: 2432  total_loss: 8.014  time: 0.2173(73.62)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:48:58] lb.utils.events INFO:  eta: 0:03:04  iteration: 153/1000  consumed samples: 2448  total_loss: 8.013  time: 0.2173(73.62)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:58] lb.utils.events INFO:  eta: 0:03:04  iteration: 154/1000  consumed samples: 2464  total_loss: 8.011  time: 0.2174(73.61)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:59] lb.utils.events INFO:  eta: 0:03:03  iteration: 155/1000  consumed samples: 2480  total_loss: 8.01  time: 0.2174(73.61)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:59] lb.utils.events INFO:  eta: 0:03:03  iteration: 156/1000  consumed samples: 2496  total_loss: 8.011  time: 0.2174(73.60)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:59] lb.utils.events INFO:  eta: 0:03:03  iteration: 157/1000  consumed samples: 2512  total_loss: 8.01  time: 0.2174(73.60)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:48:59] lb.utils.events INFO:  eta: 0:03:03  iteration: 158/1000  consumed samples: 2528  total_loss: 8.011  time: 0.2174(73.59)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:48:59] lb.utils.events INFO:  eta: 0:03:03  iteration: 159/1000  consumed samples: 2544  total_loss: 8.01  time: 0.2174(73.59)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:00] lb.utils.events INFO:  eta: 0:03:02  iteration: 160/1000  consumed samples: 2560  total_loss: 8.01  time: 0.2175(73.58)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:00] lb.utils.events INFO:  eta: 0:03:02  iteration: 161/1000  consumed samples: 2576  total_loss: 8.01  time: 0.2175(73.58)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:00] lb.utils.events INFO:  eta: 0:03:02  iteration: 162/1000  consumed samples: 2592  total_loss: 8.01  time: 0.2175(73.57)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:00] lb.utils.events INFO:  eta: 0:03:02  iteration: 163/1000  consumed samples: 2608  total_loss: 8.01  time: 0.2175(73.57)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:01] lb.utils.events INFO:  eta: 0:03:01  iteration: 164/1000  consumed samples: 2624  total_loss: 8.01  time: 0.2175(73.56)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:01] lb.utils.events INFO:  eta: 0:03:01  iteration: 165/1000  consumed samples: 2640  total_loss: 8.009  time: 0.2175(73.56)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:01] lb.utils.events INFO:  eta: 0:03:01  iteration: 166/1000  consumed samples: 2656  total_loss: 8.009  time: 0.2175(73.55)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:01] lb.utils.events INFO:  eta: 0:03:01  iteration: 167/1000  consumed samples: 2672  total_loss: 8.009  time: 0.2175(73.55)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:02] lb.utils.events INFO:  eta: 0:03:01  iteration: 168/1000  consumed samples: 2688  total_loss: 8.006  time: 0.2176(73.54)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:02] lb.utils.events INFO:  eta: 0:03:00  iteration: 169/1000  consumed samples: 2704  total_loss: 8.004  time: 0.2176(73.53)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:02] lb.utils.events INFO:  eta: 0:03:00  iteration: 170/1000  consumed samples: 2720  total_loss: 8.006  time: 0.2176(73.52)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:02] lb.utils.events INFO:  eta: 0:03:00  iteration: 171/1000  consumed samples: 2736  total_loss: 8.009  time: 0.2177(73.51)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:02] lb.utils.events INFO:  eta: 0:03:00  iteration: 172/1000  consumed samples: 2752  total_loss: 8.006  time: 0.2175(73.56)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:03] lb.utils.events INFO:  eta: 0:03:00  iteration: 173/1000  consumed samples: 2768  total_loss: 8.004  time: 0.2175(73.55)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:03] lb.utils.events INFO:  eta: 0:02:59  iteration: 174/1000  consumed samples: 2784  total_loss: 8.006  time: 0.2175(73.55)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:03] lb.utils.events INFO:  eta: 0:02:59  iteration: 175/1000  consumed samples: 2800  total_loss: 8.004  time: 0.2176(73.53)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:03] lb.utils.events INFO:  eta: 0:02:59  iteration: 176/1000  consumed samples: 2816  total_loss: 8.006  time: 0.2176(73.54)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:04] lb.utils.events INFO:  eta: 0:02:59  iteration: 177/1000  consumed samples: 2832  total_loss: 8.004  time: 0.2176(73.53)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:04] lb.utils.events INFO:  eta: 0:02:59  iteration: 178/1000  consumed samples: 2848  total_loss: 8.006  time: 0.2176(73.53)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:04] lb.utils.events INFO:  eta: 0:02:58  iteration: 179/1000  consumed samples: 2864  total_loss: 8.004  time: 0.2176(73.52)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:04] lb.utils.events INFO:  eta: 0:02:58  iteration: 180/1000  consumed samples: 2880  total_loss: 8.004  time: 0.2176(73.52)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:05] lb.utils.events INFO:  eta: 0:02:58  iteration: 181/1000  consumed samples: 2896  total_loss: 8.003  time: 0.2176(73.53)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:05] lb.utils.events INFO:  eta: 0:02:58  iteration: 182/1000  consumed samples: 2912  total_loss: 8  time: 0.2176(73.52)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:05] lb.utils.events INFO:  eta: 0:02:58  iteration: 183/1000  consumed samples: 2928  total_loss: 7.998  time: 0.2177(73.51)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:05] lb.utils.events INFO:  eta: 0:02:57  iteration: 184/1000  consumed samples: 2944  total_loss: 7.997  time: 0.2176(73.52)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:05] lb.utils.events INFO:  eta: 0:02:57  iteration: 185/1000  consumed samples: 2960  total_loss: 7.997  time: 0.2176(73.52)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:06] lb.utils.events INFO:  eta: 0:02:57  iteration: 186/1000  consumed samples: 2976  total_loss: 7.997  time: 0.2176(73.52)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:06] lb.utils.events INFO:  eta: 0:02:56  iteration: 187/1000  consumed samples: 2992  total_loss: 7.998  time: 0.2176(73.52)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:49:06] lb.utils.events INFO:  eta: 0:02:56  iteration: 188/1000  consumed samples: 3008  total_loss: 8  time: 0.2176(73.52)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:49:06] lb.utils.events INFO:  eta: 0:02:56  iteration: 189/1000  consumed samples: 3024  total_loss: 7.998  time: 0.2177(73.51)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:07] lb.utils.events INFO:  eta: 0:02:56  iteration: 190/1000  consumed samples: 3040  total_loss: 7.997  time: 0.2177(73.51)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:07] lb.utils.events INFO:  eta: 0:02:56  iteration: 191/1000  consumed samples: 3056  total_loss: 7.997  time: 0.2177(73.50)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:07] lb.utils.events INFO:  eta: 0:02:55  iteration: 192/1000  consumed samples: 3072  total_loss: 7.997  time: 0.2177(73.50)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:07] lb.utils.events INFO:  eta: 0:02:55  iteration: 193/1000  consumed samples: 3088  total_loss: 7.997  time: 0.2177(73.49)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:07] lb.utils.events INFO:  eta: 0:02:55  iteration: 194/1000  consumed samples: 3104  total_loss: 7.997  time: 0.2177(73.49)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:08] lb.utils.events INFO:  eta: 0:02:55  iteration: 195/1000  consumed samples: 3120  total_loss: 7.997  time: 0.2177(73.50)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:08] lb.utils.events INFO:  eta: 0:02:54  iteration: 196/1000  consumed samples: 3136  total_loss: 7.996  time: 0.2176(73.51)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:08] lb.utils.events INFO:  eta: 0:02:54  iteration: 197/1000  consumed samples: 3152  total_loss: 7.995  time: 0.2177(73.50)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:08] lb.utils.events INFO:  eta: 0:02:54  iteration: 198/1000  consumed samples: 3168  total_loss: 7.995  time: 0.2177(73.51)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:09] lb.utils.events INFO:  eta: 0:02:54  iteration: 199/1000  consumed samples: 3184  total_loss: 7.995  time: 0.2177(73.50)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:09] lb.utils.events INFO:  eta: 0:02:54  iteration: 200/1000  consumed samples: 3200  total_loss: 7.995  time: 0.2177(73.50)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:09] lb.utils.events INFO:  eta: 0:02:53  iteration: 201/1000  consumed samples: 3216  total_loss: 7.995  time: 0.2177(73.50)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:09] lb.utils.events INFO:  eta: 0:02:53  iteration: 202/1000  consumed samples: 3232  total_loss: 7.994  time: 0.2177(73.50)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:10] lb.utils.events INFO:  eta: 0:02:53  iteration: 203/1000  consumed samples: 3248  total_loss: 7.994  time: 0.2177(73.49)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:10] lb.utils.events INFO:  eta: 0:02:53  iteration: 204/1000  consumed samples: 3264  total_loss: 7.993  time: 0.2177(73.51)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:10] lb.utils.events INFO:  eta: 0:02:52  iteration: 205/1000  consumed samples: 3280  total_loss: 7.99  time: 0.2177(73.50)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:10] lb.utils.events INFO:  eta: 0:02:52  iteration: 206/1000  consumed samples: 3296  total_loss: 7.99  time: 0.2177(73.51)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:10] lb.utils.events INFO:  eta: 0:02:52  iteration: 207/1000  consumed samples: 3312  total_loss: 7.99  time: 0.2177(73.50)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:11] lb.utils.events INFO:  eta: 0:02:52  iteration: 208/1000  consumed samples: 3328  total_loss: 7.99  time: 0.2177(73.50)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:11] lb.utils.events INFO:  eta: 0:02:52  iteration: 209/1000  consumed samples: 3344  total_loss: 7.987  time: 0.2177(73.49)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:11] lb.utils.events INFO:  eta: 0:02:51  iteration: 210/1000  consumed samples: 3360  total_loss: 7.985  time: 0.2177(73.49)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:11] lb.utils.events INFO:  eta: 0:02:51  iteration: 211/1000  consumed samples: 3376  total_loss: 7.983  time: 0.2177(73.49)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:12] lb.utils.events INFO:  eta: 0:02:51  iteration: 212/1000  consumed samples: 3392  total_loss: 7.982  time: 0.2177(73.49)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:12] lb.utils.events INFO:  eta: 0:02:51  iteration: 213/1000  consumed samples: 3408  total_loss: 7.982  time: 0.2178(73.48)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:12] lb.utils.events INFO:  eta: 0:02:51  iteration: 214/1000  consumed samples: 3424  total_loss: 7.982  time: 0.2177(73.48)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:12] lb.utils.events INFO:  eta: 0:02:50  iteration: 215/1000  consumed samples: 3440  total_loss: 7.982  time: 0.2178(73.47)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:13] lb.utils.events INFO:  eta: 0:02:50  iteration: 216/1000  consumed samples: 3456  total_loss: 7.981  time: 0.2178(73.47)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:13] lb.utils.events INFO:  eta: 0:02:50  iteration: 217/1000  consumed samples: 3472  total_loss: 7.981  time: 0.2178(73.46)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:13] lb.utils.events INFO:  eta: 0:02:50  iteration: 218/1000  consumed samples: 3488  total_loss: 7.98  time: 0.2178(73.47)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:13] lb.utils.events INFO:  eta: 0:02:49  iteration: 219/1000  consumed samples: 3504  total_loss: 7.978  time: 0.2178(73.46)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:13] lb.utils.events INFO:  eta: 0:02:49  iteration: 220/1000  consumed samples: 3520  total_loss: 7.976  time: 0.2178(73.45)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:14] lb.utils.events INFO:  eta: 0:02:49  iteration: 221/1000  consumed samples: 3536  total_loss: 7.976  time: 0.2179(73.44)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:14] lb.utils.events INFO:  eta: 0:02:49  iteration: 222/1000  consumed samples: 3552  total_loss: 7.976  time: 0.2179(73.44)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:14] lb.utils.events INFO:  eta: 0:02:49  iteration: 223/1000  consumed samples: 3568  total_loss: 7.973  time: 0.2179(73.43)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:14] lb.utils.events INFO:  eta: 0:02:48  iteration: 224/1000  consumed samples: 3584  total_loss: 7.973  time: 0.2179(73.44)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:15] lb.utils.events INFO:  eta: 0:02:48  iteration: 225/1000  consumed samples: 3600  total_loss: 7.973  time: 0.2179(73.43)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:15] lb.utils.events INFO:  eta: 0:02:48  iteration: 226/1000  consumed samples: 3616  total_loss: 7.973  time: 0.2179(73.43)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:15] lb.utils.events INFO:  eta: 0:02:48  iteration: 227/1000  consumed samples: 3632  total_loss: 7.971  time: 0.2179(73.42)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:15] lb.utils.events INFO:  eta: 0:02:48  iteration: 228/1000  consumed samples: 3648  total_loss: 7.968  time: 0.2179(73.41)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:15] lb.utils.events INFO:  eta: 0:02:47  iteration: 229/1000  consumed samples: 3664  total_loss: 7.968  time: 0.2179(73.42)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:16] lb.utils.events INFO:  eta: 0:02:47  iteration: 230/1000  consumed samples: 3680  total_loss: 7.968  time: 0.2180(73.41)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:16] lb.utils.events INFO:  eta: 0:02:47  iteration: 231/1000  consumed samples: 3696  total_loss: 7.968  time: 0.2179(73.42)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:16] lb.utils.events INFO:  eta: 0:02:47  iteration: 232/1000  consumed samples: 3712  total_loss: 7.968  time: 0.2180(73.41)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:16] lb.utils.events INFO:  eta: 0:02:46  iteration: 233/1000  consumed samples: 3728  total_loss: 7.967  time: 0.2180(73.41)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:17] lb.utils.events INFO:  eta: 0:02:46  iteration: 234/1000  consumed samples: 3744  total_loss: 7.967  time: 0.2180(73.40)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:17] lb.utils.events INFO:  eta: 0:02:46  iteration: 235/1000  consumed samples: 3760  total_loss: 7.967  time: 0.2180(73.40)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:17] lb.utils.events INFO:  eta: 0:02:46  iteration: 236/1000  consumed samples: 3776  total_loss: 7.966  time: 0.2180(73.39)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:17] lb.utils.events INFO:  eta: 0:02:46  iteration: 237/1000  consumed samples: 3792  total_loss: 7.966  time: 0.2180(73.39)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:18] lb.utils.events INFO:  eta: 0:02:46  iteration: 238/1000  consumed samples: 3808  total_loss: 7.965  time: 0.2180(73.39)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:18] lb.utils.events INFO:  eta: 0:02:45  iteration: 239/1000  consumed samples: 3824  total_loss: 7.965  time: 0.2180(73.39)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:18] lb.utils.events INFO:  eta: 0:02:45  iteration: 240/1000  consumed samples: 3840  total_loss: 7.965  time: 0.2180(73.38)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:18] lb.utils.events INFO:  eta: 0:02:45  iteration: 241/1000  consumed samples: 3856  total_loss: 7.96  time: 0.2180(73.38)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:18] lb.utils.events INFO:  eta: 0:02:45  iteration: 242/1000  consumed samples: 3872  total_loss: 7.957  time: 0.2181(73.37)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:19] lb.utils.events INFO:  eta: 0:02:45  iteration: 243/1000  consumed samples: 3888  total_loss: 7.957  time: 0.2180(73.38)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:19] lb.utils.events INFO:  eta: 0:02:44  iteration: 244/1000  consumed samples: 3904  total_loss: 7.956  time: 0.2181(73.37)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:19] lb.utils.events INFO:  eta: 0:02:44  iteration: 245/1000  consumed samples: 3920  total_loss: 7.957  time: 0.2181(73.36)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:19] lb.utils.events INFO:  eta: 0:02:44  iteration: 246/1000  consumed samples: 3936  total_loss: 7.957  time: 0.2181(73.35)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:20] lb.utils.events INFO:  eta: 0:02:44  iteration: 247/1000  consumed samples: 3952  total_loss: 7.96  time: 0.2181(73.36)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:20] lb.utils.events INFO:  eta: 0:02:44  iteration: 248/1000  consumed samples: 3968  total_loss: 7.957  time: 0.2181(73.35)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:20] lb.utils.events INFO:  eta: 0:02:43  iteration: 249/1000  consumed samples: 3984  total_loss: 7.956  time: 0.2181(73.35)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:20] lb.utils.events INFO:  eta: 0:02:43  iteration: 250/1000  consumed samples: 4000  total_loss: 7.954  time: 0.2182(73.34)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:21] lb.utils.events INFO:  eta: 0:02:43  iteration: 251/1000  consumed samples: 4016  total_loss: 7.954  time: 0.2181(73.35)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:21] lb.utils.events INFO:  eta: 0:02:43  iteration: 252/1000  consumed samples: 4032  total_loss: 7.956  time: 0.2182(73.34)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:21] lb.utils.events INFO:  eta: 0:02:42  iteration: 253/1000  consumed samples: 4048  total_loss: 7.954  time: 0.2182(73.34)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:21] lb.utils.events INFO:  eta: 0:02:42  iteration: 254/1000  consumed samples: 4064  total_loss: 7.954  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:21] lb.utils.events INFO:  eta: 0:02:42  iteration: 255/1000  consumed samples: 4080  total_loss: 7.953  time: 0.2181(73.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:22] lb.utils.events INFO:  eta: 0:02:42  iteration: 256/1000  consumed samples: 4096  total_loss: 7.953  time: 0.2181(73.36)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:22] lb.utils.events INFO:  eta: 0:02:41  iteration: 257/1000  consumed samples: 4112  total_loss: 7.953  time: 0.2181(73.36)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:22] lb.utils.events INFO:  eta: 0:02:41  iteration: 258/1000  consumed samples: 4128  total_loss: 7.954  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:22] lb.utils.events INFO:  eta: 0:02:41  iteration: 259/1000  consumed samples: 4144  total_loss: 7.954  time: 0.2181(73.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:23] lb.utils.events INFO:  eta: 0:02:41  iteration: 260/1000  consumed samples: 4160  total_loss: 7.956  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:23] lb.utils.events INFO:  eta: 0:02:40  iteration: 261/1000  consumed samples: 4176  total_loss: 7.956  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:23] lb.utils.events INFO:  eta: 0:02:40  iteration: 262/1000  consumed samples: 4192  total_loss: 7.954  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:23] lb.utils.events INFO:  eta: 0:02:40  iteration: 263/1000  consumed samples: 4208  total_loss: 7.953  time: 0.2181(73.37)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:23] lb.utils.events INFO:  eta: 0:02:40  iteration: 264/1000  consumed samples: 4224  total_loss: 7.952  time: 0.2181(73.36)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:24] lb.utils.events INFO:  eta: 0:02:39  iteration: 265/1000  consumed samples: 4240  total_loss: 7.951  time: 0.2181(73.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:24] lb.utils.events INFO:  eta: 0:02:39  iteration: 266/1000  consumed samples: 4256  total_loss: 7.952  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:24] lb.utils.events INFO:  eta: 0:02:39  iteration: 267/1000  consumed samples: 4272  total_loss: 7.952  time: 0.2181(73.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:24] lb.utils.events INFO:  eta: 0:02:39  iteration: 268/1000  consumed samples: 4288  total_loss: 7.952  time: 0.2181(73.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:25] lb.utils.events INFO:  eta: 0:02:39  iteration: 269/1000  consumed samples: 4304  total_loss: 7.951  time: 0.2181(73.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:25] lb.utils.events INFO:  eta: 0:02:39  iteration: 270/1000  consumed samples: 4320  total_loss: 7.949  time: 0.2181(73.35)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:25] lb.utils.events INFO:  eta: 0:02:38  iteration: 271/1000  consumed samples: 4336  total_loss: 7.949  time: 0.2181(73.35)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:25] lb.utils.events INFO:  eta: 0:02:38  iteration: 272/1000  consumed samples: 4352  total_loss: 7.949  time: 0.2181(73.35)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:26] lb.utils.events INFO:  eta: 0:02:38  iteration: 273/1000  consumed samples: 4368  total_loss: 7.946  time: 0.2182(73.34)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:26] lb.utils.events INFO:  eta: 0:02:38  iteration: 274/1000  consumed samples: 4384  total_loss: 7.944  time: 0.2181(73.35)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:26] lb.utils.events INFO:  eta: 0:02:38  iteration: 275/1000  consumed samples: 4400  total_loss: 7.943  time: 0.2181(73.35)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:26] lb.utils.events INFO:  eta: 0:02:37  iteration: 276/1000  consumed samples: 4416  total_loss: 7.943  time: 0.2181(73.35)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:49:26] lb.utils.events INFO:  eta: 0:02:37  iteration: 277/1000  consumed samples: 4432  total_loss: 7.943  time: 0.2182(73.34)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:49:27] lb.utils.events INFO:  eta: 0:02:37  iteration: 278/1000  consumed samples: 4448  total_loss: 7.943  time: 0.2182(73.34)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:27] lb.utils.events INFO:  eta: 0:02:37  iteration: 279/1000  consumed samples: 4464  total_loss: 7.942  time: 0.2182(73.34)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:49:27] lb.utils.events INFO:  eta: 0:02:36  iteration: 280/1000  consumed samples: 4480  total_loss: 7.942  time: 0.2182(73.34)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:49:27] lb.utils.events INFO:  eta: 0:02:36  iteration: 281/1000  consumed samples: 4496  total_loss: 7.942  time: 0.2182(73.33)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:28] lb.utils.events INFO:  eta: 0:02:36  iteration: 282/1000  consumed samples: 4512  total_loss: 7.942  time: 0.2182(73.33)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:28] lb.utils.events INFO:  eta: 0:02:36  iteration: 283/1000  consumed samples: 4528  total_loss: 7.942  time: 0.2182(73.34)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:28] lb.utils.events INFO:  eta: 0:02:36  iteration: 284/1000  consumed samples: 4544  total_loss: 7.942  time: 0.2182(73.33)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:28] lb.utils.events INFO:  eta: 0:02:35  iteration: 285/1000  consumed samples: 4560  total_loss: 7.943  time: 0.2181(73.35)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:29] lb.utils.events INFO:  eta: 0:02:35  iteration: 286/1000  consumed samples: 4576  total_loss: 7.942  time: 0.2181(73.37)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:29] lb.utils.events INFO:  eta: 0:02:35  iteration: 287/1000  consumed samples: 4592  total_loss: 7.942  time: 0.2181(73.37)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:29] lb.utils.events INFO:  eta: 0:02:35  iteration: 288/1000  consumed samples: 4608  total_loss: 7.941  time: 0.2181(73.37)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:29] lb.utils.events INFO:  eta: 0:02:34  iteration: 289/1000  consumed samples: 4624  total_loss: 7.94  time: 0.2181(73.37)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:29] lb.utils.events INFO:  eta: 0:02:34  iteration: 290/1000  consumed samples: 4640  total_loss: 7.942  time: 0.2181(73.37)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:30] lb.utils.events INFO:  eta: 0:02:34  iteration: 291/1000  consumed samples: 4656  total_loss: 7.94  time: 0.2181(73.37)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:30] lb.utils.events INFO:  eta: 0:02:34  iteration: 292/1000  consumed samples: 4672  total_loss: 7.938  time: 0.2181(73.36)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:30] lb.utils.events INFO:  eta: 0:02:33  iteration: 293/1000  consumed samples: 4688  total_loss: 7.937  time: 0.2181(73.36)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:30] lb.utils.events INFO:  eta: 0:02:33  iteration: 294/1000  consumed samples: 4704  total_loss: 7.936  time: 0.2181(73.36)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:49:31] lb.utils.events INFO:  eta: 0:02:33  iteration: 295/1000  consumed samples: 4720  total_loss: 7.936  time: 0.2181(73.35)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:49:31] lb.utils.events INFO:  eta: 0:02:33  iteration: 296/1000  consumed samples: 4736  total_loss: 7.933  time: 0.2182(73.34)  data_time: 0.0028  lr: 1.00e-04  
[01/19 14:49:31] lb.utils.events INFO:  eta: 0:02:33  iteration: 297/1000  consumed samples: 4752  total_loss: 7.93  time: 0.2181(73.34)  data_time: 0.0027  lr: 1.00e-04  
[01/19 14:49:31] lb.utils.events INFO:  eta: 0:02:33  iteration: 298/1000  consumed samples: 4768  total_loss: 7.93  time: 0.2182(73.34)  data_time: 0.0027  lr: 1.00e-04  
[01/19 14:49:31] lb.utils.events INFO:  eta: 0:02:32  iteration: 299/1000  consumed samples: 4784  total_loss: 7.928  time: 0.2182(73.34)  data_time: 0.0028  lr: 1.00e-04  
[01/19 14:49:32] lb.utils.events INFO:  eta: 0:02:32  iteration: 300/1000  consumed samples: 4800  total_loss: 7.928  time: 0.2182(73.34)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:49:32] lb.utils.events INFO:  eta: 0:02:32  iteration: 301/1000  consumed samples: 4816  total_loss: 7.93  time: 0.2182(73.34)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:49:32] lb.utils.events INFO:  eta: 0:02:32  iteration: 302/1000  consumed samples: 4832  total_loss: 7.928  time: 0.2182(73.33)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:32] lb.utils.events INFO:  eta: 0:02:31  iteration: 303/1000  consumed samples: 4848  total_loss: 7.928  time: 0.2182(73.33)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:33] lb.utils.events INFO:  eta: 0:02:31  iteration: 304/1000  consumed samples: 4864  total_loss: 7.925  time: 0.2182(73.33)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:33] lb.utils.events INFO:  eta: 0:02:31  iteration: 305/1000  consumed samples: 4880  total_loss: 7.928  time: 0.2182(73.33)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:33] lb.utils.events INFO:  eta: 0:02:31  iteration: 306/1000  consumed samples: 4896  total_loss: 7.928  time: 0.2182(73.32)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:33] lb.utils.events INFO:  eta: 0:02:31  iteration: 307/1000  consumed samples: 4912  total_loss: 7.928  time: 0.2182(73.33)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:34] lb.utils.events INFO:  eta: 0:02:30  iteration: 308/1000  consumed samples: 4928  total_loss: 7.928  time: 0.2182(73.32)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:34] lb.utils.events INFO:  eta: 0:02:30  iteration: 309/1000  consumed samples: 4944  total_loss: 7.928  time: 0.2182(73.32)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:34] lb.utils.events INFO:  eta: 0:02:30  iteration: 310/1000  consumed samples: 4960  total_loss: 7.928  time: 0.2182(73.32)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:34] lb.utils.events INFO:  eta: 0:02:30  iteration: 311/1000  consumed samples: 4976  total_loss: 7.925  time: 0.2181(73.36)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:34] lb.utils.events INFO:  eta: 0:02:29  iteration: 312/1000  consumed samples: 4992  total_loss: 7.921  time: 0.2181(73.36)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:35] lb.utils.events INFO:  eta: 0:02:29  iteration: 313/1000  consumed samples: 5008  total_loss: 7.925  time: 0.2181(73.35)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:35] lb.utils.events INFO:  eta: 0:02:29  iteration: 314/1000  consumed samples: 5024  total_loss: 7.921  time: 0.2181(73.35)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:35] lb.utils.events INFO:  eta: 0:02:29  iteration: 315/1000  consumed samples: 5040  total_loss: 7.919  time: 0.2181(73.35)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:35] lb.utils.events INFO:  eta: 0:02:29  iteration: 316/1000  consumed samples: 5056  total_loss: 7.918  time: 0.2181(73.36)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:36] lb.utils.events INFO:  eta: 0:02:28  iteration: 317/1000  consumed samples: 5072  total_loss: 7.917  time: 0.2181(73.35)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:36] lb.utils.events INFO:  eta: 0:02:28  iteration: 318/1000  consumed samples: 5088  total_loss: 7.917  time: 0.2181(73.35)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:36] lb.utils.events INFO:  eta: 0:02:28  iteration: 319/1000  consumed samples: 5104  total_loss: 7.917  time: 0.2182(73.34)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:36] lb.utils.events INFO:  eta: 0:02:28  iteration: 320/1000  consumed samples: 5120  total_loss: 7.916  time: 0.2182(73.34)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:37] lb.utils.events INFO:  eta: 0:02:28  iteration: 321/1000  consumed samples: 5136  total_loss: 7.916  time: 0.2182(73.34)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:37] lb.utils.events INFO:  eta: 0:02:27  iteration: 322/1000  consumed samples: 5152  total_loss: 7.916  time: 0.2182(73.34)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:37] lb.utils.events INFO:  eta: 0:02:27  iteration: 323/1000  consumed samples: 5168  total_loss: 7.916  time: 0.2182(73.33)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:37] lb.utils.events INFO:  eta: 0:02:27  iteration: 324/1000  consumed samples: 5184  total_loss: 7.916  time: 0.2182(73.33)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:37] lb.utils.events INFO:  eta: 0:02:27  iteration: 325/1000  consumed samples: 5200  total_loss: 7.915  time: 0.2182(73.33)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:49:38] lb.utils.events INFO:  eta: 0:02:26  iteration: 326/1000  consumed samples: 5216  total_loss: 7.915  time: 0.2182(73.33)  data_time: 0.0028  lr: 1.00e-04  
[01/19 14:49:38] lb.utils.events INFO:  eta: 0:02:26  iteration: 327/1000  consumed samples: 5232  total_loss: 7.915  time: 0.2182(73.33)  data_time: 0.0029  lr: 1.00e-04  
[01/19 14:49:38] lb.utils.events INFO:  eta: 0:02:26  iteration: 328/1000  consumed samples: 5248  total_loss: 7.915  time: 0.2182(73.33)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:38] lb.utils.events INFO:  eta: 0:02:26  iteration: 329/1000  consumed samples: 5264  total_loss: 7.915  time: 0.2182(73.32)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:39] lb.utils.events INFO:  eta: 0:02:26  iteration: 330/1000  consumed samples: 5280  total_loss: 7.915  time: 0.2182(73.32)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:39] lb.utils.events INFO:  eta: 0:02:25  iteration: 331/1000  consumed samples: 5296  total_loss: 7.913  time: 0.2182(73.33)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:39] lb.utils.events INFO:  eta: 0:02:25  iteration: 332/1000  consumed samples: 5312  total_loss: 7.912  time: 0.2182(73.32)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:39] lb.utils.events INFO:  eta: 0:02:25  iteration: 333/1000  consumed samples: 5328  total_loss: 7.912  time: 0.2182(73.32)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:40] lb.utils.events INFO:  eta: 0:02:25  iteration: 334/1000  consumed samples: 5344  total_loss: 7.912  time: 0.2182(73.32)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:40] lb.utils.events INFO:  eta: 0:02:24  iteration: 335/1000  consumed samples: 5360  total_loss: 7.912  time: 0.2182(73.33)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:40] lb.utils.events INFO:  eta: 0:02:24  iteration: 336/1000  consumed samples: 5376  total_loss: 7.91  time: 0.2182(73.32)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:40] lb.utils.events INFO:  eta: 0:02:24  iteration: 337/1000  consumed samples: 5392  total_loss: 7.909  time: 0.2182(73.32)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:40] lb.utils.events INFO:  eta: 0:02:24  iteration: 338/1000  consumed samples: 5408  total_loss: 7.909  time: 0.2182(73.32)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:41] lb.utils.events INFO:  eta: 0:02:24  iteration: 339/1000  consumed samples: 5424  total_loss: 7.909  time: 0.2182(73.32)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:41] lb.utils.events INFO:  eta: 0:02:23  iteration: 340/1000  consumed samples: 5440  total_loss: 7.909  time: 0.2182(73.31)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:41] lb.utils.events INFO:  eta: 0:02:23  iteration: 341/1000  consumed samples: 5456  total_loss: 7.909  time: 0.2182(73.31)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:41] lb.utils.events INFO:  eta: 0:02:23  iteration: 342/1000  consumed samples: 5472  total_loss: 7.91  time: 0.2183(73.31)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:42] lb.utils.events INFO:  eta: 0:02:23  iteration: 343/1000  consumed samples: 5488  total_loss: 7.909  time: 0.2182(73.31)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:42] lb.utils.events INFO:  eta: 0:02:22  iteration: 344/1000  consumed samples: 5504  total_loss: 7.907  time: 0.2183(73.31)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:42] lb.utils.events INFO:  eta: 0:02:22  iteration: 345/1000  consumed samples: 5520  total_loss: 7.907  time: 0.2181(73.37)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:42] lb.utils.events INFO:  eta: 0:02:22  iteration: 346/1000  consumed samples: 5536  total_loss: 7.905  time: 0.2179(73.42)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:42] lb.utils.events INFO:  eta: 0:02:22  iteration: 347/1000  consumed samples: 5552  total_loss: 7.904  time: 0.2178(73.48)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:43] lb.utils.events INFO:  eta: 0:02:21  iteration: 348/1000  consumed samples: 5568  total_loss: 7.901  time: 0.2176(73.54)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:49:43] lb.utils.events INFO:  eta: 0:02:21  iteration: 349/1000  consumed samples: 5584  total_loss: 7.904  time: 0.2172(73.66)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:43] lb.utils.events INFO:  eta: 0:02:21  iteration: 350/1000  consumed samples: 5600  total_loss: 7.904  time: 0.2172(73.66)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:49:43] lb.utils.events INFO:  eta: 0:02:21  iteration: 351/1000  consumed samples: 5616  total_loss: 7.901  time: 0.2172(73.66)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:44] lb.utils.events INFO:  eta: 0:02:21  iteration: 352/1000  consumed samples: 5632  total_loss: 7.901  time: 0.2172(73.66)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:44] lb.utils.events INFO:  eta: 0:02:20  iteration: 353/1000  consumed samples: 5648  total_loss: 7.904  time: 0.2172(73.65)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:44] lb.utils.events INFO:  eta: 0:02:20  iteration: 354/1000  consumed samples: 5664  total_loss: 7.904  time: 0.2172(73.65)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:44] lb.utils.events INFO:  eta: 0:02:20  iteration: 355/1000  consumed samples: 5680  total_loss: 7.901  time: 0.2173(73.64)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:45] lb.utils.events INFO:  eta: 0:02:20  iteration: 356/1000  consumed samples: 5696  total_loss: 7.898  time: 0.2173(73.65)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:45] lb.utils.events INFO:  eta: 0:02:19  iteration: 357/1000  consumed samples: 5712  total_loss: 7.901  time: 0.2173(73.65)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:45] lb.utils.events INFO:  eta: 0:02:19  iteration: 358/1000  consumed samples: 5728  total_loss: 7.898  time: 0.2172(73.65)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:45] lb.utils.events INFO:  eta: 0:02:19  iteration: 359/1000  consumed samples: 5744  total_loss: 7.898  time: 0.2172(73.65)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:45] lb.utils.events INFO:  eta: 0:02:19  iteration: 360/1000  consumed samples: 5760  total_loss: 7.897  time: 0.2173(73.65)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:46] lb.utils.events INFO:  eta: 0:02:19  iteration: 361/1000  consumed samples: 5776  total_loss: 7.894  time: 0.2173(73.65)  data_time: 0.0030  lr: 1.00e-04  
[01/19 14:49:46] lb.utils.events INFO:  eta: 0:02:18  iteration: 362/1000  consumed samples: 5792  total_loss: 7.894  time: 0.2173(73.64)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:46] lb.utils.events INFO:  eta: 0:02:18  iteration: 363/1000  consumed samples: 5808  total_loss: 7.89  time: 0.2173(73.64)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:46] lb.utils.events INFO:  eta: 0:02:18  iteration: 364/1000  consumed samples: 5824  total_loss: 7.89  time: 0.2173(73.64)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:47] lb.utils.events INFO:  eta: 0:02:18  iteration: 365/1000  consumed samples: 5840  total_loss: 7.89  time: 0.2173(73.64)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:47] lb.utils.events INFO:  eta: 0:02:17  iteration: 366/1000  consumed samples: 5856  total_loss: 7.89  time: 0.2173(73.63)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:47] lb.utils.events INFO:  eta: 0:02:17  iteration: 367/1000  consumed samples: 5872  total_loss: 7.889  time: 0.2173(73.63)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:47] lb.utils.events INFO:  eta: 0:02:17  iteration: 368/1000  consumed samples: 5888  total_loss: 7.888  time: 0.2173(73.63)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:48] lb.utils.events INFO:  eta: 0:02:17  iteration: 369/1000  consumed samples: 5904  total_loss: 7.885  time: 0.2173(73.63)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:48] lb.utils.events INFO:  eta: 0:02:17  iteration: 370/1000  consumed samples: 5920  total_loss: 7.883  time: 0.2173(73.62)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:48] lb.utils.events INFO:  eta: 0:02:16  iteration: 371/1000  consumed samples: 5936  total_loss: 7.881  time: 0.2173(73.62)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:48] lb.utils.events INFO:  eta: 0:02:16  iteration: 372/1000  consumed samples: 5952  total_loss: 7.88  time: 0.2174(73.61)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:48] lb.utils.events INFO:  eta: 0:02:16  iteration: 373/1000  consumed samples: 5968  total_loss: 7.88  time: 0.2174(73.61)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:49] lb.utils.events INFO:  eta: 0:02:16  iteration: 374/1000  consumed samples: 5984  total_loss: 7.878  time: 0.2174(73.60)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:49] lb.utils.events INFO:  eta: 0:02:15  iteration: 375/1000  consumed samples: 6000  total_loss: 7.88  time: 0.2171(73.69)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:49] lb.utils.events INFO:  eta: 0:02:15  iteration: 376/1000  consumed samples: 6016  total_loss: 7.878  time: 0.2170(73.72)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:49] lb.utils.events INFO:  eta: 0:02:15  iteration: 377/1000  consumed samples: 6032  total_loss: 7.88  time: 0.2168(73.79)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:50] lb.utils.events INFO:  eta: 0:02:15  iteration: 378/1000  consumed samples: 6048  total_loss: 7.878  time: 0.2166(73.88)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:50] lb.utils.events INFO:  eta: 0:02:15  iteration: 379/1000  consumed samples: 6064  total_loss: 7.88  time: 0.2162(74.00)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:50] lb.utils.events INFO:  eta: 0:02:14  iteration: 380/1000  consumed samples: 6080  total_loss: 7.88  time: 0.2159(74.12)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:50] lb.utils.events INFO:  eta: 0:02:14  iteration: 381/1000  consumed samples: 6096  total_loss: 7.88  time: 0.2156(74.22)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:50] lb.utils.events INFO:  eta: 0:02:14  iteration: 382/1000  consumed samples: 6112  total_loss: 7.881  time: 0.2154(74.29)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:51] lb.utils.events INFO:  eta: 0:02:14  iteration: 383/1000  consumed samples: 6128  total_loss: 7.88  time: 0.2152(74.34)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:51] lb.utils.events INFO:  eta: 0:02:13  iteration: 384/1000  consumed samples: 6144  total_loss: 7.88  time: 0.2149(74.46)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:51] lb.utils.events INFO:  eta: 0:02:13  iteration: 385/1000  consumed samples: 6160  total_loss: 7.88  time: 0.2146(74.54)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:51] lb.utils.events INFO:  eta: 0:02:13  iteration: 386/1000  consumed samples: 6176  total_loss: 7.88  time: 0.2146(74.55)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:52] lb.utils.events INFO:  eta: 0:02:13  iteration: 387/1000  consumed samples: 6192  total_loss: 7.88  time: 0.2146(74.54)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:52] lb.utils.events INFO:  eta: 0:02:13  iteration: 388/1000  consumed samples: 6208  total_loss: 7.88  time: 0.2146(74.54)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:52] lb.utils.events INFO:  eta: 0:02:12  iteration: 389/1000  consumed samples: 6224  total_loss: 7.877  time: 0.2147(74.53)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:52] lb.utils.events INFO:  eta: 0:02:12  iteration: 390/1000  consumed samples: 6240  total_loss: 7.875  time: 0.2147(74.53)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:53] lb.utils.events INFO:  eta: 0:02:12  iteration: 391/1000  consumed samples: 6256  total_loss: 7.874  time: 0.2147(74.52)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:53] lb.utils.events INFO:  eta: 0:02:12  iteration: 392/1000  consumed samples: 6272  total_loss: 7.874  time: 0.2147(74.52)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:53] lb.utils.events INFO:  eta: 0:02:11  iteration: 393/1000  consumed samples: 6288  total_loss: 7.875  time: 0.2147(74.52)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:53] lb.utils.events INFO:  eta: 0:02:11  iteration: 394/1000  consumed samples: 6304  total_loss: 7.875  time: 0.2147(74.52)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:53] lb.utils.events INFO:  eta: 0:02:11  iteration: 395/1000  consumed samples: 6320  total_loss: 7.875  time: 0.2147(74.51)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:54] lb.utils.events INFO:  eta: 0:02:11  iteration: 396/1000  consumed samples: 6336  total_loss: 7.875  time: 0.2147(74.51)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:54] lb.utils.events INFO:  eta: 0:02:11  iteration: 397/1000  consumed samples: 6352  total_loss: 7.874  time: 0.2148(74.50)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:54] lb.utils.events INFO:  eta: 0:02:10  iteration: 398/1000  consumed samples: 6368  total_loss: 7.874  time: 0.2148(74.50)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:54] lb.utils.events INFO:  eta: 0:02:10  iteration: 399/1000  consumed samples: 6384  total_loss: 7.874  time: 0.2148(74.50)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:55] lb.utils.events INFO:  eta: 0:02:10  iteration: 400/1000  consumed samples: 6400  total_loss: 7.874  time: 0.2148(74.50)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:55] lb.utils.events INFO:  eta: 0:02:10  iteration: 401/1000  consumed samples: 6416  total_loss: 7.874  time: 0.2148(74.49)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:55] lb.utils.events INFO:  eta: 0:02:09  iteration: 402/1000  consumed samples: 6432  total_loss: 7.874  time: 0.2148(74.49)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:55] lb.utils.events INFO:  eta: 0:02:09  iteration: 403/1000  consumed samples: 6448  total_loss: 7.874  time: 0.2148(74.49)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:56] lb.utils.events INFO:  eta: 0:02:09  iteration: 404/1000  consumed samples: 6464  total_loss: 7.874  time: 0.2148(74.48)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:56] lb.utils.events INFO:  eta: 0:02:09  iteration: 405/1000  consumed samples: 6480  total_loss: 7.874  time: 0.2148(74.48)  data_time: 0.0031  lr: 1.00e-04  
[01/19 14:49:56] lb.utils.events INFO:  eta: 0:02:09  iteration: 406/1000  consumed samples: 6496  total_loss: 7.874  time: 0.2148(74.48)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:56] lb.utils.events INFO:  eta: 0:02:08  iteration: 407/1000  consumed samples: 6512  total_loss: 7.874  time: 0.2148(74.48)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:56] lb.utils.events INFO:  eta: 0:02:08  iteration: 408/1000  consumed samples: 6528  total_loss: 7.873  time: 0.2148(74.48)  data_time: 0.0032  lr: 1.00e-04  
[01/19 14:49:57] lb.utils.events INFO:  eta: 0:02:08  iteration: 409/1000  consumed samples: 6544  total_loss: 7.873  time: 0.2148(74.48)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:57] lb.utils.events INFO:  eta: 0:02:08  iteration: 410/1000  consumed samples: 6560  total_loss: 7.874  time: 0.2149(74.47)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:49:57] lb.utils.events INFO:  eta: 0:02:08  iteration: 411/1000  consumed samples: 6576  total_loss: 7.874  time: 0.2149(74.46)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:57] lb.utils.events INFO:  eta: 0:02:07  iteration: 412/1000  consumed samples: 6592  total_loss: 7.873  time: 0.2148(74.47)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:58] lb.utils.events INFO:  eta: 0:02:07  iteration: 413/1000  consumed samples: 6608  total_loss: 7.872  time: 0.2149(74.46)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:58] lb.utils.events INFO:  eta: 0:02:07  iteration: 414/1000  consumed samples: 6624  total_loss: 7.872  time: 0.2149(74.46)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:58] lb.utils.events INFO:  eta: 0:02:07  iteration: 415/1000  consumed samples: 6640  total_loss: 7.872  time: 0.2149(74.46)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:49:58] lb.utils.events INFO:  eta: 0:02:06  iteration: 416/1000  consumed samples: 6656  total_loss: 7.872  time: 0.2149(74.45)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:59] lb.utils.events INFO:  eta: 0:02:06  iteration: 417/1000  consumed samples: 6672  total_loss: 7.871  time: 0.2149(74.45)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:59] lb.utils.events INFO:  eta: 0:02:06  iteration: 418/1000  consumed samples: 6688  total_loss: 7.869  time: 0.2149(74.45)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:59] lb.utils.events INFO:  eta: 0:02:06  iteration: 419/1000  consumed samples: 6704  total_loss: 7.869  time: 0.2149(74.45)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:49:59] lb.utils.events INFO:  eta: 0:02:06  iteration: 420/1000  consumed samples: 6720  total_loss: 7.869  time: 0.2149(74.44)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:49:59] lb.utils.events INFO:  eta: 0:02:05  iteration: 421/1000  consumed samples: 6736  total_loss: 7.869  time: 0.2149(74.44)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:00] lb.utils.events INFO:  eta: 0:02:05  iteration: 422/1000  consumed samples: 6752  total_loss: 7.869  time: 0.2150(74.43)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:00] lb.utils.events INFO:  eta: 0:02:05  iteration: 423/1000  consumed samples: 6768  total_loss: 7.868  time: 0.2150(74.42)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:00] lb.utils.events INFO:  eta: 0:02:05  iteration: 424/1000  consumed samples: 6784  total_loss: 7.868  time: 0.2150(74.42)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:00] lb.utils.events INFO:  eta: 0:02:04  iteration: 425/1000  consumed samples: 6800  total_loss: 7.868  time: 0.2150(74.41)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:01] lb.utils.events INFO:  eta: 0:02:04  iteration: 426/1000  consumed samples: 6816  total_loss: 7.869  time: 0.2150(74.41)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:01] lb.utils.events INFO:  eta: 0:02:04  iteration: 427/1000  consumed samples: 6832  total_loss: 7.868  time: 0.2150(74.40)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:01] lb.utils.events INFO:  eta: 0:02:04  iteration: 428/1000  consumed samples: 6848  total_loss: 7.867  time: 0.2150(74.40)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:01] lb.utils.events INFO:  eta: 0:02:04  iteration: 429/1000  consumed samples: 6864  total_loss: 7.867  time: 0.2151(74.40)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:01] lb.utils.events INFO:  eta: 0:02:03  iteration: 430/1000  consumed samples: 6880  total_loss: 7.867  time: 0.2151(74.40)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:02] lb.utils.events INFO:  eta: 0:02:03  iteration: 431/1000  consumed samples: 6896  total_loss: 7.866  time: 0.2151(74.39)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:02] lb.utils.events INFO:  eta: 0:02:03  iteration: 432/1000  consumed samples: 6912  total_loss: 7.866  time: 0.2151(74.38)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:50:02] lb.utils.events INFO:  eta: 0:02:03  iteration: 433/1000  consumed samples: 6928  total_loss: 7.866  time: 0.2151(74.37)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:50:02] lb.utils.events INFO:  eta: 0:02:03  iteration: 434/1000  consumed samples: 6944  total_loss: 7.866  time: 0.2151(74.38)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:50:03] lb.utils.events INFO:  eta: 0:02:02  iteration: 435/1000  consumed samples: 6960  total_loss: 7.866  time: 0.2151(74.38)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:50:03] lb.utils.events INFO:  eta: 0:02:02  iteration: 436/1000  consumed samples: 6976  total_loss: 7.866  time: 0.2151(74.37)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:50:03] lb.utils.events INFO:  eta: 0:02:02  iteration: 437/1000  consumed samples: 6992  total_loss: 7.864  time: 0.2152(74.37)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:03] lb.utils.events INFO:  eta: 0:02:02  iteration: 438/1000  consumed samples: 7008  total_loss: 7.861  time: 0.2152(74.37)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 439/1000  consumed samples: 7024  total_loss: 7.858  time: 0.2152(74.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 440/1000  consumed samples: 7040  total_loss: 7.857  time: 0.2152(74.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 441/1000  consumed samples: 7056  total_loss: 7.857  time: 0.2152(74.35)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 442/1000  consumed samples: 7072  total_loss: 7.857  time: 0.2152(74.35)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:04] lb.utils.events INFO:  eta: 0:02:01  iteration: 443/1000  consumed samples: 7088  total_loss: 7.857  time: 0.2152(74.35)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 444/1000  consumed samples: 7104  total_loss: 7.857  time: 0.2152(74.34)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 445/1000  consumed samples: 7120  total_loss: 7.856  time: 0.2152(74.34)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 446/1000  consumed samples: 7136  total_loss: 7.856  time: 0.2152(74.35)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:05] lb.utils.events INFO:  eta: 0:02:00  iteration: 447/1000  consumed samples: 7152  total_loss: 7.855  time: 0.2152(74.34)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 448/1000  consumed samples: 7168  total_loss: 7.855  time: 0.2152(74.34)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 449/1000  consumed samples: 7184  total_loss: 7.854  time: 0.2153(74.33)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 450/1000  consumed samples: 7200  total_loss: 7.854  time: 0.2153(74.33)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:06] lb.utils.events INFO:  eta: 0:01:59  iteration: 451/1000  consumed samples: 7216  total_loss: 7.854  time: 0.2153(74.33)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:07] lb.utils.events INFO:  eta: 0:01:59  iteration: 452/1000  consumed samples: 7232  total_loss: 7.854  time: 0.2153(74.33)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 453/1000  consumed samples: 7248  total_loss: 7.853  time: 0.2153(74.32)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 454/1000  consumed samples: 7264  total_loss: 7.853  time: 0.2153(74.32)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 455/1000  consumed samples: 7280  total_loss: 7.852  time: 0.2153(74.31)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:07] lb.utils.events INFO:  eta: 0:01:58  iteration: 456/1000  consumed samples: 7296  total_loss: 7.852  time: 0.2153(74.31)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:08] lb.utils.events INFO:  eta: 0:01:58  iteration: 457/1000  consumed samples: 7312  total_loss: 7.851  time: 0.2153(74.31)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 458/1000  consumed samples: 7328  total_loss: 7.851  time: 0.2153(74.30)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 459/1000  consumed samples: 7344  total_loss: 7.849  time: 0.2154(74.30)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:08] lb.utils.events INFO:  eta: 0:01:57  iteration: 460/1000  consumed samples: 7360  total_loss: 7.848  time: 0.2154(74.30)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:09] lb.utils.events INFO:  eta: 0:01:57  iteration: 461/1000  consumed samples: 7376  total_loss: 7.849  time: 0.2154(74.29)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 462/1000  consumed samples: 7392  total_loss: 7.85  time: 0.2154(74.29)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 463/1000  consumed samples: 7408  total_loss: 7.85  time: 0.2154(74.28)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 464/1000  consumed samples: 7424  total_loss: 7.848  time: 0.2154(74.28)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:09] lb.utils.events INFO:  eta: 0:01:56  iteration: 465/1000  consumed samples: 7440  total_loss: 7.848  time: 0.2154(74.28)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:10] lb.utils.events INFO:  eta: 0:01:56  iteration: 466/1000  consumed samples: 7456  total_loss: 7.848  time: 0.2154(74.27)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 467/1000  consumed samples: 7472  total_loss: 7.848  time: 0.2154(74.27)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 468/1000  consumed samples: 7488  total_loss: 7.848  time: 0.2154(74.27)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:10] lb.utils.events INFO:  eta: 0:01:55  iteration: 469/1000  consumed samples: 7504  total_loss: 7.848  time: 0.2155(74.26)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:11] lb.utils.events INFO:  eta: 0:01:55  iteration: 470/1000  consumed samples: 7520  total_loss: 7.848  time: 0.2155(74.26)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:11] lb.utils.events INFO:  eta: 0:01:55  iteration: 471/1000  consumed samples: 7536  total_loss: 7.846  time: 0.2155(74.25)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:11] lb.utils.events INFO:  eta: 0:01:54  iteration: 472/1000  consumed samples: 7552  total_loss: 7.843  time: 0.2155(74.26)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:11] lb.utils.events INFO:  eta: 0:01:54  iteration: 473/1000  consumed samples: 7568  total_loss: 7.843  time: 0.2155(74.25)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:12] lb.utils.events INFO:  eta: 0:01:54  iteration: 474/1000  consumed samples: 7584  total_loss: 7.842  time: 0.2155(74.25)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:12] lb.utils.events INFO:  eta: 0:01:54  iteration: 475/1000  consumed samples: 7600  total_loss: 7.841  time: 0.2155(74.25)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:50:12] lb.utils.events INFO:  eta: 0:01:53  iteration: 476/1000  consumed samples: 7616  total_loss: 7.84  time: 0.2155(74.25)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:12] lb.utils.events INFO:  eta: 0:01:53  iteration: 477/1000  consumed samples: 7632  total_loss: 7.84  time: 0.2155(74.24)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:50:12] lb.utils.events INFO:  eta: 0:01:53  iteration: 478/1000  consumed samples: 7648  total_loss: 7.838  time: 0.2155(74.24)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:13] lb.utils.events INFO:  eta: 0:01:53  iteration: 479/1000  consumed samples: 7664  total_loss: 7.838  time: 0.2155(74.24)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:13] lb.utils.events INFO:  eta: 0:01:53  iteration: 480/1000  consumed samples: 7680  total_loss: 7.837  time: 0.2155(74.23)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:13] lb.utils.events INFO:  eta: 0:01:52  iteration: 481/1000  consumed samples: 7696  total_loss: 7.836  time: 0.2156(74.22)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:13] lb.utils.events INFO:  eta: 0:01:52  iteration: 482/1000  consumed samples: 7712  total_loss: 7.836  time: 0.2156(74.22)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:14] lb.utils.events INFO:  eta: 0:01:52  iteration: 483/1000  consumed samples: 7728  total_loss: 7.836  time: 0.2156(74.22)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:14] lb.utils.events INFO:  eta: 0:01:52  iteration: 484/1000  consumed samples: 7744  total_loss: 7.835  time: 0.2156(74.22)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:14] lb.utils.events INFO:  eta: 0:01:51  iteration: 485/1000  consumed samples: 7760  total_loss: 7.834  time: 0.2154(74.28)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:14] lb.utils.events INFO:  eta: 0:01:51  iteration: 486/1000  consumed samples: 7776  total_loss: 7.833  time: 0.2152(74.34)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:15] lb.utils.events INFO:  eta: 0:01:51  iteration: 487/1000  consumed samples: 7792  total_loss: 7.832  time: 0.2152(74.35)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:15] lb.utils.events INFO:  eta: 0:01:51  iteration: 488/1000  consumed samples: 7808  total_loss: 7.832  time: 0.2151(74.39)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:15] lb.utils.events INFO:  eta: 0:01:51  iteration: 489/1000  consumed samples: 7824  total_loss: 7.832  time: 0.2150(74.43)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:15] lb.utils.events INFO:  eta: 0:01:50  iteration: 490/1000  consumed samples: 7840  total_loss: 7.832  time: 0.2150(74.42)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:15] lb.utils.events INFO:  eta: 0:01:50  iteration: 491/1000  consumed samples: 7856  total_loss: 7.832  time: 0.2148(74.48)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:16] lb.utils.events INFO:  eta: 0:01:50  iteration: 492/1000  consumed samples: 7872  total_loss: 7.832  time: 0.2147(74.52)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:16] lb.utils.events INFO:  eta: 0:01:50  iteration: 493/1000  consumed samples: 7888  total_loss: 7.832  time: 0.2146(74.55)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:16] lb.utils.events INFO:  eta: 0:01:49  iteration: 494/1000  consumed samples: 7904  total_loss: 7.829  time: 0.2146(74.55)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:16] lb.utils.events INFO:  eta: 0:01:49  iteration: 495/1000  consumed samples: 7920  total_loss: 7.826  time: 0.2146(74.56)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:17] lb.utils.events INFO:  eta: 0:01:49  iteration: 496/1000  consumed samples: 7936  total_loss: 7.824  time: 0.2145(74.61)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:50:17] lb.utils.events INFO:  eta: 0:01:49  iteration: 497/1000  consumed samples: 7952  total_loss: 7.824  time: 0.2142(74.68)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:17] lb.utils.events INFO:  eta: 0:01:49  iteration: 498/1000  consumed samples: 7968  total_loss: 7.824  time: 0.2141(74.74)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 499/1000  consumed samples: 7984  total_loss: 7.823  time: 0.2140(74.76)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 500/1000  consumed samples: 8000  total_loss: 7.822  time: 0.2140(74.77)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 501/1000  consumed samples: 8016  total_loss: 7.819  time: 0.2140(74.77)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:18] lb.utils.events INFO:  eta: 0:01:48  iteration: 502/1000  consumed samples: 8032  total_loss: 7.819  time: 0.2139(74.79)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:18] lb.utils.events INFO:  eta: 0:01:47  iteration: 503/1000  consumed samples: 8048  total_loss: 7.816  time: 0.2139(74.79)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:18] lb.utils.events INFO:  eta: 0:01:47  iteration: 504/1000  consumed samples: 8064  total_loss: 7.816  time: 0.2138(74.85)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 505/1000  consumed samples: 8080  total_loss: 7.814  time: 0.2137(74.86)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 506/1000  consumed samples: 8096  total_loss: 7.814  time: 0.2136(74.90)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:50:19] lb.utils.events INFO:  eta: 0:01:47  iteration: 507/1000  consumed samples: 8112  total_loss: 7.813  time: 0.2135(74.94)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:19] lb.utils.events INFO:  eta: 0:01:46  iteration: 508/1000  consumed samples: 8128  total_loss: 7.812  time: 0.2134(74.96)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 509/1000  consumed samples: 8144  total_loss: 7.812  time: 0.2132(75.04)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 510/1000  consumed samples: 8160  total_loss: 7.812  time: 0.2131(75.07)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:20] lb.utils.events INFO:  eta: 0:01:46  iteration: 511/1000  consumed samples: 8176  total_loss: 7.812  time: 0.2130(75.11)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:20] lb.utils.events INFO:  eta: 0:01:45  iteration: 512/1000  consumed samples: 8192  total_loss: 7.812  time: 0.2129(75.16)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 513/1000  consumed samples: 8208  total_loss: 7.812  time: 0.2128(75.20)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 514/1000  consumed samples: 8224  total_loss: 7.812  time: 0.2127(75.22)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 515/1000  consumed samples: 8240  total_loss: 7.81  time: 0.2126(75.25)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:21] lb.utils.events INFO:  eta: 0:01:45  iteration: 516/1000  consumed samples: 8256  total_loss: 7.81  time: 0.2126(75.27)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:21] lb.utils.events INFO:  eta: 0:01:44  iteration: 517/1000  consumed samples: 8272  total_loss: 7.81  time: 0.2124(75.33)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 518/1000  consumed samples: 8288  total_loss: 7.812  time: 0.2123(75.38)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 519/1000  consumed samples: 8304  total_loss: 7.81  time: 0.2121(75.44)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:22] lb.utils.events INFO:  eta: 0:01:44  iteration: 520/1000  consumed samples: 8320  total_loss: 7.81  time: 0.2120(75.47)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:22] lb.utils.events INFO:  eta: 0:01:43  iteration: 521/1000  consumed samples: 8336  total_loss: 7.812  time: 0.2119(75.51)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:23] lb.utils.events INFO:  eta: 0:01:43  iteration: 522/1000  consumed samples: 8352  total_loss: 7.812  time: 0.2119(75.52)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:23] lb.utils.events INFO:  eta: 0:01:43  iteration: 523/1000  consumed samples: 8368  total_loss: 7.81  time: 0.2118(75.53)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:50:23] lb.utils.events INFO:  eta: 0:01:43  iteration: 524/1000  consumed samples: 8384  total_loss: 7.808  time: 0.2116(75.61)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:50:23] lb.utils.events INFO:  eta: 0:01:42  iteration: 525/1000  consumed samples: 8400  total_loss: 7.806  time: 0.2115(75.66)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:50:23] lb.utils.events INFO:  eta: 0:01:42  iteration: 526/1000  consumed samples: 8416  total_loss: 7.806  time: 0.2113(75.71)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:50:24] lb.utils.events INFO:  eta: 0:01:42  iteration: 527/1000  consumed samples: 8432  total_loss: 7.808  time: 0.2113(75.73)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:50:24] lb.utils.events INFO:  eta: 0:01:42  iteration: 528/1000  consumed samples: 8448  total_loss: 7.806  time: 0.2113(75.74)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:50:24] lb.utils.events INFO:  eta: 0:01:42  iteration: 529/1000  consumed samples: 8464  total_loss: 7.803  time: 0.2112(75.77)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:50:24] lb.utils.events INFO:  eta: 0:01:41  iteration: 530/1000  consumed samples: 8480  total_loss: 7.802  time: 0.2110(75.83)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:50:25] lb.utils.events INFO:  eta: 0:01:41  iteration: 531/1000  consumed samples: 8496  total_loss: 7.802  time: 0.2108(75.90)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:50:25] lb.utils.events INFO:  eta: 0:01:41  iteration: 532/1000  consumed samples: 8512  total_loss: 7.802  time: 0.2106(75.96)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:50:25] lb.utils.events INFO:  eta: 0:01:41  iteration: 533/1000  consumed samples: 8528  total_loss: 7.802  time: 0.2105(76.01)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:25] lb.utils.events INFO:  eta: 0:01:41  iteration: 534/1000  consumed samples: 8544  total_loss: 7.802  time: 0.2104(76.03)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:26] lb.utils.events INFO:  eta: 0:01:40  iteration: 535/1000  consumed samples: 8560  total_loss: 7.802  time: 0.2103(76.08)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:26] lb.utils.events INFO:  eta: 0:01:40  iteration: 536/1000  consumed samples: 8576  total_loss: 7.802  time: 0.2101(76.14)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:26] lb.utils.events INFO:  eta: 0:01:40  iteration: 537/1000  consumed samples: 8592  total_loss: 7.802  time: 0.2100(76.18)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:26] lb.utils.events INFO:  eta: 0:01:40  iteration: 538/1000  consumed samples: 8608  total_loss: 7.802  time: 0.2099(76.23)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:26] lb.utils.events INFO:  eta: 0:01:39  iteration: 539/1000  consumed samples: 8624  total_loss: 7.802  time: 0.2098(76.26)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:27] lb.utils.events INFO:  eta: 0:01:39  iteration: 540/1000  consumed samples: 8640  total_loss: 7.8  time: 0.2097(76.32)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:27] lb.utils.events INFO:  eta: 0:01:39  iteration: 541/1000  consumed samples: 8656  total_loss: 7.8  time: 0.2096(76.33)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:27] lb.utils.events INFO:  eta: 0:01:39  iteration: 542/1000  consumed samples: 8672  total_loss: 7.798  time: 0.2095(76.38)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:27] lb.utils.events INFO:  eta: 0:01:39  iteration: 543/1000  consumed samples: 8688  total_loss: 7.798  time: 0.2094(76.40)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:28] lb.utils.events INFO:  eta: 0:01:38  iteration: 544/1000  consumed samples: 8704  total_loss: 7.798  time: 0.2093(76.46)  data_time: 0.0088  lr: 1.00e-04  
[01/19 14:50:28] lb.utils.events INFO:  eta: 0:01:38  iteration: 545/1000  consumed samples: 8720  total_loss: 7.798  time: 0.2091(76.51)  data_time: 0.0088  lr: 1.00e-04  
[01/19 14:50:28] lb.utils.events INFO:  eta: 0:01:38  iteration: 546/1000  consumed samples: 8736  total_loss: 7.796  time: 0.2090(76.54)  data_time: 0.0087  lr: 1.00e-04  
[01/19 14:50:28] lb.utils.events INFO:  eta: 0:01:38  iteration: 547/1000  consumed samples: 8752  total_loss: 7.796  time: 0.2089(76.61)  data_time: 0.0087  lr: 1.00e-04  
[01/19 14:50:29] lb.utils.events INFO:  eta: 0:01:37  iteration: 548/1000  consumed samples: 8768  total_loss: 7.795  time: 0.2087(76.67)  data_time: 0.0087  lr: 1.00e-04  
[01/19 14:50:29] lb.utils.events INFO:  eta: 0:01:37  iteration: 549/1000  consumed samples: 8784  total_loss: 7.795  time: 0.2085(76.74)  data_time: 0.0088  lr: 1.00e-04  
[01/19 14:50:29] lb.utils.events INFO:  eta: 0:01:37  iteration: 550/1000  consumed samples: 8800  total_loss: 7.792  time: 0.2083(76.82)  data_time: 0.0087  lr: 1.00e-04  
[01/19 14:50:29] lb.utils.events INFO:  eta: 0:01:37  iteration: 551/1000  consumed samples: 8816  total_loss: 7.788  time: 0.2081(76.90)  data_time: 0.0088  lr: 1.00e-04  
[01/19 14:50:30] lb.utils.events INFO:  eta: 0:01:37  iteration: 552/1000  consumed samples: 8832  total_loss: 7.788  time: 0.2079(76.94)  data_time: 0.0088  lr: 1.00e-04  
[01/19 14:50:30] lb.utils.events INFO:  eta: 0:01:36  iteration: 553/1000  consumed samples: 8848  total_loss: 7.785  time: 0.2078(77.01)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:30] lb.utils.events INFO:  eta: 0:01:36  iteration: 554/1000  consumed samples: 8864  total_loss: 7.783  time: 0.2076(77.06)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:30] lb.utils.events INFO:  eta: 0:01:36  iteration: 555/1000  consumed samples: 8880  total_loss: 7.783  time: 0.2075(77.10)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:30] lb.utils.events INFO:  eta: 0:01:36  iteration: 556/1000  consumed samples: 8896  total_loss: 7.783  time: 0.2074(77.16)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:31] lb.utils.events INFO:  eta: 0:01:35  iteration: 557/1000  consumed samples: 8912  total_loss: 7.782  time: 0.2073(77.20)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:31] lb.utils.events INFO:  eta: 0:01:35  iteration: 558/1000  consumed samples: 8928  total_loss: 7.782  time: 0.2071(77.26)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:31] lb.utils.events INFO:  eta: 0:01:35  iteration: 559/1000  consumed samples: 8944  total_loss: 7.78  time: 0.2070(77.29)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:31] lb.utils.events INFO:  eta: 0:01:35  iteration: 560/1000  consumed samples: 8960  total_loss: 7.78  time: 0.2068(77.37)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:32] lb.utils.events INFO:  eta: 0:01:35  iteration: 561/1000  consumed samples: 8976  total_loss: 7.777  time: 0.2066(77.45)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:32] lb.utils.events INFO:  eta: 0:01:34  iteration: 562/1000  consumed samples: 8992  total_loss: 7.777  time: 0.2064(77.51)  data_time: 0.0089  lr: 1.00e-04  
[01/19 14:50:32] lb.utils.events INFO:  eta: 0:01:34  iteration: 563/1000  consumed samples: 9008  total_loss: 7.773  time: 0.2063(77.55)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:32] lb.utils.events INFO:  eta: 0:01:34  iteration: 564/1000  consumed samples: 9024  total_loss: 7.777  time: 0.2063(77.55)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:33] lb.utils.events INFO:  eta: 0:01:34  iteration: 565/1000  consumed samples: 9040  total_loss: 7.773  time: 0.2063(77.55)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:33] lb.utils.events INFO:  eta: 0:01:33  iteration: 566/1000  consumed samples: 9056  total_loss: 7.773  time: 0.2062(77.60)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:33] lb.utils.events INFO:  eta: 0:01:33  iteration: 567/1000  consumed samples: 9072  total_loss: 7.773  time: 0.2060(77.66)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:33] lb.utils.events INFO:  eta: 0:01:33  iteration: 568/1000  consumed samples: 9088  total_loss: 7.766  time: 0.2060(77.67)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:33] lb.utils.events INFO:  eta: 0:01:33  iteration: 569/1000  consumed samples: 9104  total_loss: 7.764  time: 0.2059(77.70)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:34] lb.utils.events INFO:  eta: 0:01:33  iteration: 570/1000  consumed samples: 9120  total_loss: 7.764  time: 0.2058(77.74)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:34] lb.utils.events INFO:  eta: 0:01:32  iteration: 571/1000  consumed samples: 9136  total_loss: 7.762  time: 0.2057(77.79)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:34] lb.utils.events INFO:  eta: 0:01:32  iteration: 572/1000  consumed samples: 9152  total_loss: 7.762  time: 0.2056(77.81)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:34] lb.utils.events INFO:  eta: 0:01:32  iteration: 573/1000  consumed samples: 9168  total_loss: 7.762  time: 0.2054(77.89)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:35] lb.utils.events INFO:  eta: 0:01:32  iteration: 574/1000  consumed samples: 9184  total_loss: 7.762  time: 0.2053(77.93)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:35] lb.utils.events INFO:  eta: 0:01:31  iteration: 575/1000  consumed samples: 9200  total_loss: 7.759  time: 0.2051(78.01)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:35] lb.utils.events INFO:  eta: 0:01:31  iteration: 576/1000  consumed samples: 9216  total_loss: 7.756  time: 0.2050(78.04)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:35] lb.utils.events INFO:  eta: 0:01:31  iteration: 577/1000  consumed samples: 9232  total_loss: 7.756  time: 0.2048(78.11)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:36] lb.utils.events INFO:  eta: 0:01:31  iteration: 578/1000  consumed samples: 9248  total_loss: 7.753  time: 0.2047(78.18)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:36] lb.utils.events INFO:  eta: 0:01:31  iteration: 579/1000  consumed samples: 9264  total_loss: 7.753  time: 0.2045(78.25)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:36] lb.utils.events INFO:  eta: 0:01:30  iteration: 580/1000  consumed samples: 9280  total_loss: 7.749  time: 0.2043(78.30)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:36] lb.utils.events INFO:  eta: 0:01:30  iteration: 581/1000  consumed samples: 9296  total_loss: 7.746  time: 0.2043(78.32)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:36] lb.utils.events INFO:  eta: 0:01:30  iteration: 582/1000  consumed samples: 9312  total_loss: 7.746  time: 0.2041(78.40)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:37] lb.utils.events INFO:  eta: 0:01:30  iteration: 583/1000  consumed samples: 9328  total_loss: 7.746  time: 0.2039(78.46)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:37] lb.utils.events INFO:  eta: 0:01:30  iteration: 584/1000  consumed samples: 9344  total_loss: 7.746  time: 0.2038(78.49)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:37] lb.utils.events INFO:  eta: 0:01:29  iteration: 585/1000  consumed samples: 9360  total_loss: 7.746  time: 0.2038(78.52)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:50:37] lb.utils.events INFO:  eta: 0:01:29  iteration: 586/1000  consumed samples: 9376  total_loss: 7.746  time: 0.2037(78.55)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:38] lb.utils.events INFO:  eta: 0:01:29  iteration: 587/1000  consumed samples: 9392  total_loss: 7.745  time: 0.2036(78.58)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:38] lb.utils.events INFO:  eta: 0:01:29  iteration: 588/1000  consumed samples: 9408  total_loss: 7.746  time: 0.2035(78.62)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:38] lb.utils.events INFO:  eta: 0:01:28  iteration: 589/1000  consumed samples: 9424  total_loss: 7.745  time: 0.2034(78.66)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:38] lb.utils.events INFO:  eta: 0:01:28  iteration: 590/1000  consumed samples: 9440  total_loss: 7.742  time: 0.2034(78.68)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:39] lb.utils.events INFO:  eta: 0:01:28  iteration: 591/1000  consumed samples: 9456  total_loss: 7.741  time: 0.2032(78.72)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:39] lb.utils.events INFO:  eta: 0:01:28  iteration: 592/1000  consumed samples: 9472  total_loss: 7.741  time: 0.2032(78.73)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:39] lb.utils.events INFO:  eta: 0:01:28  iteration: 593/1000  consumed samples: 9488  total_loss: 7.741  time: 0.2032(78.75)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:39] lb.utils.events INFO:  eta: 0:01:27  iteration: 594/1000  consumed samples: 9504  total_loss: 7.739  time: 0.2031(78.78)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:39] lb.utils.events INFO:  eta: 0:01:27  iteration: 595/1000  consumed samples: 9520  total_loss: 7.739  time: 0.2030(78.83)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:40] lb.utils.events INFO:  eta: 0:01:27  iteration: 596/1000  consumed samples: 9536  total_loss: 7.739  time: 0.2029(78.84)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:40] lb.utils.events INFO:  eta: 0:01:27  iteration: 597/1000  consumed samples: 9552  total_loss: 7.739  time: 0.2029(78.86)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:40] lb.utils.events INFO:  eta: 0:01:26  iteration: 598/1000  consumed samples: 9568  total_loss: 7.739  time: 0.2027(78.92)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:40] lb.utils.events INFO:  eta: 0:01:26  iteration: 599/1000  consumed samples: 9584  total_loss: 7.737  time: 0.2026(78.96)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:41] lb.utils.events INFO:  eta: 0:01:26  iteration: 600/1000  consumed samples: 9600  total_loss: 7.735  time: 0.2025(79.01)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:41] lb.utils.events INFO:  eta: 0:01:26  iteration: 601/1000  consumed samples: 9616  total_loss: 7.735  time: 0.2024(79.05)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:41] lb.utils.events INFO:  eta: 0:01:26  iteration: 602/1000  consumed samples: 9632  total_loss: 7.735  time: 0.2023(79.08)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:41] lb.utils.events INFO:  eta: 0:01:25  iteration: 603/1000  consumed samples: 9648  total_loss: 7.735  time: 0.2022(79.14)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:42] lb.utils.events INFO:  eta: 0:01:25  iteration: 604/1000  consumed samples: 9664  total_loss: 7.735  time: 0.2020(79.19)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:42] lb.utils.events INFO:  eta: 0:01:25  iteration: 605/1000  consumed samples: 9680  total_loss: 7.735  time: 0.2020(79.21)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:42] lb.utils.events INFO:  eta: 0:01:25  iteration: 606/1000  consumed samples: 9696  total_loss: 7.733  time: 0.2019(79.27)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:42] lb.utils.events INFO:  eta: 0:01:24  iteration: 607/1000  consumed samples: 9712  total_loss: 7.733  time: 0.2018(79.29)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:42] lb.utils.events INFO:  eta: 0:01:24  iteration: 608/1000  consumed samples: 9728  total_loss: 7.733  time: 0.2016(79.35)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:43] lb.utils.events INFO:  eta: 0:01:24  iteration: 609/1000  consumed samples: 9744  total_loss: 7.73  time: 0.2015(79.39)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:43] lb.utils.events INFO:  eta: 0:01:24  iteration: 610/1000  consumed samples: 9760  total_loss: 7.727  time: 0.2015(79.42)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:43] lb.utils.events INFO:  eta: 0:01:24  iteration: 611/1000  consumed samples: 9776  total_loss: 7.724  time: 0.2013(79.48)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:43] lb.utils.events INFO:  eta: 0:01:23  iteration: 612/1000  consumed samples: 9792  total_loss: 7.727  time: 0.2013(79.50)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:44] lb.utils.events INFO:  eta: 0:01:23  iteration: 613/1000  consumed samples: 9808  total_loss: 7.727  time: 0.2011(79.55)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:44] lb.utils.events INFO:  eta: 0:01:23  iteration: 614/1000  consumed samples: 9824  total_loss: 7.724  time: 0.2011(79.57)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:44] lb.utils.events INFO:  eta: 0:01:23  iteration: 615/1000  consumed samples: 9840  total_loss: 7.722  time: 0.2010(79.61)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:44] lb.utils.events INFO:  eta: 0:01:22  iteration: 616/1000  consumed samples: 9856  total_loss: 7.721  time: 0.2009(79.63)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:45] lb.utils.events INFO:  eta: 0:01:22  iteration: 617/1000  consumed samples: 9872  total_loss: 7.72  time: 0.2008(79.67)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:45] lb.utils.events INFO:  eta: 0:01:22  iteration: 618/1000  consumed samples: 9888  total_loss: 7.719  time: 0.2007(79.72)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:45] lb.utils.events INFO:  eta: 0:01:22  iteration: 619/1000  consumed samples: 9904  total_loss: 7.72  time: 0.2006(79.75)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:45] lb.utils.events INFO:  eta: 0:01:22  iteration: 620/1000  consumed samples: 9920  total_loss: 7.719  time: 0.2005(79.78)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:45] lb.utils.events INFO:  eta: 0:01:21  iteration: 621/1000  consumed samples: 9936  total_loss: 7.718  time: 0.2005(79.82)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:46] lb.utils.events INFO:  eta: 0:01:21  iteration: 622/1000  consumed samples: 9952  total_loss: 7.718  time: 0.2004(79.84)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:46] lb.utils.events INFO:  eta: 0:01:21  iteration: 623/1000  consumed samples: 9968  total_loss: 7.716  time: 0.2003(79.89)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:46] lb.utils.events INFO:  eta: 0:01:21  iteration: 624/1000  consumed samples: 9984  total_loss: 7.713  time: 0.2002(79.90)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:46] lb.utils.events INFO:  eta: 0:01:21  iteration: 625/1000  consumed samples: 10000  total_loss: 7.713  time: 0.2002(79.94)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:47] lb.utils.events INFO:  eta: 0:01:20  iteration: 626/1000  consumed samples: 10016  total_loss: 7.713  time: 0.2001(79.97)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:47] lb.utils.events INFO:  eta: 0:01:20  iteration: 627/1000  consumed samples: 10032  total_loss: 7.713  time: 0.2000(80.01)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:47] lb.utils.events INFO:  eta: 0:01:20  iteration: 628/1000  consumed samples: 10048  total_loss: 7.713  time: 0.1998(80.07)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:47] lb.utils.events INFO:  eta: 0:01:20  iteration: 629/1000  consumed samples: 10064  total_loss: 7.711  time: 0.1997(80.14)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:47] lb.utils.events INFO:  eta: 0:01:19  iteration: 630/1000  consumed samples: 10080  total_loss: 7.711  time: 0.1995(80.21)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:48] lb.utils.events INFO:  eta: 0:01:19  iteration: 631/1000  consumed samples: 10096  total_loss: 7.709  time: 0.1995(80.21)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:48] lb.utils.events INFO:  eta: 0:01:19  iteration: 632/1000  consumed samples: 10112  total_loss: 7.709  time: 0.1995(80.21)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:48] lb.utils.events INFO:  eta: 0:01:19  iteration: 633/1000  consumed samples: 10128  total_loss: 7.709  time: 0.1994(80.24)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:48] lb.utils.events INFO:  eta: 0:01:19  iteration: 634/1000  consumed samples: 10144  total_loss: 7.708  time: 0.1994(80.26)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:49] lb.utils.events INFO:  eta: 0:01:18  iteration: 635/1000  consumed samples: 10160  total_loss: 7.706  time: 0.1993(80.29)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:49] lb.utils.events INFO:  eta: 0:01:18  iteration: 636/1000  consumed samples: 10176  total_loss: 7.705  time: 0.1992(80.33)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:49] lb.utils.events INFO:  eta: 0:01:18  iteration: 637/1000  consumed samples: 10192  total_loss: 7.706  time: 0.1991(80.35)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:49] lb.utils.events INFO:  eta: 0:01:18  iteration: 638/1000  consumed samples: 10208  total_loss: 7.708  time: 0.1991(80.37)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:50] lb.utils.events INFO:  eta: 0:01:17  iteration: 639/1000  consumed samples: 10224  total_loss: 7.706  time: 0.1990(80.40)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:50] lb.utils.events INFO:  eta: 0:01:17  iteration: 640/1000  consumed samples: 10240  total_loss: 7.705  time: 0.1989(80.45)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:50] lb.utils.events INFO:  eta: 0:01:17  iteration: 641/1000  consumed samples: 10256  total_loss: 7.705  time: 0.1988(80.48)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:50] lb.utils.events INFO:  eta: 0:01:17  iteration: 642/1000  consumed samples: 10272  total_loss: 7.703  time: 0.1987(80.52)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:50] lb.utils.events INFO:  eta: 0:01:17  iteration: 643/1000  consumed samples: 10288  total_loss: 7.702  time: 0.1986(80.55)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:51] lb.utils.events INFO:  eta: 0:01:16  iteration: 644/1000  consumed samples: 10304  total_loss: 7.701  time: 0.1986(80.58)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:51] lb.utils.events INFO:  eta: 0:01:16  iteration: 645/1000  consumed samples: 10320  total_loss: 7.701  time: 0.1985(80.61)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:51] lb.utils.events INFO:  eta: 0:01:16  iteration: 646/1000  consumed samples: 10336  total_loss: 7.7  time: 0.1984(80.63)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:51] lb.utils.events INFO:  eta: 0:01:16  iteration: 647/1000  consumed samples: 10352  total_loss: 7.696  time: 0.1983(80.68)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:52] lb.utils.events INFO:  eta: 0:01:15  iteration: 648/1000  consumed samples: 10368  total_loss: 7.692  time: 0.1983(80.67)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:52] lb.utils.events INFO:  eta: 0:01:15  iteration: 649/1000  consumed samples: 10384  total_loss: 7.692  time: 0.1984(80.65)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:50:52] lb.utils.events INFO:  eta: 0:01:15  iteration: 650/1000  consumed samples: 10400  total_loss: 7.69  time: 0.1984(80.66)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:52] lb.utils.events INFO:  eta: 0:01:15  iteration: 651/1000  consumed samples: 10416  total_loss: 7.687  time: 0.1984(80.65)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:53] lb.utils.events INFO:  eta: 0:01:15  iteration: 652/1000  consumed samples: 10432  total_loss: 7.685  time: 0.1984(80.64)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:53] lb.utils.events INFO:  eta: 0:01:14  iteration: 653/1000  consumed samples: 10448  total_loss: 7.685  time: 0.1984(80.64)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:53] lb.utils.events INFO:  eta: 0:01:14  iteration: 654/1000  consumed samples: 10464  total_loss: 7.685  time: 0.1984(80.63)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:50:53] lb.utils.events INFO:  eta: 0:01:14  iteration: 655/1000  consumed samples: 10480  total_loss: 7.684  time: 0.1985(80.62)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:53] lb.utils.events INFO:  eta: 0:01:14  iteration: 656/1000  consumed samples: 10496  total_loss: 7.685  time: 0.1985(80.60)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:54] lb.utils.events INFO:  eta: 0:01:14  iteration: 657/1000  consumed samples: 10512  total_loss: 7.685  time: 0.1985(80.59)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:54] lb.utils.events INFO:  eta: 0:01:13  iteration: 658/1000  consumed samples: 10528  total_loss: 7.684  time: 0.1985(80.59)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:54] lb.utils.events INFO:  eta: 0:01:13  iteration: 659/1000  consumed samples: 10544  total_loss: 7.682  time: 0.1985(80.58)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:54] lb.utils.events INFO:  eta: 0:01:13  iteration: 660/1000  consumed samples: 10560  total_loss: 7.682  time: 0.1986(80.57)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:55] lb.utils.events INFO:  eta: 0:01:13  iteration: 661/1000  consumed samples: 10576  total_loss: 7.68  time: 0.1986(80.56)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:55] lb.utils.events INFO:  eta: 0:01:12  iteration: 662/1000  consumed samples: 10592  total_loss: 7.677  time: 0.1986(80.58)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:55] lb.utils.events INFO:  eta: 0:01:12  iteration: 663/1000  consumed samples: 10608  total_loss: 7.674  time: 0.1986(80.57)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:55] lb.utils.events INFO:  eta: 0:01:12  iteration: 664/1000  consumed samples: 10624  total_loss: 7.674  time: 0.1985(80.59)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:56] lb.utils.events INFO:  eta: 0:01:12  iteration: 665/1000  consumed samples: 10640  total_loss: 7.674  time: 0.1986(80.58)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:56] lb.utils.events INFO:  eta: 0:01:12  iteration: 666/1000  consumed samples: 10656  total_loss: 7.672  time: 0.1986(80.57)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:56] lb.utils.events INFO:  eta: 0:01:11  iteration: 667/1000  consumed samples: 10672  total_loss: 7.671  time: 0.1986(80.55)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:56] lb.utils.events INFO:  eta: 0:01:11  iteration: 668/1000  consumed samples: 10688  total_loss: 7.67  time: 0.1987(80.53)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:56] lb.utils.events INFO:  eta: 0:01:11  iteration: 669/1000  consumed samples: 10704  total_loss: 7.67  time: 0.1987(80.53)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:50:57] lb.utils.events INFO:  eta: 0:01:11  iteration: 670/1000  consumed samples: 10720  total_loss: 7.67  time: 0.1987(80.51)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:50:57] lb.utils.events INFO:  eta: 0:01:11  iteration: 671/1000  consumed samples: 10736  total_loss: 7.668  time: 0.1987(80.53)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:57] lb.utils.events INFO:  eta: 0:01:10  iteration: 672/1000  consumed samples: 10752  total_loss: 7.667  time: 0.1986(80.56)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:57] lb.utils.events INFO:  eta: 0:01:10  iteration: 673/1000  consumed samples: 10768  total_loss: 7.666  time: 0.1986(80.54)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:58] lb.utils.events INFO:  eta: 0:01:10  iteration: 674/1000  consumed samples: 10784  total_loss: 7.667  time: 0.1985(80.60)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:58] lb.utils.events INFO:  eta: 0:01:10  iteration: 675/1000  consumed samples: 10800  total_loss: 7.667  time: 0.1984(80.65)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:58] lb.utils.events INFO:  eta: 0:01:09  iteration: 676/1000  consumed samples: 10816  total_loss: 7.666  time: 0.1984(80.64)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:58] lb.utils.events INFO:  eta: 0:01:09  iteration: 677/1000  consumed samples: 10832  total_loss: 7.666  time: 0.1983(80.69)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:59] lb.utils.events INFO:  eta: 0:01:09  iteration: 678/1000  consumed samples: 10848  total_loss: 7.666  time: 0.1982(80.74)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:59] lb.utils.events INFO:  eta: 0:01:09  iteration: 679/1000  consumed samples: 10864  total_loss: 7.665  time: 0.1980(80.81)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:59] lb.utils.events INFO:  eta: 0:01:09  iteration: 680/1000  consumed samples: 10880  total_loss: 7.663  time: 0.1980(80.81)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:50:59] lb.utils.events INFO:  eta: 0:01:08  iteration: 681/1000  consumed samples: 10896  total_loss: 7.663  time: 0.1980(80.79)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:50:59] lb.utils.events INFO:  eta: 0:01:08  iteration: 682/1000  consumed samples: 10912  total_loss: 7.659  time: 0.1981(80.78)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:00] lb.utils.events INFO:  eta: 0:01:08  iteration: 683/1000  consumed samples: 10928  total_loss: 7.659  time: 0.1981(80.76)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:00] lb.utils.events INFO:  eta: 0:01:08  iteration: 684/1000  consumed samples: 10944  total_loss: 7.657  time: 0.1981(80.75)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:00] lb.utils.events INFO:  eta: 0:01:07  iteration: 685/1000  consumed samples: 10960  total_loss: 7.659  time: 0.1982(80.74)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:00] lb.utils.events INFO:  eta: 0:01:07  iteration: 686/1000  consumed samples: 10976  total_loss: 7.657  time: 0.1982(80.73)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:01] lb.utils.events INFO:  eta: 0:01:07  iteration: 687/1000  consumed samples: 10992  total_loss: 7.657  time: 0.1982(80.72)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:01] lb.utils.events INFO:  eta: 0:01:07  iteration: 688/1000  consumed samples: 11008  total_loss: 7.656  time: 0.1982(80.71)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:01] lb.utils.events INFO:  eta: 0:01:07  iteration: 689/1000  consumed samples: 11024  total_loss: 7.656  time: 0.1983(80.69)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:01] lb.utils.events INFO:  eta: 0:01:06  iteration: 690/1000  consumed samples: 11040  total_loss: 7.655  time: 0.1983(80.68)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:01] lb.utils.events INFO:  eta: 0:01:06  iteration: 691/1000  consumed samples: 11056  total_loss: 7.653  time: 0.1983(80.67)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:02] lb.utils.events INFO:  eta: 0:01:06  iteration: 692/1000  consumed samples: 11072  total_loss: 7.653  time: 0.1984(80.66)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:02] lb.utils.events INFO:  eta: 0:01:06  iteration: 693/1000  consumed samples: 11088  total_loss: 7.653  time: 0.1984(80.65)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:02] lb.utils.events INFO:  eta: 0:01:06  iteration: 694/1000  consumed samples: 11104  total_loss: 7.651  time: 0.1984(80.63)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:02] lb.utils.events INFO:  eta: 0:01:05  iteration: 695/1000  consumed samples: 11120  total_loss: 7.65  time: 0.1985(80.62)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:03] lb.utils.events INFO:  eta: 0:01:05  iteration: 696/1000  consumed samples: 11136  total_loss: 7.65  time: 0.1985(80.61)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:03] lb.utils.events INFO:  eta: 0:01:05  iteration: 697/1000  consumed samples: 11152  total_loss: 7.649  time: 0.1985(80.59)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:03] lb.utils.events INFO:  eta: 0:01:05  iteration: 698/1000  consumed samples: 11168  total_loss: 7.649  time: 0.1986(80.58)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:03] lb.utils.events INFO:  eta: 0:01:04  iteration: 699/1000  consumed samples: 11184  total_loss: 7.649  time: 0.1986(80.57)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:04] lb.utils.events INFO:  eta: 0:01:04  iteration: 700/1000  consumed samples: 11200  total_loss: 7.647  time: 0.1986(80.56)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:04] lb.utils.events INFO:  eta: 0:01:04  iteration: 701/1000  consumed samples: 11216  total_loss: 7.641  time: 0.1987(80.54)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:04] lb.utils.events INFO:  eta: 0:01:04  iteration: 702/1000  consumed samples: 11232  total_loss: 7.637  time: 0.1987(80.53)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:04] lb.utils.events INFO:  eta: 0:01:04  iteration: 703/1000  consumed samples: 11248  total_loss: 7.635  time: 0.1987(80.52)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:05] lb.utils.events INFO:  eta: 0:01:03  iteration: 704/1000  consumed samples: 11264  total_loss: 7.635  time: 0.1986(80.58)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:05] lb.utils.events INFO:  eta: 0:01:03  iteration: 705/1000  consumed samples: 11280  total_loss: 7.633  time: 0.1984(80.64)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:05] lb.utils.events INFO:  eta: 0:01:03  iteration: 706/1000  consumed samples: 11296  total_loss: 7.632  time: 0.1984(80.63)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:05] lb.utils.events INFO:  eta: 0:01:03  iteration: 707/1000  consumed samples: 11312  total_loss: 7.632  time: 0.1984(80.62)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:05] lb.utils.events INFO:  eta: 0:01:02  iteration: 708/1000  consumed samples: 11328  total_loss: 7.629  time: 0.1985(80.62)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:06] lb.utils.events INFO:  eta: 0:01:02  iteration: 709/1000  consumed samples: 11344  total_loss: 7.625  time: 0.1985(80.61)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:06] lb.utils.events INFO:  eta: 0:01:02  iteration: 710/1000  consumed samples: 11360  total_loss: 7.625  time: 0.1985(80.60)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:06] lb.utils.events INFO:  eta: 0:01:02  iteration: 711/1000  consumed samples: 11376  total_loss: 7.625  time: 0.1985(80.59)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:06] lb.utils.events INFO:  eta: 0:01:02  iteration: 712/1000  consumed samples: 11392  total_loss: 7.623  time: 0.1986(80.58)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:07] lb.utils.events INFO:  eta: 0:01:01  iteration: 713/1000  consumed samples: 11408  total_loss: 7.622  time: 0.1986(80.57)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:07] lb.utils.events INFO:  eta: 0:01:01  iteration: 714/1000  consumed samples: 11424  total_loss: 7.622  time: 0.1986(80.55)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:07] lb.utils.events INFO:  eta: 0:01:01  iteration: 715/1000  consumed samples: 11440  total_loss: 7.621  time: 0.1987(80.54)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:07] lb.utils.events INFO:  eta: 0:01:01  iteration: 716/1000  consumed samples: 11456  total_loss: 7.621  time: 0.1987(80.53)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:08] lb.utils.events INFO:  eta: 0:01:01  iteration: 717/1000  consumed samples: 11472  total_loss: 7.62  time: 0.1987(80.52)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:08] lb.utils.events INFO:  eta: 0:01:00  iteration: 718/1000  consumed samples: 11488  total_loss: 7.618  time: 0.1988(80.50)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:08] lb.utils.events INFO:  eta: 0:01:00  iteration: 719/1000  consumed samples: 11504  total_loss: 7.618  time: 0.1988(80.49)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:08] lb.utils.events INFO:  eta: 0:01:00  iteration: 720/1000  consumed samples: 11520  total_loss: 7.618  time: 0.1988(80.48)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:08] lb.utils.events INFO:  eta: 0:01:00  iteration: 721/1000  consumed samples: 11536  total_loss: 7.616  time: 0.1988(80.47)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:09] lb.utils.events INFO:  eta: 0:00:59  iteration: 722/1000  consumed samples: 11552  total_loss: 7.616  time: 0.1989(80.45)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:09] lb.utils.events INFO:  eta: 0:00:59  iteration: 723/1000  consumed samples: 11568  total_loss: 7.616  time: 0.1989(80.44)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:09] lb.utils.events INFO:  eta: 0:00:59  iteration: 724/1000  consumed samples: 11584  total_loss: 7.616  time: 0.1989(80.43)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:09] lb.utils.events INFO:  eta: 0:00:59  iteration: 725/1000  consumed samples: 11600  total_loss: 7.615  time: 0.1990(80.42)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:10] lb.utils.events INFO:  eta: 0:00:59  iteration: 726/1000  consumed samples: 11616  total_loss: 7.614  time: 0.1990(80.40)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:10] lb.utils.events INFO:  eta: 0:00:58  iteration: 727/1000  consumed samples: 11632  total_loss: 7.614  time: 0.1990(80.41)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:10] lb.utils.events INFO:  eta: 0:00:58  iteration: 728/1000  consumed samples: 11648  total_loss: 7.613  time: 0.1990(80.40)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:10] lb.utils.events INFO:  eta: 0:00:58  iteration: 729/1000  consumed samples: 11664  total_loss: 7.613  time: 0.1990(80.38)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:11] lb.utils.events INFO:  eta: 0:00:58  iteration: 730/1000  consumed samples: 11680  total_loss: 7.613  time: 0.1991(80.38)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:11] lb.utils.events INFO:  eta: 0:00:58  iteration: 731/1000  consumed samples: 11696  total_loss: 7.613  time: 0.1991(80.36)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:11] lb.utils.events INFO:  eta: 0:00:57  iteration: 732/1000  consumed samples: 11712  total_loss: 7.612  time: 0.1991(80.36)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:11] lb.utils.events INFO:  eta: 0:00:57  iteration: 733/1000  consumed samples: 11728  total_loss: 7.611  time: 0.1991(80.34)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:11] lb.utils.events INFO:  eta: 0:00:57  iteration: 734/1000  consumed samples: 11744  total_loss: 7.61  time: 0.1992(80.33)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:12] lb.utils.events INFO:  eta: 0:00:57  iteration: 735/1000  consumed samples: 11760  total_loss: 7.61  time: 0.1992(80.32)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:12] lb.utils.events INFO:  eta: 0:00:56  iteration: 736/1000  consumed samples: 11776  total_loss: 7.61  time: 0.1992(80.31)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:12] lb.utils.events INFO:  eta: 0:00:56  iteration: 737/1000  consumed samples: 11792  total_loss: 7.61  time: 0.1993(80.30)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:12] lb.utils.events INFO:  eta: 0:00:56  iteration: 738/1000  consumed samples: 11808  total_loss: 7.61  time: 0.1993(80.29)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:13] lb.utils.events INFO:  eta: 0:00:56  iteration: 739/1000  consumed samples: 11824  total_loss: 7.61  time: 0.1993(80.27)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:13] lb.utils.events INFO:  eta: 0:00:56  iteration: 740/1000  consumed samples: 11840  total_loss: 7.609  time: 0.1993(80.26)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:13] lb.utils.events INFO:  eta: 0:00:55  iteration: 741/1000  consumed samples: 11856  total_loss: 7.608  time: 0.1994(80.25)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:13] lb.utils.events INFO:  eta: 0:00:55  iteration: 742/1000  consumed samples: 11872  total_loss: 7.608  time: 0.1994(80.24)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:13] lb.utils.events INFO:  eta: 0:00:55  iteration: 743/1000  consumed samples: 11888  total_loss: 7.608  time: 0.1994(80.23)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:14] lb.utils.events INFO:  eta: 0:00:55  iteration: 744/1000  consumed samples: 11904  total_loss: 7.608  time: 0.1995(80.22)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:14] lb.utils.events INFO:  eta: 0:00:55  iteration: 745/1000  consumed samples: 11920  total_loss: 7.609  time: 0.1995(80.21)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:14] lb.utils.events INFO:  eta: 0:00:54  iteration: 746/1000  consumed samples: 11936  total_loss: 7.608  time: 0.1995(80.21)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:14] lb.utils.events INFO:  eta: 0:00:54  iteration: 747/1000  consumed samples: 11952  total_loss: 7.609  time: 0.1995(80.19)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:15] lb.utils.events INFO:  eta: 0:00:54  iteration: 748/1000  consumed samples: 11968  total_loss: 7.609  time: 0.1995(80.18)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:15] lb.utils.events INFO:  eta: 0:00:54  iteration: 749/1000  consumed samples: 11984  total_loss: 7.609  time: 0.1996(80.17)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:15] lb.utils.events INFO:  eta: 0:00:53  iteration: 750/1000  consumed samples: 12000  total_loss: 7.608  time: 0.1996(80.17)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:15] lb.utils.events INFO:  eta: 0:00:53  iteration: 751/1000  consumed samples: 12016  total_loss: 7.608  time: 0.1996(80.15)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:16] lb.utils.events INFO:  eta: 0:00:53  iteration: 752/1000  consumed samples: 12032  total_loss: 7.607  time: 0.1997(80.14)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:16] lb.utils.events INFO:  eta: 0:00:53  iteration: 753/1000  consumed samples: 12048  total_loss: 7.607  time: 0.1997(80.13)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:16] lb.utils.events INFO:  eta: 0:00:53  iteration: 754/1000  consumed samples: 12064  total_loss: 7.607  time: 0.1997(80.12)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:16] lb.utils.events INFO:  eta: 0:00:52  iteration: 755/1000  consumed samples: 12080  total_loss: 7.607  time: 0.1997(80.11)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:16] lb.utils.events INFO:  eta: 0:00:52  iteration: 756/1000  consumed samples: 12096  total_loss: 7.605  time: 0.1998(80.10)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:17] lb.utils.events INFO:  eta: 0:00:52  iteration: 757/1000  consumed samples: 12112  total_loss: 7.603  time: 0.1998(80.08)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:17] lb.utils.events INFO:  eta: 0:00:52  iteration: 758/1000  consumed samples: 12128  total_loss: 7.603  time: 0.1998(80.07)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:17] lb.utils.events INFO:  eta: 0:00:51  iteration: 759/1000  consumed samples: 12144  total_loss: 7.605  time: 0.1998(80.06)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:17] lb.utils.events INFO:  eta: 0:00:51  iteration: 760/1000  consumed samples: 12160  total_loss: 7.603  time: 0.1999(80.05)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:18] lb.utils.events INFO:  eta: 0:00:51  iteration: 761/1000  consumed samples: 12176  total_loss: 7.603  time: 0.1999(80.04)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:18] lb.utils.events INFO:  eta: 0:00:51  iteration: 762/1000  consumed samples: 12192  total_loss: 7.603  time: 0.1999(80.03)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:18] lb.utils.events INFO:  eta: 0:00:51  iteration: 763/1000  consumed samples: 12208  total_loss: 7.602  time: 0.2000(80.02)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:18] lb.utils.events INFO:  eta: 0:00:50  iteration: 764/1000  consumed samples: 12224  total_loss: 7.602  time: 0.2000(80.01)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:19] lb.utils.events INFO:  eta: 0:00:50  iteration: 765/1000  consumed samples: 12240  total_loss: 7.602  time: 0.2000(80.00)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:19] lb.utils.events INFO:  eta: 0:00:50  iteration: 766/1000  consumed samples: 12256  total_loss: 7.602  time: 0.2000(79.99)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:19] lb.utils.events INFO:  eta: 0:00:50  iteration: 767/1000  consumed samples: 12272  total_loss: 7.602  time: 0.2001(79.98)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:19] lb.utils.events INFO:  eta: 0:00:50  iteration: 768/1000  consumed samples: 12288  total_loss: 7.602  time: 0.2001(79.97)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:19] lb.utils.events INFO:  eta: 0:00:49  iteration: 769/1000  consumed samples: 12304  total_loss: 7.602  time: 0.2001(79.96)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:20] lb.utils.events INFO:  eta: 0:00:49  iteration: 770/1000  consumed samples: 12320  total_loss: 7.602  time: 0.2001(79.94)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:20] lb.utils.events INFO:  eta: 0:00:49  iteration: 771/1000  consumed samples: 12336  total_loss: 7.602  time: 0.2002(79.93)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:20] lb.utils.events INFO:  eta: 0:00:49  iteration: 772/1000  consumed samples: 12352  total_loss: 7.602  time: 0.2002(79.92)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:20] lb.utils.events INFO:  eta: 0:00:48  iteration: 773/1000  consumed samples: 12368  total_loss: 7.602  time: 0.2002(79.91)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:21] lb.utils.events INFO:  eta: 0:00:48  iteration: 774/1000  consumed samples: 12384  total_loss: 7.602  time: 0.2002(79.90)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:21] lb.utils.events INFO:  eta: 0:00:48  iteration: 775/1000  consumed samples: 12400  total_loss: 7.6  time: 0.2003(79.89)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:21] lb.utils.events INFO:  eta: 0:00:48  iteration: 776/1000  consumed samples: 12416  total_loss: 7.6  time: 0.2003(79.88)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:21] lb.utils.events INFO:  eta: 0:00:48  iteration: 777/1000  consumed samples: 12432  total_loss: 7.598  time: 0.2003(79.87)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:22] lb.utils.events INFO:  eta: 0:00:47  iteration: 778/1000  consumed samples: 12448  total_loss: 7.598  time: 0.2004(79.86)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:22] lb.utils.events INFO:  eta: 0:00:47  iteration: 779/1000  consumed samples: 12464  total_loss: 7.598  time: 0.2004(79.85)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:22] lb.utils.events INFO:  eta: 0:00:47  iteration: 780/1000  consumed samples: 12480  total_loss: 7.598  time: 0.2004(79.84)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:22] lb.utils.events INFO:  eta: 0:00:47  iteration: 781/1000  consumed samples: 12496  total_loss: 7.596  time: 0.2004(79.83)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:22] lb.utils.events INFO:  eta: 0:00:47  iteration: 782/1000  consumed samples: 12512  total_loss: 7.59  time: 0.2005(79.82)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:23] lb.utils.events INFO:  eta: 0:00:46  iteration: 783/1000  consumed samples: 12528  total_loss: 7.596  time: 0.2005(79.81)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:23] lb.utils.events INFO:  eta: 0:00:46  iteration: 784/1000  consumed samples: 12544  total_loss: 7.598  time: 0.2005(79.80)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:23] lb.utils.events INFO:  eta: 0:00:46  iteration: 785/1000  consumed samples: 12560  total_loss: 7.598  time: 0.2005(79.79)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:23] lb.utils.events INFO:  eta: 0:00:46  iteration: 786/1000  consumed samples: 12576  total_loss: 7.596  time: 0.2006(79.78)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:24] lb.utils.events INFO:  eta: 0:00:45  iteration: 787/1000  consumed samples: 12592  total_loss: 7.598  time: 0.2006(79.77)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:24] lb.utils.events INFO:  eta: 0:00:45  iteration: 788/1000  consumed samples: 12608  total_loss: 7.598  time: 0.2006(79.76)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:24] lb.utils.events INFO:  eta: 0:00:45  iteration: 789/1000  consumed samples: 12624  total_loss: 7.598  time: 0.2006(79.75)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:24] lb.utils.events INFO:  eta: 0:00:45  iteration: 790/1000  consumed samples: 12640  total_loss: 7.596  time: 0.2007(79.74)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:25] lb.utils.events INFO:  eta: 0:00:45  iteration: 791/1000  consumed samples: 12656  total_loss: 7.59  time: 0.2007(79.73)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:25] lb.utils.events INFO:  eta: 0:00:44  iteration: 792/1000  consumed samples: 12672  total_loss: 7.59  time: 0.2007(79.72)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:25] lb.utils.events INFO:  eta: 0:00:44  iteration: 793/1000  consumed samples: 12688  total_loss: 7.586  time: 0.2007(79.71)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:25] lb.utils.events INFO:  eta: 0:00:44  iteration: 794/1000  consumed samples: 12704  total_loss: 7.586  time: 0.2008(79.70)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:25] lb.utils.events INFO:  eta: 0:00:44  iteration: 795/1000  consumed samples: 12720  total_loss: 7.59  time: 0.2008(79.69)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:26] lb.utils.events INFO:  eta: 0:00:44  iteration: 796/1000  consumed samples: 12736  total_loss: 7.59  time: 0.2008(79.68)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:26] lb.utils.events INFO:  eta: 0:00:43  iteration: 797/1000  consumed samples: 12752  total_loss: 7.59  time: 0.2008(79.67)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:26] lb.utils.events INFO:  eta: 0:00:43  iteration: 798/1000  consumed samples: 12768  total_loss: 7.586  time: 0.2009(79.66)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:26] lb.utils.events INFO:  eta: 0:00:43  iteration: 799/1000  consumed samples: 12784  total_loss: 7.586  time: 0.2009(79.65)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:27] lb.utils.events INFO:  eta: 0:00:43  iteration: 800/1000  consumed samples: 12800  total_loss: 7.585  time: 0.2009(79.64)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:27] lb.utils.events INFO:  eta: 0:00:42  iteration: 801/1000  consumed samples: 12816  total_loss: 7.585  time: 0.2009(79.63)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:27] lb.utils.events INFO:  eta: 0:00:42  iteration: 802/1000  consumed samples: 12832  total_loss: 7.585  time: 0.2010(79.62)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:51:27] lb.utils.events INFO:  eta: 0:00:42  iteration: 803/1000  consumed samples: 12848  total_loss: 7.584  time: 0.2010(79.61)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:27] lb.utils.events INFO:  eta: 0:00:42  iteration: 804/1000  consumed samples: 12864  total_loss: 7.582  time: 0.2010(79.60)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:28] lb.utils.events INFO:  eta: 0:00:42  iteration: 805/1000  consumed samples: 12880  total_loss: 7.581  time: 0.2010(79.59)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:28] lb.utils.events INFO:  eta: 0:00:41  iteration: 806/1000  consumed samples: 12896  total_loss: 7.581  time: 0.2010(79.58)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:28] lb.utils.events INFO:  eta: 0:00:41  iteration: 807/1000  consumed samples: 12912  total_loss: 7.581  time: 0.2011(79.57)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:28] lb.utils.events INFO:  eta: 0:00:41  iteration: 808/1000  consumed samples: 12928  total_loss: 7.582  time: 0.2011(79.56)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:29] lb.utils.events INFO:  eta: 0:00:41  iteration: 809/1000  consumed samples: 12944  total_loss: 7.582  time: 0.2011(79.55)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:29] lb.utils.events INFO:  eta: 0:00:40  iteration: 810/1000  consumed samples: 12960  total_loss: 7.581  time: 0.2011(79.55)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:29] lb.utils.events INFO:  eta: 0:00:40  iteration: 811/1000  consumed samples: 12976  total_loss: 7.581  time: 0.2012(79.54)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:29] lb.utils.events INFO:  eta: 0:00:40  iteration: 812/1000  consumed samples: 12992  total_loss: 7.579  time: 0.2012(79.53)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:30] lb.utils.events INFO:  eta: 0:00:40  iteration: 813/1000  consumed samples: 13008  total_loss: 7.579  time: 0.2012(79.52)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:51:30] lb.utils.events INFO:  eta: 0:00:40  iteration: 814/1000  consumed samples: 13024  total_loss: 7.579  time: 0.2012(79.51)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:30] lb.utils.events INFO:  eta: 0:00:39  iteration: 815/1000  consumed samples: 13040  total_loss: 7.577  time: 0.2012(79.51)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:30] lb.utils.events INFO:  eta: 0:00:39  iteration: 816/1000  consumed samples: 13056  total_loss: 7.577  time: 0.2012(79.50)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:30] lb.utils.events INFO:  eta: 0:00:39  iteration: 817/1000  consumed samples: 13072  total_loss: 7.577  time: 0.2013(79.49)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:31] lb.utils.events INFO:  eta: 0:00:39  iteration: 818/1000  consumed samples: 13088  total_loss: 7.577  time: 0.2013(79.48)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:31] lb.utils.events INFO:  eta: 0:00:39  iteration: 819/1000  consumed samples: 13104  total_loss: 7.576  time: 0.2013(79.47)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:31] lb.utils.events INFO:  eta: 0:00:38  iteration: 820/1000  consumed samples: 13120  total_loss: 7.576  time: 0.2014(79.46)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:31] lb.utils.events INFO:  eta: 0:00:38  iteration: 821/1000  consumed samples: 13136  total_loss: 7.576  time: 0.2014(79.46)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:32] lb.utils.events INFO:  eta: 0:00:38  iteration: 822/1000  consumed samples: 13152  total_loss: 7.576  time: 0.2014(79.44)  data_time: 0.0051  lr: 1.00e-04  
[01/19 14:51:32] lb.utils.events INFO:  eta: 0:00:38  iteration: 823/1000  consumed samples: 13168  total_loss: 7.576  time: 0.2014(79.44)  data_time: 0.0050  lr: 1.00e-04  
[01/19 14:51:32] lb.utils.events INFO:  eta: 0:00:37  iteration: 824/1000  consumed samples: 13184  total_loss: 7.577  time: 0.2014(79.43)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:32] lb.utils.events INFO:  eta: 0:00:37  iteration: 825/1000  consumed samples: 13200  total_loss: 7.576  time: 0.2015(79.42)  data_time: 0.0050  lr: 1.00e-04  
[01/19 14:51:33] lb.utils.events INFO:  eta: 0:00:37  iteration: 826/1000  consumed samples: 13216  total_loss: 7.575  time: 0.2015(79.41)  data_time: 0.0051  lr: 1.00e-04  
[01/19 14:51:33] lb.utils.events INFO:  eta: 0:00:37  iteration: 827/1000  consumed samples: 13232  total_loss: 7.575  time: 0.2015(79.40)  data_time: 0.0051  lr: 1.00e-04  
[01/19 14:51:33] lb.utils.events INFO:  eta: 0:00:37  iteration: 828/1000  consumed samples: 13248  total_loss: 7.574  time: 0.2015(79.39)  data_time: 0.0051  lr: 1.00e-04  
[01/19 14:51:33] lb.utils.events INFO:  eta: 0:00:36  iteration: 829/1000  consumed samples: 13264  total_loss: 7.574  time: 0.2016(79.38)  data_time: 0.0052  lr: 1.00e-04  
[01/19 14:51:33] lb.utils.events INFO:  eta: 0:00:36  iteration: 830/1000  consumed samples: 13280  total_loss: 7.572  time: 0.2016(79.37)  data_time: 0.0054  lr: 1.00e-04  
[01/19 14:51:34] lb.utils.events INFO:  eta: 0:00:36  iteration: 831/1000  consumed samples: 13296  total_loss: 7.57  time: 0.2016(79.36)  data_time: 0.0053  lr: 1.00e-04  
[01/19 14:51:34] lb.utils.events INFO:  eta: 0:00:36  iteration: 832/1000  consumed samples: 13312  total_loss: 7.568  time: 0.2016(79.35)  data_time: 0.0052  lr: 1.00e-04  
[01/19 14:51:34] lb.utils.events INFO:  eta: 0:00:36  iteration: 833/1000  consumed samples: 13328  total_loss: 7.568  time: 0.2017(79.34)  data_time: 0.0052  lr: 1.00e-04  
[01/19 14:51:34] lb.utils.events INFO:  eta: 0:00:35  iteration: 834/1000  consumed samples: 13344  total_loss: 7.568  time: 0.2017(79.33)  data_time: 0.0054  lr: 1.00e-04  
[01/19 14:51:35] lb.utils.events INFO:  eta: 0:00:35  iteration: 835/1000  consumed samples: 13360  total_loss: 7.568  time: 0.2017(79.33)  data_time: 0.0053  lr: 1.00e-04  
[01/19 14:51:35] lb.utils.events INFO:  eta: 0:00:35  iteration: 836/1000  consumed samples: 13376  total_loss: 7.568  time: 0.2017(79.32)  data_time: 0.0053  lr: 1.00e-04  
[01/19 14:51:35] lb.utils.events INFO:  eta: 0:00:35  iteration: 837/1000  consumed samples: 13392  total_loss: 7.568  time: 0.2017(79.32)  data_time: 0.0051  lr: 1.00e-04  
[01/19 14:51:35] lb.utils.events INFO:  eta: 0:00:34  iteration: 838/1000  consumed samples: 13408  total_loss: 7.568  time: 0.2018(79.31)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:36] lb.utils.events INFO:  eta: 0:00:34  iteration: 839/1000  consumed samples: 13424  total_loss: 7.57  time: 0.2018(79.30)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:36] lb.utils.events INFO:  eta: 0:00:34  iteration: 840/1000  consumed samples: 13440  total_loss: 7.568  time: 0.2018(79.29)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:36] lb.utils.events INFO:  eta: 0:00:34  iteration: 841/1000  consumed samples: 13456  total_loss: 7.568  time: 0.2018(79.28)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:36] lb.utils.events INFO:  eta: 0:00:34  iteration: 842/1000  consumed samples: 13472  total_loss: 7.568  time: 0.2018(79.27)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:36] lb.utils.events INFO:  eta: 0:00:33  iteration: 843/1000  consumed samples: 13488  total_loss: 7.568  time: 0.2019(79.26)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:37] lb.utils.events INFO:  eta: 0:00:33  iteration: 844/1000  consumed samples: 13504  total_loss: 7.568  time: 0.2019(79.25)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:37] lb.utils.events INFO:  eta: 0:00:33  iteration: 845/1000  consumed samples: 13520  total_loss: 7.567  time: 0.2019(79.25)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:37] lb.utils.events INFO:  eta: 0:00:33  iteration: 846/1000  consumed samples: 13536  total_loss: 7.566  time: 0.2019(79.24)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:37] lb.utils.events INFO:  eta: 0:00:32  iteration: 847/1000  consumed samples: 13552  total_loss: 7.566  time: 0.2019(79.23)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:38] lb.utils.events INFO:  eta: 0:00:32  iteration: 848/1000  consumed samples: 13568  total_loss: 7.566  time: 0.2020(79.22)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:38] lb.utils.events INFO:  eta: 0:00:32  iteration: 849/1000  consumed samples: 13584  total_loss: 7.566  time: 0.2020(79.21)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:38] lb.utils.events INFO:  eta: 0:00:32  iteration: 850/1000  consumed samples: 13600  total_loss: 7.566  time: 0.2020(79.20)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:38] lb.utils.events INFO:  eta: 0:00:32  iteration: 851/1000  consumed samples: 13616  total_loss: 7.566  time: 0.2020(79.19)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:39] lb.utils.events INFO:  eta: 0:00:31  iteration: 852/1000  consumed samples: 13632  total_loss: 7.566  time: 0.2021(79.18)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:39] lb.utils.events INFO:  eta: 0:00:31  iteration: 853/1000  consumed samples: 13648  total_loss: 7.566  time: 0.2021(79.18)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:39] lb.utils.events INFO:  eta: 0:00:31  iteration: 854/1000  consumed samples: 13664  total_loss: 7.566  time: 0.2021(79.17)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:39] lb.utils.events INFO:  eta: 0:00:31  iteration: 855/1000  consumed samples: 13680  total_loss: 7.566  time: 0.2021(79.16)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:39] lb.utils.events INFO:  eta: 0:00:31  iteration: 856/1000  consumed samples: 13696  total_loss: 7.566  time: 0.2021(79.15)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:40] lb.utils.events INFO:  eta: 0:00:30  iteration: 857/1000  consumed samples: 13712  total_loss: 7.566  time: 0.2022(79.15)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:40] lb.utils.events INFO:  eta: 0:00:30  iteration: 858/1000  consumed samples: 13728  total_loss: 7.566  time: 0.2022(79.14)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:40] lb.utils.events INFO:  eta: 0:00:30  iteration: 859/1000  consumed samples: 13744  total_loss: 7.566  time: 0.2022(79.13)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:40] lb.utils.events INFO:  eta: 0:00:30  iteration: 860/1000  consumed samples: 13760  total_loss: 7.566  time: 0.2022(79.12)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:51:41] lb.utils.events INFO:  eta: 0:00:29  iteration: 861/1000  consumed samples: 13776  total_loss: 7.566  time: 0.2022(79.11)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:51:41] lb.utils.events INFO:  eta: 0:00:29  iteration: 862/1000  consumed samples: 13792  total_loss: 7.566  time: 0.2023(79.10)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:41] lb.utils.events INFO:  eta: 0:00:29  iteration: 863/1000  consumed samples: 13808  total_loss: 7.566  time: 0.2023(79.10)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:41] lb.utils.events INFO:  eta: 0:00:29  iteration: 864/1000  consumed samples: 13824  total_loss: 7.566  time: 0.2023(79.09)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:42] lb.utils.events INFO:  eta: 0:00:29  iteration: 865/1000  consumed samples: 13840  total_loss: 7.565  time: 0.2023(79.08)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:42] lb.utils.events INFO:  eta: 0:00:28  iteration: 866/1000  consumed samples: 13856  total_loss: 7.565  time: 0.2024(79.07)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:42] lb.utils.events INFO:  eta: 0:00:28  iteration: 867/1000  consumed samples: 13872  total_loss: 7.562  time: 0.2024(79.06)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:51:42] lb.utils.events INFO:  eta: 0:00:28  iteration: 868/1000  consumed samples: 13888  total_loss: 7.565  time: 0.2024(79.05)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:42] lb.utils.events INFO:  eta: 0:00:28  iteration: 869/1000  consumed samples: 13904  total_loss: 7.562  time: 0.2024(79.04)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:43] lb.utils.events INFO:  eta: 0:00:28  iteration: 870/1000  consumed samples: 13920  total_loss: 7.557  time: 0.2024(79.04)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:43] lb.utils.events INFO:  eta: 0:00:27  iteration: 871/1000  consumed samples: 13936  total_loss: 7.557  time: 0.2024(79.03)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:43] lb.utils.events INFO:  eta: 0:00:27  iteration: 872/1000  consumed samples: 13952  total_loss: 7.557  time: 0.2025(79.02)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:43] lb.utils.events INFO:  eta: 0:00:27  iteration: 873/1000  consumed samples: 13968  total_loss: 7.557  time: 0.2025(79.02)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:44] lb.utils.events INFO:  eta: 0:00:27  iteration: 874/1000  consumed samples: 13984  total_loss: 7.557  time: 0.2025(79.01)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:44] lb.utils.events INFO:  eta: 0:00:26  iteration: 875/1000  consumed samples: 14000  total_loss: 7.557  time: 0.2025(79.00)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:44] lb.utils.events INFO:  eta: 0:00:26  iteration: 876/1000  consumed samples: 14016  total_loss: 7.557  time: 0.2025(78.99)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:44] lb.utils.events INFO:  eta: 0:00:26  iteration: 877/1000  consumed samples: 14032  total_loss: 7.553  time: 0.2026(78.99)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:44] lb.utils.events INFO:  eta: 0:00:26  iteration: 878/1000  consumed samples: 14048  total_loss: 7.552  time: 0.2026(78.98)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:45] lb.utils.events INFO:  eta: 0:00:26  iteration: 879/1000  consumed samples: 14064  total_loss: 7.55  time: 0.2026(78.97)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:45] lb.utils.events INFO:  eta: 0:00:25  iteration: 880/1000  consumed samples: 14080  total_loss: 7.55  time: 0.2026(78.97)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:45] lb.utils.events INFO:  eta: 0:00:25  iteration: 881/1000  consumed samples: 14096  total_loss: 7.548  time: 0.2026(78.96)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:45] lb.utils.events INFO:  eta: 0:00:25  iteration: 882/1000  consumed samples: 14112  total_loss: 7.548  time: 0.2027(78.95)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:46] lb.utils.events INFO:  eta: 0:00:25  iteration: 883/1000  consumed samples: 14128  total_loss: 7.548  time: 0.2027(78.94)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:46] lb.utils.events INFO:  eta: 0:00:24  iteration: 884/1000  consumed samples: 14144  total_loss: 7.545  time: 0.2027(78.93)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:51:46] lb.utils.events INFO:  eta: 0:00:24  iteration: 885/1000  consumed samples: 14160  total_loss: 7.542  time: 0.2027(78.93)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:51:46] lb.utils.events INFO:  eta: 0:00:24  iteration: 886/1000  consumed samples: 14176  total_loss: 7.542  time: 0.2027(78.92)  data_time: 0.0068  lr: 1.00e-04  
[01/19 14:51:47] lb.utils.events INFO:  eta: 0:00:24  iteration: 887/1000  consumed samples: 14192  total_loss: 7.545  time: 0.2028(78.91)  data_time: 0.0068  lr: 1.00e-04  
[01/19 14:51:47] lb.utils.events INFO:  eta: 0:00:24  iteration: 888/1000  consumed samples: 14208  total_loss: 7.545  time: 0.2028(78.90)  data_time: 0.0068  lr: 1.00e-04  
[01/19 14:51:47] lb.utils.events INFO:  eta: 0:00:23  iteration: 889/1000  consumed samples: 14224  total_loss: 7.543  time: 0.2028(78.90)  data_time: 0.0068  lr: 1.00e-04  
[01/19 14:51:47] lb.utils.events INFO:  eta: 0:00:23  iteration: 890/1000  consumed samples: 14240  total_loss: 7.542  time: 0.2028(78.89)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:47] lb.utils.events INFO:  eta: 0:00:23  iteration: 891/1000  consumed samples: 14256  total_loss: 7.542  time: 0.2028(78.88)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:48] lb.utils.events INFO:  eta: 0:00:23  iteration: 892/1000  consumed samples: 14272  total_loss: 7.541  time: 0.2029(78.87)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:48] lb.utils.events INFO:  eta: 0:00:23  iteration: 893/1000  consumed samples: 14288  total_loss: 7.541  time: 0.2029(78.87)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:48] lb.utils.events INFO:  eta: 0:00:22  iteration: 894/1000  consumed samples: 14304  total_loss: 7.541  time: 0.2029(78.86)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:48] lb.utils.events INFO:  eta: 0:00:22  iteration: 895/1000  consumed samples: 14320  total_loss: 7.542  time: 0.2029(78.85)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:49] lb.utils.events INFO:  eta: 0:00:22  iteration: 896/1000  consumed samples: 14336  total_loss: 7.541  time: 0.2029(78.85)  data_time: 0.0066  lr: 1.00e-04  
[01/19 14:51:49] lb.utils.events INFO:  eta: 0:00:22  iteration: 897/1000  consumed samples: 14352  total_loss: 7.541  time: 0.2029(78.85)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:49] lb.utils.events INFO:  eta: 0:00:21  iteration: 898/1000  consumed samples: 14368  total_loss: 7.541  time: 0.2029(78.84)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:49] lb.utils.events INFO:  eta: 0:00:21  iteration: 899/1000  consumed samples: 14384  total_loss: 7.541  time: 0.2030(78.83)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:50] lb.utils.events INFO:  eta: 0:00:21  iteration: 900/1000  consumed samples: 14400  total_loss: 7.54  time: 0.2030(78.82)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:50] lb.utils.events INFO:  eta: 0:00:21  iteration: 901/1000  consumed samples: 14416  total_loss: 7.54  time: 0.2030(78.82)  data_time: 0.0085  lr: 1.00e-04  
[01/19 14:51:50] lb.utils.events INFO:  eta: 0:00:21  iteration: 902/1000  consumed samples: 14432  total_loss: 7.537  time: 0.2030(78.81)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:50] lb.utils.events INFO:  eta: 0:00:20  iteration: 903/1000  consumed samples: 14448  total_loss: 7.537  time: 0.2030(78.80)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:50] lb.utils.events INFO:  eta: 0:00:20  iteration: 904/1000  consumed samples: 14464  total_loss: 7.54  time: 0.2031(78.79)  data_time: 0.0084  lr: 1.00e-04  
[01/19 14:51:51] lb.utils.events INFO:  eta: 0:00:20  iteration: 905/1000  consumed samples: 14480  total_loss: 7.54  time: 0.2031(78.79)  data_time: 0.0085  lr: 1.00e-04  
[01/19 14:51:51] lb.utils.events INFO:  eta: 0:00:20  iteration: 906/1000  consumed samples: 14496  total_loss: 7.541  time: 0.2031(78.78)  data_time: 0.0057  lr: 1.00e-04  
[01/19 14:51:51] lb.utils.events INFO:  eta: 0:00:19  iteration: 907/1000  consumed samples: 14512  total_loss: 7.541  time: 0.2031(78.77)  data_time: 0.0057  lr: 1.00e-04  
[01/19 14:51:51] lb.utils.events INFO:  eta: 0:00:19  iteration: 908/1000  consumed samples: 14528  total_loss: 7.54  time: 0.2031(78.76)  data_time: 0.0057  lr: 1.00e-04  
[01/19 14:51:52] lb.utils.events INFO:  eta: 0:00:19  iteration: 909/1000  consumed samples: 14544  total_loss: 7.54  time: 0.2032(78.76)  data_time: 0.0057  lr: 1.00e-04  
[01/19 14:51:52] lb.utils.events INFO:  eta: 0:00:19  iteration: 910/1000  consumed samples: 14560  total_loss: 7.537  time: 0.2032(78.75)  data_time: 0.0058  lr: 1.00e-04  
[01/19 14:51:52] lb.utils.events INFO:  eta: 0:00:19  iteration: 911/1000  consumed samples: 14576  total_loss: 7.54  time: 0.2032(78.74)  data_time: 0.0059  lr: 1.00e-04  
[01/19 14:51:52] lb.utils.events INFO:  eta: 0:00:18  iteration: 912/1000  consumed samples: 14592  total_loss: 7.54  time: 0.2032(78.73)  data_time: 0.0059  lr: 1.00e-04  
[01/19 14:51:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 913/1000  consumed samples: 14608  total_loss: 7.54  time: 0.2032(78.73)  data_time: 0.0059  lr: 1.00e-04  
[01/19 14:51:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 914/1000  consumed samples: 14624  total_loss: 7.537  time: 0.2033(78.72)  data_time: 0.0059  lr: 1.00e-04  
[01/19 14:51:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 915/1000  consumed samples: 14640  total_loss: 7.533  time: 0.2033(78.71)  data_time: 0.0059  lr: 1.00e-04  
[01/19 14:51:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 916/1000  consumed samples: 14656  total_loss: 7.531  time: 0.2033(78.70)  data_time: 0.0059  lr: 1.00e-04  
[01/19 14:51:53] lb.utils.events INFO:  eta: 0:00:17  iteration: 917/1000  consumed samples: 14672  total_loss: 7.533  time: 0.2033(78.70)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 918/1000  consumed samples: 14688  total_loss: 7.533  time: 0.2033(78.69)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 919/1000  consumed samples: 14704  total_loss: 7.537  time: 0.2033(78.68)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:51:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 920/1000  consumed samples: 14720  total_loss: 7.533  time: 0.2034(78.67)  data_time: 0.0041  lr: 1.00e-04  
[01/19 14:51:54] lb.utils.events INFO:  eta: 0:00:16  iteration: 921/1000  consumed samples: 14736  total_loss: 7.531  time: 0.2034(78.67)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:55] lb.utils.events INFO:  eta: 0:00:16  iteration: 922/1000  consumed samples: 14752  total_loss: 7.531  time: 0.2034(78.66)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:55] lb.utils.events INFO:  eta: 0:00:16  iteration: 923/1000  consumed samples: 14768  total_loss: 7.529  time: 0.2034(78.65)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:55] lb.utils.events INFO:  eta: 0:00:16  iteration: 924/1000  consumed samples: 14784  total_loss: 7.529  time: 0.2034(78.64)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:55] lb.utils.events INFO:  eta: 0:00:16  iteration: 925/1000  consumed samples: 14800  total_loss: 7.528  time: 0.2035(78.64)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:51:56] lb.utils.events INFO:  eta: 0:00:15  iteration: 926/1000  consumed samples: 14816  total_loss: 7.527  time: 0.2035(78.63)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:56] lb.utils.events INFO:  eta: 0:00:15  iteration: 927/1000  consumed samples: 14832  total_loss: 7.525  time: 0.2035(78.62)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:56] lb.utils.events INFO:  eta: 0:00:15  iteration: 928/1000  consumed samples: 14848  total_loss: 7.527  time: 0.2035(78.62)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:56] lb.utils.events INFO:  eta: 0:00:15  iteration: 929/1000  consumed samples: 14864  total_loss: 7.527  time: 0.2035(78.61)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:56] lb.utils.events INFO:  eta: 0:00:14  iteration: 930/1000  consumed samples: 14880  total_loss: 7.525  time: 0.2036(78.60)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:57] lb.utils.events INFO:  eta: 0:00:14  iteration: 931/1000  consumed samples: 14896  total_loss: 7.525  time: 0.2036(78.60)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:57] lb.utils.events INFO:  eta: 0:00:14  iteration: 932/1000  consumed samples: 14912  total_loss: 7.521  time: 0.2036(78.59)  data_time: 0.0049  lr: 1.00e-04  
[01/19 14:51:57] lb.utils.events INFO:  eta: 0:00:14  iteration: 933/1000  consumed samples: 14928  total_loss: 7.521  time: 0.2036(78.58)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:51:57] lb.utils.events INFO:  eta: 0:00:14  iteration: 934/1000  consumed samples: 14944  total_loss: 7.518  time: 0.2036(78.58)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:51:58] lb.utils.events INFO:  eta: 0:00:13  iteration: 935/1000  consumed samples: 14960  total_loss: 7.518  time: 0.2036(78.57)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:51:58] lb.utils.events INFO:  eta: 0:00:13  iteration: 936/1000  consumed samples: 14976  total_loss: 7.516  time: 0.2037(78.56)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:51:58] lb.utils.events INFO:  eta: 0:00:13  iteration: 937/1000  consumed samples: 14992  total_loss: 7.514  time: 0.2037(78.55)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:51:58] lb.utils.events INFO:  eta: 0:00:13  iteration: 938/1000  consumed samples: 15008  total_loss: 7.514  time: 0.2037(78.55)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:51:58] lb.utils.events INFO:  eta: 0:00:13  iteration: 939/1000  consumed samples: 15024  total_loss: 7.514  time: 0.2037(78.54)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:51:59] lb.utils.events INFO:  eta: 0:00:12  iteration: 940/1000  consumed samples: 15040  total_loss: 7.514  time: 0.2037(78.54)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:51:59] lb.utils.events INFO:  eta: 0:00:12  iteration: 941/1000  consumed samples: 15056  total_loss: 7.514  time: 0.2038(78.53)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:51:59] lb.utils.events INFO:  eta: 0:00:12  iteration: 942/1000  consumed samples: 15072  total_loss: 7.512  time: 0.2038(78.52)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:51:59] lb.utils.events INFO:  eta: 0:00:12  iteration: 943/1000  consumed samples: 15088  total_loss: 7.512  time: 0.2038(78.51)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:00] lb.utils.events INFO:  eta: 0:00:11  iteration: 944/1000  consumed samples: 15104  total_loss: 7.508  time: 0.2038(78.51)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:00] lb.utils.events INFO:  eta: 0:00:11  iteration: 945/1000  consumed samples: 15120  total_loss: 7.505  time: 0.2038(78.50)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:00] lb.utils.events INFO:  eta: 0:00:11  iteration: 946/1000  consumed samples: 15136  total_loss: 7.504  time: 0.2038(78.49)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:00] lb.utils.events INFO:  eta: 0:00:11  iteration: 947/1000  consumed samples: 15152  total_loss: 7.503  time: 0.2039(78.49)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:01] lb.utils.events INFO:  eta: 0:00:11  iteration: 948/1000  consumed samples: 15168  total_loss: 7.503  time: 0.2039(78.48)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:01] lb.utils.events INFO:  eta: 0:00:10  iteration: 949/1000  consumed samples: 15184  total_loss: 7.503  time: 0.2039(78.47)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:01] lb.utils.events INFO:  eta: 0:00:10  iteration: 950/1000  consumed samples: 15200  total_loss: 7.503  time: 0.2039(78.47)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:52:01] lb.utils.events INFO:  eta: 0:00:10  iteration: 951/1000  consumed samples: 15216  total_loss: 7.502  time: 0.2039(78.46)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:52:01] lb.utils.events INFO:  eta: 0:00:10  iteration: 952/1000  consumed samples: 15232  total_loss: 7.502  time: 0.2039(78.45)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:52:02] lb.utils.events INFO:  eta: 0:00:09  iteration: 953/1000  consumed samples: 15248  total_loss: 7.503  time: 0.2040(78.45)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:52:02] lb.utils.events INFO:  eta: 0:00:09  iteration: 954/1000  consumed samples: 15264  total_loss: 7.503  time: 0.2040(78.44)  data_time: 0.0033  lr: 1.00e-04  
[01/19 14:52:02] lb.utils.events INFO:  eta: 0:00:09  iteration: 955/1000  consumed samples: 15280  total_loss: 7.504  time: 0.2040(78.43)  data_time: 0.0034  lr: 1.00e-04  
[01/19 14:52:02] lb.utils.events INFO:  eta: 0:00:09  iteration: 956/1000  consumed samples: 15296  total_loss: 7.504  time: 0.2040(78.42)  data_time: 0.0035  lr: 1.00e-04  
[01/19 14:52:03] lb.utils.events INFO:  eta: 0:00:09  iteration: 957/1000  consumed samples: 15312  total_loss: 7.504  time: 0.2040(78.42)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:03] lb.utils.events INFO:  eta: 0:00:08  iteration: 958/1000  consumed samples: 15328  total_loss: 7.503  time: 0.2041(78.41)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:03] lb.utils.events INFO:  eta: 0:00:08  iteration: 959/1000  consumed samples: 15344  total_loss: 7.502  time: 0.2041(78.41)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:03] lb.utils.events INFO:  eta: 0:00:08  iteration: 960/1000  consumed samples: 15360  total_loss: 7.503  time: 0.2041(78.40)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:04] lb.utils.events INFO:  eta: 0:00:08  iteration: 961/1000  consumed samples: 15376  total_loss: 7.502  time: 0.2041(78.39)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:04] lb.utils.events INFO:  eta: 0:00:08  iteration: 962/1000  consumed samples: 15392  total_loss: 7.502  time: 0.2041(78.38)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:04] lb.utils.events INFO:  eta: 0:00:07  iteration: 963/1000  consumed samples: 15408  total_loss: 7.5  time: 0.2041(78.38)  data_time: 0.0036  lr: 1.00e-04  
[01/19 14:52:04] lb.utils.events INFO:  eta: 0:00:07  iteration: 964/1000  consumed samples: 15424  total_loss: 7.503  time: 0.2042(78.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:04] lb.utils.events INFO:  eta: 0:00:07  iteration: 965/1000  consumed samples: 15440  total_loss: 7.504  time: 0.2042(78.37)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:05] lb.utils.events INFO:  eta: 0:00:07  iteration: 966/1000  consumed samples: 15456  total_loss: 7.503  time: 0.2042(78.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:05] lb.utils.events INFO:  eta: 0:00:06  iteration: 967/1000  consumed samples: 15472  total_loss: 7.5  time: 0.2042(78.36)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:05] lb.utils.events INFO:  eta: 0:00:06  iteration: 968/1000  consumed samples: 15488  total_loss: 7.496  time: 0.2042(78.35)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:05] lb.utils.events INFO:  eta: 0:00:06  iteration: 969/1000  consumed samples: 15504  total_loss: 7.494  time: 0.2042(78.35)  data_time: 0.0037  lr: 1.00e-04  
[01/19 14:52:06] lb.utils.events INFO:  eta: 0:00:06  iteration: 970/1000  consumed samples: 15520  total_loss: 7.493  time: 0.2042(78.34)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:52:06] lb.utils.events INFO:  eta: 0:00:06  iteration: 971/1000  consumed samples: 15536  total_loss: 7.494  time: 0.2043(78.33)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:52:06] lb.utils.events INFO:  eta: 0:00:05  iteration: 972/1000  consumed samples: 15552  total_loss: 7.494  time: 0.2043(78.33)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:52:06] lb.utils.events INFO:  eta: 0:00:05  iteration: 973/1000  consumed samples: 15568  total_loss: 7.494  time: 0.2043(78.33)  data_time: 0.0038  lr: 1.00e-04  
[01/19 14:52:07] lb.utils.events INFO:  eta: 0:00:05  iteration: 974/1000  consumed samples: 15584  total_loss: 7.494  time: 0.2043(78.32)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:52:07] lb.utils.events INFO:  eta: 0:00:05  iteration: 975/1000  consumed samples: 15600  total_loss: 7.493  time: 0.2043(78.31)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:07] lb.utils.events INFO:  eta: 0:00:04  iteration: 976/1000  consumed samples: 15616  total_loss: 7.494  time: 0.2043(78.31)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:07] lb.utils.events INFO:  eta: 0:00:04  iteration: 977/1000  consumed samples: 15632  total_loss: 7.493  time: 0.2043(78.30)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:07] lb.utils.events INFO:  eta: 0:00:04  iteration: 978/1000  consumed samples: 15648  total_loss: 7.493  time: 0.2044(78.30)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:52:08] lb.utils.events INFO:  eta: 0:00:04  iteration: 979/1000  consumed samples: 15664  total_loss: 7.493  time: 0.2044(78.29)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:08] lb.utils.events INFO:  eta: 0:00:04  iteration: 980/1000  consumed samples: 15680  total_loss: 7.491  time: 0.2044(78.28)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:08] lb.utils.events INFO:  eta: 0:00:03  iteration: 981/1000  consumed samples: 15696  total_loss: 7.491  time: 0.2044(78.28)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:08] lb.utils.events INFO:  eta: 0:00:03  iteration: 982/1000  consumed samples: 15712  total_loss: 7.493  time: 0.2044(78.27)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:09] lb.utils.events INFO:  eta: 0:00:03  iteration: 983/1000  consumed samples: 15728  total_loss: 7.493  time: 0.2044(78.26)  data_time: 0.0046  lr: 1.00e-04  
[01/19 14:52:09] lb.utils.events INFO:  eta: 0:00:03  iteration: 984/1000  consumed samples: 15744  total_loss: 7.491  time: 0.2044(78.26)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:09] lb.utils.events INFO:  eta: 0:00:03  iteration: 985/1000  consumed samples: 15760  total_loss: 7.491  time: 0.2045(78.25)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:09] lb.utils.events INFO:  eta: 0:00:02  iteration: 986/1000  consumed samples: 15776  total_loss: 7.493  time: 0.2045(78.25)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:10] lb.utils.events INFO:  eta: 0:00:02  iteration: 987/1000  consumed samples: 15792  total_loss: 7.493  time: 0.2045(78.24)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:10] lb.utils.events INFO:  eta: 0:00:02  iteration: 988/1000  consumed samples: 15808  total_loss: 7.491  time: 0.2045(78.24)  data_time: 0.0044  lr: 1.00e-04  
[01/19 14:52:10] lb.utils.events INFO:  eta: 0:00:02  iteration: 989/1000  consumed samples: 15824  total_loss: 7.489  time: 0.2045(78.23)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:10] lb.utils.events INFO:  eta: 0:00:01  iteration: 990/1000  consumed samples: 15840  total_loss: 7.489  time: 0.2045(78.22)  data_time: 0.0045  lr: 1.00e-04  
[01/19 14:52:10] lb.utils.events INFO:  eta: 0:00:01  iteration: 991/1000  consumed samples: 15856  total_loss: 7.491  time: 0.2046(78.22)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:52:11] lb.utils.events INFO:  eta: 0:00:01  iteration: 992/1000  consumed samples: 15872  total_loss: 7.491  time: 0.2046(78.21)  data_time: 0.0048  lr: 1.00e-04  
[01/19 14:52:11] lb.utils.events INFO:  eta: 0:00:01  iteration: 993/1000  consumed samples: 15888  total_loss: 7.491  time: 0.2046(78.21)  data_time: 0.0047  lr: 1.00e-04  
[01/19 14:52:11] lb.utils.events INFO:  eta: 0:00:01  iteration: 994/1000  consumed samples: 15904  total_loss: 7.489  time: 0.2046(78.20)  data_time: 0.0043  lr: 1.00e-04  
[01/19 14:52:11] lb.utils.events INFO:  eta: 0:00:00  iteration: 995/1000  consumed samples: 15920  total_loss: 7.489  time: 0.2046(78.19)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:52:12] lb.utils.events INFO:  eta: 0:00:00  iteration: 996/1000  consumed samples: 15936  total_loss: 7.489  time: 0.2046(78.19)  data_time: 0.0042  lr: 1.00e-04  
[01/19 14:52:12] lb.utils.events INFO:  eta: 0:00:00  iteration: 997/1000  consumed samples: 15952  total_loss: 7.487  time: 0.2047(78.18)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:52:12] lb.utils.events INFO:  eta: 0:00:00  iteration: 998/1000  consumed samples: 15968  total_loss: 7.486  time: 0.2047(78.17)  data_time: 0.0039  lr: 1.00e-04  
[01/19 14:52:12] lb.utils.events INFO:  eta: 0:00:00  iteration: 999/1000  consumed samples: 15984  total_loss: 7.483  time: 0.2047(78.17)  data_time: 0.0040  lr: 1.00e-04  
[01/19 14:52:12] lb.utils.checkpoint INFO: Saving checkpoint to ./demo_output/test_config/model_final
[01/19 14:52:13] lb.trainer.hooks INFO: Overall training speed: 998 iterations in 0:03:24 (0.2047 s / it)
[01/19 14:52:13] lb.trainer.hooks INFO: Total training time: 0:03:49 (0:00:24 on hooks)
[01/19 14:57:29] libai INFO: Rank of current process: 0. World size: 1
[01/19 14:57:29] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 14:57:30] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 14:57:30] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 14:57:30] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 14:57:30] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 14:57:33] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 14:57:33] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 14:57:33] libai INFO: Loadding megatron weight
[01/19 14:57:33] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 14:57:33] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 14:57:33] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 14:57:33] lb.data.build INFO:     train:      16000
[01/19 14:57:33] lb.data.build INFO:     validation: 160000
[01/19 14:57:33] lb.data.build INFO:     test:       160000
[01/19 14:57:33] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 14:57:33] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     reading document index...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 14:57:33] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 14:57:33] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.046985 seconds
[01/19 14:57:33] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 14:57:33] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 14:57:33] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 14:57:33] lb.data.dataset_utils INFO:  > dataset split:
[01/19 14:57:33] lb.data.dataset_utils INFO:     train:
[01/19 14:57:33] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 14:57:33] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 14:57:33] lb.data.dataset_utils INFO:     validation:
[01/19 14:57:33] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 14:57:33] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 14:57:33] lb.data.dataset_utils INFO:     test:
[01/19 14:57:33] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 14:57:33] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 14:57:33] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 14:57:33] lb.data.dataset_utils INFO:     loaded indexed file in 0.020 seconds
[01/19 14:57:33] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 14:57:33] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 14:57:33] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 14:57:33] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 14:57:33] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 14:57:33] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 14:57:33] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 14:57:33] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 14:57:34] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:04:51] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:04:51] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 15:04:51] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:04:51] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:04:51] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:04:51] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:04:54] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:04:54] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 15:04:54] libai INFO: Loadding megatron weight
[01/19 15:04:54] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:04:55] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:04:55] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:04:55] lb.data.build INFO:     train:      16000
[01/19 15:04:55] lb.data.build INFO:     validation: 160000
[01/19 15:04:55] lb.data.build INFO:     test:       160000
[01/19 15:04:55] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:04:55] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:04:55] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:04:55] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.048813 seconds
[01/19 15:04:55] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:04:55] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:04:55] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:04:55] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:04:55] lb.data.dataset_utils INFO:     train:
[01/19 15:04:55] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:04:55] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:04:55] lb.data.dataset_utils INFO:     validation:
[01/19 15:04:55] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:04:55] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:04:55] lb.data.dataset_utils INFO:     test:
[01/19 15:04:55] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:04:55] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:04:55] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:04:55] lb.data.dataset_utils INFO:     loaded indexed file in 0.019 seconds
[01/19 15:04:55] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:04:55] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:04:55] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:04:55] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:04:55] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:04:55] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:04:55] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:04:55] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:04:56] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:12:10] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:12:10] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 15:12:10] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:12:10] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:12:10] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:12:10] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:12:13] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:12:13] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 15:12:13] libai INFO: Loadding megatron weight
[01/19 15:12:13] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:12:14] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:12:14] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:12:14] lb.data.build INFO:     train:      16000
[01/19 15:12:14] lb.data.build INFO:     validation: 160000
[01/19 15:12:14] lb.data.build INFO:     test:       160000
[01/19 15:12:14] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:12:14] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:12:14] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:12:14] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.046746 seconds
[01/19 15:12:14] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:12:14] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:12:14] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:12:14] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:12:14] lb.data.dataset_utils INFO:     train:
[01/19 15:12:14] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:12:14] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:12:14] lb.data.dataset_utils INFO:     validation:
[01/19 15:12:14] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:12:14] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:12:14] lb.data.dataset_utils INFO:     test:
[01/19 15:12:14] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:12:14] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:12:14] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:12:14] lb.data.dataset_utils INFO:     loaded indexed file in 0.020 seconds
[01/19 15:12:14] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:12:14] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:12:14] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:12:14] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:12:14] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:12:14] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:12:14] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:12:14] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:12:14] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:13:13] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:13:13] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 15:13:13] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:13:13] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:13:13] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:13:14] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:13:17] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:13:17] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 15:13:17] libai INFO: Loadding megatron weight
[01/19 15:13:17] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:13:17] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:13:17] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:13:17] lb.data.build INFO:     train:      16000
[01/19 15:13:17] lb.data.build INFO:     validation: 160000
[01/19 15:13:17] lb.data.build INFO:     test:       160000
[01/19 15:13:17] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:13:17] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:13:17] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:13:17] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.049845 seconds
[01/19 15:13:17] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:13:17] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:13:17] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:13:17] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:13:17] lb.data.dataset_utils INFO:     train:
[01/19 15:13:17] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:13:17] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:13:17] lb.data.dataset_utils INFO:     validation:
[01/19 15:13:17] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:13:17] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:13:17] lb.data.dataset_utils INFO:     test:
[01/19 15:13:17] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:13:17] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:13:17] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:13:17] lb.data.dataset_utils INFO:     loaded indexed file in 0.018 seconds
[01/19 15:13:17] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:13:17] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:13:17] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:13:17] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:13:17] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:13:17] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:13:17] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:13:17] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:13:18] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:32:15] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:32:15] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 15:32:16] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:32:16] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:32:16] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:32:16] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:32:19] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:32:19] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 15:32:19] libai INFO: Loadding megatron weight
[01/19 15:32:19] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:32:19] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:32:19] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:32:19] lb.data.build INFO:     train:      16000
[01/19 15:32:19] lb.data.build INFO:     validation: 160000
[01/19 15:32:19] lb.data.build INFO:     test:       160000
[01/19 15:32:19] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:32:19] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:32:19] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:32:19] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.103581 seconds
[01/19 15:32:19] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:32:19] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:32:19] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:32:19] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:32:19] lb.data.dataset_utils INFO:     train:
[01/19 15:32:19] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:32:19] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:32:19] lb.data.dataset_utils INFO:     validation:
[01/19 15:32:19] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:32:19] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:32:19] lb.data.dataset_utils INFO:     test:
[01/19 15:32:19] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:32:19] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:32:20] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:32:20] lb.data.dataset_utils INFO:     loaded indexed file in 0.019 seconds
[01/19 15:32:20] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:32:20] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:32:20] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:32:20] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:32:20] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:32:20] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:32:20] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:32:20] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:32:20] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:33:08] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:33:08] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 15:33:08] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:33:08] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:33:08] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:33:08] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:33:11] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:33:11] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 15:33:11] libai INFO: Loadding megatron weight
[01/19 15:33:11] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:33:12] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:33:12] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:33:12] lb.data.build INFO:     train:      16000
[01/19 15:33:12] lb.data.build INFO:     validation: 160000
[01/19 15:33:12] lb.data.build INFO:     test:       160000
[01/19 15:33:12] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:33:12] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:33:12] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:33:12] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.051555 seconds
[01/19 15:33:12] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:33:12] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:33:12] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:33:12] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:33:12] lb.data.dataset_utils INFO:     train:
[01/19 15:33:12] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:33:12] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:33:12] lb.data.dataset_utils INFO:     validation:
[01/19 15:33:12] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:33:12] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:33:12] lb.data.dataset_utils INFO:     test:
[01/19 15:33:12] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:33:12] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:33:12] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:33:12] lb.data.dataset_utils INFO:     loaded indexed file in 0.024 seconds
[01/19 15:33:12] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:33:12] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:33:12] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:33:12] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:33:12] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:33:12] lb.data.dataset_utils INFO:     loaded indexed file in 0.002 seconds
[01/19 15:33:12] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:33:12] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:33:13] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:33:14] lb.trainer.trainer ERROR: Exception during training:
Traceback (most recent call last):
  File "./libai/trainer/trainer.py", line 140, in train
    self.run_step()
  File "./libai/trainer/default.py", line 364, in run_step
    self._trainer.run_step(self.get_batch)
  File "./libai/trainer/trainer.py", line 279, in run_step
    losses.backward()
  File "/home/dev/.local/lib/python3.6/site-packages/oneflow/framework/tensor.py", line 79, in _backward
    flow.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/dev/.local/lib/python3.6/site-packages/oneflow/autograd/autograd.py", line 52, in backward
    create_graph,
oneflow._oneflow_internal.exception.RuntimeException: 
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/api/python/autograd/autograd.cpp", line 74, in Backward
    one::GetThreadLocalAutogradEngine()->RunBackwardAndSaveGrads4LeafTensorIf( outputs, *gradients, retain_graph, create_graph)
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp", line 438, in RunBackwardAndSaveGrads4LeafTensor
    graph_task.Apply( true)
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp", line 414, in Apply
    node->Apply(create_graph_)
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/autograd_engine.cpp", line 176, in Apply
    (*backward_fn_)(output_grads, &input_grads, create_graph)
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/framework/op_interpreter/op_interpreter.cpp", line 108, in operator()
    grad_closure->Apply(out_grads, in_grads)
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/autograd/gradient_funcs/narrow.cpp", line 69, in Apply
    dy->device()
  File "/home/ci-user/runners/release/_work/oneflow/oneflow/oneflow/core/framework/tensor.h", line 510, in device
    RuntimeError : Only local tensors have 'device'. Please use '.placement' for consistent tensors.
[01/19 15:33:14] lb.trainer.hooks INFO: Total training time: 0:00:01 (0:00:00 on hooks)
[01/19 15:33:37] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:33:37] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/19 15:33:38] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:33:38] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:33:38] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:33:38] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:33:41] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:33:41] lb.scheduler.lr_scheduler WARNING: warmup iters equals to zero, return MultiStepLR
[01/19 15:33:41] libai INFO: Loadding megatron weight
[01/19 15:33:41] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:33:41] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:33:41] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:33:41] lb.data.build INFO:     train:      16000
[01/19 15:33:41] lb.data.build INFO:     validation: 160000
[01/19 15:33:41] lb.data.build INFO:     test:       160000
[01/19 15:33:41] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:33:41] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:33:41] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:33:41] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.104438 seconds
[01/19 15:33:41] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:33:41] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:33:41] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:33:41] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:33:41] lb.data.dataset_utils INFO:     train:
[01/19 15:33:41] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:33:41] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:33:41] lb.data.dataset_utils INFO:     validation:
[01/19 15:33:41] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:33:41] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:33:41] lb.data.dataset_utils INFO:     test:
[01/19 15:33:41] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:33:41] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:33:42] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:33:42] lb.data.dataset_utils INFO:     loaded indexed file in 0.019 seconds
[01/19 15:33:42] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:33:42] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:33:42] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:33:42] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:33:42] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:33:42] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 15:33:42] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:33:42] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:33:42] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:43:19] libai INFO: Rank of current process: 0. World size: 1
[01/19 15:43:19] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 15:43:19] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 15:43:19] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 15:43:19] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 15:43:19] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 15:43:22] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 15:43:22] libai INFO: Loadding megatron weight
[01/19 15:43:22] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 15:43:22] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 15:43:22] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 15:43:22] lb.data.build INFO:     train:      16000
[01/19 15:43:22] lb.data.build INFO:     validation: 160000
[01/19 15:43:22] lb.data.build INFO:     test:       160000
[01/19 15:43:22] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 15:43:22] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     reading document index...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 15:43:22] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 15:43:22] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.048533 seconds
[01/19 15:43:22] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 15:43:22] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 15:43:22] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 15:43:22] lb.data.dataset_utils INFO:  > dataset split:
[01/19 15:43:22] lb.data.dataset_utils INFO:     train:
[01/19 15:43:22] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 15:43:22] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 15:43:22] lb.data.dataset_utils INFO:     validation:
[01/19 15:43:22] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 15:43:22] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 15:43:22] lb.data.dataset_utils INFO:     test:
[01/19 15:43:22] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 15:43:22] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 15:43:22] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 15:43:22] lb.data.dataset_utils INFO:     loaded indexed file in 0.006 seconds
[01/19 15:43:22] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 15:43:22] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:43:22] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 15:43:22] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 15:43:22] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 15:43:22] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/19 15:43:22] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 15:43:22] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 15:43:23] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 15:43:28] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.0622  lr: 0.00e+00  
[01/19 15:43:29] lb.utils.events INFO:  eta: 0:14:27  iteration: 2/1000  consumed samples: 32  total_loss: 8.65  data_time: 0.0350  lr: 1.00e-05  
[01/19 15:43:29] lb.utils.events INFO:  eta: 0:01:16  iteration: 3/1000  consumed samples: 48  total_loss: 8.663  time: 0.0767(208.49)  data_time: 0.0250  lr: 2.00e-05  
[01/19 15:43:29] lb.utils.events INFO:  eta: 0:01:19  iteration: 4/1000  consumed samples: 64  total_loss: 8.65  time: 0.0803(199.22)  data_time: 0.0199  lr: 3.00e-05  
[01/19 15:43:29] lb.utils.events INFO:  eta: 0:01:23  iteration: 5/1000  consumed samples: 80  total_loss: 8.663  time: 0.0863(185.46)  data_time: 0.0168  lr: 4.00e-05  
[01/19 15:43:30] lb.utils.events INFO:  eta: 0:01:25  iteration: 6/1000  consumed samples: 96  total_loss: 8.675  time: 0.0868(184.33)  data_time: 0.0148  lr: 5.00e-05  
[01/19 15:43:30] lb.utils.events INFO:  eta: 0:01:27  iteration: 7/1000  consumed samples: 112  total_loss: 8.663  time: 0.0933(171.57)  data_time: 0.0132  lr: 6.00e-05  
[01/19 15:43:30] lb.utils.events INFO:  eta: 0:01:32  iteration: 8/1000  consumed samples: 128  total_loss: 8.65  time: 0.0982(162.85)  data_time: 0.0121  lr: 7.00e-05  
[01/19 15:43:30] lb.utils.events INFO:  eta: 0:01:27  iteration: 9/1000  consumed samples: 144  total_loss: 8.637  time: 0.0950(168.43)  data_time: 0.0112  lr: 8.00e-05  
[01/19 15:43:30] lb.utils.events INFO:  eta: 0:01:29  iteration: 10/1000  consumed samples: 160  total_loss: 8.635  time: 0.0946(169.12)  data_time: 0.0105  lr: 9.00e-05  
[01/19 15:43:31] lb.utils.events INFO:  eta: 0:01:30  iteration: 11/1000  consumed samples: 176  total_loss: 8.633  time: 0.0962(166.27)  data_time: 0.0100  lr: 9.00e-05  
[01/19 15:43:31] lb.utils.events INFO:  eta: 0:01:33  iteration: 12/1000  consumed samples: 192  total_loss: 8.632  time: 0.0976(164.01)  data_time: 0.0095  lr: 9.00e-05  
[01/19 15:43:31] lb.utils.events INFO:  eta: 0:01:36  iteration: 13/1000  consumed samples: 208  total_loss: 8.631  time: 0.0988(161.98)  data_time: 0.0091  lr: 9.00e-05  
[01/19 15:43:31] lb.utils.events INFO:  eta: 0:01:42  iteration: 14/1000  consumed samples: 224  total_loss: 8.632  time: 0.1005(159.17)  data_time: 0.0088  lr: 9.00e-05  
[01/19 15:43:31] lb.utils.events INFO:  eta: 0:01:36  iteration: 15/1000  consumed samples: 240  total_loss: 8.631  time: 0.0987(162.18)  data_time: 0.0086  lr: 9.00e-05  
[01/19 15:43:32] lb.utils.events INFO:  eta: 0:01:41  iteration: 16/1000  consumed samples: 256  total_loss: 8.63  time: 0.1011(158.24)  data_time: 0.0083  lr: 9.00e-05  
[01/19 15:43:32] lb.utils.events INFO:  eta: 0:01:47  iteration: 17/1000  consumed samples: 272  total_loss: 8.629  time: 0.1032(154.99)  data_time: 0.0081  lr: 9.00e-05  
[01/19 15:43:32] lb.utils.events INFO:  eta: 0:01:41  iteration: 18/1000  consumed samples: 288  total_loss: 8.621  time: 0.1015(157.64)  data_time: 0.0078  lr: 9.00e-05  
[01/19 15:43:32] lb.utils.events INFO:  eta: 0:01:36  iteration: 19/1000  consumed samples: 304  total_loss: 8.613  time: 0.0999(160.14)  data_time: 0.0076  lr: 9.00e-05  
[01/19 15:43:32] lb.utils.events INFO:  eta: 0:01:33  iteration: 20/1000  consumed samples: 320  total_loss: 8.613  time: 0.0988(161.96)  data_time: 0.0074  lr: 9.00e-05  
[01/19 15:43:33] lb.utils.events INFO:  eta: 0:01:36  iteration: 21/1000  consumed samples: 336  total_loss: 8.613  time: 0.0993(161.20)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:33] lb.utils.events INFO:  eta: 0:01:40  iteration: 22/1000  consumed samples: 352  total_loss: 8.595  time: 0.0998(160.30)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:33] lb.utils.events INFO:  eta: 0:01:45  iteration: 23/1000  consumed samples: 368  total_loss: 8.578  time: 0.1026(155.89)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:33] lb.utils.events INFO:  eta: 0:01:40  iteration: 24/1000  consumed samples: 384  total_loss: 8.536  time: 0.1016(157.42)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:33] lb.utils.events INFO:  eta: 0:01:35  iteration: 25/1000  consumed samples: 400  total_loss: 8.494  time: 0.1005(159.14)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:34] lb.utils.events INFO:  eta: 0:01:32  iteration: 26/1000  consumed samples: 416  total_loss: 8.491  time: 0.1001(159.78)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:34] lb.utils.events INFO:  eta: 0:01:29  iteration: 27/1000  consumed samples: 432  total_loss: 8.489  time: 0.0992(161.29)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:34] lb.utils.events INFO:  eta: 0:01:28  iteration: 28/1000  consumed samples: 448  total_loss: 8.488  time: 0.0983(162.70)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:34] lb.utils.events INFO:  eta: 0:01:28  iteration: 29/1000  consumed samples: 464  total_loss: 8.487  time: 0.0975(164.13)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:35] lb.utils.events INFO:  eta: 0:01:26  iteration: 30/1000  consumed samples: 480  total_loss: 8.484  time: 0.0967(165.45)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:35] lb.utils.events INFO:  eta: 0:01:27  iteration: 31/1000  consumed samples: 496  total_loss: 8.48  time: 0.0965(165.85)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:35] lb.utils.events INFO:  eta: 0:01:26  iteration: 32/1000  consumed samples: 512  total_loss: 8.471  time: 0.0958(167.06)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:35] lb.utils.events INFO:  eta: 0:01:26  iteration: 33/1000  consumed samples: 528  total_loss: 8.463  time: 0.0962(166.31)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:35] lb.utils.events INFO:  eta: 0:01:27  iteration: 34/1000  consumed samples: 544  total_loss: 8.448  time: 0.0979(163.48)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:36] lb.utils.events INFO:  eta: 0:01:26  iteration: 35/1000  consumed samples: 560  total_loss: 8.432  time: 0.0974(164.32)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:36] lb.utils.events INFO:  eta: 0:01:27  iteration: 36/1000  consumed samples: 576  total_loss: 8.423  time: 0.0972(164.55)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:36] lb.utils.events INFO:  eta: 0:01:27  iteration: 37/1000  consumed samples: 592  total_loss: 8.413  time: 0.0982(162.98)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:36] lb.utils.events INFO:  eta: 0:01:26  iteration: 38/1000  consumed samples: 608  total_loss: 8.412  time: 0.0975(164.03)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:36] lb.utils.events INFO:  eta: 0:01:27  iteration: 39/1000  consumed samples: 624  total_loss: 8.41  time: 0.0980(163.20)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:37] lb.utils.events INFO:  eta: 0:01:27  iteration: 40/1000  consumed samples: 640  total_loss: 8.392  time: 0.0989(161.76)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:37] lb.utils.events INFO:  eta: 0:01:27  iteration: 41/1000  consumed samples: 656  total_loss: 8.374  time: 0.0984(162.67)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:37] lb.utils.events INFO:  eta: 0:01:27  iteration: 42/1000  consumed samples: 672  total_loss: 8.355  time: 0.0989(161.86)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:37] lb.utils.events INFO:  eta: 0:01:27  iteration: 43/1000  consumed samples: 688  total_loss: 8.335  time: 0.1002(159.76)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:37] lb.utils.events INFO:  eta: 0:01:27  iteration: 44/1000  consumed samples: 704  total_loss: 8.332  time: 0.0996(160.62)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:38] lb.utils.events INFO:  eta: 0:01:27  iteration: 45/1000  consumed samples: 720  total_loss: 8.328  time: 0.1000(159.94)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:38] lb.utils.events INFO:  eta: 0:01:28  iteration: 46/1000  consumed samples: 736  total_loss: 8.328  time: 0.1008(158.70)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:38] lb.utils.events INFO:  eta: 0:01:28  iteration: 47/1000  consumed samples: 752  total_loss: 8.327  time: 0.1023(156.39)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:38] lb.utils.events INFO:  eta: 0:01:27  iteration: 48/1000  consumed samples: 768  total_loss: 8.303  time: 0.1018(157.22)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:38] lb.utils.events INFO:  eta: 0:01:28  iteration: 49/1000  consumed samples: 784  total_loss: 8.278  time: 0.1023(156.47)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:39] lb.utils.events INFO:  eta: 0:01:30  iteration: 50/1000  consumed samples: 800  total_loss: 8.278  time: 0.1034(154.75)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:39] lb.utils.events INFO:  eta: 0:01:27  iteration: 51/1000  consumed samples: 816  total_loss: 8.277  time: 0.1029(155.56)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:39] lb.utils.events INFO:  eta: 0:01:30  iteration: 52/1000  consumed samples: 832  total_loss: 8.276  time: 0.1030(155.32)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:39] lb.utils.events INFO:  eta: 0:01:32  iteration: 53/1000  consumed samples: 848  total_loss: 8.274  time: 0.1041(153.64)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:39] lb.utils.events INFO:  eta: 0:01:37  iteration: 54/1000  consumed samples: 864  total_loss: 8.259  time: 0.1049(152.59)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:40] lb.utils.events INFO:  eta: 0:01:37  iteration: 55/1000  consumed samples: 880  total_loss: 8.244  time: 0.1048(152.63)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:40] lb.utils.events INFO:  eta: 0:01:39  iteration: 56/1000  consumed samples: 896  total_loss: 8.244  time: 0.1052(152.05)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:40] lb.utils.events INFO:  eta: 0:01:37  iteration: 57/1000  consumed samples: 912  total_loss: 8.243  time: 0.1048(152.61)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:40] lb.utils.events INFO:  eta: 0:01:39  iteration: 58/1000  consumed samples: 928  total_loss: 8.242  time: 0.1053(151.93)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:40] lb.utils.events INFO:  eta: 0:01:41  iteration: 59/1000  consumed samples: 944  total_loss: 8.24  time: 0.1059(151.02)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 60/1000  consumed samples: 960  total_loss: 8.236  time: 0.1061(150.74)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:41] lb.utils.events INFO:  eta: 0:01:42  iteration: 61/1000  consumed samples: 976  total_loss: 8.231  time: 0.1072(149.29)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 62/1000  consumed samples: 992  total_loss: 8.229  time: 0.1072(149.26)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 63/1000  consumed samples: 1008  total_loss: 8.227  time: 0.1070(149.50)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:41] lb.utils.events INFO:  eta: 0:01:41  iteration: 64/1000  consumed samples: 1024  total_loss: 8.227  time: 0.1070(149.57)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 65/1000  consumed samples: 1040  total_loss: 8.227  time: 0.1074(148.99)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:42] lb.utils.events INFO:  eta: 0:01:40  iteration: 66/1000  consumed samples: 1056  total_loss: 8.227  time: 0.1069(149.70)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 67/1000  consumed samples: 1072  total_loss: 8.227  time: 0.1073(149.07)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 68/1000  consumed samples: 1088  total_loss: 8.222  time: 0.1080(148.14)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 69/1000  consumed samples: 1104  total_loss: 8.218  time: 0.1085(147.42)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:43] lb.utils.events INFO:  eta: 0:01:41  iteration: 70/1000  consumed samples: 1120  total_loss: 8.214  time: 0.1089(146.90)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:43] lb.utils.events INFO:  eta: 0:01:41  iteration: 71/1000  consumed samples: 1136  total_loss: 8.209  time: 0.1088(147.02)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 72/1000  consumed samples: 1152  total_loss: 8.198  time: 0.1084(147.64)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:43] lb.utils.events INFO:  eta: 0:01:41  iteration: 73/1000  consumed samples: 1168  total_loss: 8.186  time: 0.1086(147.27)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 74/1000  consumed samples: 1184  total_loss: 8.185  time: 0.1089(146.92)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:44] lb.utils.events INFO:  eta: 0:01:40  iteration: 75/1000  consumed samples: 1200  total_loss: 8.183  time: 0.1093(146.43)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:44] lb.utils.events INFO:  eta: 0:01:40  iteration: 76/1000  consumed samples: 1216  total_loss: 8.178  time: 0.1088(147.00)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:44] lb.utils.events INFO:  eta: 0:01:40  iteration: 77/1000  consumed samples: 1232  total_loss: 8.172  time: 0.1091(146.63)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:44] lb.utils.events INFO:  eta: 0:01:40  iteration: 78/1000  consumed samples: 1248  total_loss: 8.172  time: 0.1088(147.02)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:44] lb.utils.events INFO:  eta: 0:01:40  iteration: 79/1000  consumed samples: 1264  total_loss: 8.172  time: 0.1088(147.09)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:45] lb.utils.events INFO:  eta: 0:01:39  iteration: 80/1000  consumed samples: 1280  total_loss: 8.169  time: 0.1086(147.32)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:45] lb.utils.events INFO:  eta: 0:01:39  iteration: 81/1000  consumed samples: 1296  total_loss: 8.167  time: 0.1082(147.88)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:45] lb.utils.events INFO:  eta: 0:01:39  iteration: 82/1000  consumed samples: 1312  total_loss: 8.161  time: 0.1080(148.14)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 83/1000  consumed samples: 1328  total_loss: 8.155  time: 0.1076(148.66)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 84/1000  consumed samples: 1344  total_loss: 8.15  time: 0.1073(149.18)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:46] lb.utils.events INFO:  eta: 0:01:35  iteration: 85/1000  consumed samples: 1360  total_loss: 8.145  time: 0.1071(149.45)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:46] lb.utils.events INFO:  eta: 0:01:35  iteration: 86/1000  consumed samples: 1376  total_loss: 8.141  time: 0.1070(149.55)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:46] lb.utils.events INFO:  eta: 0:01:34  iteration: 87/1000  consumed samples: 1392  total_loss: 8.136  time: 0.1066(150.06)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:46] lb.utils.events INFO:  eta: 0:01:35  iteration: 88/1000  consumed samples: 1408  total_loss: 8.129  time: 0.1069(149.64)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:47] lb.utils.events INFO:  eta: 0:01:35  iteration: 89/1000  consumed samples: 1424  total_loss: 8.121  time: 0.1073(149.12)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:47] lb.utils.events INFO:  eta: 0:01:36  iteration: 90/1000  consumed samples: 1440  total_loss: 8.115  time: 0.1077(148.50)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:47] lb.utils.events INFO:  eta: 0:01:37  iteration: 91/1000  consumed samples: 1456  total_loss: 8.108  time: 0.1080(148.13)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:47] lb.utils.events INFO:  eta: 0:01:38  iteration: 92/1000  consumed samples: 1472  total_loss: 8.103  time: 0.1084(147.57)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:47] lb.utils.events INFO:  eta: 0:01:38  iteration: 93/1000  consumed samples: 1488  total_loss: 8.097  time: 0.1088(147.01)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:48] lb.utils.events INFO:  eta: 0:01:37  iteration: 94/1000  consumed samples: 1504  total_loss: 8.103  time: 0.1087(147.24)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:43:48] lb.utils.events INFO:  eta: 0:01:37  iteration: 95/1000  consumed samples: 1520  total_loss: 8.097  time: 0.1083(147.70)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 96/1000  consumed samples: 1536  total_loss: 8.092  time: 0.1081(148.02)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:48] lb.utils.events INFO:  eta: 0:01:34  iteration: 97/1000  consumed samples: 1552  total_loss: 8.087  time: 0.1079(148.34)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:48] lb.utils.events INFO:  eta: 0:01:33  iteration: 98/1000  consumed samples: 1568  total_loss: 8.081  time: 0.1078(148.45)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:49] lb.utils.events INFO:  eta: 0:01:33  iteration: 99/1000  consumed samples: 1584  total_loss: 8.075  time: 0.1077(148.51)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:49] lb.utils.events INFO:  eta: 0:01:33  iteration: 100/1000  consumed samples: 1600  total_loss: 8.072  time: 0.1079(148.31)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 101/1000  consumed samples: 1616  total_loss: 8.069  time: 0.1083(147.75)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:49] lb.utils.events INFO:  eta: 0:01:33  iteration: 102/1000  consumed samples: 1632  total_loss: 8.066  time: 0.1080(148.19)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:49] lb.utils.events INFO:  eta: 0:01:33  iteration: 103/1000  consumed samples: 1648  total_loss: 8.063  time: 0.1078(148.36)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:50] lb.utils.events INFO:  eta: 0:01:32  iteration: 104/1000  consumed samples: 1664  total_loss: 8.061  time: 0.1076(148.69)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:50] lb.utils.events INFO:  eta: 0:01:32  iteration: 105/1000  consumed samples: 1680  total_loss: 8.058  time: 0.1073(149.12)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:50] lb.utils.events INFO:  eta: 0:01:32  iteration: 106/1000  consumed samples: 1696  total_loss: 8.058  time: 0.1072(149.24)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:50] lb.utils.events INFO:  eta: 0:01:32  iteration: 107/1000  consumed samples: 1712  total_loss: 8.058  time: 0.1078(148.36)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:50] lb.utils.events INFO:  eta: 0:01:32  iteration: 108/1000  consumed samples: 1728  total_loss: 8.058  time: 0.1079(148.34)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 109/1000  consumed samples: 1744  total_loss: 8.058  time: 0.1083(147.69)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 110/1000  consumed samples: 1760  total_loss: 8.058  time: 0.1089(146.91)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 111/1000  consumed samples: 1776  total_loss: 8.058  time: 0.1086(147.30)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:51] lb.utils.events INFO:  eta: 0:01:31  iteration: 112/1000  consumed samples: 1792  total_loss: 8.058  time: 0.1083(147.70)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 113/1000  consumed samples: 1808  total_loss: 8.058  time: 0.1080(148.09)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 114/1000  consumed samples: 1824  total_loss: 8.058  time: 0.1084(147.67)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 115/1000  consumed samples: 1840  total_loss: 8.058  time: 0.1086(147.27)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 116/1000  consumed samples: 1856  total_loss: 8.058  time: 0.1084(147.64)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 117/1000  consumed samples: 1872  total_loss: 8.058  time: 0.1083(147.80)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 118/1000  consumed samples: 1888  total_loss: 8.058  time: 0.1080(148.13)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 119/1000  consumed samples: 1904  total_loss: 8.058  time: 0.1084(147.63)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:53] lb.utils.events INFO:  eta: 0:01:31  iteration: 120/1000  consumed samples: 1920  total_loss: 8.058  time: 0.1090(146.74)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:53] lb.utils.events INFO:  eta: 0:01:31  iteration: 121/1000  consumed samples: 1936  total_loss: 8.058  time: 0.1092(146.49)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:53] lb.utils.events INFO:  eta: 0:01:31  iteration: 122/1000  consumed samples: 1952  total_loss: 8.058  time: 0.1096(146.03)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:54] lb.utils.events INFO:  eta: 0:01:30  iteration: 123/1000  consumed samples: 1968  total_loss: 8.058  time: 0.1093(146.38)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:54] lb.utils.events INFO:  eta: 0:01:31  iteration: 124/1000  consumed samples: 1984  total_loss: 8.057  time: 0.1094(146.30)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:54] lb.utils.events INFO:  eta: 0:01:31  iteration: 125/1000  consumed samples: 2000  total_loss: 8.058  time: 0.1096(146.04)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:54] lb.utils.events INFO:  eta: 0:01:31  iteration: 126/1000  consumed samples: 2016  total_loss: 8.057  time: 0.1093(146.39)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:55] lb.utils.events INFO:  eta: 0:01:31  iteration: 127/1000  consumed samples: 2032  total_loss: 8.056  time: 0.1095(146.14)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:55] lb.utils.events INFO:  eta: 0:01:30  iteration: 128/1000  consumed samples: 2048  total_loss: 8.055  time: 0.1092(146.48)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:55] lb.utils.events INFO:  eta: 0:01:30  iteration: 129/1000  consumed samples: 2064  total_loss: 8.054  time: 0.1091(146.59)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:55] lb.utils.events INFO:  eta: 0:01:29  iteration: 130/1000  consumed samples: 2080  total_loss: 8.054  time: 0.1090(146.81)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:55] lb.utils.events INFO:  eta: 0:01:30  iteration: 131/1000  consumed samples: 2096  total_loss: 8.053  time: 0.1090(146.78)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:56] lb.utils.events INFO:  eta: 0:01:29  iteration: 132/1000  consumed samples: 2112  total_loss: 8.052  time: 0.1088(147.10)  data_time: 0.0045  lr: 9.00e-05  
[01/19 15:43:56] lb.utils.events INFO:  eta: 0:01:29  iteration: 133/1000  consumed samples: 2128  total_loss: 8.052  time: 0.1089(146.91)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:56] lb.utils.events INFO:  eta: 0:01:29  iteration: 134/1000  consumed samples: 2144  total_loss: 8.049  time: 0.1087(147.24)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:56] lb.utils.events INFO:  eta: 0:01:29  iteration: 135/1000  consumed samples: 2160  total_loss: 8.046  time: 0.1088(147.12)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:56] lb.utils.events INFO:  eta: 0:01:29  iteration: 136/1000  consumed samples: 2176  total_loss: 8.049  time: 0.1090(146.82)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:57] lb.utils.events INFO:  eta: 0:01:29  iteration: 137/1000  consumed samples: 2192  total_loss: 8.052  time: 0.1087(147.14)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:43:57] lb.utils.events INFO:  eta: 0:01:29  iteration: 138/1000  consumed samples: 2208  total_loss: 8.049  time: 0.1093(146.41)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:43:57] lb.utils.events INFO:  eta: 0:01:30  iteration: 139/1000  consumed samples: 2224  total_loss: 8.046  time: 0.1098(145.66)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:57] lb.utils.events INFO:  eta: 0:01:31  iteration: 140/1000  consumed samples: 2240  total_loss: 8.043  time: 0.1104(144.94)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:43:57] lb.utils.events INFO:  eta: 0:01:32  iteration: 141/1000  consumed samples: 2256  total_loss: 8.041  time: 0.1110(144.17)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:43:58] lb.utils.events INFO:  eta: 0:01:32  iteration: 142/1000  consumed samples: 2272  total_loss: 8.039  time: 0.1116(143.42)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:43:58] lb.utils.events INFO:  eta: 0:01:32  iteration: 143/1000  consumed samples: 2288  total_loss: 8.037  time: 0.1122(142.66)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:43:58] lb.utils.events INFO:  eta: 0:01:33  iteration: 144/1000  consumed samples: 2304  total_loss: 8.034  time: 0.1127(141.93)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:43:58] lb.utils.events INFO:  eta: 0:01:33  iteration: 145/1000  consumed samples: 2320  total_loss: 8.03  time: 0.1133(141.18)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:43:58] lb.utils.events INFO:  eta: 0:01:33  iteration: 146/1000  consumed samples: 2336  total_loss: 8.034  time: 0.1139(140.52)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:43:59] lb.utils.events INFO:  eta: 0:01:33  iteration: 147/1000  consumed samples: 2352  total_loss: 8.03  time: 0.1144(139.81)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:43:59] lb.utils.events INFO:  eta: 0:01:32  iteration: 148/1000  consumed samples: 2368  total_loss: 8.034  time: 0.1150(139.18)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:43:59] lb.utils.events INFO:  eta: 0:01:32  iteration: 149/1000  consumed samples: 2384  total_loss: 8.03  time: 0.1155(138.50)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:43:59] lb.utils.events INFO:  eta: 0:01:32  iteration: 150/1000  consumed samples: 2400  total_loss: 8.029  time: 0.1160(137.89)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:43:59] lb.utils.events INFO:  eta: 0:01:32  iteration: 151/1000  consumed samples: 2416  total_loss: 8.028  time: 0.1166(137.25)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:00] lb.utils.events INFO:  eta: 0:01:33  iteration: 152/1000  consumed samples: 2432  total_loss: 8.026  time: 0.1171(136.68)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:00] lb.utils.events INFO:  eta: 0:01:33  iteration: 153/1000  consumed samples: 2448  total_loss: 8.023  time: 0.1176(136.06)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:00] lb.utils.events INFO:  eta: 0:01:33  iteration: 154/1000  consumed samples: 2464  total_loss: 8.021  time: 0.1181(135.50)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:00] lb.utils.events INFO:  eta: 0:01:33  iteration: 155/1000  consumed samples: 2480  total_loss: 8.02  time: 0.1186(134.91)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:00] lb.utils.events INFO:  eta: 0:01:33  iteration: 156/1000  consumed samples: 2496  total_loss: 8.017  time: 0.1191(134.38)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:01] lb.utils.events INFO:  eta: 0:01:33  iteration: 157/1000  consumed samples: 2512  total_loss: 8.014  time: 0.1196(133.80)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:01] lb.utils.events INFO:  eta: 0:01:33  iteration: 158/1000  consumed samples: 2528  total_loss: 8.017  time: 0.1201(133.24)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:01] lb.utils.events INFO:  eta: 0:01:33  iteration: 159/1000  consumed samples: 2544  total_loss: 8.014  time: 0.1205(132.75)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:01] lb.utils.events INFO:  eta: 0:01:35  iteration: 160/1000  consumed samples: 2560  total_loss: 8.014  time: 0.1210(132.22)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:01] lb.utils.events INFO:  eta: 0:01:37  iteration: 161/1000  consumed samples: 2576  total_loss: 8.014  time: 0.1214(131.74)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:02] lb.utils.events INFO:  eta: 0:01:37  iteration: 162/1000  consumed samples: 2592  total_loss: 8.013  time: 0.1219(131.23)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:02] lb.utils.events INFO:  eta: 0:01:37  iteration: 163/1000  consumed samples: 2608  total_loss: 8.014  time: 0.1224(130.72)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:02] lb.utils.events INFO:  eta: 0:01:37  iteration: 164/1000  consumed samples: 2624  total_loss: 8.013  time: 0.1228(130.27)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:02] lb.utils.events INFO:  eta: 0:01:37  iteration: 165/1000  consumed samples: 2640  total_loss: 8.013  time: 0.1233(129.79)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:02] lb.utils.events INFO:  eta: 0:01:37  iteration: 166/1000  consumed samples: 2656  total_loss: 8.012  time: 0.1237(129.31)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:03] lb.utils.events INFO:  eta: 0:01:38  iteration: 167/1000  consumed samples: 2672  total_loss: 8.011  time: 0.1242(128.83)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:03] lb.utils.events INFO:  eta: 0:01:38  iteration: 168/1000  consumed samples: 2688  total_loss: 8.011  time: 0.1246(128.41)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:03] lb.utils.events INFO:  eta: 0:01:38  iteration: 169/1000  consumed samples: 2704  total_loss: 8.01  time: 0.1250(127.96)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:03] lb.utils.events INFO:  eta: 0:01:38  iteration: 170/1000  consumed samples: 2720  total_loss: 8.011  time: 0.1255(127.52)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:03] lb.utils.events INFO:  eta: 0:01:38  iteration: 171/1000  consumed samples: 2736  total_loss: 8.01  time: 0.1259(127.13)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:04] lb.utils.events INFO:  eta: 0:01:38  iteration: 172/1000  consumed samples: 2752  total_loss: 8.008  time: 0.1263(126.71)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:04] lb.utils.events INFO:  eta: 0:01:38  iteration: 173/1000  consumed samples: 2768  total_loss: 8.005  time: 0.1266(126.34)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:04] lb.utils.events INFO:  eta: 0:01:38  iteration: 174/1000  consumed samples: 2784  total_loss: 8.008  time: 0.1271(125.92)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:04] lb.utils.events INFO:  eta: 0:01:38  iteration: 175/1000  consumed samples: 2800  total_loss: 8.005  time: 0.1274(125.57)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:05] lb.utils.events INFO:  eta: 0:01:39  iteration: 176/1000  consumed samples: 2816  total_loss: 8.008  time: 0.1278(125.17)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:05] lb.utils.events INFO:  eta: 0:01:40  iteration: 177/1000  consumed samples: 2832  total_loss: 8.005  time: 0.1282(124.81)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:05] lb.utils.events INFO:  eta: 0:01:40  iteration: 178/1000  consumed samples: 2848  total_loss: 8.008  time: 0.1286(124.43)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:05] lb.utils.events INFO:  eta: 0:01:41  iteration: 179/1000  consumed samples: 2864  total_loss: 8.005  time: 0.1289(124.09)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:05] lb.utils.events INFO:  eta: 0:01:41  iteration: 180/1000  consumed samples: 2880  total_loss: 8.005  time: 0.1293(123.71)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:06] lb.utils.events INFO:  eta: 0:01:42  iteration: 181/1000  consumed samples: 2896  total_loss: 8.004  time: 0.1297(123.38)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:06] lb.utils.events INFO:  eta: 0:01:42  iteration: 182/1000  consumed samples: 2912  total_loss: 8.004  time: 0.1301(123.02)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 183/1000  consumed samples: 2928  total_loss: 8.003  time: 0.1304(122.70)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 184/1000  consumed samples: 2944  total_loss: 8  time: 0.1308(122.36)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 185/1000  consumed samples: 2960  total_loss: 7.997  time: 0.1311(122.05)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 186/1000  consumed samples: 2976  total_loss: 8  time: 0.1315(121.71)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 187/1000  consumed samples: 2992  total_loss: 8.003  time: 0.1318(121.41)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 188/1000  consumed samples: 3008  total_loss: 8.004  time: 0.1321(121.08)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 189/1000  consumed samples: 3024  total_loss: 8.003  time: 0.1325(120.80)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 190/1000  consumed samples: 3040  total_loss: 8  time: 0.1328(120.48)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:08] lb.utils.events INFO:  eta: 0:01:44  iteration: 191/1000  consumed samples: 3056  total_loss: 7.997  time: 0.1331(120.20)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:08] lb.utils.events INFO:  eta: 0:01:44  iteration: 192/1000  consumed samples: 3072  total_loss: 7.997  time: 0.1335(119.89)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:08] lb.utils.events INFO:  eta: 0:01:44  iteration: 193/1000  consumed samples: 3088  total_loss: 7.997  time: 0.1338(119.58)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:08] lb.utils.events INFO:  eta: 0:01:45  iteration: 194/1000  consumed samples: 3104  total_loss: 7.997  time: 0.1341(119.28)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:08] lb.utils.events INFO:  eta: 0:01:45  iteration: 195/1000  consumed samples: 3120  total_loss: 7.997  time: 0.1344(119.01)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:09] lb.utils.events INFO:  eta: 0:01:45  iteration: 196/1000  consumed samples: 3136  total_loss: 7.996  time: 0.1348(118.72)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:09] lb.utils.events INFO:  eta: 0:01:45  iteration: 197/1000  consumed samples: 3152  total_loss: 7.996  time: 0.1351(118.47)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:09] lb.utils.events INFO:  eta: 0:01:45  iteration: 198/1000  consumed samples: 3168  total_loss: 7.996  time: 0.1354(118.18)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:09] lb.utils.events INFO:  eta: 0:01:45  iteration: 199/1000  consumed samples: 3184  total_loss: 7.996  time: 0.1357(117.90)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:09] lb.utils.events INFO:  eta: 0:01:45  iteration: 200/1000  consumed samples: 3200  total_loss: 7.996  time: 0.1360(117.63)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:10] lb.utils.events INFO:  eta: 0:01:45  iteration: 201/1000  consumed samples: 3216  total_loss: 7.996  time: 0.1363(117.35)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:10] lb.utils.events INFO:  eta: 0:01:45  iteration: 202/1000  consumed samples: 3232  total_loss: 7.994  time: 0.1366(117.12)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:10] lb.utils.events INFO:  eta: 0:01:45  iteration: 203/1000  consumed samples: 3248  total_loss: 7.994  time: 0.1369(116.85)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:10] lb.utils.events INFO:  eta: 0:01:45  iteration: 204/1000  consumed samples: 3264  total_loss: 7.991  time: 0.1372(116.62)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:10] lb.utils.events INFO:  eta: 0:01:45  iteration: 205/1000  consumed samples: 3280  total_loss: 7.99  time: 0.1375(116.39)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:11] lb.utils.events INFO:  eta: 0:01:45  iteration: 206/1000  consumed samples: 3296  total_loss: 7.99  time: 0.1378(116.13)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:11] lb.utils.events INFO:  eta: 0:01:45  iteration: 207/1000  consumed samples: 3312  total_loss: 7.988  time: 0.1380(115.91)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:11] lb.utils.events INFO:  eta: 0:01:45  iteration: 208/1000  consumed samples: 3328  total_loss: 7.988  time: 0.1383(115.70)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:11] lb.utils.events INFO:  eta: 0:01:45  iteration: 209/1000  consumed samples: 3344  total_loss: 7.987  time: 0.1386(115.45)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:11] lb.utils.events INFO:  eta: 0:01:44  iteration: 210/1000  consumed samples: 3360  total_loss: 7.986  time: 0.1388(115.28)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:12] lb.utils.events INFO:  eta: 0:01:44  iteration: 211/1000  consumed samples: 3376  total_loss: 7.983  time: 0.1391(115.05)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:12] lb.utils.events INFO:  eta: 0:01:44  iteration: 212/1000  consumed samples: 3392  total_loss: 7.98  time: 0.1393(114.84)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:12] lb.utils.events INFO:  eta: 0:01:44  iteration: 213/1000  consumed samples: 3408  total_loss: 7.979  time: 0.1396(114.61)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:12] lb.utils.events INFO:  eta: 0:01:44  iteration: 214/1000  consumed samples: 3424  total_loss: 7.979  time: 0.1399(114.40)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:12] lb.utils.events INFO:  eta: 0:01:45  iteration: 215/1000  consumed samples: 3440  total_loss: 7.976  time: 0.1401(114.17)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:13] lb.utils.events INFO:  eta: 0:01:45  iteration: 216/1000  consumed samples: 3456  total_loss: 7.974  time: 0.1404(113.97)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:13] lb.utils.events INFO:  eta: 0:01:45  iteration: 217/1000  consumed samples: 3472  total_loss: 7.974  time: 0.1406(113.78)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:13] lb.utils.events INFO:  eta: 0:01:45  iteration: 218/1000  consumed samples: 3488  total_loss: 7.973  time: 0.1409(113.59)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:13] lb.utils.events INFO:  eta: 0:01:45  iteration: 219/1000  consumed samples: 3504  total_loss: 7.97  time: 0.1411(113.37)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:13] lb.utils.events INFO:  eta: 0:01:45  iteration: 220/1000  consumed samples: 3520  total_loss: 7.969  time: 0.1414(113.18)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:14] lb.utils.events INFO:  eta: 0:01:45  iteration: 221/1000  consumed samples: 3536  total_loss: 7.969  time: 0.1416(112.97)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:14] lb.utils.events INFO:  eta: 0:01:46  iteration: 222/1000  consumed samples: 3552  total_loss: 7.969  time: 0.1419(112.79)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:14] lb.utils.events INFO:  eta: 0:01:47  iteration: 223/1000  consumed samples: 3568  total_loss: 7.968  time: 0.1421(112.58)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:14] lb.utils.events INFO:  eta: 0:01:47  iteration: 224/1000  consumed samples: 3584  total_loss: 7.966  time: 0.1424(112.40)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:15] lb.utils.events INFO:  eta: 0:01:47  iteration: 225/1000  consumed samples: 3600  total_loss: 7.966  time: 0.1426(112.19)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:15] lb.utils.events INFO:  eta: 0:01:48  iteration: 226/1000  consumed samples: 3616  total_loss: 7.965  time: 0.1428(112.02)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:15] lb.utils.events INFO:  eta: 0:01:49  iteration: 227/1000  consumed samples: 3632  total_loss: 7.962  time: 0.1431(111.82)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:15] lb.utils.events INFO:  eta: 0:01:48  iteration: 228/1000  consumed samples: 3648  total_loss: 7.961  time: 0.1429(111.95)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:15] lb.utils.events INFO:  eta: 0:01:48  iteration: 229/1000  consumed samples: 3664  total_loss: 7.961  time: 0.1429(111.95)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:16] lb.utils.events INFO:  eta: 0:01:48  iteration: 230/1000  consumed samples: 3680  total_loss: 7.96  time: 0.1431(111.80)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:16] lb.utils.events INFO:  eta: 0:01:48  iteration: 231/1000  consumed samples: 3696  total_loss: 7.959  time: 0.1433(111.67)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:16] lb.utils.events INFO:  eta: 0:01:48  iteration: 232/1000  consumed samples: 3712  total_loss: 7.958  time: 0.1435(111.48)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:16] lb.utils.events INFO:  eta: 0:01:48  iteration: 233/1000  consumed samples: 3728  total_loss: 7.958  time: 0.1437(111.31)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:16] lb.utils.events INFO:  eta: 0:01:48  iteration: 234/1000  consumed samples: 3744  total_loss: 7.958  time: 0.1440(111.12)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 235/1000  consumed samples: 3760  total_loss: 7.956  time: 0.1442(110.94)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 236/1000  consumed samples: 3776  total_loss: 7.955  time: 0.1443(110.85)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 237/1000  consumed samples: 3792  total_loss: 7.955  time: 0.1446(110.67)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 238/1000  consumed samples: 3808  total_loss: 7.955  time: 0.1448(110.52)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:17] lb.utils.events INFO:  eta: 0:01:48  iteration: 239/1000  consumed samples: 3824  total_loss: 7.955  time: 0.1450(110.34)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:18] lb.utils.events INFO:  eta: 0:01:49  iteration: 240/1000  consumed samples: 3840  total_loss: 7.955  time: 0.1452(110.19)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:18] lb.utils.events INFO:  eta: 0:01:49  iteration: 241/1000  consumed samples: 3856  total_loss: 7.954  time: 0.1454(110.01)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:18] lb.utils.events INFO:  eta: 0:01:50  iteration: 242/1000  consumed samples: 3872  total_loss: 7.952  time: 0.1456(109.86)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:18] lb.utils.events INFO:  eta: 0:01:50  iteration: 243/1000  consumed samples: 3888  total_loss: 7.952  time: 0.1459(109.69)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:18] lb.utils.events INFO:  eta: 0:01:50  iteration: 244/1000  consumed samples: 3904  total_loss: 7.95  time: 0.1460(109.57)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:19] lb.utils.events INFO:  eta: 0:01:50  iteration: 245/1000  consumed samples: 3920  total_loss: 7.952  time: 0.1462(109.40)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:19] lb.utils.events INFO:  eta: 0:01:51  iteration: 246/1000  consumed samples: 3936  total_loss: 7.952  time: 0.1464(109.25)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:19] lb.utils.events INFO:  eta: 0:01:51  iteration: 247/1000  consumed samples: 3952  total_loss: 7.954  time: 0.1467(109.08)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:19] lb.utils.events INFO:  eta: 0:01:51  iteration: 248/1000  consumed samples: 3968  total_loss: 7.952  time: 0.1469(108.94)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:19] lb.utils.events INFO:  eta: 0:01:52  iteration: 249/1000  consumed samples: 3984  total_loss: 7.95  time: 0.1471(108.77)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:20] lb.utils.events INFO:  eta: 0:01:52  iteration: 250/1000  consumed samples: 4000  total_loss: 7.949  time: 0.1473(108.64)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:20] lb.utils.events INFO:  eta: 0:01:52  iteration: 251/1000  consumed samples: 4016  total_loss: 7.948  time: 0.1475(108.47)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:20] lb.utils.events INFO:  eta: 0:01:52  iteration: 252/1000  consumed samples: 4032  total_loss: 7.948  time: 0.1477(108.33)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:20] lb.utils.events INFO:  eta: 0:01:52  iteration: 253/1000  consumed samples: 4048  total_loss: 7.945  time: 0.1479(108.17)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:20] lb.utils.events INFO:  eta: 0:01:52  iteration: 254/1000  consumed samples: 4064  total_loss: 7.943  time: 0.1481(108.04)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:21] lb.utils.events INFO:  eta: 0:01:53  iteration: 255/1000  consumed samples: 4080  total_loss: 7.942  time: 0.1482(107.93)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:21] lb.utils.events INFO:  eta: 0:01:52  iteration: 256/1000  consumed samples: 4096  total_loss: 7.942  time: 0.1484(107.80)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:21] lb.utils.events INFO:  eta: 0:01:52  iteration: 257/1000  consumed samples: 4112  total_loss: 7.942  time: 0.1486(107.64)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:21] lb.utils.events INFO:  eta: 0:01:52  iteration: 258/1000  consumed samples: 4128  total_loss: 7.942  time: 0.1484(107.84)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:22] lb.utils.events INFO:  eta: 0:01:52  iteration: 259/1000  consumed samples: 4144  total_loss: 7.942  time: 0.1486(107.69)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:22] lb.utils.events INFO:  eta: 0:01:54  iteration: 260/1000  consumed samples: 4160  total_loss: 7.942  time: 0.1488(107.54)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:22] lb.utils.events INFO:  eta: 0:01:55  iteration: 261/1000  consumed samples: 4176  total_loss: 7.942  time: 0.1490(107.40)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:22] lb.utils.events INFO:  eta: 0:01:56  iteration: 262/1000  consumed samples: 4192  total_loss: 7.941  time: 0.1492(107.25)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:22] lb.utils.events INFO:  eta: 0:01:56  iteration: 263/1000  consumed samples: 4208  total_loss: 7.941  time: 0.1494(107.12)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:23] lb.utils.events INFO:  eta: 0:01:56  iteration: 264/1000  consumed samples: 4224  total_loss: 7.938  time: 0.1496(106.98)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:23] lb.utils.events INFO:  eta: 0:01:56  iteration: 265/1000  consumed samples: 4240  total_loss: 7.933  time: 0.1497(106.86)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:23] lb.utils.events INFO:  eta: 0:01:57  iteration: 266/1000  consumed samples: 4256  total_loss: 7.933  time: 0.1499(106.72)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:23] lb.utils.events INFO:  eta: 0:01:57  iteration: 267/1000  consumed samples: 4272  total_loss: 7.933  time: 0.1501(106.59)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:23] lb.utils.events INFO:  eta: 0:01:59  iteration: 268/1000  consumed samples: 4288  total_loss: 7.933  time: 0.1502(106.49)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:24] lb.utils.events INFO:  eta: 0:02:01  iteration: 269/1000  consumed samples: 4304  total_loss: 7.93  time: 0.1504(106.36)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:24] lb.utils.events INFO:  eta: 0:02:02  iteration: 270/1000  consumed samples: 4320  total_loss: 7.929  time: 0.1506(106.24)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:24] lb.utils.events INFO:  eta: 0:02:02  iteration: 271/1000  consumed samples: 4336  total_loss: 7.929  time: 0.1508(106.13)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:24] lb.utils.events INFO:  eta: 0:02:02  iteration: 272/1000  consumed samples: 4352  total_loss: 7.927  time: 0.1510(105.99)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:25] lb.utils.events INFO:  eta: 0:02:03  iteration: 273/1000  consumed samples: 4368  total_loss: 7.924  time: 0.1511(105.86)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:25] lb.utils.events INFO:  eta: 0:02:03  iteration: 274/1000  consumed samples: 4384  total_loss: 7.923  time: 0.1513(105.75)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:25] lb.utils.events INFO:  eta: 0:02:03  iteration: 275/1000  consumed samples: 4400  total_loss: 7.923  time: 0.1515(105.62)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:25] lb.utils.events INFO:  eta: 0:02:04  iteration: 276/1000  consumed samples: 4416  total_loss: 7.923  time: 0.1517(105.50)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:25] lb.utils.events INFO:  eta: 0:02:05  iteration: 277/1000  consumed samples: 4432  total_loss: 7.923  time: 0.1518(105.39)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:26] lb.utils.events INFO:  eta: 0:02:07  iteration: 278/1000  consumed samples: 4448  total_loss: 7.922  time: 0.1520(105.28)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:26] lb.utils.events INFO:  eta: 0:02:10  iteration: 279/1000  consumed samples: 4464  total_loss: 7.921  time: 0.1521(105.16)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:26] lb.utils.events INFO:  eta: 0:02:10  iteration: 280/1000  consumed samples: 4480  total_loss: 7.921  time: 0.1523(105.05)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:26] lb.utils.events INFO:  eta: 0:02:10  iteration: 281/1000  consumed samples: 4496  total_loss: 7.921  time: 0.1525(104.95)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:26] lb.utils.events INFO:  eta: 0:02:10  iteration: 282/1000  consumed samples: 4512  total_loss: 7.921  time: 0.1526(104.83)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:27] lb.utils.events INFO:  eta: 0:02:10  iteration: 283/1000  consumed samples: 4528  total_loss: 7.921  time: 0.1528(104.71)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:27] lb.utils.events INFO:  eta: 0:02:11  iteration: 284/1000  consumed samples: 4544  total_loss: 7.921  time: 0.1529(104.61)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:27] lb.utils.events INFO:  eta: 0:02:11  iteration: 285/1000  consumed samples: 4560  total_loss: 7.921  time: 0.1531(104.50)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:27] lb.utils.events INFO:  eta: 0:02:11  iteration: 286/1000  consumed samples: 4576  total_loss: 7.92  time: 0.1532(104.40)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:27] lb.utils.events INFO:  eta: 0:02:11  iteration: 287/1000  consumed samples: 4592  total_loss: 7.92  time: 0.1534(104.31)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:28] lb.utils.events INFO:  eta: 0:02:11  iteration: 288/1000  consumed samples: 4608  total_loss: 7.919  time: 0.1535(104.21)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:28] lb.utils.events INFO:  eta: 0:02:11  iteration: 289/1000  consumed samples: 4624  total_loss: 7.918  time: 0.1537(104.10)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:28] lb.utils.events INFO:  eta: 0:02:11  iteration: 290/1000  consumed samples: 4640  total_loss: 7.919  time: 0.1538(104.01)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:28] lb.utils.events INFO:  eta: 0:02:11  iteration: 291/1000  consumed samples: 4656  total_loss: 7.918  time: 0.1540(103.90)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:28] lb.utils.events INFO:  eta: 0:02:11  iteration: 292/1000  consumed samples: 4672  total_loss: 7.917  time: 0.1541(103.81)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:29] lb.utils.events INFO:  eta: 0:02:11  iteration: 293/1000  consumed samples: 4688  total_loss: 7.915  time: 0.1543(103.70)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:29] lb.utils.events INFO:  eta: 0:02:11  iteration: 294/1000  consumed samples: 4704  total_loss: 7.914  time: 0.1544(103.61)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:29] lb.utils.events INFO:  eta: 0:02:12  iteration: 295/1000  consumed samples: 4720  total_loss: 7.914  time: 0.1546(103.50)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:29] lb.utils.events INFO:  eta: 0:02:12  iteration: 296/1000  consumed samples: 4736  total_loss: 7.912  time: 0.1547(103.41)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:29] lb.utils.events INFO:  eta: 0:02:12  iteration: 297/1000  consumed samples: 4752  total_loss: 7.911  time: 0.1549(103.31)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:30] lb.utils.events INFO:  eta: 0:02:11  iteration: 298/1000  consumed samples: 4768  total_loss: 7.911  time: 0.1550(103.22)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:30] lb.utils.events INFO:  eta: 0:02:11  iteration: 299/1000  consumed samples: 4784  total_loss: 7.91  time: 0.1552(103.11)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:30] lb.utils.events INFO:  eta: 0:02:11  iteration: 300/1000  consumed samples: 4800  total_loss: 7.91  time: 0.1553(103.02)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:30] lb.utils.events INFO:  eta: 0:02:11  iteration: 301/1000  consumed samples: 4816  total_loss: 7.91  time: 0.1555(102.92)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:31] lb.utils.events INFO:  eta: 0:02:11  iteration: 302/1000  consumed samples: 4832  total_loss: 7.909  time: 0.1556(102.83)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:31] lb.utils.events INFO:  eta: 0:02:12  iteration: 303/1000  consumed samples: 4848  total_loss: 7.908  time: 0.1558(102.73)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:31] lb.utils.events INFO:  eta: 0:02:11  iteration: 304/1000  consumed samples: 4864  total_loss: 7.907  time: 0.1559(102.64)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:31] lb.utils.events INFO:  eta: 0:02:11  iteration: 305/1000  consumed samples: 4880  total_loss: 7.908  time: 0.1560(102.54)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:31] lb.utils.events INFO:  eta: 0:02:11  iteration: 306/1000  consumed samples: 4896  total_loss: 7.908  time: 0.1562(102.46)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:32] lb.utils.events INFO:  eta: 0:02:11  iteration: 307/1000  consumed samples: 4912  total_loss: 7.908  time: 0.1563(102.36)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:32] lb.utils.events INFO:  eta: 0:02:11  iteration: 308/1000  consumed samples: 4928  total_loss: 7.908  time: 0.1564(102.28)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:32] lb.utils.events INFO:  eta: 0:02:11  iteration: 309/1000  consumed samples: 4944  total_loss: 7.908  time: 0.1566(102.18)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:32] lb.utils.events INFO:  eta: 0:02:11  iteration: 310/1000  consumed samples: 4960  total_loss: 7.908  time: 0.1567(102.10)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:32] lb.utils.events INFO:  eta: 0:02:10  iteration: 311/1000  consumed samples: 4976  total_loss: 7.907  time: 0.1569(102.00)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:33] lb.utils.events INFO:  eta: 0:02:10  iteration: 312/1000  consumed samples: 4992  total_loss: 7.905  time: 0.1570(101.92)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:33] lb.utils.events INFO:  eta: 0:02:10  iteration: 313/1000  consumed samples: 5008  total_loss: 7.907  time: 0.1571(101.83)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:33] lb.utils.events INFO:  eta: 0:02:10  iteration: 314/1000  consumed samples: 5024  total_loss: 7.905  time: 0.1572(101.75)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:33] lb.utils.events INFO:  eta: 0:02:10  iteration: 315/1000  consumed samples: 5040  total_loss: 7.901  time: 0.1574(101.66)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:33] lb.utils.events INFO:  eta: 0:02:10  iteration: 316/1000  consumed samples: 5056  total_loss: 7.899  time: 0.1575(101.58)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:34] lb.utils.events INFO:  eta: 0:02:09  iteration: 317/1000  consumed samples: 5072  total_loss: 7.898  time: 0.1577(101.49)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:34] lb.utils.events INFO:  eta: 0:02:09  iteration: 318/1000  consumed samples: 5088  total_loss: 7.898  time: 0.1578(101.41)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:34] lb.utils.events INFO:  eta: 0:02:09  iteration: 319/1000  consumed samples: 5104  total_loss: 7.898  time: 0.1579(101.32)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:34] lb.utils.events INFO:  eta: 0:02:09  iteration: 320/1000  consumed samples: 5120  total_loss: 7.897  time: 0.1580(101.25)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:34] lb.utils.events INFO:  eta: 0:02:09  iteration: 321/1000  consumed samples: 5136  total_loss: 7.896  time: 0.1582(101.16)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:35] lb.utils.events INFO:  eta: 0:02:09  iteration: 322/1000  consumed samples: 5152  total_loss: 7.896  time: 0.1583(101.08)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:35] lb.utils.events INFO:  eta: 0:02:09  iteration: 323/1000  consumed samples: 5168  total_loss: 7.895  time: 0.1584(100.99)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:35] lb.utils.events INFO:  eta: 0:02:08  iteration: 324/1000  consumed samples: 5184  total_loss: 7.895  time: 0.1585(100.92)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:35] lb.utils.events INFO:  eta: 0:02:08  iteration: 325/1000  consumed samples: 5200  total_loss: 7.893  time: 0.1586(100.85)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:35] lb.utils.events INFO:  eta: 0:02:08  iteration: 326/1000  consumed samples: 5216  total_loss: 7.89  time: 0.1588(100.78)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:36] lb.utils.events INFO:  eta: 0:02:08  iteration: 327/1000  consumed samples: 5232  total_loss: 7.888  time: 0.1589(100.70)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:36] lb.utils.events INFO:  eta: 0:02:08  iteration: 328/1000  consumed samples: 5248  total_loss: 7.888  time: 0.1590(100.63)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:36] lb.utils.events INFO:  eta: 0:02:08  iteration: 329/1000  consumed samples: 5264  total_loss: 7.888  time: 0.1591(100.55)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:36] lb.utils.events INFO:  eta: 0:02:07  iteration: 330/1000  consumed samples: 5280  total_loss: 7.884  time: 0.1592(100.49)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:37] lb.utils.events INFO:  eta: 0:02:07  iteration: 331/1000  consumed samples: 5296  total_loss: 7.881  time: 0.1594(100.41)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:37] lb.utils.events INFO:  eta: 0:02:07  iteration: 332/1000  consumed samples: 5312  total_loss: 7.878  time: 0.1595(100.34)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:37] lb.utils.events INFO:  eta: 0:02:07  iteration: 333/1000  consumed samples: 5328  total_loss: 7.878  time: 0.1596(100.27)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:37] lb.utils.events INFO:  eta: 0:02:07  iteration: 334/1000  consumed samples: 5344  total_loss: 7.878  time: 0.1597(100.20)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:37] lb.utils.events INFO:  eta: 0:02:06  iteration: 335/1000  consumed samples: 5360  total_loss: 7.875  time: 0.1598(100.13)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:38] lb.utils.events INFO:  eta: 0:02:06  iteration: 336/1000  consumed samples: 5376  total_loss: 7.873  time: 0.1599(100.06)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:38] lb.utils.events INFO:  eta: 0:02:06  iteration: 337/1000  consumed samples: 5392  total_loss: 7.873  time: 0.1600(99.99)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:38] lb.utils.events INFO:  eta: 0:02:06  iteration: 338/1000  consumed samples: 5408  total_loss: 7.873  time: 0.1601(99.93)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:38] lb.utils.events INFO:  eta: 0:02:06  iteration: 339/1000  consumed samples: 5424  total_loss: 7.873  time: 0.1602(99.85)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:38] lb.utils.events INFO:  eta: 0:02:06  iteration: 340/1000  consumed samples: 5440  total_loss: 7.873  time: 0.1603(99.79)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:39] lb.utils.events INFO:  eta: 0:02:06  iteration: 341/1000  consumed samples: 5456  total_loss: 7.873  time: 0.1605(99.71)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:39] lb.utils.events INFO:  eta: 0:02:05  iteration: 342/1000  consumed samples: 5472  total_loss: 7.873  time: 0.1606(99.65)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:39] lb.utils.events INFO:  eta: 0:02:05  iteration: 343/1000  consumed samples: 5488  total_loss: 7.873  time: 0.1607(99.57)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:39] lb.utils.events INFO:  eta: 0:02:05  iteration: 344/1000  consumed samples: 5504  total_loss: 7.872  time: 0.1608(99.51)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:39] lb.utils.events INFO:  eta: 0:02:05  iteration: 345/1000  consumed samples: 5520  total_loss: 7.872  time: 0.1609(99.44)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:40] lb.utils.events INFO:  eta: 0:02:05  iteration: 346/1000  consumed samples: 5536  total_loss: 7.871  time: 0.1610(99.38)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:40] lb.utils.events INFO:  eta: 0:02:05  iteration: 347/1000  consumed samples: 5552  total_loss: 7.869  time: 0.1611(99.30)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:40] lb.utils.events INFO:  eta: 0:02:04  iteration: 348/1000  consumed samples: 5568  total_loss: 7.868  time: 0.1612(99.24)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:40] lb.utils.events INFO:  eta: 0:02:04  iteration: 349/1000  consumed samples: 5584  total_loss: 7.868  time: 0.1613(99.17)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:40] lb.utils.events INFO:  eta: 0:02:04  iteration: 350/1000  consumed samples: 5600  total_loss: 7.868  time: 0.1614(99.11)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:41] lb.utils.events INFO:  eta: 0:02:04  iteration: 351/1000  consumed samples: 5616  total_loss: 7.866  time: 0.1615(99.04)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:41] lb.utils.events INFO:  eta: 0:02:04  iteration: 352/1000  consumed samples: 5632  total_loss: 7.866  time: 0.1616(98.98)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:41] lb.utils.events INFO:  eta: 0:02:04  iteration: 353/1000  consumed samples: 5648  total_loss: 7.868  time: 0.1618(98.91)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:41] lb.utils.events INFO:  eta: 0:02:03  iteration: 354/1000  consumed samples: 5664  total_loss: 7.868  time: 0.1615(99.06)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:41] lb.utils.events INFO:  eta: 0:02:03  iteration: 355/1000  consumed samples: 5680  total_loss: 7.866  time: 0.1616(98.99)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:42] lb.utils.events INFO:  eta: 0:02:03  iteration: 356/1000  consumed samples: 5696  total_loss: 7.864  time: 0.1617(98.92)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:42] lb.utils.events INFO:  eta: 0:02:03  iteration: 357/1000  consumed samples: 5712  total_loss: 7.866  time: 0.1618(98.87)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:42] lb.utils.events INFO:  eta: 0:02:03  iteration: 358/1000  consumed samples: 5728  total_loss: 7.864  time: 0.1619(98.81)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:42] lb.utils.events INFO:  eta: 0:02:02  iteration: 359/1000  consumed samples: 5744  total_loss: 7.864  time: 0.1620(98.74)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:43] lb.utils.events INFO:  eta: 0:02:02  iteration: 360/1000  consumed samples: 5760  total_loss: 7.863  time: 0.1621(98.69)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:43] lb.utils.events INFO:  eta: 0:02:02  iteration: 361/1000  consumed samples: 5776  total_loss: 7.862  time: 0.1622(98.62)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:43] lb.utils.events INFO:  eta: 0:02:02  iteration: 362/1000  consumed samples: 5792  total_loss: 7.862  time: 0.1623(98.57)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:43] lb.utils.events INFO:  eta: 0:02:02  iteration: 363/1000  consumed samples: 5808  total_loss: 7.861  time: 0.1624(98.50)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:43] lb.utils.events INFO:  eta: 0:02:02  iteration: 364/1000  consumed samples: 5824  total_loss: 7.861  time: 0.1625(98.43)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:44] lb.utils.events INFO:  eta: 0:02:01  iteration: 365/1000  consumed samples: 5840  total_loss: 7.861  time: 0.1627(98.37)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:44] lb.utils.events INFO:  eta: 0:02:01  iteration: 366/1000  consumed samples: 5856  total_loss: 7.858  time: 0.1627(98.32)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:44] lb.utils.events INFO:  eta: 0:02:01  iteration: 367/1000  consumed samples: 5872  total_loss: 7.852  time: 0.1628(98.25)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:44] lb.utils.events INFO:  eta: 0:02:01  iteration: 368/1000  consumed samples: 5888  total_loss: 7.848  time: 0.1629(98.20)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:44] lb.utils.events INFO:  eta: 0:02:01  iteration: 369/1000  consumed samples: 5904  total_loss: 7.847  time: 0.1630(98.14)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:45] lb.utils.events INFO:  eta: 0:02:01  iteration: 370/1000  consumed samples: 5920  total_loss: 7.847  time: 0.1631(98.09)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:45] lb.utils.events INFO:  eta: 0:02:00  iteration: 371/1000  consumed samples: 5936  total_loss: 7.846  time: 0.1632(98.02)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:45] lb.utils.events INFO:  eta: 0:02:00  iteration: 372/1000  consumed samples: 5952  total_loss: 7.845  time: 0.1633(97.98)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:45] lb.utils.events INFO:  eta: 0:02:00  iteration: 373/1000  consumed samples: 5968  total_loss: 7.843  time: 0.1634(97.95)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:45] lb.utils.events INFO:  eta: 0:02:00  iteration: 374/1000  consumed samples: 5984  total_loss: 7.842  time: 0.1634(97.89)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:46] lb.utils.events INFO:  eta: 0:02:00  iteration: 375/1000  consumed samples: 6000  total_loss: 7.843  time: 0.1635(97.85)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:46] lb.utils.events INFO:  eta: 0:01:59  iteration: 376/1000  consumed samples: 6016  total_loss: 7.842  time: 0.1636(97.81)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:46] lb.utils.events INFO:  eta: 0:01:59  iteration: 377/1000  consumed samples: 6032  total_loss: 7.843  time: 0.1637(97.75)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:46] lb.utils.events INFO:  eta: 0:01:59  iteration: 378/1000  consumed samples: 6048  total_loss: 7.842  time: 0.1635(97.87)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:47] lb.utils.events INFO:  eta: 0:01:59  iteration: 379/1000  consumed samples: 6064  total_loss: 7.843  time: 0.1632(98.01)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:47] lb.utils.events INFO:  eta: 0:01:59  iteration: 380/1000  consumed samples: 6080  total_loss: 7.843  time: 0.1630(98.15)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:48] lb.utils.events INFO:  eta: 0:01:58  iteration: 381/1000  consumed samples: 6096  total_loss: 7.843  time: 0.1628(98.28)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:48] lb.utils.events INFO:  eta: 0:01:58  iteration: 382/1000  consumed samples: 6112  total_loss: 7.845  time: 0.1626(98.42)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:49] lb.utils.events INFO:  eta: 0:01:58  iteration: 383/1000  consumed samples: 6128  total_loss: 7.843  time: 0.1623(98.55)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:49] lb.utils.events INFO:  eta: 0:01:58  iteration: 384/1000  consumed samples: 6144  total_loss: 7.843  time: 0.1621(98.69)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:50] lb.utils.events INFO:  eta: 0:01:58  iteration: 385/1000  consumed samples: 6160  total_loss: 7.841  time: 0.1619(98.82)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:50] lb.utils.events INFO:  eta: 0:01:57  iteration: 386/1000  consumed samples: 6176  total_loss: 7.84  time: 0.1619(98.83)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:50] lb.utils.events INFO:  eta: 0:01:57  iteration: 387/1000  consumed samples: 6192  total_loss: 7.839  time: 0.1620(98.79)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:50] lb.utils.events INFO:  eta: 0:01:57  iteration: 388/1000  consumed samples: 6208  total_loss: 7.837  time: 0.1620(98.78)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:51] lb.utils.events INFO:  eta: 0:01:57  iteration: 389/1000  consumed samples: 6224  total_loss: 7.834  time: 0.1620(98.79)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:51] lb.utils.events INFO:  eta: 0:01:57  iteration: 390/1000  consumed samples: 6240  total_loss: 7.831  time: 0.1621(98.73)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:51] lb.utils.events INFO:  eta: 0:01:56  iteration: 391/1000  consumed samples: 6256  total_loss: 7.83  time: 0.1621(98.68)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:51] lb.utils.events INFO:  eta: 0:01:56  iteration: 392/1000  consumed samples: 6272  total_loss: 7.83  time: 0.1622(98.62)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:51] lb.utils.events INFO:  eta: 0:01:56  iteration: 393/1000  consumed samples: 6288  total_loss: 7.831  time: 0.1623(98.57)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:44:52] lb.utils.events INFO:  eta: 0:01:56  iteration: 394/1000  consumed samples: 6304  total_loss: 7.831  time: 0.1624(98.51)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:44:52] lb.utils.events INFO:  eta: 0:01:56  iteration: 395/1000  consumed samples: 6320  total_loss: 7.831  time: 0.1625(98.47)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:44:52] lb.utils.events INFO:  eta: 0:01:55  iteration: 396/1000  consumed samples: 6336  total_loss: 7.831  time: 0.1626(98.41)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:44:52] lb.utils.events INFO:  eta: 0:01:55  iteration: 397/1000  consumed samples: 6352  total_loss: 7.83  time: 0.1627(98.36)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:44:52] lb.utils.events INFO:  eta: 0:01:55  iteration: 398/1000  consumed samples: 6368  total_loss: 7.83  time: 0.1628(98.31)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:44:53] lb.utils.events INFO:  eta: 0:01:55  iteration: 399/1000  consumed samples: 6384  total_loss: 7.829  time: 0.1629(98.25)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:44:53] lb.utils.events INFO:  eta: 0:01:55  iteration: 400/1000  consumed samples: 6400  total_loss: 7.829  time: 0.1629(98.19)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:44:53] lb.utils.events INFO:  eta: 0:01:55  iteration: 401/1000  consumed samples: 6416  total_loss: 7.829  time: 0.1630(98.16)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:44:53] lb.utils.events INFO:  eta: 0:01:54  iteration: 402/1000  consumed samples: 6432  total_loss: 7.829  time: 0.1631(98.10)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:53] lb.utils.events INFO:  eta: 0:01:54  iteration: 403/1000  consumed samples: 6448  total_loss: 7.828  time: 0.1631(98.11)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:44:54] lb.utils.events INFO:  eta: 0:01:54  iteration: 404/1000  consumed samples: 6464  total_loss: 7.828  time: 0.1631(98.07)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:44:54] lb.utils.events INFO:  eta: 0:01:54  iteration: 405/1000  consumed samples: 6480  total_loss: 7.828  time: 0.1632(98.03)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:44:54] lb.utils.events INFO:  eta: 0:01:54  iteration: 406/1000  consumed samples: 6496  total_loss: 7.828  time: 0.1633(97.97)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:54] lb.utils.events INFO:  eta: 0:01:53  iteration: 407/1000  consumed samples: 6512  total_loss: 7.828  time: 0.1634(97.91)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:54] lb.utils.events INFO:  eta: 0:01:53  iteration: 408/1000  consumed samples: 6528  total_loss: 7.826  time: 0.1635(97.87)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:44:55] lb.utils.events INFO:  eta: 0:01:53  iteration: 409/1000  consumed samples: 6544  total_loss: 7.826  time: 0.1636(97.83)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:44:55] lb.utils.events INFO:  eta: 0:01:53  iteration: 410/1000  consumed samples: 6560  total_loss: 7.828  time: 0.1636(97.79)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:44:55] lb.utils.events INFO:  eta: 0:01:53  iteration: 411/1000  consumed samples: 6576  total_loss: 7.826  time: 0.1637(97.74)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:55] lb.utils.events INFO:  eta: 0:01:53  iteration: 412/1000  consumed samples: 6592  total_loss: 7.823  time: 0.1638(97.70)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:44:56] lb.utils.events INFO:  eta: 0:01:52  iteration: 413/1000  consumed samples: 6608  total_loss: 7.821  time: 0.1639(97.64)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:56] lb.utils.events INFO:  eta: 0:01:52  iteration: 414/1000  consumed samples: 6624  total_loss: 7.821  time: 0.1640(97.59)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:56] lb.utils.events INFO:  eta: 0:01:52  iteration: 415/1000  consumed samples: 6640  total_loss: 7.821  time: 0.1640(97.55)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:56] lb.utils.events INFO:  eta: 0:01:52  iteration: 416/1000  consumed samples: 6656  total_loss: 7.821  time: 0.1641(97.49)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:56] lb.utils.events INFO:  eta: 0:01:52  iteration: 417/1000  consumed samples: 6672  total_loss: 7.82  time: 0.1642(97.44)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:57] lb.utils.events INFO:  eta: 0:01:51  iteration: 418/1000  consumed samples: 6688  total_loss: 7.818  time: 0.1643(97.41)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:44:57] lb.utils.events INFO:  eta: 0:01:51  iteration: 419/1000  consumed samples: 6704  total_loss: 7.817  time: 0.1643(97.36)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:57] lb.utils.events INFO:  eta: 0:01:51  iteration: 420/1000  consumed samples: 6720  total_loss: 7.815  time: 0.1644(97.31)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:57] lb.utils.events INFO:  eta: 0:01:51  iteration: 421/1000  consumed samples: 6736  total_loss: 7.815  time: 0.1645(97.26)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:57] lb.utils.events INFO:  eta: 0:01:51  iteration: 422/1000  consumed samples: 6752  total_loss: 7.813  time: 0.1646(97.22)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:58] lb.utils.events INFO:  eta: 0:01:51  iteration: 423/1000  consumed samples: 6768  total_loss: 7.813  time: 0.1647(97.16)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:58] lb.utils.events INFO:  eta: 0:01:50  iteration: 424/1000  consumed samples: 6784  total_loss: 7.812  time: 0.1648(97.11)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:58] lb.utils.events INFO:  eta: 0:01:50  iteration: 425/1000  consumed samples: 6800  total_loss: 7.812  time: 0.1649(97.06)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:58] lb.utils.events INFO:  eta: 0:01:50  iteration: 426/1000  consumed samples: 6816  total_loss: 7.813  time: 0.1649(97.02)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:58] lb.utils.events INFO:  eta: 0:01:50  iteration: 427/1000  consumed samples: 6832  total_loss: 7.812  time: 0.1650(96.96)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:59] lb.utils.events INFO:  eta: 0:01:50  iteration: 428/1000  consumed samples: 6848  total_loss: 7.811  time: 0.1651(96.92)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:44:59] lb.utils.events INFO:  eta: 0:01:49  iteration: 429/1000  consumed samples: 6864  total_loss: 7.811  time: 0.1652(96.87)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:44:59] lb.utils.events INFO:  eta: 0:01:49  iteration: 430/1000  consumed samples: 6880  total_loss: 7.811  time: 0.1652(96.83)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:44:59] lb.utils.events INFO:  eta: 0:01:49  iteration: 431/1000  consumed samples: 6896  total_loss: 7.809  time: 0.1653(96.78)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:44:59] lb.utils.events INFO:  eta: 0:01:49  iteration: 432/1000  consumed samples: 6912  total_loss: 7.809  time: 0.1654(96.74)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:00] lb.utils.events INFO:  eta: 0:01:49  iteration: 433/1000  consumed samples: 6928  total_loss: 7.808  time: 0.1655(96.69)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:00] lb.utils.events INFO:  eta: 0:01:49  iteration: 434/1000  consumed samples: 6944  total_loss: 7.807  time: 0.1656(96.65)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:00] lb.utils.events INFO:  eta: 0:01:48  iteration: 435/1000  consumed samples: 6960  total_loss: 7.807  time: 0.1656(96.61)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:00] lb.utils.events INFO:  eta: 0:01:48  iteration: 436/1000  consumed samples: 6976  total_loss: 7.807  time: 0.1657(96.58)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:00] lb.utils.events INFO:  eta: 0:01:48  iteration: 437/1000  consumed samples: 6992  total_loss: 7.803  time: 0.1658(96.53)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:01] lb.utils.events INFO:  eta: 0:01:48  iteration: 438/1000  consumed samples: 7008  total_loss: 7.799  time: 0.1658(96.49)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:01] lb.utils.events INFO:  eta: 0:01:48  iteration: 439/1000  consumed samples: 7024  total_loss: 7.798  time: 0.1659(96.44)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:01] lb.utils.events INFO:  eta: 0:01:47  iteration: 440/1000  consumed samples: 7040  total_loss: 7.796  time: 0.1660(96.40)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:45:01] lb.utils.events INFO:  eta: 0:01:47  iteration: 441/1000  consumed samples: 7056  total_loss: 7.796  time: 0.1660(96.37)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:45:02] lb.utils.events INFO:  eta: 0:01:47  iteration: 442/1000  consumed samples: 7072  total_loss: 7.796  time: 0.1661(96.33)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:45:02] lb.utils.events INFO:  eta: 0:01:47  iteration: 443/1000  consumed samples: 7088  total_loss: 7.795  time: 0.1662(96.28)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:02] lb.utils.events INFO:  eta: 0:01:47  iteration: 444/1000  consumed samples: 7104  total_loss: 7.793  time: 0.1662(96.24)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:02] lb.utils.events INFO:  eta: 0:01:46  iteration: 445/1000  consumed samples: 7120  total_loss: 7.791  time: 0.1663(96.21)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:45:02] lb.utils.events INFO:  eta: 0:01:46  iteration: 446/1000  consumed samples: 7136  total_loss: 7.79  time: 0.1664(96.17)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:45:03] lb.utils.events INFO:  eta: 0:01:46  iteration: 447/1000  consumed samples: 7152  total_loss: 7.79  time: 0.1665(96.12)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:45:03] lb.utils.events INFO:  eta: 0:01:46  iteration: 448/1000  consumed samples: 7168  total_loss: 7.79  time: 0.1665(96.08)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:03] lb.utils.events INFO:  eta: 0:01:46  iteration: 449/1000  consumed samples: 7184  total_loss: 7.79  time: 0.1666(96.04)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:03] lb.utils.events INFO:  eta: 0:01:46  iteration: 450/1000  consumed samples: 7200  total_loss: 7.79  time: 0.1667(96.00)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:45:03] lb.utils.events INFO:  eta: 0:01:45  iteration: 451/1000  consumed samples: 7216  total_loss: 7.79  time: 0.1667(95.96)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:45:04] lb.utils.events INFO:  eta: 0:01:45  iteration: 452/1000  consumed samples: 7232  total_loss: 7.789  time: 0.1668(95.92)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:45:04] lb.utils.events INFO:  eta: 0:01:45  iteration: 453/1000  consumed samples: 7248  total_loss: 7.788  time: 0.1669(95.87)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:45:04] lb.utils.events INFO:  eta: 0:01:45  iteration: 454/1000  consumed samples: 7264  total_loss: 7.787  time: 0.1669(95.84)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:04] lb.utils.events INFO:  eta: 0:01:45  iteration: 455/1000  consumed samples: 7280  total_loss: 7.786  time: 0.1670(95.79)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:04] lb.utils.events INFO:  eta: 0:01:45  iteration: 456/1000  consumed samples: 7296  total_loss: 7.783  time: 0.1671(95.76)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:45:05] lb.utils.events INFO:  eta: 0:01:44  iteration: 457/1000  consumed samples: 7312  total_loss: 7.78  time: 0.1672(95.72)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:45:05] lb.utils.events INFO:  eta: 0:01:44  iteration: 458/1000  consumed samples: 7328  total_loss: 7.78  time: 0.1672(95.68)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:05] lb.utils.events INFO:  eta: 0:01:44  iteration: 459/1000  consumed samples: 7344  total_loss: 7.779  time: 0.1673(95.64)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:05] lb.utils.events INFO:  eta: 0:01:44  iteration: 460/1000  consumed samples: 7360  total_loss: 7.778  time: 0.1674(95.61)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:05] lb.utils.events INFO:  eta: 0:01:44  iteration: 461/1000  consumed samples: 7376  total_loss: 7.779  time: 0.1674(95.56)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:45:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 462/1000  consumed samples: 7392  total_loss: 7.779  time: 0.1675(95.53)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:45:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 463/1000  consumed samples: 7408  total_loss: 7.779  time: 0.1676(95.49)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:45:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 464/1000  consumed samples: 7424  total_loss: 7.778  time: 0.1676(95.46)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:06] lb.utils.events INFO:  eta: 0:01:43  iteration: 465/1000  consumed samples: 7440  total_loss: 7.778  time: 0.1677(95.42)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 466/1000  consumed samples: 7456  total_loss: 7.778  time: 0.1677(95.39)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:07] lb.utils.events INFO:  eta: 0:01:43  iteration: 467/1000  consumed samples: 7472  total_loss: 7.778  time: 0.1678(95.34)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:07] lb.utils.events INFO:  eta: 0:01:42  iteration: 468/1000  consumed samples: 7488  total_loss: 7.778  time: 0.1679(95.31)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:07] lb.utils.events INFO:  eta: 0:01:42  iteration: 469/1000  consumed samples: 7504  total_loss: 7.778  time: 0.1679(95.27)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:07] lb.utils.events INFO:  eta: 0:01:42  iteration: 470/1000  consumed samples: 7520  total_loss: 7.778  time: 0.1680(95.24)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:08] lb.utils.events INFO:  eta: 0:01:42  iteration: 471/1000  consumed samples: 7536  total_loss: 7.777  time: 0.1681(95.20)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:08] lb.utils.events INFO:  eta: 0:01:42  iteration: 472/1000  consumed samples: 7552  total_loss: 7.774  time: 0.1681(95.17)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:08] lb.utils.events INFO:  eta: 0:01:41  iteration: 473/1000  consumed samples: 7568  total_loss: 7.774  time: 0.1682(95.12)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:08] lb.utils.events INFO:  eta: 0:01:41  iteration: 474/1000  consumed samples: 7584  total_loss: 7.773  time: 0.1683(95.09)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:08] lb.utils.events INFO:  eta: 0:01:41  iteration: 475/1000  consumed samples: 7600  total_loss: 7.772  time: 0.1683(95.05)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:09] lb.utils.events INFO:  eta: 0:01:41  iteration: 476/1000  consumed samples: 7616  total_loss: 7.772  time: 0.1684(95.02)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:09] lb.utils.events INFO:  eta: 0:01:41  iteration: 477/1000  consumed samples: 7632  total_loss: 7.772  time: 0.1685(94.98)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:09] lb.utils.events INFO:  eta: 0:01:40  iteration: 478/1000  consumed samples: 7648  total_loss: 7.771  time: 0.1685(94.95)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:09] lb.utils.events INFO:  eta: 0:01:40  iteration: 479/1000  consumed samples: 7664  total_loss: 7.771  time: 0.1686(94.91)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:09] lb.utils.events INFO:  eta: 0:01:40  iteration: 480/1000  consumed samples: 7680  total_loss: 7.769  time: 0.1686(94.88)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:10] lb.utils.events INFO:  eta: 0:01:40  iteration: 481/1000  consumed samples: 7696  total_loss: 7.767  time: 0.1687(94.84)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:10] lb.utils.events INFO:  eta: 0:01:40  iteration: 482/1000  consumed samples: 7712  total_loss: 7.767  time: 0.1688(94.81)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:10] lb.utils.events INFO:  eta: 0:01:40  iteration: 483/1000  consumed samples: 7728  total_loss: 7.767  time: 0.1688(94.77)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:10] lb.utils.events INFO:  eta: 0:01:39  iteration: 484/1000  consumed samples: 7744  total_loss: 7.766  time: 0.1689(94.74)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:10] lb.utils.events INFO:  eta: 0:01:39  iteration: 485/1000  consumed samples: 7760  total_loss: 7.763  time: 0.1690(94.70)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:11] lb.utils.events INFO:  eta: 0:01:39  iteration: 486/1000  consumed samples: 7776  total_loss: 7.761  time: 0.1690(94.67)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:11] lb.utils.events INFO:  eta: 0:01:39  iteration: 487/1000  consumed samples: 7792  total_loss: 7.759  time: 0.1691(94.63)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:11] lb.utils.events INFO:  eta: 0:01:39  iteration: 488/1000  consumed samples: 7808  total_loss: 7.759  time: 0.1691(94.60)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:11] lb.utils.events INFO:  eta: 0:01:38  iteration: 489/1000  consumed samples: 7824  total_loss: 7.759  time: 0.1692(94.56)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:11] lb.utils.events INFO:  eta: 0:01:38  iteration: 490/1000  consumed samples: 7840  total_loss: 7.759  time: 0.1693(94.53)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:12] lb.utils.events INFO:  eta: 0:01:38  iteration: 491/1000  consumed samples: 7856  total_loss: 7.759  time: 0.1693(94.49)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:12] lb.utils.events INFO:  eta: 0:01:38  iteration: 492/1000  consumed samples: 7872  total_loss: 7.758  time: 0.1694(94.46)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:12] lb.utils.events INFO:  eta: 0:01:38  iteration: 493/1000  consumed samples: 7888  total_loss: 7.758  time: 0.1694(94.43)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:12] lb.utils.events INFO:  eta: 0:01:38  iteration: 494/1000  consumed samples: 7904  total_loss: 7.756  time: 0.1695(94.40)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:13] lb.utils.events INFO:  eta: 0:01:37  iteration: 495/1000  consumed samples: 7920  total_loss: 7.755  time: 0.1696(94.36)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:13] lb.utils.events INFO:  eta: 0:01:37  iteration: 496/1000  consumed samples: 7936  total_loss: 7.754  time: 0.1696(94.33)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:13] lb.utils.events INFO:  eta: 0:01:37  iteration: 497/1000  consumed samples: 7952  total_loss: 7.754  time: 0.1697(94.30)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:13] lb.utils.events INFO:  eta: 0:01:37  iteration: 498/1000  consumed samples: 7968  total_loss: 7.753  time: 0.1697(94.27)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:13] lb.utils.events INFO:  eta: 0:01:37  iteration: 499/1000  consumed samples: 7984  total_loss: 7.753  time: 0.1698(94.23)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:14] lb.utils.events INFO:  eta: 0:01:36  iteration: 500/1000  consumed samples: 8000  total_loss: 7.752  time: 0.1698(94.21)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:14] lb.utils.events INFO:  eta: 0:01:36  iteration: 501/1000  consumed samples: 8016  total_loss: 7.751  time: 0.1699(94.17)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:14] lb.utils.events INFO:  eta: 0:01:36  iteration: 502/1000  consumed samples: 8032  total_loss: 7.751  time: 0.1700(94.14)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:14] lb.utils.events INFO:  eta: 0:01:36  iteration: 503/1000  consumed samples: 8048  total_loss: 7.747  time: 0.1700(94.11)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:14] lb.utils.events INFO:  eta: 0:01:36  iteration: 504/1000  consumed samples: 8064  total_loss: 7.747  time: 0.1701(94.08)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:15] lb.utils.events INFO:  eta: 0:01:35  iteration: 505/1000  consumed samples: 8080  total_loss: 7.742  time: 0.1701(94.05)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:15] lb.utils.events INFO:  eta: 0:01:35  iteration: 506/1000  consumed samples: 8096  total_loss: 7.742  time: 0.1702(94.01)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:15] lb.utils.events INFO:  eta: 0:01:35  iteration: 507/1000  consumed samples: 8112  total_loss: 7.738  time: 0.1702(93.99)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:15] lb.utils.events INFO:  eta: 0:01:35  iteration: 508/1000  consumed samples: 8128  total_loss: 7.735  time: 0.1703(93.95)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:15] lb.utils.events INFO:  eta: 0:01:35  iteration: 509/1000  consumed samples: 8144  total_loss: 7.735  time: 0.1703(93.94)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:16] lb.utils.events INFO:  eta: 0:01:34  iteration: 510/1000  consumed samples: 8160  total_loss: 7.735  time: 0.1703(93.94)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:16] lb.utils.events INFO:  eta: 0:01:34  iteration: 511/1000  consumed samples: 8176  total_loss: 7.735  time: 0.1704(93.91)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:16] lb.utils.events INFO:  eta: 0:01:34  iteration: 512/1000  consumed samples: 8192  total_loss: 7.732  time: 0.1704(93.88)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:16] lb.utils.events INFO:  eta: 0:01:34  iteration: 513/1000  consumed samples: 8208  total_loss: 7.732  time: 0.1705(93.85)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:16] lb.utils.events INFO:  eta: 0:01:34  iteration: 514/1000  consumed samples: 8224  total_loss: 7.732  time: 0.1706(93.81)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:17] lb.utils.events INFO:  eta: 0:01:33  iteration: 515/1000  consumed samples: 8240  total_loss: 7.729  time: 0.1706(93.78)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:17] lb.utils.events INFO:  eta: 0:01:33  iteration: 516/1000  consumed samples: 8256  total_loss: 7.729  time: 0.1707(93.75)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:17] lb.utils.events INFO:  eta: 0:01:33  iteration: 517/1000  consumed samples: 8272  total_loss: 7.729  time: 0.1707(93.72)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:17] lb.utils.events INFO:  eta: 0:01:33  iteration: 518/1000  consumed samples: 8288  total_loss: 7.729  time: 0.1708(93.69)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:17] lb.utils.events INFO:  eta: 0:01:33  iteration: 519/1000  consumed samples: 8304  total_loss: 7.726  time: 0.1708(93.66)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:18] lb.utils.events INFO:  eta: 0:01:33  iteration: 520/1000  consumed samples: 8320  total_loss: 7.726  time: 0.1709(93.64)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:18] lb.utils.events INFO:  eta: 0:01:32  iteration: 521/1000  consumed samples: 8336  total_loss: 7.729  time: 0.1709(93.61)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:18] lb.utils.events INFO:  eta: 0:01:32  iteration: 522/1000  consumed samples: 8352  total_loss: 7.726  time: 0.1710(93.58)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:45:18] lb.utils.events INFO:  eta: 0:01:32  iteration: 523/1000  consumed samples: 8368  total_loss: 7.725  time: 0.1710(93.55)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:45:19] lb.utils.events INFO:  eta: 0:01:32  iteration: 524/1000  consumed samples: 8384  total_loss: 7.725  time: 0.1711(93.52)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:19] lb.utils.events INFO:  eta: 0:01:32  iteration: 525/1000  consumed samples: 8400  total_loss: 7.724  time: 0.1711(93.49)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:19] lb.utils.events INFO:  eta: 0:01:31  iteration: 526/1000  consumed samples: 8416  total_loss: 7.724  time: 0.1712(93.46)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:19] lb.utils.events INFO:  eta: 0:01:31  iteration: 527/1000  consumed samples: 8432  total_loss: 7.725  time: 0.1712(93.43)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:45:20] lb.utils.events INFO:  eta: 0:01:31  iteration: 528/1000  consumed samples: 8448  total_loss: 7.724  time: 0.1711(93.53)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:20] lb.utils.events INFO:  eta: 0:01:31  iteration: 529/1000  consumed samples: 8464  total_loss: 7.723  time: 0.1709(93.63)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:21] lb.utils.events INFO:  eta: 0:01:31  iteration: 530/1000  consumed samples: 8480  total_loss: 7.722  time: 0.1707(93.72)  data_time: 0.0041  lr: 9.00e-05  
[01/19 15:45:21] lb.utils.events INFO:  eta: 0:01:30  iteration: 531/1000  consumed samples: 8496  total_loss: 7.722  time: 0.1705(93.82)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:45:22] lb.utils.events INFO:  eta: 0:01:30  iteration: 532/1000  consumed samples: 8512  total_loss: 7.722  time: 0.1704(93.91)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:45:22] lb.utils.events INFO:  eta: 0:01:30  iteration: 533/1000  consumed samples: 8528  total_loss: 7.722  time: 0.1702(94.01)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:45:22] lb.utils.events INFO:  eta: 0:01:30  iteration: 534/1000  consumed samples: 8544  total_loss: 7.72  time: 0.1700(94.11)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:45:23] lb.utils.events INFO:  eta: 0:01:30  iteration: 535/1000  consumed samples: 8560  total_loss: 7.72  time: 0.1698(94.21)  data_time: 0.0042  lr: 9.00e-05  
[01/19 15:45:23] lb.utils.events INFO:  eta: 0:01:29  iteration: 536/1000  consumed samples: 8576  total_loss: 7.72  time: 0.1697(94.26)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:45:23] lb.utils.events INFO:  eta: 0:01:29  iteration: 537/1000  consumed samples: 8592  total_loss: 7.72  time: 0.1698(94.25)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:45:23] lb.utils.events INFO:  eta: 0:01:29  iteration: 538/1000  consumed samples: 8608  total_loss: 7.722  time: 0.1698(94.25)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:45:23] lb.utils.events INFO:  eta: 0:01:29  iteration: 539/1000  consumed samples: 8624  total_loss: 7.72  time: 0.1698(94.22)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:45:24] lb.utils.events INFO:  eta: 0:01:29  iteration: 540/1000  consumed samples: 8640  total_loss: 7.718  time: 0.1699(94.19)  data_time: 0.0043  lr: 9.00e-05  
[01/19 15:45:24] lb.utils.events INFO:  eta: 0:01:28  iteration: 541/1000  consumed samples: 8656  total_loss: 7.718  time: 0.1699(94.16)  data_time: 0.0044  lr: 9.00e-05  
[01/19 15:45:24] lb.utils.events INFO:  eta: 0:01:28  iteration: 542/1000  consumed samples: 8672  total_loss: 7.716  time: 0.1700(94.13)  data_time: 0.0089  lr: 9.00e-05  
[01/19 15:45:24] lb.utils.events INFO:  eta: 0:01:28  iteration: 543/1000  consumed samples: 8688  total_loss: 7.712  time: 0.1700(94.11)  data_time: 0.0088  lr: 9.00e-05  
[01/19 15:45:24] lb.utils.events INFO:  eta: 0:01:28  iteration: 544/1000  consumed samples: 8704  total_loss: 7.712  time: 0.1701(94.08)  data_time: 0.0087  lr: 9.00e-05  
[01/19 15:45:25] lb.utils.events INFO:  eta: 0:01:28  iteration: 545/1000  consumed samples: 8720  total_loss: 7.711  time: 0.1701(94.05)  data_time: 0.0087  lr: 9.00e-05  
[01/19 15:45:25] lb.utils.events INFO:  eta: 0:01:27  iteration: 546/1000  consumed samples: 8736  total_loss: 7.711  time: 0.1702(94.01)  data_time: 0.0086  lr: 9.00e-05  
[01/19 15:45:25] lb.utils.events INFO:  eta: 0:01:27  iteration: 547/1000  consumed samples: 8752  total_loss: 7.711  time: 0.1702(93.99)  data_time: 0.0085  lr: 9.00e-05  
[01/19 15:45:25] lb.utils.events INFO:  eta: 0:01:27  iteration: 548/1000  consumed samples: 8768  total_loss: 7.709  time: 0.1703(93.95)  data_time: 0.0083  lr: 9.00e-05  
[01/19 15:45:25] lb.utils.events INFO:  eta: 0:01:27  iteration: 549/1000  consumed samples: 8784  total_loss: 7.709  time: 0.1704(93.92)  data_time: 0.0083  lr: 9.00e-05  
[01/19 15:45:26] lb.utils.events INFO:  eta: 0:01:27  iteration: 550/1000  consumed samples: 8800  total_loss: 7.709  time: 0.1704(93.89)  data_time: 0.0081  lr: 9.00e-05  
[01/19 15:45:26] lb.utils.events INFO:  eta: 0:01:27  iteration: 551/1000  consumed samples: 8816  total_loss: 7.706  time: 0.1705(93.87)  data_time: 0.0081  lr: 9.00e-05  
[01/19 15:45:26] lb.utils.events INFO:  eta: 0:01:26  iteration: 552/1000  consumed samples: 8832  total_loss: 7.706  time: 0.1705(93.83)  data_time: 0.0081  lr: 9.00e-05  
[01/19 15:45:26] lb.utils.events INFO:  eta: 0:01:26  iteration: 553/1000  consumed samples: 8848  total_loss: 7.704  time: 0.1705(93.83)  data_time: 0.0081  lr: 9.00e-05  
[01/19 15:45:27] lb.utils.events INFO:  eta: 0:01:26  iteration: 554/1000  consumed samples: 8864  total_loss: 7.701  time: 0.1706(93.80)  data_time: 0.0080  lr: 9.00e-05  
[01/19 15:45:27] lb.utils.events INFO:  eta: 0:01:26  iteration: 555/1000  consumed samples: 8880  total_loss: 7.701  time: 0.1706(93.78)  data_time: 0.0080  lr: 9.00e-05  
[01/19 15:45:27] lb.utils.events INFO:  eta: 0:01:26  iteration: 556/1000  consumed samples: 8896  total_loss: 7.701  time: 0.1707(93.74)  data_time: 0.0080  lr: 9.00e-05  
[01/19 15:45:27] lb.utils.events INFO:  eta: 0:01:25  iteration: 557/1000  consumed samples: 8912  total_loss: 7.698  time: 0.1707(93.72)  data_time: 0.0080  lr: 9.00e-05  
[01/19 15:45:27] lb.utils.events INFO:  eta: 0:01:25  iteration: 558/1000  consumed samples: 8928  total_loss: 7.698  time: 0.1708(93.68)  data_time: 0.0079  lr: 9.00e-05  
[01/19 15:45:28] lb.utils.events INFO:  eta: 0:01:25  iteration: 559/1000  consumed samples: 8944  total_loss: 7.694  time: 0.1708(93.66)  data_time: 0.0079  lr: 9.00e-05  
[01/19 15:45:28] lb.utils.events INFO:  eta: 0:01:25  iteration: 560/1000  consumed samples: 8960  total_loss: 7.694  time: 0.1709(93.63)  data_time: 0.0080  lr: 9.00e-05  
[01/19 15:45:28] lb.utils.events INFO:  eta: 0:01:25  iteration: 561/1000  consumed samples: 8976  total_loss: 7.692  time: 0.1709(93.60)  data_time: 0.0080  lr: 9.00e-05  
[01/19 15:45:28] lb.utils.events INFO:  eta: 0:01:24  iteration: 562/1000  consumed samples: 8992  total_loss: 7.692  time: 0.1710(93.57)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:28] lb.utils.events INFO:  eta: 0:01:24  iteration: 563/1000  consumed samples: 9008  total_loss: 7.691  time: 0.1710(93.54)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:29] lb.utils.events INFO:  eta: 0:01:24  iteration: 564/1000  consumed samples: 9024  total_loss: 7.692  time: 0.1711(93.51)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:29] lb.utils.events INFO:  eta: 0:01:24  iteration: 565/1000  consumed samples: 9040  total_loss: 7.691  time: 0.1711(93.49)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:29] lb.utils.events INFO:  eta: 0:01:24  iteration: 566/1000  consumed samples: 9056  total_loss: 7.687  time: 0.1712(93.46)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:45:29] lb.utils.events INFO:  eta: 0:01:24  iteration: 567/1000  consumed samples: 9072  total_loss: 7.687  time: 0.1712(93.44)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:45:29] lb.utils.events INFO:  eta: 0:01:23  iteration: 568/1000  consumed samples: 9088  total_loss: 7.683  time: 0.1713(93.41)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:30] lb.utils.events INFO:  eta: 0:01:23  iteration: 569/1000  consumed samples: 9104  total_loss: 7.683  time: 0.1713(93.38)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:30] lb.utils.events INFO:  eta: 0:01:23  iteration: 570/1000  consumed samples: 9120  total_loss: 7.683  time: 0.1714(93.35)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:30] lb.utils.events INFO:  eta: 0:01:23  iteration: 571/1000  consumed samples: 9136  total_loss: 7.683  time: 0.1714(93.33)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:30] lb.utils.events INFO:  eta: 0:01:23  iteration: 572/1000  consumed samples: 9152  total_loss: 7.683  time: 0.1715(93.30)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:30] lb.utils.events INFO:  eta: 0:01:22  iteration: 573/1000  consumed samples: 9168  total_loss: 7.687  time: 0.1715(93.29)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:31] lb.utils.events INFO:  eta: 0:01:22  iteration: 574/1000  consumed samples: 9184  total_loss: 7.687  time: 0.1716(93.26)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:45:31] lb.utils.events INFO:  eta: 0:01:22  iteration: 575/1000  consumed samples: 9200  total_loss: 7.683  time: 0.1716(93.23)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:45:31] lb.utils.events INFO:  eta: 0:01:22  iteration: 576/1000  consumed samples: 9216  total_loss: 7.682  time: 0.1717(93.20)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:45:31] lb.utils.events INFO:  eta: 0:01:22  iteration: 577/1000  consumed samples: 9232  total_loss: 7.682  time: 0.1717(93.18)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:45:32] lb.utils.events INFO:  eta: 0:01:21  iteration: 578/1000  consumed samples: 9248  total_loss: 7.678  time: 0.1718(93.15)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:45:32] lb.utils.events INFO:  eta: 0:01:21  iteration: 579/1000  consumed samples: 9264  total_loss: 7.674  time: 0.1718(93.13)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:45:32] lb.utils.events INFO:  eta: 0:01:21  iteration: 580/1000  consumed samples: 9280  total_loss: 7.674  time: 0.1719(93.10)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:45:32] lb.utils.events INFO:  eta: 0:01:21  iteration: 581/1000  consumed samples: 9296  total_loss: 7.673  time: 0.1719(93.09)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:45:32] lb.utils.events INFO:  eta: 0:01:21  iteration: 582/1000  consumed samples: 9312  total_loss: 7.673  time: 0.1719(93.06)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:33] lb.utils.events INFO:  eta: 0:01:20  iteration: 583/1000  consumed samples: 9328  total_loss: 7.673  time: 0.1720(93.04)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:33] lb.utils.events INFO:  eta: 0:01:20  iteration: 584/1000  consumed samples: 9344  total_loss: 7.669  time: 0.1720(93.01)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:33] lb.utils.events INFO:  eta: 0:01:20  iteration: 585/1000  consumed samples: 9360  total_loss: 7.669  time: 0.1720(93.00)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:33] lb.utils.events INFO:  eta: 0:01:20  iteration: 586/1000  consumed samples: 9376  total_loss: 7.669  time: 0.1721(92.97)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:33] lb.utils.events INFO:  eta: 0:01:20  iteration: 587/1000  consumed samples: 9392  total_loss: 7.665  time: 0.1721(92.95)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:34] lb.utils.events INFO:  eta: 0:01:19  iteration: 588/1000  consumed samples: 9408  total_loss: 7.669  time: 0.1722(92.92)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:34] lb.utils.events INFO:  eta: 0:01:19  iteration: 589/1000  consumed samples: 9424  total_loss: 7.665  time: 0.1722(92.90)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:34] lb.utils.events INFO:  eta: 0:01:19  iteration: 590/1000  consumed samples: 9440  total_loss: 7.663  time: 0.1723(92.88)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:34] lb.utils.events INFO:  eta: 0:01:19  iteration: 591/1000  consumed samples: 9456  total_loss: 7.662  time: 0.1723(92.86)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:34] lb.utils.events INFO:  eta: 0:01:19  iteration: 592/1000  consumed samples: 9472  total_loss: 7.66  time: 0.1723(92.83)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:35] lb.utils.events INFO:  eta: 0:01:19  iteration: 593/1000  consumed samples: 9488  total_loss: 7.66  time: 0.1724(92.81)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:35] lb.utils.events INFO:  eta: 0:01:18  iteration: 594/1000  consumed samples: 9504  total_loss: 7.658  time: 0.1724(92.78)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:35] lb.utils.events INFO:  eta: 0:01:18  iteration: 595/1000  consumed samples: 9520  total_loss: 7.658  time: 0.1725(92.77)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:35] lb.utils.events INFO:  eta: 0:01:18  iteration: 596/1000  consumed samples: 9536  total_loss: 7.658  time: 0.1725(92.74)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:35] lb.utils.events INFO:  eta: 0:01:18  iteration: 597/1000  consumed samples: 9552  total_loss: 7.658  time: 0.1725(92.73)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:36] lb.utils.events INFO:  eta: 0:01:18  iteration: 598/1000  consumed samples: 9568  total_loss: 7.658  time: 0.1726(92.71)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:36] lb.utils.events INFO:  eta: 0:01:17  iteration: 599/1000  consumed samples: 9584  total_loss: 7.656  time: 0.1726(92.69)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:36] lb.utils.events INFO:  eta: 0:01:17  iteration: 600/1000  consumed samples: 9600  total_loss: 7.655  time: 0.1727(92.66)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:36] lb.utils.events INFO:  eta: 0:01:17  iteration: 601/1000  consumed samples: 9616  total_loss: 7.655  time: 0.1727(92.64)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:36] lb.utils.events INFO:  eta: 0:01:17  iteration: 602/1000  consumed samples: 9632  total_loss: 7.654  time: 0.1728(92.61)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:37] lb.utils.events INFO:  eta: 0:01:17  iteration: 603/1000  consumed samples: 9648  total_loss: 7.655  time: 0.1728(92.59)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:37] lb.utils.events INFO:  eta: 0:01:16  iteration: 604/1000  consumed samples: 9664  total_loss: 7.654  time: 0.1729(92.56)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:37] lb.utils.events INFO:  eta: 0:01:16  iteration: 605/1000  consumed samples: 9680  total_loss: 7.654  time: 0.1729(92.54)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:37] lb.utils.events INFO:  eta: 0:01:16  iteration: 606/1000  consumed samples: 9696  total_loss: 7.651  time: 0.1729(92.52)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:38] lb.utils.events INFO:  eta: 0:01:16  iteration: 607/1000  consumed samples: 9712  total_loss: 7.651  time: 0.1730(92.50)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:38] lb.utils.events INFO:  eta: 0:01:16  iteration: 608/1000  consumed samples: 9728  total_loss: 7.651  time: 0.1730(92.47)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:38] lb.utils.events INFO:  eta: 0:01:15  iteration: 609/1000  consumed samples: 9744  total_loss: 7.648  time: 0.1731(92.45)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:38] lb.utils.events INFO:  eta: 0:01:15  iteration: 610/1000  consumed samples: 9760  total_loss: 7.646  time: 0.1731(92.43)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:38] lb.utils.events INFO:  eta: 0:01:15  iteration: 611/1000  consumed samples: 9776  total_loss: 7.645  time: 0.1732(92.40)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:39] lb.utils.events INFO:  eta: 0:01:15  iteration: 612/1000  consumed samples: 9792  total_loss: 7.646  time: 0.1732(92.38)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:39] lb.utils.events INFO:  eta: 0:01:15  iteration: 613/1000  consumed samples: 9808  total_loss: 7.646  time: 0.1732(92.36)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:39] lb.utils.events INFO:  eta: 0:01:15  iteration: 614/1000  consumed samples: 9824  total_loss: 7.645  time: 0.1733(92.33)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:39] lb.utils.events INFO:  eta: 0:01:14  iteration: 615/1000  consumed samples: 9840  total_loss: 7.644  time: 0.1733(92.31)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:39] lb.utils.events INFO:  eta: 0:01:14  iteration: 616/1000  consumed samples: 9856  total_loss: 7.644  time: 0.1734(92.29)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:40] lb.utils.events INFO:  eta: 0:01:14  iteration: 617/1000  consumed samples: 9872  total_loss: 7.643  time: 0.1734(92.27)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:40] lb.utils.events INFO:  eta: 0:01:14  iteration: 618/1000  consumed samples: 9888  total_loss: 7.641  time: 0.1735(92.24)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:40] lb.utils.events INFO:  eta: 0:01:14  iteration: 619/1000  consumed samples: 9904  total_loss: 7.641  time: 0.1735(92.22)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:40] lb.utils.events INFO:  eta: 0:01:13  iteration: 620/1000  consumed samples: 9920  total_loss: 7.641  time: 0.1735(92.20)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:40] lb.utils.events INFO:  eta: 0:01:13  iteration: 621/1000  consumed samples: 9936  total_loss: 7.639  time: 0.1736(92.18)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:41] lb.utils.events INFO:  eta: 0:01:13  iteration: 622/1000  consumed samples: 9952  total_loss: 7.639  time: 0.1736(92.16)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:41] lb.utils.events INFO:  eta: 0:01:13  iteration: 623/1000  consumed samples: 9968  total_loss: 7.638  time: 0.1736(92.14)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:41] lb.utils.events INFO:  eta: 0:01:13  iteration: 624/1000  consumed samples: 9984  total_loss: 7.638  time: 0.1737(92.12)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:41] lb.utils.events INFO:  eta: 0:01:12  iteration: 625/1000  consumed samples: 10000  total_loss: 7.638  time: 0.1737(92.10)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:45:41] lb.utils.events INFO:  eta: 0:01:12  iteration: 626/1000  consumed samples: 10016  total_loss: 7.637  time: 0.1738(92.08)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:45:42] lb.utils.events INFO:  eta: 0:01:12  iteration: 627/1000  consumed samples: 10032  total_loss: 7.637  time: 0.1738(92.06)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:45:42] lb.utils.events INFO:  eta: 0:01:12  iteration: 628/1000  consumed samples: 10048  total_loss: 7.637  time: 0.1738(92.04)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:45:42] lb.utils.events INFO:  eta: 0:01:12  iteration: 629/1000  consumed samples: 10064  total_loss: 7.636  time: 0.1739(92.02)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:42] lb.utils.events INFO:  eta: 0:01:11  iteration: 630/1000  consumed samples: 10080  total_loss: 7.636  time: 0.1739(91.99)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:43] lb.utils.events INFO:  eta: 0:01:11  iteration: 631/1000  consumed samples: 10096  total_loss: 7.636  time: 0.1740(91.98)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:43] lb.utils.events INFO:  eta: 0:01:11  iteration: 632/1000  consumed samples: 10112  total_loss: 7.636  time: 0.1740(91.95)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:43] lb.utils.events INFO:  eta: 0:01:11  iteration: 633/1000  consumed samples: 10128  total_loss: 7.636  time: 0.1740(91.93)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:43] lb.utils.events INFO:  eta: 0:01:11  iteration: 634/1000  consumed samples: 10144  total_loss: 7.635  time: 0.1741(91.91)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:43] lb.utils.events INFO:  eta: 0:01:10  iteration: 635/1000  consumed samples: 10160  total_loss: 7.633  time: 0.1741(91.89)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:44] lb.utils.events INFO:  eta: 0:01:10  iteration: 636/1000  consumed samples: 10176  total_loss: 7.632  time: 0.1742(91.87)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:44] lb.utils.events INFO:  eta: 0:01:10  iteration: 637/1000  consumed samples: 10192  total_loss: 7.633  time: 0.1742(91.85)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:44] lb.utils.events INFO:  eta: 0:01:10  iteration: 638/1000  consumed samples: 10208  total_loss: 7.635  time: 0.1742(91.82)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:44] lb.utils.events INFO:  eta: 0:01:10  iteration: 639/1000  consumed samples: 10224  total_loss: 7.633  time: 0.1743(91.81)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:44] lb.utils.events INFO:  eta: 0:01:09  iteration: 640/1000  consumed samples: 10240  total_loss: 7.632  time: 0.1743(91.78)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:45] lb.utils.events INFO:  eta: 0:01:09  iteration: 641/1000  consumed samples: 10256  total_loss: 7.632  time: 0.1744(91.76)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:45] lb.utils.events INFO:  eta: 0:01:09  iteration: 642/1000  consumed samples: 10272  total_loss: 7.631  time: 0.1744(91.74)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:45] lb.utils.events INFO:  eta: 0:01:09  iteration: 643/1000  consumed samples: 10288  total_loss: 7.63  time: 0.1744(91.73)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:45] lb.utils.events INFO:  eta: 0:01:09  iteration: 644/1000  consumed samples: 10304  total_loss: 7.628  time: 0.1745(91.70)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:45] lb.utils.events INFO:  eta: 0:01:09  iteration: 645/1000  consumed samples: 10320  total_loss: 7.628  time: 0.1744(91.74)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:46] lb.utils.events INFO:  eta: 0:01:08  iteration: 646/1000  consumed samples: 10336  total_loss: 7.626  time: 0.1744(91.73)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:46] lb.utils.events INFO:  eta: 0:01:08  iteration: 647/1000  consumed samples: 10352  total_loss: 7.625  time: 0.1745(91.71)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:46] lb.utils.events INFO:  eta: 0:01:08  iteration: 648/1000  consumed samples: 10368  total_loss: 7.624  time: 0.1745(91.69)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:46] lb.utils.events INFO:  eta: 0:01:08  iteration: 649/1000  consumed samples: 10384  total_loss: 7.624  time: 0.1745(91.67)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:46] lb.utils.events INFO:  eta: 0:01:08  iteration: 650/1000  consumed samples: 10400  total_loss: 7.622  time: 0.1746(91.65)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:47] lb.utils.events INFO:  eta: 0:01:07  iteration: 651/1000  consumed samples: 10416  total_loss: 7.62  time: 0.1746(91.63)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:47] lb.utils.events INFO:  eta: 0:01:07  iteration: 652/1000  consumed samples: 10432  total_loss: 7.618  time: 0.1746(91.61)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:47] lb.utils.events INFO:  eta: 0:01:07  iteration: 653/1000  consumed samples: 10448  total_loss: 7.618  time: 0.1747(91.59)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:47] lb.utils.events INFO:  eta: 0:01:07  iteration: 654/1000  consumed samples: 10464  total_loss: 7.618  time: 0.1747(91.58)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:47] lb.utils.events INFO:  eta: 0:01:07  iteration: 655/1000  consumed samples: 10480  total_loss: 7.617  time: 0.1748(91.55)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:48] lb.utils.events INFO:  eta: 0:01:06  iteration: 656/1000  consumed samples: 10496  total_loss: 7.618  time: 0.1748(91.54)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:48] lb.utils.events INFO:  eta: 0:01:06  iteration: 657/1000  consumed samples: 10512  total_loss: 7.618  time: 0.1748(91.52)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:48] lb.utils.events INFO:  eta: 0:01:06  iteration: 658/1000  consumed samples: 10528  total_loss: 7.617  time: 0.1749(91.50)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:48] lb.utils.events INFO:  eta: 0:01:06  iteration: 659/1000  consumed samples: 10544  total_loss: 7.615  time: 0.1749(91.50)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:49] lb.utils.events INFO:  eta: 0:01:06  iteration: 660/1000  consumed samples: 10560  total_loss: 7.615  time: 0.1749(91.47)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:49] lb.utils.events INFO:  eta: 0:01:05  iteration: 661/1000  consumed samples: 10576  total_loss: 7.615  time: 0.1749(91.46)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:49] lb.utils.events INFO:  eta: 0:01:05  iteration: 662/1000  consumed samples: 10592  total_loss: 7.614  time: 0.1750(91.44)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:45:49] lb.utils.events INFO:  eta: 0:01:05  iteration: 663/1000  consumed samples: 10608  total_loss: 7.614  time: 0.1750(91.42)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:49] lb.utils.events INFO:  eta: 0:01:05  iteration: 664/1000  consumed samples: 10624  total_loss: 7.614  time: 0.1751(91.40)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:50] lb.utils.events INFO:  eta: 0:01:05  iteration: 665/1000  consumed samples: 10640  total_loss: 7.614  time: 0.1751(91.38)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:50] lb.utils.events INFO:  eta: 0:01:04  iteration: 666/1000  consumed samples: 10656  total_loss: 7.611  time: 0.1751(91.36)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:50] lb.utils.events INFO:  eta: 0:01:04  iteration: 667/1000  consumed samples: 10672  total_loss: 7.611  time: 0.1752(91.34)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:50] lb.utils.events INFO:  eta: 0:01:04  iteration: 668/1000  consumed samples: 10688  total_loss: 7.607  time: 0.1752(91.32)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:50] lb.utils.events INFO:  eta: 0:01:04  iteration: 669/1000  consumed samples: 10704  total_loss: 7.607  time: 0.1752(91.31)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:51] lb.utils.events INFO:  eta: 0:01:04  iteration: 670/1000  consumed samples: 10720  total_loss: 7.607  time: 0.1753(91.29)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:51] lb.utils.events INFO:  eta: 0:01:03  iteration: 671/1000  consumed samples: 10736  total_loss: 7.606  time: 0.1753(91.27)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:51] lb.utils.events INFO:  eta: 0:01:03  iteration: 672/1000  consumed samples: 10752  total_loss: 7.604  time: 0.1753(91.25)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:51] lb.utils.events INFO:  eta: 0:01:03  iteration: 673/1000  consumed samples: 10768  total_loss: 7.603  time: 0.1754(91.23)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:51] lb.utils.events INFO:  eta: 0:01:03  iteration: 674/1000  consumed samples: 10784  total_loss: 7.604  time: 0.1754(91.21)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:52] lb.utils.events INFO:  eta: 0:01:03  iteration: 675/1000  consumed samples: 10800  total_loss: 7.604  time: 0.1754(91.20)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:52] lb.utils.events INFO:  eta: 0:01:03  iteration: 676/1000  consumed samples: 10816  total_loss: 7.603  time: 0.1755(91.18)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:52] lb.utils.events INFO:  eta: 0:01:02  iteration: 677/1000  consumed samples: 10832  total_loss: 7.603  time: 0.1755(91.16)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:52] lb.utils.events INFO:  eta: 0:01:02  iteration: 678/1000  consumed samples: 10848  total_loss: 7.603  time: 0.1756(91.14)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:45:52] lb.utils.events INFO:  eta: 0:01:02  iteration: 679/1000  consumed samples: 10864  total_loss: 7.602  time: 0.1756(91.12)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:53] lb.utils.events INFO:  eta: 0:01:02  iteration: 680/1000  consumed samples: 10880  total_loss: 7.599  time: 0.1756(91.10)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:53] lb.utils.events INFO:  eta: 0:01:02  iteration: 681/1000  consumed samples: 10896  total_loss: 7.599  time: 0.1757(91.08)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:53] lb.utils.events INFO:  eta: 0:01:01  iteration: 682/1000  consumed samples: 10912  total_loss: 7.596  time: 0.1757(91.07)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:53] lb.utils.events INFO:  eta: 0:01:01  iteration: 683/1000  consumed samples: 10928  total_loss: 7.596  time: 0.1757(91.05)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:54] lb.utils.events INFO:  eta: 0:01:01  iteration: 684/1000  consumed samples: 10944  total_loss: 7.596  time: 0.1758(91.03)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:54] lb.utils.events INFO:  eta: 0:01:01  iteration: 685/1000  consumed samples: 10960  total_loss: 7.599  time: 0.1758(91.02)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:45:54] lb.utils.events INFO:  eta: 0:01:01  iteration: 686/1000  consumed samples: 10976  total_loss: 7.596  time: 0.1758(91.00)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:54] lb.utils.events INFO:  eta: 0:01:00  iteration: 687/1000  consumed samples: 10992  total_loss: 7.596  time: 0.1759(90.98)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:54] lb.utils.events INFO:  eta: 0:01:00  iteration: 688/1000  consumed samples: 11008  total_loss: 7.591  time: 0.1759(90.96)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:55] lb.utils.events INFO:  eta: 0:01:00  iteration: 689/1000  consumed samples: 11024  total_loss: 7.596  time: 0.1759(90.94)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:55] lb.utils.events INFO:  eta: 0:01:00  iteration: 690/1000  consumed samples: 11040  total_loss: 7.591  time: 0.1760(90.93)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:55] lb.utils.events INFO:  eta: 0:01:00  iteration: 691/1000  consumed samples: 11056  total_loss: 7.585  time: 0.1760(90.91)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:55] lb.utils.events INFO:  eta: 0:00:59  iteration: 692/1000  consumed samples: 11072  total_loss: 7.581  time: 0.1760(90.89)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:55] lb.utils.events INFO:  eta: 0:00:59  iteration: 693/1000  consumed samples: 11088  total_loss: 7.581  time: 0.1761(90.88)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:56] lb.utils.events INFO:  eta: 0:00:59  iteration: 694/1000  consumed samples: 11104  total_loss: 7.58  time: 0.1761(90.86)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:45:56] lb.utils.events INFO:  eta: 0:00:59  iteration: 695/1000  consumed samples: 11120  total_loss: 7.58  time: 0.1761(90.85)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:56] lb.utils.events INFO:  eta: 0:00:59  iteration: 696/1000  consumed samples: 11136  total_loss: 7.58  time: 0.1762(90.83)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:56] lb.utils.events INFO:  eta: 0:00:58  iteration: 697/1000  consumed samples: 11152  total_loss: 7.579  time: 0.1762(90.82)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:56] lb.utils.events INFO:  eta: 0:00:58  iteration: 698/1000  consumed samples: 11168  total_loss: 7.579  time: 0.1762(90.80)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:57] lb.utils.events INFO:  eta: 0:00:58  iteration: 699/1000  consumed samples: 11184  total_loss: 7.579  time: 0.1762(90.78)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:57] lb.utils.events INFO:  eta: 0:00:58  iteration: 700/1000  consumed samples: 11200  total_loss: 7.578  time: 0.1763(90.76)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:45:57] lb.utils.events INFO:  eta: 0:00:58  iteration: 701/1000  consumed samples: 11216  total_loss: 7.575  time: 0.1763(90.75)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:57] lb.utils.events INFO:  eta: 0:00:58  iteration: 702/1000  consumed samples: 11232  total_loss: 7.573  time: 0.1763(90.73)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:57] lb.utils.events INFO:  eta: 0:00:57  iteration: 703/1000  consumed samples: 11248  total_loss: 7.57  time: 0.1764(90.72)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:58] lb.utils.events INFO:  eta: 0:00:57  iteration: 704/1000  consumed samples: 11264  total_loss: 7.57  time: 0.1764(90.70)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:58] lb.utils.events INFO:  eta: 0:00:57  iteration: 705/1000  consumed samples: 11280  total_loss: 7.567  time: 0.1764(90.68)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:58] lb.utils.events INFO:  eta: 0:00:57  iteration: 706/1000  consumed samples: 11296  total_loss: 7.566  time: 0.1764(90.68)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:58] lb.utils.events INFO:  eta: 0:00:57  iteration: 707/1000  consumed samples: 11312  total_loss: 7.566  time: 0.1765(90.66)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:58] lb.utils.events INFO:  eta: 0:00:56  iteration: 708/1000  consumed samples: 11328  total_loss: 7.566  time: 0.1765(90.65)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:59] lb.utils.events INFO:  eta: 0:00:56  iteration: 709/1000  consumed samples: 11344  total_loss: 7.564  time: 0.1765(90.63)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:59] lb.utils.events INFO:  eta: 0:00:56  iteration: 710/1000  consumed samples: 11360  total_loss: 7.565  time: 0.1766(90.62)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:59] lb.utils.events INFO:  eta: 0:00:56  iteration: 711/1000  consumed samples: 11376  total_loss: 7.565  time: 0.1766(90.60)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:45:59] lb.utils.events INFO:  eta: 0:00:56  iteration: 712/1000  consumed samples: 11392  total_loss: 7.564  time: 0.1766(90.59)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:00] lb.utils.events INFO:  eta: 0:00:55  iteration: 713/1000  consumed samples: 11408  total_loss: 7.564  time: 0.1767(90.57)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:00] lb.utils.events INFO:  eta: 0:00:55  iteration: 714/1000  consumed samples: 11424  total_loss: 7.564  time: 0.1767(90.56)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:00] lb.utils.events INFO:  eta: 0:00:55  iteration: 715/1000  consumed samples: 11440  total_loss: 7.564  time: 0.1767(90.54)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:00] lb.utils.events INFO:  eta: 0:00:55  iteration: 716/1000  consumed samples: 11456  total_loss: 7.564  time: 0.1767(90.52)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:00] lb.utils.events INFO:  eta: 0:00:55  iteration: 717/1000  consumed samples: 11472  total_loss: 7.564  time: 0.1768(90.51)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:01] lb.utils.events INFO:  eta: 0:00:54  iteration: 718/1000  consumed samples: 11488  total_loss: 7.563  time: 0.1768(90.49)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:01] lb.utils.events INFO:  eta: 0:00:54  iteration: 719/1000  consumed samples: 11504  total_loss: 7.563  time: 0.1768(90.47)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:01] lb.utils.events INFO:  eta: 0:00:54  iteration: 720/1000  consumed samples: 11520  total_loss: 7.563  time: 0.1769(90.46)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:01] lb.utils.events INFO:  eta: 0:00:54  iteration: 721/1000  consumed samples: 11536  total_loss: 7.563  time: 0.1769(90.44)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:01] lb.utils.events INFO:  eta: 0:00:54  iteration: 722/1000  consumed samples: 11552  total_loss: 7.562  time: 0.1769(90.43)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:02] lb.utils.events INFO:  eta: 0:00:53  iteration: 723/1000  consumed samples: 11568  total_loss: 7.562  time: 0.1770(90.42)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:02] lb.utils.events INFO:  eta: 0:00:53  iteration: 724/1000  consumed samples: 11584  total_loss: 7.562  time: 0.1770(90.40)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:02] lb.utils.events INFO:  eta: 0:00:53  iteration: 725/1000  consumed samples: 11600  total_loss: 7.561  time: 0.1770(90.39)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:02] lb.utils.events INFO:  eta: 0:00:53  iteration: 726/1000  consumed samples: 11616  total_loss: 7.561  time: 0.1770(90.37)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:02] lb.utils.events INFO:  eta: 0:00:53  iteration: 727/1000  consumed samples: 11632  total_loss: 7.561  time: 0.1771(90.35)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:03] lb.utils.events INFO:  eta: 0:00:52  iteration: 728/1000  consumed samples: 11648  total_loss: 7.561  time: 0.1771(90.34)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:03] lb.utils.events INFO:  eta: 0:00:52  iteration: 729/1000  consumed samples: 11664  total_loss: 7.561  time: 0.1771(90.32)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:03] lb.utils.events INFO:  eta: 0:00:52  iteration: 730/1000  consumed samples: 11680  total_loss: 7.557  time: 0.1772(90.31)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:03] lb.utils.events INFO:  eta: 0:00:52  iteration: 731/1000  consumed samples: 11696  total_loss: 7.557  time: 0.1772(90.30)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:03] lb.utils.events INFO:  eta: 0:00:52  iteration: 732/1000  consumed samples: 11712  total_loss: 7.553  time: 0.1772(90.28)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:04] lb.utils.events INFO:  eta: 0:00:51  iteration: 733/1000  consumed samples: 11728  total_loss: 7.551  time: 0.1773(90.27)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:04] lb.utils.events INFO:  eta: 0:00:51  iteration: 734/1000  consumed samples: 11744  total_loss: 7.551  time: 0.1773(90.25)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:04] lb.utils.events INFO:  eta: 0:00:51  iteration: 735/1000  consumed samples: 11760  total_loss: 7.551  time: 0.1773(90.23)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:04] lb.utils.events INFO:  eta: 0:00:51  iteration: 736/1000  consumed samples: 11776  total_loss: 7.551  time: 0.1774(90.22)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:05] lb.utils.events INFO:  eta: 0:00:51  iteration: 737/1000  consumed samples: 11792  total_loss: 7.551  time: 0.1774(90.20)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:05] lb.utils.events INFO:  eta: 0:00:51  iteration: 738/1000  consumed samples: 11808  total_loss: 7.551  time: 0.1774(90.19)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:05] lb.utils.events INFO:  eta: 0:00:50  iteration: 739/1000  consumed samples: 11824  total_loss: 7.551  time: 0.1774(90.17)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:05] lb.utils.events INFO:  eta: 0:00:50  iteration: 740/1000  consumed samples: 11840  total_loss: 7.55  time: 0.1775(90.16)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:05] lb.utils.events INFO:  eta: 0:00:50  iteration: 741/1000  consumed samples: 11856  total_loss: 7.549  time: 0.1775(90.15)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:06] lb.utils.events INFO:  eta: 0:00:50  iteration: 742/1000  consumed samples: 11872  total_loss: 7.549  time: 0.1775(90.13)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:06] lb.utils.events INFO:  eta: 0:00:50  iteration: 743/1000  consumed samples: 11888  total_loss: 7.549  time: 0.1776(90.12)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:06] lb.utils.events INFO:  eta: 0:00:49  iteration: 744/1000  consumed samples: 11904  total_loss: 7.549  time: 0.1776(90.10)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:06] lb.utils.events INFO:  eta: 0:00:49  iteration: 745/1000  consumed samples: 11920  total_loss: 7.55  time: 0.1776(90.09)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:06] lb.utils.events INFO:  eta: 0:00:49  iteration: 746/1000  consumed samples: 11936  total_loss: 7.55  time: 0.1776(90.07)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:07] lb.utils.events INFO:  eta: 0:00:49  iteration: 747/1000  consumed samples: 11952  total_loss: 7.551  time: 0.1777(90.06)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:07] lb.utils.events INFO:  eta: 0:00:49  iteration: 748/1000  consumed samples: 11968  total_loss: 7.551  time: 0.1777(90.04)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:07] lb.utils.events INFO:  eta: 0:00:48  iteration: 749/1000  consumed samples: 11984  total_loss: 7.551  time: 0.1777(90.03)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:07] lb.utils.events INFO:  eta: 0:00:48  iteration: 750/1000  consumed samples: 12000  total_loss: 7.55  time: 0.1778(90.01)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:07] lb.utils.events INFO:  eta: 0:00:48  iteration: 751/1000  consumed samples: 12016  total_loss: 7.55  time: 0.1778(90.00)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:08] lb.utils.events INFO:  eta: 0:00:48  iteration: 752/1000  consumed samples: 12032  total_loss: 7.549  time: 0.1778(89.98)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:08] lb.utils.events INFO:  eta: 0:00:48  iteration: 753/1000  consumed samples: 12048  total_loss: 7.548  time: 0.1778(89.97)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:08] lb.utils.events INFO:  eta: 0:00:47  iteration: 754/1000  consumed samples: 12064  total_loss: 7.549  time: 0.1779(89.95)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:08] lb.utils.events INFO:  eta: 0:00:47  iteration: 755/1000  consumed samples: 12080  total_loss: 7.548  time: 0.1779(89.94)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:08] lb.utils.events INFO:  eta: 0:00:47  iteration: 756/1000  consumed samples: 12096  total_loss: 7.547  time: 0.1779(89.93)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:09] lb.utils.events INFO:  eta: 0:00:47  iteration: 757/1000  consumed samples: 12112  total_loss: 7.545  time: 0.1779(89.92)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:09] lb.utils.events INFO:  eta: 0:00:47  iteration: 758/1000  consumed samples: 12128  total_loss: 7.544  time: 0.1780(89.91)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:09] lb.utils.events INFO:  eta: 0:00:46  iteration: 759/1000  consumed samples: 12144  total_loss: 7.545  time: 0.1780(89.89)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:09] lb.utils.events INFO:  eta: 0:00:46  iteration: 760/1000  consumed samples: 12160  total_loss: 7.544  time: 0.1780(89.88)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:09] lb.utils.events INFO:  eta: 0:00:46  iteration: 761/1000  consumed samples: 12176  total_loss: 7.544  time: 0.1780(89.86)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:10] lb.utils.events INFO:  eta: 0:00:46  iteration: 762/1000  consumed samples: 12192  total_loss: 7.543  time: 0.1781(89.85)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:10] lb.utils.events INFO:  eta: 0:00:46  iteration: 763/1000  consumed samples: 12208  total_loss: 7.541  time: 0.1781(89.84)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:10] lb.utils.events INFO:  eta: 0:00:45  iteration: 764/1000  consumed samples: 12224  total_loss: 7.539  time: 0.1781(89.82)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:10] lb.utils.events INFO:  eta: 0:00:45  iteration: 765/1000  consumed samples: 12240  total_loss: 7.539  time: 0.1782(89.81)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:11] lb.utils.events INFO:  eta: 0:00:45  iteration: 766/1000  consumed samples: 12256  total_loss: 7.539  time: 0.1782(89.80)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:11] lb.utils.events INFO:  eta: 0:00:45  iteration: 767/1000  consumed samples: 12272  total_loss: 7.539  time: 0.1782(89.78)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:11] lb.utils.events INFO:  eta: 0:00:45  iteration: 768/1000  consumed samples: 12288  total_loss: 7.539  time: 0.1782(89.77)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:11] lb.utils.events INFO:  eta: 0:00:44  iteration: 769/1000  consumed samples: 12304  total_loss: 7.537  time: 0.1783(89.75)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:11] lb.utils.events INFO:  eta: 0:00:44  iteration: 770/1000  consumed samples: 12320  total_loss: 7.539  time: 0.1783(89.74)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:12] lb.utils.events INFO:  eta: 0:00:44  iteration: 771/1000  consumed samples: 12336  total_loss: 7.537  time: 0.1783(89.73)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:12] lb.utils.events INFO:  eta: 0:00:44  iteration: 772/1000  consumed samples: 12352  total_loss: 7.537  time: 0.1783(89.72)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:12] lb.utils.events INFO:  eta: 0:00:44  iteration: 773/1000  consumed samples: 12368  total_loss: 7.537  time: 0.1784(89.70)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:12] lb.utils.events INFO:  eta: 0:00:44  iteration: 774/1000  consumed samples: 12384  total_loss: 7.537  time: 0.1784(89.69)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:12] lb.utils.events INFO:  eta: 0:00:43  iteration: 775/1000  consumed samples: 12400  total_loss: 7.535  time: 0.1784(89.67)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:13] lb.utils.events INFO:  eta: 0:00:43  iteration: 776/1000  consumed samples: 12416  total_loss: 7.535  time: 0.1784(89.66)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:13] lb.utils.events INFO:  eta: 0:00:43  iteration: 777/1000  consumed samples: 12432  total_loss: 7.534  time: 0.1785(89.65)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:13] lb.utils.events INFO:  eta: 0:00:43  iteration: 778/1000  consumed samples: 12448  total_loss: 7.533  time: 0.1785(89.63)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:13] lb.utils.events INFO:  eta: 0:00:43  iteration: 779/1000  consumed samples: 12464  total_loss: 7.533  time: 0.1785(89.62)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:13] lb.utils.events INFO:  eta: 0:00:42  iteration: 780/1000  consumed samples: 12480  total_loss: 7.533  time: 0.1786(89.60)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:14] lb.utils.events INFO:  eta: 0:00:42  iteration: 781/1000  consumed samples: 12496  total_loss: 7.533  time: 0.1786(89.59)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:14] lb.utils.events INFO:  eta: 0:00:42  iteration: 782/1000  consumed samples: 12512  total_loss: 7.532  time: 0.1786(89.58)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:14] lb.utils.events INFO:  eta: 0:00:42  iteration: 783/1000  consumed samples: 12528  total_loss: 7.533  time: 0.1786(89.56)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:14] lb.utils.events INFO:  eta: 0:00:42  iteration: 784/1000  consumed samples: 12544  total_loss: 7.533  time: 0.1787(89.55)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:14] lb.utils.events INFO:  eta: 0:00:41  iteration: 785/1000  consumed samples: 12560  total_loss: 7.533  time: 0.1787(89.54)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:15] lb.utils.events INFO:  eta: 0:00:41  iteration: 786/1000  consumed samples: 12576  total_loss: 7.533  time: 0.1787(89.52)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:15] lb.utils.events INFO:  eta: 0:00:41  iteration: 787/1000  consumed samples: 12592  total_loss: 7.533  time: 0.1788(89.51)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:15] lb.utils.events INFO:  eta: 0:00:41  iteration: 788/1000  consumed samples: 12608  total_loss: 7.533  time: 0.1788(89.50)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:15] lb.utils.events INFO:  eta: 0:00:41  iteration: 789/1000  consumed samples: 12624  total_loss: 7.533  time: 0.1788(89.48)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:16] lb.utils.events INFO:  eta: 0:00:40  iteration: 790/1000  consumed samples: 12640  total_loss: 7.533  time: 0.1788(89.48)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:16] lb.utils.events INFO:  eta: 0:00:40  iteration: 791/1000  consumed samples: 12656  total_loss: 7.532  time: 0.1788(89.47)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:16] lb.utils.events INFO:  eta: 0:00:40  iteration: 792/1000  consumed samples: 12672  total_loss: 7.532  time: 0.1789(89.46)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:16] lb.utils.events INFO:  eta: 0:00:40  iteration: 793/1000  consumed samples: 12688  total_loss: 7.531  time: 0.1789(89.45)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:16] lb.utils.events INFO:  eta: 0:00:40  iteration: 794/1000  consumed samples: 12704  total_loss: 7.53  time: 0.1789(89.43)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:17] lb.utils.events INFO:  eta: 0:00:39  iteration: 795/1000  consumed samples: 12720  total_loss: 7.531  time: 0.1789(89.42)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:17] lb.utils.events INFO:  eta: 0:00:39  iteration: 796/1000  consumed samples: 12736  total_loss: 7.531  time: 0.1789(89.41)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:17] lb.utils.events INFO:  eta: 0:00:39  iteration: 797/1000  consumed samples: 12752  total_loss: 7.531  time: 0.1790(89.41)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:17] lb.utils.events INFO:  eta: 0:00:39  iteration: 798/1000  consumed samples: 12768  total_loss: 7.53  time: 0.1790(89.40)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:17] lb.utils.events INFO:  eta: 0:00:39  iteration: 799/1000  consumed samples: 12784  total_loss: 7.53  time: 0.1790(89.38)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:18] lb.utils.events INFO:  eta: 0:00:38  iteration: 800/1000  consumed samples: 12800  total_loss: 7.53  time: 0.1790(89.37)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:18] lb.utils.events INFO:  eta: 0:00:38  iteration: 801/1000  consumed samples: 12816  total_loss: 7.53  time: 0.1791(89.36)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:18] lb.utils.events INFO:  eta: 0:00:38  iteration: 802/1000  consumed samples: 12832  total_loss: 7.53  time: 0.1791(89.35)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:18] lb.utils.events INFO:  eta: 0:00:38  iteration: 803/1000  consumed samples: 12848  total_loss: 7.529  time: 0.1791(89.33)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:18] lb.utils.events INFO:  eta: 0:00:38  iteration: 804/1000  consumed samples: 12864  total_loss: 7.528  time: 0.1791(89.32)  data_time: 0.0024  lr: 9.00e-05  
[01/19 15:46:19] lb.utils.events INFO:  eta: 0:00:37  iteration: 805/1000  consumed samples: 12880  total_loss: 7.526  time: 0.1792(89.31)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:19] lb.utils.events INFO:  eta: 0:00:37  iteration: 806/1000  consumed samples: 12896  total_loss: 7.526  time: 0.1792(89.29)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:19] lb.utils.events INFO:  eta: 0:00:37  iteration: 807/1000  consumed samples: 12912  total_loss: 7.526  time: 0.1792(89.28)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:19] lb.utils.events INFO:  eta: 0:00:37  iteration: 808/1000  consumed samples: 12928  total_loss: 7.528  time: 0.1792(89.27)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:19] lb.utils.events INFO:  eta: 0:00:37  iteration: 809/1000  consumed samples: 12944  total_loss: 7.528  time: 0.1793(89.26)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:20] lb.utils.events INFO:  eta: 0:00:36  iteration: 810/1000  consumed samples: 12960  total_loss: 7.526  time: 0.1793(89.24)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:20] lb.utils.events INFO:  eta: 0:00:36  iteration: 811/1000  consumed samples: 12976  total_loss: 7.526  time: 0.1793(89.23)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:20] lb.utils.events INFO:  eta: 0:00:36  iteration: 812/1000  consumed samples: 12992  total_loss: 7.525  time: 0.1793(89.22)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:20] lb.utils.events INFO:  eta: 0:00:36  iteration: 813/1000  consumed samples: 13008  total_loss: 7.525  time: 0.1794(89.21)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:21] lb.utils.events INFO:  eta: 0:00:36  iteration: 814/1000  consumed samples: 13024  total_loss: 7.525  time: 0.1794(89.19)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:21] lb.utils.events INFO:  eta: 0:00:36  iteration: 815/1000  consumed samples: 13040  total_loss: 7.525  time: 0.1794(89.18)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:21] lb.utils.events INFO:  eta: 0:00:35  iteration: 816/1000  consumed samples: 13056  total_loss: 7.525  time: 0.1794(89.17)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:21] lb.utils.events INFO:  eta: 0:00:35  iteration: 817/1000  consumed samples: 13072  total_loss: 7.525  time: 0.1795(89.16)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:21] lb.utils.events INFO:  eta: 0:00:35  iteration: 818/1000  consumed samples: 13088  total_loss: 7.525  time: 0.1795(89.14)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:22] lb.utils.events INFO:  eta: 0:00:35  iteration: 819/1000  consumed samples: 13104  total_loss: 7.524  time: 0.1795(89.14)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:22] lb.utils.events INFO:  eta: 0:00:35  iteration: 820/1000  consumed samples: 13120  total_loss: 7.524  time: 0.1795(89.12)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:22] lb.utils.events INFO:  eta: 0:00:34  iteration: 821/1000  consumed samples: 13136  total_loss: 7.524  time: 0.1795(89.11)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:22] lb.utils.events INFO:  eta: 0:00:34  iteration: 822/1000  consumed samples: 13152  total_loss: 7.524  time: 0.1796(89.10)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:22] lb.utils.events INFO:  eta: 0:00:34  iteration: 823/1000  consumed samples: 13168  total_loss: 7.524  time: 0.1796(89.09)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:23] lb.utils.events INFO:  eta: 0:00:34  iteration: 824/1000  consumed samples: 13184  total_loss: 7.525  time: 0.1796(89.07)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:23] lb.utils.events INFO:  eta: 0:00:34  iteration: 825/1000  consumed samples: 13200  total_loss: 7.524  time: 0.1796(89.06)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:23] lb.utils.events INFO:  eta: 0:00:33  iteration: 826/1000  consumed samples: 13216  total_loss: 7.523  time: 0.1797(89.06)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:23] lb.utils.events INFO:  eta: 0:00:33  iteration: 827/1000  consumed samples: 13232  total_loss: 7.523  time: 0.1797(89.05)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:23] lb.utils.events INFO:  eta: 0:00:33  iteration: 828/1000  consumed samples: 13248  total_loss: 7.521  time: 0.1797(89.03)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:24] lb.utils.events INFO:  eta: 0:00:33  iteration: 829/1000  consumed samples: 13264  total_loss: 7.521  time: 0.1797(89.03)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:24] lb.utils.events INFO:  eta: 0:00:33  iteration: 830/1000  consumed samples: 13280  total_loss: 7.52  time: 0.1797(89.02)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:24] lb.utils.events INFO:  eta: 0:00:32  iteration: 831/1000  consumed samples: 13296  total_loss: 7.519  time: 0.1798(89.01)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:24] lb.utils.events INFO:  eta: 0:00:32  iteration: 832/1000  consumed samples: 13312  total_loss: 7.518  time: 0.1798(89.00)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:24] lb.utils.events INFO:  eta: 0:00:32  iteration: 833/1000  consumed samples: 13328  total_loss: 7.518  time: 0.1798(88.99)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:25] lb.utils.events INFO:  eta: 0:00:32  iteration: 834/1000  consumed samples: 13344  total_loss: 7.516  time: 0.1798(88.98)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:25] lb.utils.events INFO:  eta: 0:00:32  iteration: 835/1000  consumed samples: 13360  total_loss: 7.516  time: 0.1798(88.96)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:25] lb.utils.events INFO:  eta: 0:00:31  iteration: 836/1000  consumed samples: 13376  total_loss: 7.518  time: 0.1799(88.96)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:25] lb.utils.events INFO:  eta: 0:00:31  iteration: 837/1000  consumed samples: 13392  total_loss: 7.518  time: 0.1799(88.94)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:25] lb.utils.events INFO:  eta: 0:00:31  iteration: 838/1000  consumed samples: 13408  total_loss: 7.518  time: 0.1799(88.93)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:26] lb.utils.events INFO:  eta: 0:00:31  iteration: 839/1000  consumed samples: 13424  total_loss: 7.519  time: 0.1799(88.92)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:26] lb.utils.events INFO:  eta: 0:00:31  iteration: 840/1000  consumed samples: 13440  total_loss: 7.518  time: 0.1800(88.91)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:26] lb.utils.events INFO:  eta: 0:00:30  iteration: 841/1000  consumed samples: 13456  total_loss: 7.516  time: 0.1800(88.90)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:26] lb.utils.events INFO:  eta: 0:00:30  iteration: 842/1000  consumed samples: 13472  total_loss: 7.516  time: 0.1800(88.89)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:27] lb.utils.events INFO:  eta: 0:00:30  iteration: 843/1000  consumed samples: 13488  total_loss: 7.516  time: 0.1800(88.88)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:27] lb.utils.events INFO:  eta: 0:00:30  iteration: 844/1000  consumed samples: 13504  total_loss: 7.516  time: 0.1801(88.86)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:27] lb.utils.events INFO:  eta: 0:00:30  iteration: 845/1000  consumed samples: 13520  total_loss: 7.515  time: 0.1801(88.85)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:27] lb.utils.events INFO:  eta: 0:00:29  iteration: 846/1000  consumed samples: 13536  total_loss: 7.513  time: 0.1801(88.84)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:27] lb.utils.events INFO:  eta: 0:00:29  iteration: 847/1000  consumed samples: 13552  total_loss: 7.51  time: 0.1801(88.83)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:28] lb.utils.events INFO:  eta: 0:00:29  iteration: 848/1000  consumed samples: 13568  total_loss: 7.51  time: 0.1801(88.82)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:28] lb.utils.events INFO:  eta: 0:00:29  iteration: 849/1000  consumed samples: 13584  total_loss: 7.51  time: 0.1802(88.81)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:28] lb.utils.events INFO:  eta: 0:00:29  iteration: 850/1000  consumed samples: 13600  total_loss: 7.51  time: 0.1802(88.80)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:28] lb.utils.events INFO:  eta: 0:00:28  iteration: 851/1000  consumed samples: 13616  total_loss: 7.513  time: 0.1802(88.79)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:28] lb.utils.events INFO:  eta: 0:00:28  iteration: 852/1000  consumed samples: 13632  total_loss: 7.513  time: 0.1802(88.78)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:29] lb.utils.events INFO:  eta: 0:00:28  iteration: 853/1000  consumed samples: 13648  total_loss: 7.51  time: 0.1802(88.77)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:29] lb.utils.events INFO:  eta: 0:00:28  iteration: 854/1000  consumed samples: 13664  total_loss: 7.509  time: 0.1803(88.76)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:29] lb.utils.events INFO:  eta: 0:00:28  iteration: 855/1000  consumed samples: 13680  total_loss: 7.51  time: 0.1803(88.75)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:29] lb.utils.events INFO:  eta: 0:00:27  iteration: 856/1000  consumed samples: 13696  total_loss: 7.51  time: 0.1803(88.75)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:29] lb.utils.events INFO:  eta: 0:00:27  iteration: 857/1000  consumed samples: 13712  total_loss: 7.509  time: 0.1803(88.73)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 858/1000  consumed samples: 13728  total_loss: 7.51  time: 0.1803(88.73)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 859/1000  consumed samples: 13744  total_loss: 7.51  time: 0.1804(88.72)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 860/1000  consumed samples: 13760  total_loss: 7.51  time: 0.1804(88.71)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 861/1000  consumed samples: 13776  total_loss: 7.509  time: 0.1804(88.69)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:30] lb.utils.events INFO:  eta: 0:00:26  iteration: 862/1000  consumed samples: 13792  total_loss: 7.509  time: 0.1804(88.69)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:46:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 863/1000  consumed samples: 13808  total_loss: 7.509  time: 0.1804(88.67)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:46:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 864/1000  consumed samples: 13824  total_loss: 7.509  time: 0.1805(88.67)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:46:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 865/1000  consumed samples: 13840  total_loss: 7.506  time: 0.1805(88.65)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 866/1000  consumed samples: 13856  total_loss: 7.506  time: 0.1805(88.64)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:32] lb.utils.events INFO:  eta: 0:00:25  iteration: 867/1000  consumed samples: 13872  total_loss: 7.503  time: 0.1805(88.63)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:32] lb.utils.events INFO:  eta: 0:00:25  iteration: 868/1000  consumed samples: 13888  total_loss: 7.506  time: 0.1805(88.62)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:32] lb.utils.events INFO:  eta: 0:00:25  iteration: 869/1000  consumed samples: 13904  total_loss: 7.503  time: 0.1806(88.61)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:32] lb.utils.events INFO:  eta: 0:00:25  iteration: 870/1000  consumed samples: 13920  total_loss: 7.502  time: 0.1806(88.60)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:32] lb.utils.events INFO:  eta: 0:00:25  iteration: 871/1000  consumed samples: 13936  total_loss: 7.502  time: 0.1806(88.59)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:33] lb.utils.events INFO:  eta: 0:00:24  iteration: 872/1000  consumed samples: 13952  total_loss: 7.502  time: 0.1806(88.58)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:33] lb.utils.events INFO:  eta: 0:00:24  iteration: 873/1000  consumed samples: 13968  total_loss: 7.502  time: 0.1807(88.57)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:33] lb.utils.events INFO:  eta: 0:00:24  iteration: 874/1000  consumed samples: 13984  total_loss: 7.502  time: 0.1807(88.56)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:33] lb.utils.events INFO:  eta: 0:00:24  iteration: 875/1000  consumed samples: 14000  total_loss: 7.502  time: 0.1807(88.55)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:33] lb.utils.events INFO:  eta: 0:00:24  iteration: 876/1000  consumed samples: 14016  total_loss: 7.502  time: 0.1807(88.54)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:34] lb.utils.events INFO:  eta: 0:00:23  iteration: 877/1000  consumed samples: 14032  total_loss: 7.502  time: 0.1807(88.53)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:34] lb.utils.events INFO:  eta: 0:00:23  iteration: 878/1000  consumed samples: 14048  total_loss: 7.501  time: 0.1808(88.52)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:34] lb.utils.events INFO:  eta: 0:00:23  iteration: 879/1000  consumed samples: 14064  total_loss: 7.499  time: 0.1808(88.51)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:34] lb.utils.events INFO:  eta: 0:00:23  iteration: 880/1000  consumed samples: 14080  total_loss: 7.499  time: 0.1808(88.50)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:34] lb.utils.events INFO:  eta: 0:00:23  iteration: 881/1000  consumed samples: 14096  total_loss: 7.498  time: 0.1808(88.50)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:35] lb.utils.events INFO:  eta: 0:00:22  iteration: 882/1000  consumed samples: 14112  total_loss: 7.498  time: 0.1808(88.49)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:35] lb.utils.events INFO:  eta: 0:00:22  iteration: 883/1000  consumed samples: 14128  total_loss: 7.497  time: 0.1808(88.48)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:35] lb.utils.events INFO:  eta: 0:00:22  iteration: 884/1000  consumed samples: 14144  total_loss: 7.497  time: 0.1809(88.47)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:35] lb.utils.events INFO:  eta: 0:00:22  iteration: 885/1000  consumed samples: 14160  total_loss: 7.495  time: 0.1809(88.46)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:35] lb.utils.events INFO:  eta: 0:00:22  iteration: 886/1000  consumed samples: 14176  total_loss: 7.495  time: 0.1809(88.45)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:36] lb.utils.events INFO:  eta: 0:00:21  iteration: 887/1000  consumed samples: 14192  total_loss: 7.497  time: 0.1809(88.44)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:36] lb.utils.events INFO:  eta: 0:00:21  iteration: 888/1000  consumed samples: 14208  total_loss: 7.497  time: 0.1809(88.43)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:36] lb.utils.events INFO:  eta: 0:00:21  iteration: 889/1000  consumed samples: 14224  total_loss: 7.497  time: 0.1810(88.42)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:36] lb.utils.events INFO:  eta: 0:00:21  iteration: 890/1000  consumed samples: 14240  total_loss: 7.495  time: 0.1810(88.41)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:37] lb.utils.events INFO:  eta: 0:00:21  iteration: 891/1000  consumed samples: 14256  total_loss: 7.495  time: 0.1810(88.40)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:37] lb.utils.events INFO:  eta: 0:00:20  iteration: 892/1000  consumed samples: 14272  total_loss: 7.494  time: 0.1810(88.39)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:37] lb.utils.events INFO:  eta: 0:00:20  iteration: 893/1000  consumed samples: 14288  total_loss: 7.495  time: 0.1810(88.38)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:37] lb.utils.events INFO:  eta: 0:00:20  iteration: 894/1000  consumed samples: 14304  total_loss: 7.495  time: 0.1811(88.37)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:37] lb.utils.events INFO:  eta: 0:00:20  iteration: 895/1000  consumed samples: 14320  total_loss: 7.497  time: 0.1811(88.36)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:38] lb.utils.events INFO:  eta: 0:00:20  iteration: 896/1000  consumed samples: 14336  total_loss: 7.495  time: 0.1811(88.35)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:38] lb.utils.events INFO:  eta: 0:00:19  iteration: 897/1000  consumed samples: 14352  total_loss: 7.495  time: 0.1811(88.35)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:38] lb.utils.events INFO:  eta: 0:00:19  iteration: 898/1000  consumed samples: 14368  total_loss: 7.495  time: 0.1811(88.34)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:38] lb.utils.events INFO:  eta: 0:00:19  iteration: 899/1000  consumed samples: 14384  total_loss: 7.495  time: 0.1811(88.33)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:38] lb.utils.events INFO:  eta: 0:00:19  iteration: 900/1000  consumed samples: 14400  total_loss: 7.494  time: 0.1812(88.32)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:39] lb.utils.events INFO:  eta: 0:00:19  iteration: 901/1000  consumed samples: 14416  total_loss: 7.494  time: 0.1812(88.31)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:39] lb.utils.events INFO:  eta: 0:00:18  iteration: 902/1000  consumed samples: 14432  total_loss: 7.491  time: 0.1812(88.30)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:39] lb.utils.events INFO:  eta: 0:00:18  iteration: 903/1000  consumed samples: 14448  total_loss: 7.491  time: 0.1812(88.29)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:39] lb.utils.events INFO:  eta: 0:00:18  iteration: 904/1000  consumed samples: 14464  total_loss: 7.494  time: 0.1812(88.28)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:39] lb.utils.events INFO:  eta: 0:00:18  iteration: 905/1000  consumed samples: 14480  total_loss: 7.494  time: 0.1813(88.27)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:40] lb.utils.events INFO:  eta: 0:00:18  iteration: 906/1000  consumed samples: 14496  total_loss: 7.495  time: 0.1813(88.27)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:40] lb.utils.events INFO:  eta: 0:00:18  iteration: 907/1000  consumed samples: 14512  total_loss: 7.495  time: 0.1813(88.26)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:40] lb.utils.events INFO:  eta: 0:00:17  iteration: 908/1000  consumed samples: 14528  total_loss: 7.494  time: 0.1813(88.25)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:40] lb.utils.events INFO:  eta: 0:00:17  iteration: 909/1000  consumed samples: 14544  total_loss: 7.494  time: 0.1813(88.24)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:40] lb.utils.events INFO:  eta: 0:00:17  iteration: 910/1000  consumed samples: 14560  total_loss: 7.491  time: 0.1813(88.24)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:41] lb.utils.events INFO:  eta: 0:00:17  iteration: 911/1000  consumed samples: 14576  total_loss: 7.494  time: 0.1814(88.22)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:41] lb.utils.events INFO:  eta: 0:00:17  iteration: 912/1000  consumed samples: 14592  total_loss: 7.494  time: 0.1814(88.22)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:41] lb.utils.events INFO:  eta: 0:00:16  iteration: 913/1000  consumed samples: 14608  total_loss: 7.494  time: 0.1814(88.21)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:41] lb.utils.events INFO:  eta: 0:00:16  iteration: 914/1000  consumed samples: 14624  total_loss: 7.491  time: 0.1814(88.20)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:41] lb.utils.events INFO:  eta: 0:00:16  iteration: 915/1000  consumed samples: 14640  total_loss: 7.486  time: 0.1814(88.19)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:42] lb.utils.events INFO:  eta: 0:00:16  iteration: 916/1000  consumed samples: 14656  total_loss: 7.48  time: 0.1814(88.18)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:42] lb.utils.events INFO:  eta: 0:00:16  iteration: 917/1000  consumed samples: 14672  total_loss: 7.486  time: 0.1815(88.17)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:42] lb.utils.events INFO:  eta: 0:00:15  iteration: 918/1000  consumed samples: 14688  total_loss: 7.486  time: 0.1815(88.16)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:42] lb.utils.events INFO:  eta: 0:00:15  iteration: 919/1000  consumed samples: 14704  total_loss: 7.491  time: 0.1815(88.15)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:43] lb.utils.events INFO:  eta: 0:00:15  iteration: 920/1000  consumed samples: 14720  total_loss: 7.486  time: 0.1815(88.14)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:43] lb.utils.events INFO:  eta: 0:00:15  iteration: 921/1000  consumed samples: 14736  total_loss: 7.48  time: 0.1815(88.13)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:43] lb.utils.events INFO:  eta: 0:00:15  iteration: 922/1000  consumed samples: 14752  total_loss: 7.48  time: 0.1816(88.13)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:43] lb.utils.events INFO:  eta: 0:00:14  iteration: 923/1000  consumed samples: 14768  total_loss: 7.478  time: 0.1816(88.12)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:43] lb.utils.events INFO:  eta: 0:00:14  iteration: 924/1000  consumed samples: 14784  total_loss: 7.478  time: 0.1816(88.11)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:44] lb.utils.events INFO:  eta: 0:00:14  iteration: 925/1000  consumed samples: 14800  total_loss: 7.478  time: 0.1816(88.10)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:44] lb.utils.events INFO:  eta: 0:00:14  iteration: 926/1000  consumed samples: 14816  total_loss: 7.477  time: 0.1816(88.09)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:44] lb.utils.events INFO:  eta: 0:00:14  iteration: 927/1000  consumed samples: 14832  total_loss: 7.475  time: 0.1816(88.08)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:44] lb.utils.events INFO:  eta: 0:00:13  iteration: 928/1000  consumed samples: 14848  total_loss: 7.477  time: 0.1817(88.08)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:44] lb.utils.events INFO:  eta: 0:00:13  iteration: 929/1000  consumed samples: 14864  total_loss: 7.477  time: 0.1817(88.07)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:45] lb.utils.events INFO:  eta: 0:00:13  iteration: 930/1000  consumed samples: 14880  total_loss: 7.475  time: 0.1817(88.06)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:45] lb.utils.events INFO:  eta: 0:00:13  iteration: 931/1000  consumed samples: 14896  total_loss: 7.475  time: 0.1817(88.06)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:45] lb.utils.events INFO:  eta: 0:00:13  iteration: 932/1000  consumed samples: 14912  total_loss: 7.472  time: 0.1817(88.05)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:45] lb.utils.events INFO:  eta: 0:00:12  iteration: 933/1000  consumed samples: 14928  total_loss: 7.472  time: 0.1817(88.04)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:45] lb.utils.events INFO:  eta: 0:00:12  iteration: 934/1000  consumed samples: 14944  total_loss: 7.47  time: 0.1817(88.03)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:46] lb.utils.events INFO:  eta: 0:00:12  iteration: 935/1000  consumed samples: 14960  total_loss: 7.47  time: 0.1818(88.03)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:46] lb.utils.events INFO:  eta: 0:00:12  iteration: 936/1000  consumed samples: 14976  total_loss: 7.468  time: 0.1818(88.02)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:46] lb.utils.events INFO:  eta: 0:00:12  iteration: 937/1000  consumed samples: 14992  total_loss: 7.466  time: 0.1818(88.01)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:46] lb.utils.events INFO:  eta: 0:00:11  iteration: 938/1000  consumed samples: 15008  total_loss: 7.466  time: 0.1818(88.01)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:46] lb.utils.events INFO:  eta: 0:00:11  iteration: 939/1000  consumed samples: 15024  total_loss: 7.465  time: 0.1818(88.00)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:47] lb.utils.events INFO:  eta: 0:00:11  iteration: 940/1000  consumed samples: 15040  total_loss: 7.465  time: 0.1818(87.99)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:47] lb.utils.events INFO:  eta: 0:00:11  iteration: 941/1000  consumed samples: 15056  total_loss: 7.466  time: 0.1819(87.98)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:47] lb.utils.events INFO:  eta: 0:00:11  iteration: 942/1000  consumed samples: 15072  total_loss: 7.464  time: 0.1819(87.97)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:47] lb.utils.events INFO:  eta: 0:00:10  iteration: 943/1000  consumed samples: 15088  total_loss: 7.464  time: 0.1819(87.97)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:48] lb.utils.events INFO:  eta: 0:00:10  iteration: 944/1000  consumed samples: 15104  total_loss: 7.463  time: 0.1819(87.96)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:48] lb.utils.events INFO:  eta: 0:00:10  iteration: 945/1000  consumed samples: 15120  total_loss: 7.461  time: 0.1819(87.95)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:48] lb.utils.events INFO:  eta: 0:00:10  iteration: 946/1000  consumed samples: 15136  total_loss: 7.458  time: 0.1819(87.94)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:48] lb.utils.events INFO:  eta: 0:00:10  iteration: 947/1000  consumed samples: 15152  total_loss: 7.455  time: 0.1820(87.93)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:48] lb.utils.events INFO:  eta: 0:00:09  iteration: 948/1000  consumed samples: 15168  total_loss: 7.455  time: 0.1820(87.92)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:49] lb.utils.events INFO:  eta: 0:00:09  iteration: 949/1000  consumed samples: 15184  total_loss: 7.455  time: 0.1820(87.92)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:49] lb.utils.events INFO:  eta: 0:00:09  iteration: 950/1000  consumed samples: 15200  total_loss: 7.455  time: 0.1820(87.91)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:46:49] lb.utils.events INFO:  eta: 0:00:09  iteration: 951/1000  consumed samples: 15216  total_loss: 7.454  time: 0.1820(87.90)  data_time: 0.0040  lr: 9.00e-05  
[01/19 15:46:49] lb.utils.events INFO:  eta: 0:00:09  iteration: 952/1000  consumed samples: 15232  total_loss: 7.454  time: 0.1820(87.89)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:49] lb.utils.events INFO:  eta: 0:00:09  iteration: 953/1000  consumed samples: 15248  total_loss: 7.455  time: 0.1821(87.89)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:50] lb.utils.events INFO:  eta: 0:00:08  iteration: 954/1000  consumed samples: 15264  total_loss: 7.455  time: 0.1821(87.88)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:50] lb.utils.events INFO:  eta: 0:00:08  iteration: 955/1000  consumed samples: 15280  total_loss: 7.458  time: 0.1821(87.86)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:50] lb.utils.events INFO:  eta: 0:00:08  iteration: 956/1000  consumed samples: 15296  total_loss: 7.458  time: 0.1821(87.86)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:50] lb.utils.events INFO:  eta: 0:00:08  iteration: 957/1000  consumed samples: 15312  total_loss: 7.458  time: 0.1821(87.85)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:50] lb.utils.events INFO:  eta: 0:00:08  iteration: 958/1000  consumed samples: 15328  total_loss: 7.455  time: 0.1821(87.84)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:51] lb.utils.events INFO:  eta: 0:00:07  iteration: 959/1000  consumed samples: 15344  total_loss: 7.454  time: 0.1822(87.83)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:51] lb.utils.events INFO:  eta: 0:00:07  iteration: 960/1000  consumed samples: 15360  total_loss: 7.455  time: 0.1822(87.82)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:51] lb.utils.events INFO:  eta: 0:00:07  iteration: 961/1000  consumed samples: 15376  total_loss: 7.454  time: 0.1822(87.82)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:51] lb.utils.events INFO:  eta: 0:00:07  iteration: 962/1000  consumed samples: 15392  total_loss: 7.454  time: 0.1822(87.81)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:51] lb.utils.events INFO:  eta: 0:00:07  iteration: 963/1000  consumed samples: 15408  total_loss: 7.454  time: 0.1822(87.80)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:52] lb.utils.events INFO:  eta: 0:00:06  iteration: 964/1000  consumed samples: 15424  total_loss: 7.455  time: 0.1823(87.79)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:52] lb.utils.events INFO:  eta: 0:00:06  iteration: 965/1000  consumed samples: 15440  total_loss: 7.458  time: 0.1823(87.78)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:52] lb.utils.events INFO:  eta: 0:00:06  iteration: 966/1000  consumed samples: 15456  total_loss: 7.455  time: 0.1823(87.77)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:52] lb.utils.events INFO:  eta: 0:00:06  iteration: 967/1000  consumed samples: 15472  total_loss: 7.454  time: 0.1823(87.77)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:52] lb.utils.events INFO:  eta: 0:00:06  iteration: 968/1000  consumed samples: 15488  total_loss: 7.452  time: 0.1823(87.76)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:53] lb.utils.events INFO:  eta: 0:00:05  iteration: 969/1000  consumed samples: 15504  total_loss: 7.449  time: 0.1823(87.75)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:53] lb.utils.events INFO:  eta: 0:00:05  iteration: 970/1000  consumed samples: 15520  total_loss: 7.445  time: 0.1823(87.74)  data_time: 0.0026  lr: 9.00e-05  
[01/19 15:46:53] lb.utils.events INFO:  eta: 0:00:05  iteration: 971/1000  consumed samples: 15536  total_loss: 7.449  time: 0.1824(87.74)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:53] lb.utils.events INFO:  eta: 0:00:05  iteration: 972/1000  consumed samples: 15552  total_loss: 7.449  time: 0.1824(87.73)  data_time: 0.0025  lr: 9.00e-05  
[01/19 15:46:54] lb.utils.events INFO:  eta: 0:00:05  iteration: 973/1000  consumed samples: 15568  total_loss: 7.449  time: 0.1824(87.72)  data_time: 0.0027  lr: 9.00e-05  
[01/19 15:46:54] lb.utils.events INFO:  eta: 0:00:04  iteration: 974/1000  consumed samples: 15584  total_loss: 7.449  time: 0.1824(87.71)  data_time: 0.0028  lr: 9.00e-05  
[01/19 15:46:54] lb.utils.events INFO:  eta: 0:00:04  iteration: 975/1000  consumed samples: 15600  total_loss: 7.445  time: 0.1824(87.71)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:54] lb.utils.events INFO:  eta: 0:00:04  iteration: 976/1000  consumed samples: 15616  total_loss: 7.449  time: 0.1824(87.70)  data_time: 0.0029  lr: 9.00e-05  
[01/19 15:46:54] lb.utils.events INFO:  eta: 0:00:04  iteration: 977/1000  consumed samples: 15632  total_loss: 7.445  time: 0.1825(87.69)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:55] lb.utils.events INFO:  eta: 0:00:04  iteration: 978/1000  consumed samples: 15648  total_loss: 7.445  time: 0.1825(87.68)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:55] lb.utils.events INFO:  eta: 0:00:03  iteration: 979/1000  consumed samples: 15664  total_loss: 7.445  time: 0.1825(87.68)  data_time: 0.0030  lr: 9.00e-05  
[01/19 15:46:55] lb.utils.events INFO:  eta: 0:00:03  iteration: 980/1000  consumed samples: 15680  total_loss: 7.443  time: 0.1825(87.69)  data_time: 0.0031  lr: 9.00e-05  
[01/19 15:46:55] lb.utils.events INFO:  eta: 0:00:03  iteration: 981/1000  consumed samples: 15696  total_loss: 7.443  time: 0.1825(87.68)  data_time: 0.0032  lr: 9.00e-05  
[01/19 15:46:55] lb.utils.events INFO:  eta: 0:00:03  iteration: 982/1000  consumed samples: 15712  total_loss: 7.445  time: 0.1825(87.68)  data_time: 0.0033  lr: 9.00e-05  
[01/19 15:46:56] lb.utils.events INFO:  eta: 0:00:03  iteration: 983/1000  consumed samples: 15728  total_loss: 7.445  time: 0.1825(87.68)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:56] lb.utils.events INFO:  eta: 0:00:02  iteration: 984/1000  consumed samples: 15744  total_loss: 7.443  time: 0.1825(87.67)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:56] lb.utils.events INFO:  eta: 0:00:02  iteration: 985/1000  consumed samples: 15760  total_loss: 7.443  time: 0.1825(87.66)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:56] lb.utils.events INFO:  eta: 0:00:02  iteration: 986/1000  consumed samples: 15776  total_loss: 7.445  time: 0.1825(87.65)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:56] lb.utils.events INFO:  eta: 0:00:02  iteration: 987/1000  consumed samples: 15792  total_loss: 7.445  time: 0.1825(87.65)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:57] lb.utils.events INFO:  eta: 0:00:02  iteration: 988/1000  consumed samples: 15808  total_loss: 7.443  time: 0.1826(87.64)  data_time: 0.0038  lr: 9.00e-05  
[01/19 15:46:57] lb.utils.events INFO:  eta: 0:00:01  iteration: 989/1000  consumed samples: 15824  total_loss: 7.441  time: 0.1826(87.64)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:57] lb.utils.events INFO:  eta: 0:00:01  iteration: 990/1000  consumed samples: 15840  total_loss: 7.441  time: 0.1826(87.63)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:57] lb.utils.events INFO:  eta: 0:00:01  iteration: 991/1000  consumed samples: 15856  total_loss: 7.443  time: 0.1826(87.62)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:57] lb.utils.events INFO:  eta: 0:00:01  iteration: 992/1000  consumed samples: 15872  total_loss: 7.443  time: 0.1826(87.61)  data_time: 0.0039  lr: 9.00e-05  
[01/19 15:46:58] lb.utils.events INFO:  eta: 0:00:01  iteration: 993/1000  consumed samples: 15888  total_loss: 7.443  time: 0.1826(87.61)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:58] lb.utils.events INFO:  eta: 0:00:00  iteration: 994/1000  consumed samples: 15904  total_loss: 7.441  time: 0.1826(87.60)  data_time: 0.0037  lr: 9.00e-05  
[01/19 15:46:58] lb.utils.events INFO:  eta: 0:00:00  iteration: 995/1000  consumed samples: 15920  total_loss: 7.441  time: 0.1827(87.60)  data_time: 0.0036  lr: 9.00e-05  
[01/19 15:46:58] lb.utils.events INFO:  eta: 0:00:00  iteration: 996/1000  consumed samples: 15936  total_loss: 7.441  time: 0.1827(87.59)  data_time: 0.0035  lr: 9.00e-05  
[01/19 15:46:59] lb.utils.events INFO:  eta: 0:00:00  iteration: 997/1000  consumed samples: 15952  total_loss: 7.44  time: 0.1827(87.59)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:59] lb.utils.events INFO:  eta: 0:00:00  iteration: 998/1000  consumed samples: 15968  total_loss: 7.438  time: 0.1827(87.58)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:59] lb.utils.events INFO:  eta: 0:00:00  iteration: 999/1000  consumed samples: 15984  total_loss: 7.436  time: 0.1827(87.57)  data_time: 0.0034  lr: 9.00e-05  
[01/19 15:46:59] lb.utils.checkpoint INFO: Saving checkpoint to ./demo_output/test_config/model_final
[01/19 15:47:00] lb.trainer.hooks INFO: Overall training speed: 998 iterations in 0:03:02 (0.1827 s / it)
[01/19 15:47:00] lb.trainer.hooks INFO: Total training time: 0:03:30 (0:00:28 on hooks)
[01/19 16:06:34] libai INFO: Rank of current process: 0. World size: 1
[01/19 16:06:34] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 16:06:34] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 16:06:34] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 16:06:34] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 16:06:34] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 16:06:42] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 16:06:42] libai INFO: Loadding megatron weight
[01/19 16:06:42] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 16:06:44] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 16:06:44] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 16:06:44] lb.data.build INFO:     train:      16000
[01/19 16:06:44] lb.data.build INFO:     validation: 160000
[01/19 16:06:44] lb.data.build INFO:     test:       160000
[01/19 16:06:44] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 16:06:44] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     reading document index...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 16:06:44] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 16:06:44] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.179780 seconds
[01/19 16:06:44] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 16:06:44] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 16:06:44] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 16:06:44] lb.data.dataset_utils INFO:  > dataset split:
[01/19 16:06:44] lb.data.dataset_utils INFO:     train:
[01/19 16:06:44] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 16:06:44] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 16:06:44] lb.data.dataset_utils INFO:     validation:
[01/19 16:06:44] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 16:06:44] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 16:06:44] lb.data.dataset_utils INFO:     test:
[01/19 16:06:44] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 16:06:44] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 16:06:44] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 16:06:44] lb.data.dataset_utils INFO:     loaded indexed file in 0.011 seconds
[01/19 16:06:44] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 16:06:44] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 16:06:44] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 16:06:44] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 16:06:44] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 16:06:44] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 16:06:44] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 16:06:44] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 16:06:46] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 16:06:58] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.0264  lr: 0.00e+00  
[01/19 16:06:58] lb.utils.events INFO:  eta: 0:06:17  iteration: 2/1000  consumed samples: 32  total_loss: 8.65  data_time: 0.0160  lr: 1.00e-05  
[01/19 16:06:58] lb.utils.events INFO:  eta: 0:01:23  iteration: 3/1000  consumed samples: 48  total_loss: 8.663  time: 0.0840(190.39)  data_time: 0.0129  lr: 2.00e-05  
[01/19 16:08:48] libai INFO: Rank of current process: 0. World size: 1
[01/19 16:08:48] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/19 16:08:48] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mdist[39m[38;5;197m.[39m[38;5;15mpipeline_num_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/19 16:08:48] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/19 16:08:48] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/19 16:08:48] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/19 16:08:54] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/19 16:08:54] libai INFO: Loadding megatron weight
[01/19 16:08:54] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/19 16:08:55] lb.data.build INFO: > building train, validation, and test datasets ...
[01/19 16:08:55] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/19 16:08:55] lb.data.build INFO:     train:      16000
[01/19 16:08:55] lb.data.build INFO:     validation: 160000
[01/19 16:08:55] lb.data.build INFO:     test:       160000
[01/19 16:08:55] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/19 16:08:55] lb.data.dataset_utils INFO:  > building dataset index ...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     reading sizes...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     reading pointers...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     reading document index...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/19 16:08:55] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/19 16:08:55] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.085572 seconds
[01/19 16:08:55] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/19 16:08:55] lb.data.dataset_utils INFO:     number of documents: 50000
[01/19 16:08:55] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/19 16:08:55] lb.data.dataset_utils INFO:  > dataset split:
[01/19 16:08:55] lb.data.dataset_utils INFO:     train:
[01/19 16:08:55] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/19 16:08:55] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/19 16:08:55] lb.data.dataset_utils INFO:     validation:
[01/19 16:08:55] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/19 16:08:55] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/19 16:08:55] lb.data.dataset_utils INFO:     test:
[01/19 16:08:55] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/19 16:08:55] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/19 16:08:55] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/19 16:08:55] lb.data.dataset_utils INFO:     loaded indexed file in 0.011 seconds
[01/19 16:08:55] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/19 16:08:55] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 16:08:55] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 16:08:55] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/19 16:08:55] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/19 16:08:55] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/19 16:08:55] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/19 16:08:55] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/19 16:08:56] lb.trainer.trainer INFO: Starting training from iteration 0
[01/19 16:09:08] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.0227  lr: 0.00e+00  
[01/19 16:09:08] lb.utils.events INFO:  eta: 0:03:53  iteration: 2/1000  consumed samples: 32  total_loss: 8.65  data_time: 0.0129  lr: 1.00e-05  
[01/19 16:09:09] lb.utils.events INFO:  eta: 0:03:15  iteration: 3/1000  consumed samples: 48  total_loss: 8.663  time: 0.1959(81.66)  data_time: 0.0193  lr: 2.00e-05  
[01/19 16:09:09] lb.utils.events INFO:  eta: 0:03:20  iteration: 4/1000  consumed samples: 64  total_loss: 8.65  time: 0.2012(79.52)  data_time: 0.0151  lr: 3.00e-05  
[01/19 16:09:09] lb.utils.events INFO:  eta: 0:03:18  iteration: 5/1000  consumed samples: 80  total_loss: 8.663  time: 0.2007(79.72)  data_time: 0.0138  lr: 4.00e-05  
[01/19 16:09:09] lb.utils.events INFO:  eta: 0:03:16  iteration: 6/1000  consumed samples: 96  total_loss: 8.675  time: 0.1988(80.49)  data_time: 0.0131  lr: 5.00e-05  
[01/19 16:09:09] lb.utils.events INFO:  eta: 0:03:17  iteration: 7/1000  consumed samples: 112  total_loss: 8.663  time: 0.1988(80.47)  data_time: 0.0185  lr: 6.00e-05  
[01/19 16:09:10] lb.utils.events INFO:  eta: 0:03:17  iteration: 8/1000  consumed samples: 128  total_loss: 8.65  time: 0.1996(80.16)  data_time: 0.0170  lr: 7.00e-05  
[01/19 16:09:10] lb.utils.events INFO:  eta: 0:03:17  iteration: 9/1000  consumed samples: 144  total_loss: 8.637  time: 0.1996(80.15)  data_time: 0.0157  lr: 8.00e-05  
[01/19 16:09:10] lb.utils.events INFO:  eta: 0:03:17  iteration: 10/1000  consumed samples: 160  total_loss: 8.635  time: 0.1985(80.62)  data_time: 0.0152  lr: 9.00e-05  
[01/19 16:09:10] lb.utils.events INFO:  eta: 0:03:16  iteration: 11/1000  consumed samples: 176  total_loss: 8.633  time: 0.1985(80.61)  data_time: 0.0140  lr: 9.00e-05  
[01/19 16:09:10] lb.utils.events INFO:  eta: 0:03:16  iteration: 12/1000  consumed samples: 192  total_loss: 8.632  time: 0.1987(80.51)  data_time: 0.0133  lr: 9.00e-05  
[01/19 16:09:11] lb.utils.events INFO:  eta: 0:03:16  iteration: 13/1000  consumed samples: 208  total_loss: 8.631  time: 0.1993(80.29)  data_time: 0.0125  lr: 9.00e-05  
[01/19 16:09:11] lb.utils.events INFO:  eta: 0:03:16  iteration: 14/1000  consumed samples: 224  total_loss: 8.632  time: 0.1993(80.28)  data_time: 0.0143  lr: 9.00e-05  
[01/19 16:09:11] lb.utils.events INFO:  eta: 0:03:16  iteration: 15/1000  consumed samples: 240  total_loss: 8.631  time: 0.1906(83.96)  data_time: 0.0137  lr: 9.00e-05  
[01/19 16:09:11] lb.utils.events INFO:  eta: 0:03:15  iteration: 16/1000  consumed samples: 256  total_loss: 8.63  time: 0.1907(83.92)  data_time: 0.0132  lr: 9.00e-05  
[01/19 16:09:12] lb.utils.events INFO:  eta: 0:03:15  iteration: 17/1000  consumed samples: 272  total_loss: 8.629  time: 0.1902(84.14)  data_time: 0.0132  lr: 9.00e-05  
[01/19 16:09:12] lb.utils.events INFO:  eta: 0:03:15  iteration: 18/1000  consumed samples: 288  total_loss: 8.621  time: 0.1907(83.90)  data_time: 0.0147  lr: 9.00e-05  
[01/19 16:09:12] lb.utils.events INFO:  eta: 0:03:14  iteration: 19/1000  consumed samples: 304  total_loss: 8.613  time: 0.1908(83.86)  data_time: 0.0141  lr: 9.00e-05  
[01/19 16:09:12] lb.utils.events INFO:  eta: 0:03:14  iteration: 20/1000  consumed samples: 320  total_loss: 8.613  time: 0.1915(83.53)  data_time: 0.0135  lr: 9.00e-05  
[01/19 16:09:12] lb.utils.events INFO:  eta: 0:03:14  iteration: 21/1000  consumed samples: 336  total_loss: 8.613  time: 0.1917(83.47)  data_time: 0.0125  lr: 9.00e-05  
[01/19 16:09:13] lb.utils.events INFO:  eta: 0:03:14  iteration: 22/1000  consumed samples: 352  total_loss: 8.595  time: 0.1907(83.90)  data_time: 0.0130  lr: 9.00e-05  
[01/19 16:09:13] lb.utils.events INFO:  eta: 0:03:14  iteration: 23/1000  consumed samples: 368  total_loss: 8.578  time: 0.1912(83.68)  data_time: 0.0124  lr: 9.00e-05  
[01/19 16:09:13] lb.utils.events INFO:  eta: 0:03:13  iteration: 24/1000  consumed samples: 384  total_loss: 8.536  time: 0.1914(83.61)  data_time: 0.0126  lr: 9.00e-05  
[01/19 16:09:13] lb.utils.events INFO:  eta: 0:03:13  iteration: 25/1000  consumed samples: 400  total_loss: 8.494  time: 0.1919(83.38)  data_time: 0.0132  lr: 9.00e-05  
[01/19 16:09:14] lb.utils.events INFO:  eta: 0:03:13  iteration: 26/1000  consumed samples: 416  total_loss: 8.491  time: 0.1920(83.34)  data_time: 0.0134  lr: 9.00e-05  
[01/19 16:09:14] lb.utils.events INFO:  eta: 0:03:12  iteration: 27/1000  consumed samples: 432  total_loss: 8.489  time: 0.1922(83.25)  data_time: 0.0159  lr: 9.00e-05  
[01/19 16:09:14] lb.utils.events INFO:  eta: 0:03:11  iteration: 28/1000  consumed samples: 448  total_loss: 8.488  time: 0.1919(83.36)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:09:14] lb.utils.events INFO:  eta: 0:03:12  iteration: 29/1000  consumed samples: 464  total_loss: 8.487  time: 0.1924(83.18)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:09:14] lb.utils.events INFO:  eta: 0:03:11  iteration: 30/1000  consumed samples: 480  total_loss: 8.484  time: 0.1920(83.33)  data_time: 0.0153  lr: 9.00e-05  
[01/19 16:09:15] lb.utils.events INFO:  eta: 0:03:10  iteration: 31/1000  consumed samples: 496  total_loss: 8.48  time: 0.1921(83.27)  data_time: 0.0173  lr: 9.00e-05  
[01/19 16:09:15] lb.utils.events INFO:  eta: 0:03:10  iteration: 32/1000  consumed samples: 512  total_loss: 8.471  time: 0.1923(83.20)  data_time: 0.0172  lr: 9.00e-05  
[01/19 16:09:15] lb.utils.events INFO:  eta: 0:03:10  iteration: 33/1000  consumed samples: 528  total_loss: 8.463  time: 0.1923(83.22)  data_time: 0.0192  lr: 9.00e-05  
[01/19 16:09:15] lb.utils.events INFO:  eta: 0:03:09  iteration: 34/1000  consumed samples: 544  total_loss: 8.448  time: 0.1921(83.29)  data_time: 0.0175  lr: 9.00e-05  
[01/19 16:09:15] lb.utils.events INFO:  eta: 0:03:08  iteration: 35/1000  consumed samples: 560  total_loss: 8.432  time: 0.1920(83.33)  data_time: 0.0200  lr: 9.00e-05  
[01/19 16:09:16] lb.utils.events INFO:  eta: 0:03:09  iteration: 36/1000  consumed samples: 576  total_loss: 8.423  time: 0.1923(83.22)  data_time: 0.0221  lr: 9.00e-05  
[01/19 16:09:16] lb.utils.events INFO:  eta: 0:03:08  iteration: 37/1000  consumed samples: 592  total_loss: 8.413  time: 0.1919(83.38)  data_time: 0.0216  lr: 9.00e-05  
[01/19 16:09:17] lb.utils.events INFO:  eta: 0:03:08  iteration: 38/1000  consumed samples: 608  total_loss: 8.412  time: 0.1891(84.59)  data_time: 0.0197  lr: 9.00e-05  
[01/19 16:09:17] lb.utils.events INFO:  eta: 0:03:07  iteration: 39/1000  consumed samples: 624  total_loss: 8.41  time: 0.1862(85.92)  data_time: 0.0197  lr: 9.00e-05  
[01/19 16:09:18] lb.utils.events INFO:  eta: 0:03:07  iteration: 40/1000  consumed samples: 640  total_loss: 8.392  time: 0.1841(86.92)  data_time: 0.0199  lr: 9.00e-05  
[01/19 16:09:18] lb.utils.events INFO:  eta: 0:03:06  iteration: 41/1000  consumed samples: 656  total_loss: 8.374  time: 0.1820(87.92)  data_time: 0.0204  lr: 9.00e-05  
[01/19 16:09:19] lb.utils.events INFO:  eta: 0:03:06  iteration: 42/1000  consumed samples: 672  total_loss: 8.355  time: 0.1795(89.13)  data_time: 0.0200  lr: 9.00e-05  
[01/19 16:09:20] lb.utils.events INFO:  eta: 0:03:05  iteration: 43/1000  consumed samples: 688  total_loss: 8.335  time: 0.1774(90.17)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:20] lb.utils.events INFO:  eta: 0:03:05  iteration: 44/1000  consumed samples: 704  total_loss: 8.332  time: 0.1754(91.24)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:20] lb.utils.events INFO:  eta: 0:03:05  iteration: 45/1000  consumed samples: 720  total_loss: 8.328  time: 0.1739(92.02)  data_time: 0.0195  lr: 9.00e-05  
[01/19 16:09:21] lb.utils.events INFO:  eta: 0:03:05  iteration: 46/1000  consumed samples: 736  total_loss: 8.328  time: 0.1753(91.29)  data_time: 0.0254  lr: 9.00e-05  
[01/19 16:09:21] lb.utils.events INFO:  eta: 0:03:04  iteration: 47/1000  consumed samples: 752  total_loss: 8.327  time: 0.1734(92.29)  data_time: 0.0205  lr: 9.00e-05  
[01/19 16:09:22] lb.utils.events INFO:  eta: 0:03:04  iteration: 48/1000  consumed samples: 768  total_loss: 8.303  time: 0.1717(93.19)  data_time: 0.0207  lr: 9.00e-05  
[01/19 16:09:22] lb.utils.events INFO:  eta: 0:03:03  iteration: 49/1000  consumed samples: 784  total_loss: 8.278  time: 0.1701(94.09)  data_time: 0.0208  lr: 9.00e-05  
[01/19 16:09:22] lb.utils.events INFO:  eta: 0:03:02  iteration: 50/1000  consumed samples: 800  total_loss: 8.278  time: 0.1686(94.88)  data_time: 0.0214  lr: 9.00e-05  
[01/19 16:09:22] lb.utils.events INFO:  eta: 0:03:02  iteration: 51/1000  consumed samples: 816  total_loss: 8.277  time: 0.1672(95.67)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:22] lb.utils.events INFO:  eta: 0:03:01  iteration: 52/1000  consumed samples: 832  total_loss: 8.276  time: 0.1675(95.53)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:23] lb.utils.events INFO:  eta: 0:03:01  iteration: 53/1000  consumed samples: 848  total_loss: 8.274  time: 0.1677(95.38)  data_time: 0.0178  lr: 9.00e-05  
[01/19 16:09:23] lb.utils.events INFO:  eta: 0:03:00  iteration: 54/1000  consumed samples: 864  total_loss: 8.259  time: 0.1682(95.14)  data_time: 0.0213  lr: 9.00e-05  
[01/19 16:09:23] lb.utils.events INFO:  eta: 0:03:00  iteration: 55/1000  consumed samples: 880  total_loss: 8.244  time: 0.1685(94.94)  data_time: 0.0212  lr: 9.00e-05  
[01/19 16:09:23] lb.utils.events INFO:  eta: 0:02:59  iteration: 56/1000  consumed samples: 896  total_loss: 8.244  time: 0.1685(94.93)  data_time: 0.0190  lr: 9.00e-05  
[01/19 16:09:23] lb.utils.events INFO:  eta: 0:02:59  iteration: 57/1000  consumed samples: 912  total_loss: 8.243  time: 0.1688(94.79)  data_time: 0.0190  lr: 9.00e-05  
[01/19 16:09:24] lb.utils.events INFO:  eta: 0:02:58  iteration: 58/1000  consumed samples: 928  total_loss: 8.242  time: 0.1688(94.76)  data_time: 0.0193  lr: 9.00e-05  
[01/19 16:09:24] lb.utils.events INFO:  eta: 0:02:58  iteration: 59/1000  consumed samples: 944  total_loss: 8.24  time: 0.1679(95.28)  data_time: 0.0204  lr: 9.00e-05  
[01/19 16:09:25] lb.utils.events INFO:  eta: 0:02:57  iteration: 60/1000  consumed samples: 960  total_loss: 8.236  time: 0.1676(95.46)  data_time: 0.0234  lr: 9.00e-05  
[01/19 16:09:26] lb.utils.events INFO:  eta: 0:02:57  iteration: 61/1000  consumed samples: 976  total_loss: 8.231  time: 0.1662(96.26)  data_time: 0.0229  lr: 9.00e-05  
[01/19 16:09:27] lb.utils.events INFO:  eta: 0:02:56  iteration: 62/1000  consumed samples: 992  total_loss: 8.229  time: 0.1649(97.04)  data_time: 0.0228  lr: 9.00e-05  
[01/19 16:09:27] lb.utils.events INFO:  eta: 0:02:54  iteration: 63/1000  consumed samples: 1008  total_loss: 8.227  time: 0.1637(97.74)  data_time: 0.0227  lr: 9.00e-05  
[01/19 16:09:27] lb.utils.events INFO:  eta: 0:02:54  iteration: 64/1000  consumed samples: 1024  total_loss: 8.227  time: 0.1628(98.30)  data_time: 0.0232  lr: 9.00e-05  
[01/19 16:09:28] lb.utils.events INFO:  eta: 0:02:54  iteration: 65/1000  consumed samples: 1040  total_loss: 8.227  time: 0.1616(99.01)  data_time: 0.0223  lr: 9.00e-05  
[01/19 16:09:28] lb.utils.events INFO:  eta: 0:02:53  iteration: 66/1000  consumed samples: 1056  total_loss: 8.227  time: 0.1614(99.11)  data_time: 0.0159  lr: 9.00e-05  
[01/19 16:09:28] lb.utils.events INFO:  eta: 0:02:53  iteration: 67/1000  consumed samples: 1072  total_loss: 8.227  time: 0.1605(99.68)  data_time: 0.0166  lr: 9.00e-05  
[01/19 16:09:28] lb.utils.events INFO:  eta: 0:02:51  iteration: 68/1000  consumed samples: 1088  total_loss: 8.222  time: 0.1599(100.09)  data_time: 0.0174  lr: 9.00e-05  
[01/19 16:09:29] lb.utils.events INFO:  eta: 0:02:50  iteration: 69/1000  consumed samples: 1104  total_loss: 8.218  time: 0.1588(100.77)  data_time: 0.0172  lr: 9.00e-05  
[01/19 16:09:29] lb.utils.events INFO:  eta: 0:02:49  iteration: 70/1000  consumed samples: 1120  total_loss: 8.214  time: 0.1588(100.73)  data_time: 0.0179  lr: 9.00e-05  
[01/19 16:09:29] lb.utils.events INFO:  eta: 0:02:49  iteration: 71/1000  consumed samples: 1136  total_loss: 8.209  time: 0.1590(100.63)  data_time: 0.0179  lr: 9.00e-05  
[01/19 16:09:29] lb.utils.events INFO:  eta: 0:02:49  iteration: 72/1000  consumed samples: 1152  total_loss: 8.198  time: 0.1581(101.19)  data_time: 0.0185  lr: 9.00e-05  
[01/19 16:09:30] lb.utils.events INFO:  eta: 0:02:48  iteration: 73/1000  consumed samples: 1168  total_loss: 8.186  time: 0.1571(101.87)  data_time: 0.0182  lr: 9.00e-05  
[01/19 16:09:30] lb.utils.events INFO:  eta: 0:02:48  iteration: 74/1000  consumed samples: 1184  total_loss: 8.185  time: 0.1571(101.86)  data_time: 0.0183  lr: 9.00e-05  
[01/19 16:09:30] lb.utils.events INFO:  eta: 0:02:47  iteration: 75/1000  consumed samples: 1200  total_loss: 8.183  time: 0.1560(102.53)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:09:32] lb.utils.events INFO:  eta: 0:02:46  iteration: 76/1000  consumed samples: 1216  total_loss: 8.178  time: 0.1551(103.15)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:09:32] lb.utils.events INFO:  eta: 0:02:45  iteration: 77/1000  consumed samples: 1232  total_loss: 8.172  time: 0.1543(103.67)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:09:32] lb.utils.events INFO:  eta: 0:02:45  iteration: 78/1000  consumed samples: 1248  total_loss: 8.172  time: 0.1545(103.57)  data_time: 0.0155  lr: 9.00e-05  
[01/19 16:09:32] lb.utils.events INFO:  eta: 0:02:44  iteration: 79/1000  consumed samples: 1264  total_loss: 8.172  time: 0.1536(104.14)  data_time: 0.0144  lr: 9.00e-05  
[01/19 16:09:33] lb.utils.events INFO:  eta: 0:02:41  iteration: 80/1000  consumed samples: 1280  total_loss: 8.169  time: 0.1530(104.57)  data_time: 0.0122  lr: 9.00e-05  
[01/19 16:09:34] lb.utils.events INFO:  eta: 0:02:37  iteration: 81/1000  consumed samples: 1296  total_loss: 8.167  time: 0.1523(105.04)  data_time: 0.0128  lr: 9.00e-05  
[01/19 16:09:34] lb.utils.events INFO:  eta: 0:02:37  iteration: 82/1000  consumed samples: 1312  total_loss: 8.161  time: 0.1516(105.53)  data_time: 0.0128  lr: 9.00e-05  
[01/19 16:09:34] lb.utils.events INFO:  eta: 0:02:37  iteration: 83/1000  consumed samples: 1328  total_loss: 8.155  time: 0.1508(106.12)  data_time: 0.0126  lr: 9.00e-05  
[01/19 16:09:35] lb.utils.events INFO:  eta: 0:02:36  iteration: 84/1000  consumed samples: 1344  total_loss: 8.15  time: 0.1500(106.69)  data_time: 0.0118  lr: 9.00e-05  
[01/19 16:09:35] lb.utils.events INFO:  eta: 0:02:35  iteration: 85/1000  consumed samples: 1360  total_loss: 8.145  time: 0.1493(107.20)  data_time: 0.0120  lr: 9.00e-05  
[01/19 16:09:35] lb.utils.events INFO:  eta: 0:02:35  iteration: 86/1000  consumed samples: 1376  total_loss: 8.141  time: 0.1485(107.74)  data_time: 0.0120  lr: 9.00e-05  
[01/19 16:09:36] lb.utils.events INFO:  eta: 0:02:34  iteration: 87/1000  consumed samples: 1392  total_loss: 8.136  time: 0.1478(108.26)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:09:36] lb.utils.events INFO:  eta: 0:02:32  iteration: 88/1000  consumed samples: 1408  total_loss: 8.129  time: 0.1471(108.77)  data_time: 0.0103  lr: 9.00e-05  
[01/19 16:09:37] lb.utils.events INFO:  eta: 0:02:30  iteration: 89/1000  consumed samples: 1424  total_loss: 8.121  time: 0.1466(109.12)  data_time: 0.0112  lr: 9.00e-05  
[01/19 16:09:38] lb.utils.events INFO:  eta: 0:02:29  iteration: 90/1000  consumed samples: 1440  total_loss: 8.115  time: 0.1461(109.54)  data_time: 0.0106  lr: 9.00e-05  
[01/19 16:09:38] lb.utils.events INFO:  eta: 0:02:28  iteration: 91/1000  consumed samples: 1456  total_loss: 8.108  time: 0.1454(110.04)  data_time: 0.0106  lr: 9.00e-05  
[01/19 16:09:39] lb.utils.events INFO:  eta: 0:02:25  iteration: 92/1000  consumed samples: 1472  total_loss: 8.103  time: 0.1447(110.54)  data_time: 0.0100  lr: 9.00e-05  
[01/19 16:09:39] lb.utils.events INFO:  eta: 0:02:27  iteration: 93/1000  consumed samples: 1488  total_loss: 8.097  time: 0.1452(110.19)  data_time: 0.0099  lr: 9.00e-05  
[01/19 16:09:39] lb.utils.events INFO:  eta: 0:02:28  iteration: 94/1000  consumed samples: 1504  total_loss: 8.103  time: 0.1456(109.91)  data_time: 0.0083  lr: 9.00e-05  
[01/19 16:09:39] lb.utils.events INFO:  eta: 0:02:29  iteration: 95/1000  consumed samples: 1520  total_loss: 8.097  time: 0.1461(109.51)  data_time: 0.0083  lr: 9.00e-05  
[01/19 16:09:39] lb.utils.events INFO:  eta: 0:02:31  iteration: 96/1000  consumed samples: 1536  total_loss: 8.092  time: 0.1464(109.28)  data_time: 0.0086  lr: 9.00e-05  
[01/19 16:09:40] lb.utils.events INFO:  eta: 0:02:33  iteration: 97/1000  consumed samples: 1552  total_loss: 8.087  time: 0.1470(108.87)  data_time: 0.0086  lr: 9.00e-05  
[01/19 16:09:40] lb.utils.events INFO:  eta: 0:02:32  iteration: 98/1000  consumed samples: 1568  total_loss: 8.081  time: 0.1475(108.44)  data_time: 0.0085  lr: 9.00e-05  
[01/19 16:09:40] lb.utils.events INFO:  eta: 0:02:32  iteration: 99/1000  consumed samples: 1584  total_loss: 8.075  time: 0.1481(108.05)  data_time: 0.0085  lr: 9.00e-05  
[01/19 16:09:40] lb.utils.events INFO:  eta: 0:02:32  iteration: 100/1000  consumed samples: 1600  total_loss: 8.072  time: 0.1483(107.90)  data_time: 0.0089  lr: 9.00e-05  
[01/19 16:09:40] lb.utils.events INFO:  eta: 0:02:32  iteration: 101/1000  consumed samples: 1616  total_loss: 8.069  time: 0.1485(107.72)  data_time: 0.0083  lr: 9.00e-05  
[01/19 16:09:41] lb.utils.events INFO:  eta: 0:02:33  iteration: 102/1000  consumed samples: 1632  total_loss: 8.066  time: 0.1488(107.50)  data_time: 0.0083  lr: 9.00e-05  
[01/19 16:09:41] lb.utils.events INFO:  eta: 0:02:33  iteration: 103/1000  consumed samples: 1648  total_loss: 8.063  time: 0.1493(107.16)  data_time: 0.0103  lr: 9.00e-05  
[01/19 16:09:41] lb.utils.events INFO:  eta: 0:02:33  iteration: 104/1000  consumed samples: 1664  total_loss: 8.061  time: 0.1497(106.87)  data_time: 0.0104  lr: 9.00e-05  
[01/19 16:09:41] lb.utils.events INFO:  eta: 0:02:33  iteration: 105/1000  consumed samples: 1680  total_loss: 8.058  time: 0.1503(106.48)  data_time: 0.0103  lr: 9.00e-05  
[01/19 16:09:42] lb.utils.events INFO:  eta: 0:02:33  iteration: 106/1000  consumed samples: 1696  total_loss: 8.058  time: 0.1508(106.11)  data_time: 0.0105  lr: 9.00e-05  
[01/19 16:09:42] lb.utils.events INFO:  eta: 0:02:33  iteration: 107/1000  consumed samples: 1712  total_loss: 8.058  time: 0.1502(106.50)  data_time: 0.0108  lr: 9.00e-05  
[01/19 16:09:42] lb.utils.events INFO:  eta: 0:02:33  iteration: 108/1000  consumed samples: 1728  total_loss: 8.058  time: 0.1504(106.40)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:09:42] lb.utils.events INFO:  eta: 0:02:33  iteration: 109/1000  consumed samples: 1744  total_loss: 8.058  time: 0.1509(106.02)  data_time: 0.0107  lr: 9.00e-05  
[01/19 16:09:43] lb.utils.events INFO:  eta: 0:02:32  iteration: 110/1000  consumed samples: 1760  total_loss: 8.058  time: 0.1514(105.70)  data_time: 0.0103  lr: 9.00e-05  
[01/19 16:09:43] lb.utils.events INFO:  eta: 0:02:32  iteration: 111/1000  consumed samples: 1776  total_loss: 8.058  time: 0.1518(105.42)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:09:43] lb.utils.events INFO:  eta: 0:02:34  iteration: 112/1000  consumed samples: 1792  total_loss: 8.058  time: 0.1523(105.05)  data_time: 0.0119  lr: 9.00e-05  
[01/19 16:09:43] lb.utils.events INFO:  eta: 0:02:32  iteration: 113/1000  consumed samples: 1808  total_loss: 8.058  time: 0.1524(104.96)  data_time: 0.0131  lr: 9.00e-05  
[01/19 16:09:43] lb.utils.events INFO:  eta: 0:02:33  iteration: 114/1000  consumed samples: 1824  total_loss: 8.058  time: 0.1528(104.71)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:09:44] lb.utils.events INFO:  eta: 0:02:35  iteration: 115/1000  consumed samples: 1840  total_loss: 8.058  time: 0.1532(104.47)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:09:44] lb.utils.events INFO:  eta: 0:02:36  iteration: 116/1000  consumed samples: 1856  total_loss: 8.058  time: 0.1536(104.16)  data_time: 0.0107  lr: 9.00e-05  
[01/19 16:09:44] lb.utils.events INFO:  eta: 0:02:37  iteration: 117/1000  consumed samples: 1872  total_loss: 8.058  time: 0.1541(103.82)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:09:44] lb.utils.events INFO:  eta: 0:02:37  iteration: 118/1000  consumed samples: 1888  total_loss: 8.058  time: 0.1545(103.53)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:09:45] lb.utils.events INFO:  eta: 0:02:37  iteration: 119/1000  consumed samples: 1904  total_loss: 8.058  time: 0.1550(103.21)  data_time: 0.0117  lr: 9.00e-05  
[01/19 16:09:45] lb.utils.events INFO:  eta: 0:02:37  iteration: 120/1000  consumed samples: 1920  total_loss: 8.058  time: 0.1554(102.95)  data_time: 0.0135  lr: 9.00e-05  
[01/19 16:09:45] lb.utils.events INFO:  eta: 0:02:37  iteration: 121/1000  consumed samples: 1936  total_loss: 8.058  time: 0.1558(102.69)  data_time: 0.0137  lr: 9.00e-05  
[01/19 16:09:45] lb.utils.events INFO:  eta: 0:02:37  iteration: 122/1000  consumed samples: 1952  total_loss: 8.058  time: 0.1562(102.43)  data_time: 0.0138  lr: 9.00e-05  
[01/19 16:09:45] lb.utils.events INFO:  eta: 0:02:37  iteration: 123/1000  consumed samples: 1968  total_loss: 8.058  time: 0.1566(102.18)  data_time: 0.0118  lr: 9.00e-05  
[01/19 16:09:46] lb.utils.events INFO:  eta: 0:02:37  iteration: 124/1000  consumed samples: 1984  total_loss: 8.057  time: 0.1570(101.93)  data_time: 0.0151  lr: 9.00e-05  
[01/19 16:09:46] lb.utils.events INFO:  eta: 0:02:38  iteration: 125/1000  consumed samples: 2000  total_loss: 8.058  time: 0.1573(101.70)  data_time: 0.0149  lr: 9.00e-05  
[01/19 16:09:46] lb.utils.events INFO:  eta: 0:02:38  iteration: 126/1000  consumed samples: 2016  total_loss: 8.057  time: 0.1577(101.43)  data_time: 0.0149  lr: 9.00e-05  
[01/19 16:09:46] lb.utils.events INFO:  eta: 0:02:39  iteration: 127/1000  consumed samples: 2032  total_loss: 8.056  time: 0.1584(100.99)  data_time: 0.0225  lr: 9.00e-05  
[01/19 16:09:47] lb.utils.events INFO:  eta: 0:02:39  iteration: 128/1000  consumed samples: 2048  total_loss: 8.055  time: 0.1588(100.76)  data_time: 0.0221  lr: 9.00e-05  
[01/19 16:09:47] lb.utils.events INFO:  eta: 0:02:39  iteration: 129/1000  consumed samples: 2064  total_loss: 8.054  time: 0.1591(100.56)  data_time: 0.0216  lr: 9.00e-05  
[01/19 16:09:47] lb.utils.events INFO:  eta: 0:02:38  iteration: 130/1000  consumed samples: 2080  total_loss: 8.054  time: 0.1585(100.95)  data_time: 0.0213  lr: 9.00e-05  
[01/19 16:09:48] lb.utils.events INFO:  eta: 0:02:38  iteration: 131/1000  consumed samples: 2096  total_loss: 8.053  time: 0.1583(101.07)  data_time: 0.0207  lr: 9.00e-05  
[01/19 16:09:48] lb.utils.events INFO:  eta: 0:02:38  iteration: 132/1000  consumed samples: 2112  total_loss: 8.052  time: 0.1586(100.88)  data_time: 0.0201  lr: 9.00e-05  
[01/19 16:09:48] lb.utils.events INFO:  eta: 0:02:38  iteration: 133/1000  consumed samples: 2128  total_loss: 8.052  time: 0.1590(100.63)  data_time: 0.0189  lr: 9.00e-05  
[01/19 16:09:48] lb.utils.events INFO:  eta: 0:02:38  iteration: 134/1000  consumed samples: 2144  total_loss: 8.049  time: 0.1593(100.41)  data_time: 0.0190  lr: 9.00e-05  
[01/19 16:09:49] lb.utils.events INFO:  eta: 0:02:38  iteration: 135/1000  consumed samples: 2160  total_loss: 8.046  time: 0.1597(100.20)  data_time: 0.0240  lr: 9.00e-05  
[01/19 16:09:49] lb.utils.events INFO:  eta: 0:02:39  iteration: 136/1000  consumed samples: 2176  total_loss: 8.049  time: 0.1601(99.96)  data_time: 0.0241  lr: 9.00e-05  
[01/19 16:09:49] lb.utils.events INFO:  eta: 0:02:40  iteration: 137/1000  consumed samples: 2192  total_loss: 8.052  time: 0.1604(99.76)  data_time: 0.0248  lr: 9.00e-05  
[01/19 16:09:49] lb.utils.events INFO:  eta: 0:02:38  iteration: 138/1000  consumed samples: 2208  total_loss: 8.049  time: 0.1601(99.96)  data_time: 0.0248  lr: 9.00e-05  
[01/19 16:09:50] lb.utils.events INFO:  eta: 0:02:37  iteration: 139/1000  consumed samples: 2224  total_loss: 8.046  time: 0.1597(100.17)  data_time: 0.0256  lr: 9.00e-05  
[01/19 16:09:50] lb.utils.events INFO:  eta: 0:02:38  iteration: 140/1000  consumed samples: 2240  total_loss: 8.043  time: 0.1599(100.05)  data_time: 0.0223  lr: 9.00e-05  
[01/19 16:09:50] lb.utils.events INFO:  eta: 0:02:39  iteration: 141/1000  consumed samples: 2256  total_loss: 8.041  time: 0.1602(99.85)  data_time: 0.0223  lr: 9.00e-05  
[01/19 16:09:50] lb.utils.events INFO:  eta: 0:02:39  iteration: 142/1000  consumed samples: 2272  total_loss: 8.039  time: 0.1606(99.65)  data_time: 0.0222  lr: 9.00e-05  
[01/19 16:09:50] lb.utils.events INFO:  eta: 0:02:39  iteration: 143/1000  consumed samples: 2288  total_loss: 8.037  time: 0.1608(99.50)  data_time: 0.0226  lr: 9.00e-05  
[01/19 16:09:51] lb.utils.events INFO:  eta: 0:02:39  iteration: 144/1000  consumed samples: 2304  total_loss: 8.034  time: 0.1611(99.31)  data_time: 0.0192  lr: 9.00e-05  
[01/19 16:09:51] lb.utils.events INFO:  eta: 0:02:39  iteration: 145/1000  consumed samples: 2320  total_loss: 8.03  time: 0.1614(99.15)  data_time: 0.0207  lr: 9.00e-05  
[01/19 16:09:51] lb.utils.events INFO:  eta: 0:02:39  iteration: 146/1000  consumed samples: 2336  total_loss: 8.034  time: 0.1616(99.00)  data_time: 0.0208  lr: 9.00e-05  
[01/19 16:09:51] lb.utils.events INFO:  eta: 0:02:39  iteration: 147/1000  consumed samples: 2352  total_loss: 8.03  time: 0.1611(99.31)  data_time: 0.0129  lr: 9.00e-05  
[01/19 16:09:52] lb.utils.events INFO:  eta: 0:02:38  iteration: 148/1000  consumed samples: 2368  total_loss: 8.034  time: 0.1606(99.63)  data_time: 0.0129  lr: 9.00e-05  
[01/19 16:09:52] lb.utils.events INFO:  eta: 0:02:38  iteration: 149/1000  consumed samples: 2384  total_loss: 8.03  time: 0.1601(99.91)  data_time: 0.0131  lr: 9.00e-05  
[01/19 16:09:52] lb.utils.events INFO:  eta: 0:02:37  iteration: 150/1000  consumed samples: 2400  total_loss: 8.029  time: 0.1597(100.20)  data_time: 0.0135  lr: 9.00e-05  
[01/19 16:09:53] lb.utils.events INFO:  eta: 0:02:37  iteration: 151/1000  consumed samples: 2416  total_loss: 8.028  time: 0.1594(100.37)  data_time: 0.0149  lr: 9.00e-05  
[01/19 16:09:53] lb.utils.events INFO:  eta: 0:02:36  iteration: 152/1000  consumed samples: 2432  total_loss: 8.026  time: 0.1595(100.29)  data_time: 0.0159  lr: 9.00e-05  
[01/19 16:09:53] lb.utils.events INFO:  eta: 0:02:37  iteration: 153/1000  consumed samples: 2448  total_loss: 8.023  time: 0.1597(100.18)  data_time: 0.0164  lr: 9.00e-05  
[01/19 16:09:53] lb.utils.events INFO:  eta: 0:02:37  iteration: 154/1000  consumed samples: 2464  total_loss: 8.021  time: 0.1599(100.06)  data_time: 0.0164  lr: 9.00e-05  
[01/19 16:09:54] lb.utils.events INFO:  eta: 0:02:37  iteration: 155/1000  consumed samples: 2480  total_loss: 8.02  time: 0.1602(99.85)  data_time: 0.0119  lr: 9.00e-05  
[01/19 16:09:54] lb.utils.events INFO:  eta: 0:02:36  iteration: 156/1000  consumed samples: 2496  total_loss: 8.017  time: 0.1598(100.12)  data_time: 0.0120  lr: 9.00e-05  
[01/19 16:09:54] lb.utils.events INFO:  eta: 0:02:36  iteration: 157/1000  consumed samples: 2512  total_loss: 8.014  time: 0.1596(100.26)  data_time: 0.0125  lr: 9.00e-05  
[01/19 16:09:54] lb.utils.events INFO:  eta: 0:02:36  iteration: 158/1000  consumed samples: 2528  total_loss: 8.017  time: 0.1598(100.15)  data_time: 0.0125  lr: 9.00e-05  
[01/19 16:09:54] lb.utils.events INFO:  eta: 0:02:35  iteration: 159/1000  consumed samples: 2544  total_loss: 8.014  time: 0.1599(100.07)  data_time: 0.0124  lr: 9.00e-05  
[01/19 16:09:55] lb.utils.events INFO:  eta: 0:02:36  iteration: 160/1000  consumed samples: 2560  total_loss: 8.014  time: 0.1601(99.91)  data_time: 0.0125  lr: 9.00e-05  
[01/19 16:09:55] lb.utils.events INFO:  eta: 0:02:35  iteration: 161/1000  consumed samples: 2576  total_loss: 8.014  time: 0.1603(99.84)  data_time: 0.0124  lr: 9.00e-05  
[01/19 16:09:55] lb.utils.events INFO:  eta: 0:02:35  iteration: 162/1000  consumed samples: 2592  total_loss: 8.013  time: 0.1604(99.73)  data_time: 0.0123  lr: 9.00e-05  
[01/19 16:09:55] lb.utils.events INFO:  eta: 0:02:35  iteration: 163/1000  consumed samples: 2608  total_loss: 8.014  time: 0.1609(99.45)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:56] lb.utils.events INFO:  eta: 0:02:35  iteration: 164/1000  consumed samples: 2624  total_loss: 8.013  time: 0.1612(99.26)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:56] lb.utils.events INFO:  eta: 0:02:35  iteration: 165/1000  consumed samples: 2640  total_loss: 8.013  time: 0.1615(99.09)  data_time: 0.0179  lr: 9.00e-05  
[01/19 16:09:56] lb.utils.events INFO:  eta: 0:02:35  iteration: 166/1000  consumed samples: 2656  total_loss: 8.012  time: 0.1618(98.90)  data_time: 0.0177  lr: 9.00e-05  
[01/19 16:09:56] lb.utils.events INFO:  eta: 0:02:35  iteration: 167/1000  consumed samples: 2672  total_loss: 8.011  time: 0.1620(98.74)  data_time: 0.0194  lr: 9.00e-05  
[01/19 16:09:57] lb.utils.events INFO:  eta: 0:02:34  iteration: 168/1000  consumed samples: 2688  total_loss: 8.011  time: 0.1617(98.96)  data_time: 0.0201  lr: 9.00e-05  
[01/19 16:09:57] lb.utils.events INFO:  eta: 0:02:34  iteration: 169/1000  consumed samples: 2704  total_loss: 8.01  time: 0.1612(99.24)  data_time: 0.0199  lr: 9.00e-05  
[01/19 16:09:57] lb.utils.events INFO:  eta: 0:02:34  iteration: 170/1000  consumed samples: 2720  total_loss: 8.011  time: 0.1609(99.46)  data_time: 0.0204  lr: 9.00e-05  
[01/19 16:09:58] lb.utils.events INFO:  eta: 0:02:34  iteration: 171/1000  consumed samples: 2736  total_loss: 8.01  time: 0.1606(99.63)  data_time: 0.0199  lr: 9.00e-05  
[01/19 16:09:58] lb.utils.events INFO:  eta: 0:02:33  iteration: 172/1000  consumed samples: 2752  total_loss: 8.008  time: 0.1602(99.86)  data_time: 0.0195  lr: 9.00e-05  
[01/19 16:09:58] lb.utils.events INFO:  eta: 0:02:33  iteration: 173/1000  consumed samples: 2768  total_loss: 8.005  time: 0.1600(99.98)  data_time: 0.0209  lr: 9.00e-05  
[01/19 16:09:59] lb.utils.events INFO:  eta: 0:02:32  iteration: 174/1000  consumed samples: 2784  total_loss: 8.008  time: 0.1596(100.25)  data_time: 0.0209  lr: 9.00e-05  
[01/19 16:09:59] lb.utils.events INFO:  eta: 0:02:30  iteration: 175/1000  consumed samples: 2800  total_loss: 8.005  time: 0.1595(100.30)  data_time: 0.0203  lr: 9.00e-05  
[01/19 16:09:59] lb.utils.events INFO:  eta: 0:02:30  iteration: 176/1000  consumed samples: 2816  total_loss: 8.008  time: 0.1594(100.39)  data_time: 0.0202  lr: 9.00e-05  
[01/19 16:10:00] lb.utils.events INFO:  eta: 0:02:30  iteration: 177/1000  consumed samples: 2832  total_loss: 8.005  time: 0.1594(100.37)  data_time: 0.0210  lr: 9.00e-05  
[01/19 16:10:00] lb.utils.events INFO:  eta: 0:02:29  iteration: 178/1000  consumed samples: 2848  total_loss: 8.008  time: 0.1591(100.58)  data_time: 0.0218  lr: 9.00e-05  
[01/19 16:10:01] lb.utils.events INFO:  eta: 0:02:29  iteration: 179/1000  consumed samples: 2864  total_loss: 8.005  time: 0.1587(100.83)  data_time: 0.0209  lr: 9.00e-05  
[01/19 16:10:01] lb.utils.events INFO:  eta: 0:02:28  iteration: 180/1000  consumed samples: 2880  total_loss: 8.005  time: 0.1583(101.08)  data_time: 0.0208  lr: 9.00e-05  
[01/19 16:10:01] lb.utils.events INFO:  eta: 0:02:28  iteration: 181/1000  consumed samples: 2896  total_loss: 8.004  time: 0.1581(101.23)  data_time: 0.0219  lr: 9.00e-05  
[01/19 16:10:01] lb.utils.events INFO:  eta: 0:02:27  iteration: 182/1000  consumed samples: 2912  total_loss: 8.004  time: 0.1577(101.46)  data_time: 0.0220  lr: 9.00e-05  
[01/19 16:10:02] lb.utils.events INFO:  eta: 0:02:26  iteration: 183/1000  consumed samples: 2928  total_loss: 8.003  time: 0.1573(101.71)  data_time: 0.0145  lr: 9.00e-05  
[01/19 16:10:02] lb.utils.events INFO:  eta: 0:02:26  iteration: 184/1000  consumed samples: 2944  total_loss: 8  time: 0.1570(101.88)  data_time: 0.0155  lr: 9.00e-05  
[01/19 16:10:02] lb.utils.events INFO:  eta: 0:02:26  iteration: 185/1000  consumed samples: 2960  total_loss: 7.997  time: 0.1572(101.79)  data_time: 0.0155  lr: 9.00e-05  
[01/19 16:10:02] lb.utils.events INFO:  eta: 0:02:26  iteration: 186/1000  consumed samples: 2976  total_loss: 8  time: 0.1573(101.69)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:10:03] lb.utils.events INFO:  eta: 0:02:26  iteration: 187/1000  consumed samples: 2992  total_loss: 8.003  time: 0.1575(101.59)  data_time: 0.0141  lr: 9.00e-05  
[01/19 16:10:03] lb.utils.events INFO:  eta: 0:02:27  iteration: 188/1000  consumed samples: 3008  total_loss: 8.004  time: 0.1577(101.48)  data_time: 0.0152  lr: 9.00e-05  
[01/19 16:10:03] lb.utils.events INFO:  eta: 0:02:26  iteration: 189/1000  consumed samples: 3024  total_loss: 8.003  time: 0.1577(101.44)  data_time: 0.0152  lr: 9.00e-05  
[01/19 16:10:03] lb.utils.events INFO:  eta: 0:02:26  iteration: 190/1000  consumed samples: 3040  total_loss: 8  time: 0.1580(101.25)  data_time: 0.0144  lr: 9.00e-05  
[01/19 16:10:03] lb.utils.events INFO:  eta: 0:02:27  iteration: 191/1000  consumed samples: 3056  total_loss: 7.997  time: 0.1583(101.11)  data_time: 0.0140  lr: 9.00e-05  
[01/19 16:10:04] lb.utils.events INFO:  eta: 0:02:27  iteration: 192/1000  consumed samples: 3072  total_loss: 7.997  time: 0.1585(100.93)  data_time: 0.0140  lr: 9.00e-05  
[01/19 16:10:04] lb.utils.events INFO:  eta: 0:02:27  iteration: 193/1000  consumed samples: 3088  total_loss: 7.997  time: 0.1588(100.77)  data_time: 0.0121  lr: 9.00e-05  
[01/19 16:10:04] lb.utils.events INFO:  eta: 0:02:27  iteration: 194/1000  consumed samples: 3104  total_loss: 7.997  time: 0.1589(100.67)  data_time: 0.0121  lr: 9.00e-05  
[01/19 16:10:04] lb.utils.events INFO:  eta: 0:02:27  iteration: 195/1000  consumed samples: 3120  total_loss: 7.997  time: 0.1592(100.52)  data_time: 0.0121  lr: 9.00e-05  
[01/19 16:10:05] lb.utils.events INFO:  eta: 0:02:26  iteration: 196/1000  consumed samples: 3136  total_loss: 7.996  time: 0.1594(100.35)  data_time: 0.0133  lr: 9.00e-05  
[01/19 16:10:05] lb.utils.events INFO:  eta: 0:02:26  iteration: 197/1000  consumed samples: 3152  total_loss: 7.996  time: 0.1596(100.27)  data_time: 0.0106  lr: 9.00e-05  
[01/19 16:10:05] lb.utils.events INFO:  eta: 0:02:27  iteration: 198/1000  consumed samples: 3168  total_loss: 7.996  time: 0.1598(100.14)  data_time: 0.0101  lr: 9.00e-05  
[01/19 16:10:05] lb.utils.events INFO:  eta: 0:02:28  iteration: 199/1000  consumed samples: 3184  total_loss: 7.996  time: 0.1599(100.06)  data_time: 0.0100  lr: 9.00e-05  
[01/19 16:10:05] lb.utils.events INFO:  eta: 0:02:28  iteration: 200/1000  consumed samples: 3200  total_loss: 7.996  time: 0.1601(99.92)  data_time: 0.0106  lr: 9.00e-05  
[01/19 16:10:06] lb.utils.events INFO:  eta: 0:02:28  iteration: 201/1000  consumed samples: 3216  total_loss: 7.996  time: 0.1604(99.78)  data_time: 0.0095  lr: 9.00e-05  
[01/19 16:10:06] lb.utils.events INFO:  eta: 0:02:28  iteration: 202/1000  consumed samples: 3232  total_loss: 7.994  time: 0.1605(99.72)  data_time: 0.0119  lr: 9.00e-05  
[01/19 16:10:06] lb.utils.events INFO:  eta: 0:02:27  iteration: 203/1000  consumed samples: 3248  total_loss: 7.994  time: 0.1601(99.94)  data_time: 0.0119  lr: 9.00e-05  
[01/19 16:10:06] lb.utils.events INFO:  eta: 0:02:27  iteration: 204/1000  consumed samples: 3264  total_loss: 7.991  time: 0.1603(99.83)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:10:07] lb.utils.events INFO:  eta: 0:02:27  iteration: 205/1000  consumed samples: 3280  total_loss: 7.99  time: 0.1605(99.66)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:10:07] lb.utils.events INFO:  eta: 0:02:27  iteration: 206/1000  consumed samples: 3296  total_loss: 7.99  time: 0.1608(99.52)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:10:07] lb.utils.events INFO:  eta: 0:02:27  iteration: 207/1000  consumed samples: 3312  total_loss: 7.988  time: 0.1610(99.35)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:10:07] lb.utils.events INFO:  eta: 0:02:27  iteration: 208/1000  consumed samples: 3328  total_loss: 7.988  time: 0.1613(99.21)  data_time: 0.0100  lr: 9.00e-05  
[01/19 16:10:07] lb.utils.events INFO:  eta: 0:02:26  iteration: 209/1000  consumed samples: 3344  total_loss: 7.987  time: 0.1612(99.27)  data_time: 0.0108  lr: 9.00e-05  
[01/19 16:10:08] lb.utils.events INFO:  eta: 0:02:26  iteration: 210/1000  consumed samples: 3360  total_loss: 7.986  time: 0.1611(99.32)  data_time: 0.0109  lr: 9.00e-05  
[01/19 16:10:08] lb.utils.events INFO:  eta: 0:02:26  iteration: 211/1000  consumed samples: 3376  total_loss: 7.983  time: 0.1611(99.30)  data_time: 0.0106  lr: 9.00e-05  
[01/19 16:10:08] lb.utils.events INFO:  eta: 0:02:26  iteration: 212/1000  consumed samples: 3392  total_loss: 7.98  time: 0.1611(99.31)  data_time: 0.0105  lr: 9.00e-05  
[01/19 16:10:08] lb.utils.events INFO:  eta: 0:02:26  iteration: 213/1000  consumed samples: 3408  total_loss: 7.979  time: 0.1612(99.23)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:10:09] lb.utils.events INFO:  eta: 0:02:25  iteration: 214/1000  consumed samples: 3424  total_loss: 7.979  time: 0.1610(99.38)  data_time: 0.0123  lr: 9.00e-05  
[01/19 16:10:09] lb.utils.events INFO:  eta: 0:02:25  iteration: 215/1000  consumed samples: 3440  total_loss: 7.976  time: 0.1609(99.41)  data_time: 0.0129  lr: 9.00e-05  
[01/19 16:10:09] lb.utils.events INFO:  eta: 0:02:25  iteration: 216/1000  consumed samples: 3456  total_loss: 7.974  time: 0.1612(99.29)  data_time: 0.0167  lr: 9.00e-05  
[01/19 16:10:09] lb.utils.events INFO:  eta: 0:02:25  iteration: 217/1000  consumed samples: 3472  total_loss: 7.974  time: 0.1614(99.15)  data_time: 0.0168  lr: 9.00e-05  
[01/19 16:10:09] lb.utils.events INFO:  eta: 0:02:25  iteration: 218/1000  consumed samples: 3488  total_loss: 7.973  time: 0.1616(99.02)  data_time: 0.0165  lr: 9.00e-05  
[01/19 16:10:10] lb.utils.events INFO:  eta: 0:02:25  iteration: 219/1000  consumed samples: 3504  total_loss: 7.97  time: 0.1618(98.89)  data_time: 0.0165  lr: 9.00e-05  
[01/19 16:10:10] lb.utils.events INFO:  eta: 0:02:24  iteration: 220/1000  consumed samples: 3520  total_loss: 7.969  time: 0.1620(98.76)  data_time: 0.0176  lr: 9.00e-05  
[01/19 16:10:10] lb.utils.events INFO:  eta: 0:02:24  iteration: 221/1000  consumed samples: 3536  total_loss: 7.969  time: 0.1622(98.64)  data_time: 0.0180  lr: 9.00e-05  
[01/19 16:10:10] lb.utils.events INFO:  eta: 0:02:24  iteration: 222/1000  consumed samples: 3552  total_loss: 7.969  time: 0.1624(98.49)  data_time: 0.0155  lr: 9.00e-05  
[01/19 16:10:11] lb.utils.events INFO:  eta: 0:02:24  iteration: 223/1000  consumed samples: 3568  total_loss: 7.968  time: 0.1627(98.37)  data_time: 0.0157  lr: 9.00e-05  
[01/19 16:10:11] lb.utils.events INFO:  eta: 0:02:24  iteration: 224/1000  consumed samples: 3584  total_loss: 7.966  time: 0.1629(98.23)  data_time: 0.0168  lr: 9.00e-05  
[01/19 16:10:11] lb.utils.events INFO:  eta: 0:02:24  iteration: 225/1000  consumed samples: 3600  total_loss: 7.966  time: 0.1631(98.10)  data_time: 0.0171  lr: 9.00e-05  
[01/19 16:10:12] lb.utils.events INFO:  eta: 0:02:23  iteration: 226/1000  consumed samples: 3616  total_loss: 7.965  time: 0.1627(98.32)  data_time: 0.0167  lr: 9.00e-05  
[01/19 16:10:12] lb.utils.events INFO:  eta: 0:02:23  iteration: 227/1000  consumed samples: 3632  total_loss: 7.962  time: 0.1630(98.15)  data_time: 0.0227  lr: 9.00e-05  
[01/19 16:10:12] lb.utils.events INFO:  eta: 0:02:23  iteration: 228/1000  consumed samples: 3648  total_loss: 7.961  time: 0.1627(98.34)  data_time: 0.0221  lr: 9.00e-05  
[01/19 16:10:12] lb.utils.events INFO:  eta: 0:02:23  iteration: 229/1000  consumed samples: 3664  total_loss: 7.961  time: 0.1627(98.32)  data_time: 0.0213  lr: 9.00e-05  
[01/19 16:10:13] lb.utils.events INFO:  eta: 0:02:23  iteration: 230/1000  consumed samples: 3680  total_loss: 7.96  time: 0.1628(98.28)  data_time: 0.0211  lr: 9.00e-05  
[01/19 16:10:13] lb.utils.events INFO:  eta: 0:02:23  iteration: 231/1000  consumed samples: 3696  total_loss: 7.959  time: 0.1629(98.21)  data_time: 0.0218  lr: 9.00e-05  
[01/19 16:10:13] lb.utils.events INFO:  eta: 0:02:22  iteration: 232/1000  consumed samples: 3712  total_loss: 7.958  time: 0.1627(98.37)  data_time: 0.0215  lr: 9.00e-05  
[01/19 16:10:13] lb.utils.events INFO:  eta: 0:02:22  iteration: 233/1000  consumed samples: 3728  total_loss: 7.958  time: 0.1625(98.43)  data_time: 0.0212  lr: 9.00e-05  
[01/19 16:10:14] lb.utils.events INFO:  eta: 0:02:22  iteration: 234/1000  consumed samples: 3744  total_loss: 7.958  time: 0.1626(98.43)  data_time: 0.0207  lr: 9.00e-05  
[01/19 16:10:14] lb.utils.events INFO:  eta: 0:02:22  iteration: 235/1000  consumed samples: 3760  total_loss: 7.956  time: 0.1626(98.38)  data_time: 0.0206  lr: 9.00e-05  
[01/19 16:10:14] lb.utils.events INFO:  eta: 0:02:21  iteration: 236/1000  consumed samples: 3776  total_loss: 7.955  time: 0.1628(98.30)  data_time: 0.0182  lr: 9.00e-05  
[01/19 16:10:14] lb.utils.events INFO:  eta: 0:02:21  iteration: 237/1000  consumed samples: 3792  total_loss: 7.955  time: 0.1629(98.20)  data_time: 0.0182  lr: 9.00e-05  
[01/19 16:10:14] lb.utils.events INFO:  eta: 0:02:21  iteration: 238/1000  consumed samples: 3808  total_loss: 7.955  time: 0.1631(98.10)  data_time: 0.0183  lr: 9.00e-05  
[01/19 16:10:15] lb.utils.events INFO:  eta: 0:02:21  iteration: 239/1000  consumed samples: 3824  total_loss: 7.955  time: 0.1632(98.02)  data_time: 0.0183  lr: 9.00e-05  
[01/19 16:10:15] lb.utils.events INFO:  eta: 0:02:21  iteration: 240/1000  consumed samples: 3840  total_loss: 7.955  time: 0.1634(97.93)  data_time: 0.0193  lr: 9.00e-05  
[01/19 16:10:15] lb.utils.events INFO:  eta: 0:02:21  iteration: 241/1000  consumed samples: 3856  total_loss: 7.954  time: 0.1634(97.94)  data_time: 0.0192  lr: 9.00e-05  
[01/19 16:10:15] lb.utils.events INFO:  eta: 0:02:21  iteration: 242/1000  consumed samples: 3872  total_loss: 7.952  time: 0.1635(97.83)  data_time: 0.0192  lr: 9.00e-05  
[01/19 16:10:16] lb.utils.events INFO:  eta: 0:02:20  iteration: 243/1000  consumed samples: 3888  total_loss: 7.952  time: 0.1636(97.82)  data_time: 0.0192  lr: 9.00e-05  
[01/19 16:10:16] lb.utils.events INFO:  eta: 0:02:20  iteration: 244/1000  consumed samples: 3904  total_loss: 7.95  time: 0.1638(97.68)  data_time: 0.0236  lr: 9.00e-05  
[01/19 16:10:16] lb.utils.events INFO:  eta: 0:02:20  iteration: 245/1000  consumed samples: 3920  total_loss: 7.952  time: 0.1636(97.81)  data_time: 0.0233  lr: 9.00e-05  
[01/19 16:10:16] lb.utils.events INFO:  eta: 0:02:20  iteration: 246/1000  consumed samples: 3936  total_loss: 7.952  time: 0.1633(97.99)  data_time: 0.0233  lr: 9.00e-05  
[01/19 16:10:17] lb.utils.events INFO:  eta: 0:02:20  iteration: 247/1000  consumed samples: 3952  total_loss: 7.954  time: 0.1630(98.17)  data_time: 0.0175  lr: 9.00e-05  
[01/19 16:10:17] lb.utils.events INFO:  eta: 0:02:19  iteration: 248/1000  consumed samples: 3968  total_loss: 7.952  time: 0.1627(98.33)  data_time: 0.0178  lr: 9.00e-05  
[01/19 16:10:17] lb.utils.events INFO:  eta: 0:02:19  iteration: 249/1000  consumed samples: 3984  total_loss: 7.95  time: 0.1626(98.41)  data_time: 0.0178  lr: 9.00e-05  
[01/19 16:10:17] lb.utils.events INFO:  eta: 0:02:19  iteration: 250/1000  consumed samples: 4000  total_loss: 7.949  time: 0.1627(98.37)  data_time: 0.0217  lr: 9.00e-05  
[01/19 16:10:18] lb.utils.events INFO:  eta: 0:02:19  iteration: 251/1000  consumed samples: 4016  total_loss: 7.948  time: 0.1626(98.41)  data_time: 0.0207  lr: 9.00e-05  
[01/19 16:10:18] lb.utils.events INFO:  eta: 0:02:18  iteration: 252/1000  consumed samples: 4032  total_loss: 7.948  time: 0.1625(98.46)  data_time: 0.0204  lr: 9.00e-05  
[01/19 16:10:18] lb.utils.events INFO:  eta: 0:02:18  iteration: 253/1000  consumed samples: 4048  total_loss: 7.945  time: 0.1623(98.61)  data_time: 0.0199  lr: 9.00e-05  
[01/19 16:10:18] lb.utils.events INFO:  eta: 0:02:17  iteration: 254/1000  consumed samples: 4064  total_loss: 7.943  time: 0.1620(98.74)  data_time: 0.0205  lr: 9.00e-05  
[01/19 16:10:19] lb.utils.events INFO:  eta: 0:02:18  iteration: 255/1000  consumed samples: 4080  total_loss: 7.942  time: 0.1623(98.59)  data_time: 0.0267  lr: 9.00e-05  
[01/19 16:10:19] lb.utils.events INFO:  eta: 0:02:17  iteration: 256/1000  consumed samples: 4096  total_loss: 7.942  time: 0.1620(98.78)  data_time: 0.0240  lr: 9.00e-05  
[01/19 16:10:19] lb.utils.events INFO:  eta: 0:02:16  iteration: 257/1000  consumed samples: 4112  total_loss: 7.942  time: 0.1619(98.80)  data_time: 0.0240  lr: 9.00e-05  
[01/19 16:10:19] lb.utils.events INFO:  eta: 0:02:15  iteration: 258/1000  consumed samples: 4128  total_loss: 7.942  time: 0.1618(98.91)  data_time: 0.0239  lr: 9.00e-05  
[01/19 16:10:20] lb.utils.events INFO:  eta: 0:02:15  iteration: 259/1000  consumed samples: 4144  total_loss: 7.942  time: 0.1615(99.04)  data_time: 0.0248  lr: 9.00e-05  
[01/19 16:10:20] lb.utils.events INFO:  eta: 0:02:15  iteration: 260/1000  consumed samples: 4160  total_loss: 7.942  time: 0.1613(99.22)  data_time: 0.0221  lr: 9.00e-05  
[01/19 16:10:20] lb.utils.events INFO:  eta: 0:02:14  iteration: 261/1000  consumed samples: 4176  total_loss: 7.942  time: 0.1610(99.36)  data_time: 0.0226  lr: 9.00e-05  
[01/19 16:10:20] lb.utils.events INFO:  eta: 0:02:14  iteration: 262/1000  consumed samples: 4192  total_loss: 7.941  time: 0.1608(99.52)  data_time: 0.0229  lr: 9.00e-05  
[01/19 16:10:21] lb.utils.events INFO:  eta: 0:02:14  iteration: 263/1000  consumed samples: 4208  total_loss: 7.941  time: 0.1606(99.60)  data_time: 0.0243  lr: 9.00e-05  
[01/19 16:10:21] lb.utils.events INFO:  eta: 0:02:14  iteration: 264/1000  consumed samples: 4224  total_loss: 7.938  time: 0.1604(99.73)  data_time: 0.0187  lr: 9.00e-05  
[01/19 16:10:21] lb.utils.events INFO:  eta: 0:02:13  iteration: 265/1000  consumed samples: 4240  total_loss: 7.933  time: 0.1602(99.86)  data_time: 0.0197  lr: 9.00e-05  
[01/19 16:10:22] lb.utils.events INFO:  eta: 0:02:13  iteration: 266/1000  consumed samples: 4256  total_loss: 7.933  time: 0.1600(100.01)  data_time: 0.0200  lr: 9.00e-05  
[01/19 16:10:22] lb.utils.events INFO:  eta: 0:02:12  iteration: 267/1000  consumed samples: 4272  total_loss: 7.933  time: 0.1598(100.09)  data_time: 0.0217  lr: 9.00e-05  
[01/19 16:10:22] lb.utils.events INFO:  eta: 0:02:12  iteration: 268/1000  consumed samples: 4288  total_loss: 7.933  time: 0.1597(100.20)  data_time: 0.0213  lr: 9.00e-05  
[01/19 16:10:23] lb.utils.events INFO:  eta: 0:02:12  iteration: 269/1000  consumed samples: 4304  total_loss: 7.93  time: 0.1596(100.27)  data_time: 0.0217  lr: 9.00e-05  
[01/19 16:10:23] lb.utils.events INFO:  eta: 0:02:11  iteration: 270/1000  consumed samples: 4320  total_loss: 7.929  time: 0.1594(100.37)  data_time: 0.0192  lr: 9.00e-05  
[01/19 16:10:23] lb.utils.events INFO:  eta: 0:02:11  iteration: 271/1000  consumed samples: 4336  total_loss: 7.929  time: 0.1591(100.54)  data_time: 0.0190  lr: 9.00e-05  
[01/19 16:10:24] lb.utils.events INFO:  eta: 0:02:10  iteration: 272/1000  consumed samples: 4352  total_loss: 7.927  time: 0.1589(100.71)  data_time: 0.0190  lr: 9.00e-05  
[01/19 16:10:24] lb.utils.events INFO:  eta: 0:02:10  iteration: 273/1000  consumed samples: 4368  total_loss: 7.924  time: 0.1586(100.87)  data_time: 0.0191  lr: 9.00e-05  
[01/19 16:10:24] lb.utils.events INFO:  eta: 0:02:10  iteration: 274/1000  consumed samples: 4384  total_loss: 7.923  time: 0.1584(101.04)  data_time: 0.0182  lr: 9.00e-05  
[01/19 16:10:24] lb.utils.events INFO:  eta: 0:02:09  iteration: 275/1000  consumed samples: 4400  total_loss: 7.923  time: 0.1583(101.04)  data_time: 0.0149  lr: 9.00e-05  
[01/19 16:10:25] lb.utils.events INFO:  eta: 0:02:09  iteration: 276/1000  consumed samples: 4416  total_loss: 7.923  time: 0.1581(101.22)  data_time: 0.0148  lr: 9.00e-05  
[01/19 16:10:25] lb.utils.events INFO:  eta: 0:02:09  iteration: 277/1000  consumed samples: 4432  total_loss: 7.923  time: 0.1578(101.37)  data_time: 0.0148  lr: 9.00e-05  
[01/19 16:10:25] lb.utils.events INFO:  eta: 0:02:09  iteration: 278/1000  consumed samples: 4448  total_loss: 7.922  time: 0.1576(101.54)  data_time: 0.0148  lr: 9.00e-05  
[01/19 16:10:26] lb.utils.events INFO:  eta: 0:02:08  iteration: 279/1000  consumed samples: 4464  total_loss: 7.921  time: 0.1574(101.68)  data_time: 0.0144  lr: 9.00e-05  
[01/19 16:10:26] lb.utils.events INFO:  eta: 0:02:08  iteration: 280/1000  consumed samples: 4480  total_loss: 7.921  time: 0.1572(101.79)  data_time: 0.0154  lr: 9.00e-05  
[01/19 16:10:26] lb.utils.events INFO:  eta: 0:02:08  iteration: 281/1000  consumed samples: 4496  total_loss: 7.921  time: 0.1570(101.89)  data_time: 0.0154  lr: 9.00e-05  
[01/19 16:10:27] lb.utils.events INFO:  eta: 0:02:07  iteration: 282/1000  consumed samples: 4512  total_loss: 7.921  time: 0.1568(102.06)  data_time: 0.0151  lr: 9.00e-05  
[01/19 16:10:27] lb.utils.events INFO:  eta: 0:02:07  iteration: 283/1000  consumed samples: 4528  total_loss: 7.921  time: 0.1565(102.23)  data_time: 0.0134  lr: 9.00e-05  
[01/19 16:10:28] lb.utils.events INFO:  eta: 0:02:07  iteration: 284/1000  consumed samples: 4544  total_loss: 7.921  time: 0.1563(102.34)  data_time: 0.0142  lr: 9.00e-05  
[01/19 16:10:29] lb.utils.events INFO:  eta: 0:02:06  iteration: 285/1000  consumed samples: 4560  total_loss: 7.921  time: 0.1563(102.37)  data_time: 0.0156  lr: 9.00e-05  
[01/19 16:10:29] lb.utils.events INFO:  eta: 0:02:06  iteration: 286/1000  consumed samples: 4576  total_loss: 7.92  time: 0.1561(102.51)  data_time: 0.0155  lr: 9.00e-05  
[01/19 16:10:29] lb.utils.events INFO:  eta: 0:02:06  iteration: 287/1000  consumed samples: 4592  total_loss: 7.92  time: 0.1559(102.66)  data_time: 0.0136  lr: 9.00e-05  
[01/19 16:10:29] lb.utils.events INFO:  eta: 0:02:05  iteration: 288/1000  consumed samples: 4608  total_loss: 7.919  time: 0.1557(102.76)  data_time: 0.0150  lr: 9.00e-05  
[01/19 16:10:30] lb.utils.events INFO:  eta: 0:02:04  iteration: 289/1000  consumed samples: 4624  total_loss: 7.918  time: 0.1555(102.92)  data_time: 0.0148  lr: 9.00e-05  
[01/19 16:10:30] lb.utils.events INFO:  eta: 0:02:03  iteration: 290/1000  consumed samples: 4640  total_loss: 7.919  time: 0.1552(103.08)  data_time: 0.0134  lr: 9.00e-05  
[01/19 16:10:30] lb.utils.events INFO:  eta: 0:02:02  iteration: 291/1000  consumed samples: 4656  total_loss: 7.918  time: 0.1550(103.23)  data_time: 0.0135  lr: 9.00e-05  
[01/19 16:10:31] lb.utils.events INFO:  eta: 0:02:01  iteration: 292/1000  consumed samples: 4672  total_loss: 7.917  time: 0.1548(103.38)  data_time: 0.0138  lr: 9.00e-05  
[01/19 16:10:31] lb.utils.events INFO:  eta: 0:02:01  iteration: 293/1000  consumed samples: 4688  total_loss: 7.915  time: 0.1545(103.54)  data_time: 0.0137  lr: 9.00e-05  
[01/19 16:10:31] lb.utils.events INFO:  eta: 0:02:01  iteration: 294/1000  consumed samples: 4704  total_loss: 7.914  time: 0.1543(103.69)  data_time: 0.0136  lr: 9.00e-05  
[01/19 16:10:32] lb.utils.events INFO:  eta: 0:02:01  iteration: 295/1000  consumed samples: 4720  total_loss: 7.914  time: 0.1541(103.86)  data_time: 0.0102  lr: 9.00e-05  
[01/19 16:10:32] lb.utils.events INFO:  eta: 0:02:00  iteration: 296/1000  consumed samples: 4736  total_loss: 7.912  time: 0.1539(103.98)  data_time: 0.0109  lr: 9.00e-05  
[01/19 16:10:32] lb.utils.events INFO:  eta: 0:02:00  iteration: 297/1000  consumed samples: 4752  total_loss: 7.911  time: 0.1536(104.14)  data_time: 0.0110  lr: 9.00e-05  
[01/19 16:10:33] lb.utils.events INFO:  eta: 0:01:59  iteration: 298/1000  consumed samples: 4768  total_loss: 7.911  time: 0.1534(104.30)  data_time: 0.0110  lr: 9.00e-05  
[01/19 16:10:33] lb.utils.events INFO:  eta: 0:01:58  iteration: 299/1000  consumed samples: 4784  total_loss: 7.91  time: 0.1532(104.46)  data_time: 0.0106  lr: 9.00e-05  
[01/19 16:10:33] lb.utils.events INFO:  eta: 0:01:58  iteration: 300/1000  consumed samples: 4800  total_loss: 7.91  time: 0.1529(104.62)  data_time: 0.0097  lr: 9.00e-05  
[01/19 16:10:34] lb.utils.events INFO:  eta: 0:01:58  iteration: 301/1000  consumed samples: 4816  total_loss: 7.91  time: 0.1527(104.78)  data_time: 0.0090  lr: 9.00e-05  
[01/19 16:10:34] lb.utils.events INFO:  eta: 0:01:58  iteration: 302/1000  consumed samples: 4832  total_loss: 7.909  time: 0.1525(104.94)  data_time: 0.0090  lr: 9.00e-05  
[01/19 16:10:34] lb.utils.events INFO:  eta: 0:01:58  iteration: 303/1000  consumed samples: 4848  total_loss: 7.908  time: 0.1522(105.11)  data_time: 0.0090  lr: 9.00e-05  
[01/19 16:10:35] lb.utils.events INFO:  eta: 0:01:57  iteration: 304/1000  consumed samples: 4864  total_loss: 7.907  time: 0.1522(105.16)  data_time: 0.0107  lr: 9.00e-05  
[01/19 16:10:35] lb.utils.events INFO:  eta: 0:01:57  iteration: 305/1000  consumed samples: 4880  total_loss: 7.908  time: 0.1519(105.31)  data_time: 0.0082  lr: 9.00e-05  
[01/19 16:10:35] lb.utils.events INFO:  eta: 0:01:56  iteration: 306/1000  consumed samples: 4896  total_loss: 7.908  time: 0.1517(105.46)  data_time: 0.0084  lr: 9.00e-05  
[01/19 16:10:36] lb.utils.events INFO:  eta: 0:01:56  iteration: 307/1000  consumed samples: 4912  total_loss: 7.908  time: 0.1515(105.62)  data_time: 0.0084  lr: 9.00e-05  
[01/19 16:10:36] lb.utils.events INFO:  eta: 0:01:56  iteration: 308/1000  consumed samples: 4928  total_loss: 7.908  time: 0.1513(105.77)  data_time: 0.0071  lr: 9.00e-05  
[01/19 16:10:36] lb.utils.events INFO:  eta: 0:01:55  iteration: 309/1000  consumed samples: 4944  total_loss: 7.908  time: 0.1510(105.93)  data_time: 0.0070  lr: 9.00e-05  
[01/19 16:10:36] lb.utils.events INFO:  eta: 0:01:55  iteration: 310/1000  consumed samples: 4960  total_loss: 7.908  time: 0.1508(106.08)  data_time: 0.0071  lr: 9.00e-05  
[01/19 16:10:37] lb.utils.events INFO:  eta: 0:01:55  iteration: 311/1000  consumed samples: 4976  total_loss: 7.907  time: 0.1506(106.24)  data_time: 0.0070  lr: 9.00e-05  
[01/19 16:10:37] lb.utils.events INFO:  eta: 0:01:54  iteration: 312/1000  consumed samples: 4992  total_loss: 7.905  time: 0.1504(106.38)  data_time: 0.0070  lr: 9.00e-05  
[01/19 16:10:37] lb.utils.events INFO:  eta: 0:01:53  iteration: 313/1000  consumed samples: 5008  total_loss: 7.907  time: 0.1502(106.53)  data_time: 0.0072  lr: 9.00e-05  
[01/19 16:10:37] lb.utils.events INFO:  eta: 0:01:53  iteration: 314/1000  consumed samples: 5024  total_loss: 7.905  time: 0.1500(106.68)  data_time: 0.0074  lr: 9.00e-05  
[01/19 16:10:38] lb.utils.events INFO:  eta: 0:01:52  iteration: 315/1000  consumed samples: 5040  total_loss: 7.901  time: 0.1498(106.83)  data_time: 0.0074  lr: 9.00e-05  
[01/19 16:10:38] lb.utils.events INFO:  eta: 0:01:52  iteration: 316/1000  consumed samples: 5056  total_loss: 7.899  time: 0.1496(106.98)  data_time: 0.0068  lr: 9.00e-05  
[01/19 16:10:39] lb.utils.events INFO:  eta: 0:01:51  iteration: 317/1000  consumed samples: 5072  total_loss: 7.898  time: 0.1494(107.13)  data_time: 0.0069  lr: 9.00e-05  
[01/19 16:10:39] lb.utils.events INFO:  eta: 0:01:51  iteration: 318/1000  consumed samples: 5088  total_loss: 7.898  time: 0.1491(107.29)  data_time: 0.0068  lr: 9.00e-05  
[01/19 16:10:40] lb.utils.events INFO:  eta: 0:01:51  iteration: 319/1000  consumed samples: 5104  total_loss: 7.898  time: 0.1489(107.44)  data_time: 0.0069  lr: 9.00e-05  
[01/19 16:10:40] lb.utils.events INFO:  eta: 0:01:50  iteration: 320/1000  consumed samples: 5120  total_loss: 7.897  time: 0.1487(107.59)  data_time: 0.0067  lr: 9.00e-05  
[01/19 16:10:40] lb.utils.events INFO:  eta: 0:01:50  iteration: 321/1000  consumed samples: 5136  total_loss: 7.896  time: 0.1485(107.74)  data_time: 0.0069  lr: 9.00e-05  
[01/19 16:10:41] lb.utils.events INFO:  eta: 0:01:49  iteration: 322/1000  consumed samples: 5152  total_loss: 7.896  time: 0.1483(107.88)  data_time: 0.0069  lr: 9.00e-05  
[01/19 16:10:41] lb.utils.events INFO:  eta: 0:01:48  iteration: 323/1000  consumed samples: 5168  total_loss: 7.895  time: 0.1481(108.03)  data_time: 0.0069  lr: 9.00e-05  
[01/19 16:10:41] lb.utils.events INFO:  eta: 0:01:47  iteration: 324/1000  consumed samples: 5184  total_loss: 7.895  time: 0.1479(108.18)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:10:42] lb.utils.events INFO:  eta: 0:01:46  iteration: 325/1000  consumed samples: 5200  total_loss: 7.893  time: 0.1477(108.32)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:10:42] lb.utils.events INFO:  eta: 0:01:46  iteration: 326/1000  consumed samples: 5216  total_loss: 7.89  time: 0.1475(108.48)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:10:42] lb.utils.events INFO:  eta: 0:01:46  iteration: 327/1000  consumed samples: 5232  total_loss: 7.888  time: 0.1474(108.57)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:10:42] lb.utils.events INFO:  eta: 0:01:45  iteration: 328/1000  consumed samples: 5248  total_loss: 7.888  time: 0.1472(108.70)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:10:43] lb.utils.events INFO:  eta: 0:01:44  iteration: 329/1000  consumed samples: 5264  total_loss: 7.888  time: 0.1470(108.85)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:10:43] lb.utils.events INFO:  eta: 0:01:43  iteration: 330/1000  consumed samples: 5280  total_loss: 7.884  time: 0.1470(108.86)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:10:43] lb.utils.events INFO:  eta: 0:01:42  iteration: 331/1000  consumed samples: 5296  total_loss: 7.881  time: 0.1468(109.01)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:10:44] lb.utils.events INFO:  eta: 0:01:41  iteration: 332/1000  consumed samples: 5312  total_loss: 7.878  time: 0.1466(109.15)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:10:44] lb.utils.events INFO:  eta: 0:01:40  iteration: 333/1000  consumed samples: 5328  total_loss: 7.878  time: 0.1464(109.27)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:10:45] lb.utils.events INFO:  eta: 0:01:40  iteration: 334/1000  consumed samples: 5344  total_loss: 7.878  time: 0.1462(109.42)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:10:45] lb.utils.events INFO:  eta: 0:01:40  iteration: 335/1000  consumed samples: 5360  total_loss: 7.875  time: 0.1460(109.56)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:10:45] lb.utils.events INFO:  eta: 0:01:39  iteration: 336/1000  consumed samples: 5376  total_loss: 7.873  time: 0.1459(109.69)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:10:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 337/1000  consumed samples: 5392  total_loss: 7.873  time: 0.1457(109.82)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:10:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 338/1000  consumed samples: 5408  total_loss: 7.873  time: 0.1455(109.96)  data_time: 0.0052  lr: 9.00e-05  
[01/19 16:10:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 339/1000  consumed samples: 5424  total_loss: 7.873  time: 0.1453(110.10)  data_time: 0.0052  lr: 9.00e-05  
[01/19 16:10:46] lb.utils.events INFO:  eta: 0:01:35  iteration: 340/1000  consumed samples: 5440  total_loss: 7.873  time: 0.1451(110.23)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:47] lb.utils.events INFO:  eta: 0:01:34  iteration: 341/1000  consumed samples: 5456  total_loss: 7.873  time: 0.1450(110.36)  data_time: 0.0052  lr: 9.00e-05  
[01/19 16:10:47] lb.utils.events INFO:  eta: 0:01:34  iteration: 342/1000  consumed samples: 5472  total_loss: 7.873  time: 0.1448(110.50)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:47] lb.utils.events INFO:  eta: 0:01:33  iteration: 343/1000  consumed samples: 5488  total_loss: 7.873  time: 0.1446(110.62)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:47] lb.utils.events INFO:  eta: 0:01:33  iteration: 344/1000  consumed samples: 5504  total_loss: 7.872  time: 0.1445(110.76)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:10:48] lb.utils.events INFO:  eta: 0:01:33  iteration: 345/1000  consumed samples: 5520  total_loss: 7.872  time: 0.1443(110.89)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:48] lb.utils.events INFO:  eta: 0:01:33  iteration: 346/1000  consumed samples: 5536  total_loss: 7.871  time: 0.1441(111.03)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:48] lb.utils.events INFO:  eta: 0:01:32  iteration: 347/1000  consumed samples: 5552  total_loss: 7.869  time: 0.1439(111.18)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:49] lb.utils.events INFO:  eta: 0:01:32  iteration: 348/1000  consumed samples: 5568  total_loss: 7.868  time: 0.1438(111.30)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:10:49] lb.utils.events INFO:  eta: 0:01:32  iteration: 349/1000  consumed samples: 5584  total_loss: 7.868  time: 0.1436(111.44)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:50] lb.utils.events INFO:  eta: 0:01:31  iteration: 350/1000  consumed samples: 5600  total_loss: 7.868  time: 0.1434(111.56)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:50] lb.utils.events INFO:  eta: 0:01:29  iteration: 351/1000  consumed samples: 5616  total_loss: 7.866  time: 0.1433(111.68)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:10:50] lb.utils.events INFO:  eta: 0:01:29  iteration: 352/1000  consumed samples: 5632  total_loss: 7.866  time: 0.1432(111.70)  data_time: 0.0071  lr: 9.00e-05  
[01/19 16:10:51] lb.utils.events INFO:  eta: 0:01:29  iteration: 353/1000  consumed samples: 5648  total_loss: 7.868  time: 0.1432(111.77)  data_time: 0.0066  lr: 9.00e-05  
[01/19 16:10:52] lb.utils.events INFO:  eta: 0:01:28  iteration: 354/1000  consumed samples: 5664  total_loss: 7.868  time: 0.1430(111.88)  data_time: 0.0072  lr: 9.00e-05  
[01/19 16:10:52] lb.utils.events INFO:  eta: 0:01:27  iteration: 355/1000  consumed samples: 5680  total_loss: 7.866  time: 0.1429(112.00)  data_time: 0.0073  lr: 9.00e-05  
[01/19 16:10:53] lb.utils.events INFO:  eta: 0:01:28  iteration: 356/1000  consumed samples: 5696  total_loss: 7.864  time: 0.1429(112.00)  data_time: 0.0097  lr: 9.00e-05  
[01/19 16:10:53] lb.utils.events INFO:  eta: 0:01:27  iteration: 357/1000  consumed samples: 5712  total_loss: 7.866  time: 0.1427(112.12)  data_time: 0.0094  lr: 9.00e-05  
[01/19 16:10:54] lb.utils.events INFO:  eta: 0:01:27  iteration: 358/1000  consumed samples: 5728  total_loss: 7.864  time: 0.1425(112.25)  data_time: 0.0092  lr: 9.00e-05  
[01/19 16:10:54] lb.utils.events INFO:  eta: 0:01:27  iteration: 359/1000  consumed samples: 5744  total_loss: 7.864  time: 0.1424(112.36)  data_time: 0.0092  lr: 9.00e-05  
[01/19 16:10:55] lb.utils.events INFO:  eta: 0:01:26  iteration: 360/1000  consumed samples: 5760  total_loss: 7.863  time: 0.1423(112.44)  data_time: 0.0099  lr: 9.00e-05  
[01/19 16:10:55] lb.utils.events INFO:  eta: 0:01:25  iteration: 361/1000  consumed samples: 5776  total_loss: 7.862  time: 0.1422(112.55)  data_time: 0.0098  lr: 9.00e-05  
[01/19 16:10:56] lb.utils.events INFO:  eta: 0:01:24  iteration: 362/1000  consumed samples: 5792  total_loss: 7.862  time: 0.1420(112.67)  data_time: 0.0098  lr: 9.00e-05  
[01/19 16:10:56] lb.utils.events INFO:  eta: 0:01:23  iteration: 363/1000  consumed samples: 5808  total_loss: 7.861  time: 0.1419(112.79)  data_time: 0.0098  lr: 9.00e-05  
[01/19 16:10:57] lb.utils.events INFO:  eta: 0:01:22  iteration: 364/1000  consumed samples: 5824  total_loss: 7.861  time: 0.1417(112.89)  data_time: 0.0101  lr: 9.00e-05  
[01/19 16:10:57] lb.utils.events INFO:  eta: 0:01:22  iteration: 365/1000  consumed samples: 5840  total_loss: 7.861  time: 0.1416(113.02)  data_time: 0.0099  lr: 9.00e-05  
[01/19 16:10:57] lb.utils.events INFO:  eta: 0:01:21  iteration: 366/1000  consumed samples: 5856  total_loss: 7.858  time: 0.1414(113.13)  data_time: 0.0099  lr: 9.00e-05  
[01/19 16:10:58] lb.utils.events INFO:  eta: 0:01:21  iteration: 367/1000  consumed samples: 5872  total_loss: 7.852  time: 0.1413(113.24)  data_time: 0.0102  lr: 9.00e-05  
[01/19 16:10:59] lb.utils.events INFO:  eta: 0:01:21  iteration: 368/1000  consumed samples: 5888  total_loss: 7.848  time: 0.1414(113.11)  data_time: 0.0155  lr: 9.00e-05  
[01/19 16:10:59] lb.utils.events INFO:  eta: 0:01:20  iteration: 369/1000  consumed samples: 5904  total_loss: 7.847  time: 0.1413(113.23)  data_time: 0.0153  lr: 9.00e-05  
[01/19 16:10:59] lb.utils.events INFO:  eta: 0:01:20  iteration: 370/1000  consumed samples: 5920  total_loss: 7.847  time: 0.1412(113.33)  data_time: 0.0153  lr: 9.00e-05  
[01/19 16:11:00] lb.utils.events INFO:  eta: 0:01:20  iteration: 371/1000  consumed samples: 5936  total_loss: 7.846  time: 0.1410(113.46)  data_time: 0.0154  lr: 9.00e-05  
[01/19 16:11:00] lb.utils.events INFO:  eta: 0:01:19  iteration: 372/1000  consumed samples: 5952  total_loss: 7.845  time: 0.1409(113.52)  data_time: 0.0138  lr: 9.00e-05  
[01/19 16:11:00] lb.utils.events INFO:  eta: 0:01:19  iteration: 373/1000  consumed samples: 5968  total_loss: 7.843  time: 0.1408(113.63)  data_time: 0.0136  lr: 9.00e-05  
[01/19 16:11:00] lb.utils.events INFO:  eta: 0:01:18  iteration: 374/1000  consumed samples: 5984  total_loss: 7.842  time: 0.1407(113.71)  data_time: 0.0129  lr: 9.00e-05  
[01/19 16:11:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 375/1000  consumed samples: 6000  total_loss: 7.843  time: 0.1407(113.73)  data_time: 0.0147  lr: 9.00e-05  
[01/19 16:11:01] lb.utils.events INFO:  eta: 0:01:18  iteration: 376/1000  consumed samples: 6016  total_loss: 7.842  time: 0.1406(113.80)  data_time: 0.0120  lr: 9.00e-05  
[01/19 16:11:02] lb.utils.events INFO:  eta: 0:01:18  iteration: 377/1000  consumed samples: 6032  total_loss: 7.843  time: 0.1404(113.92)  data_time: 0.0120  lr: 9.00e-05  
[01/19 16:11:02] lb.utils.events INFO:  eta: 0:01:18  iteration: 378/1000  consumed samples: 6048  total_loss: 7.842  time: 0.1403(114.04)  data_time: 0.0120  lr: 9.00e-05  
[01/19 16:11:02] lb.utils.events INFO:  eta: 0:01:18  iteration: 379/1000  consumed samples: 6064  total_loss: 7.843  time: 0.1403(114.07)  data_time: 0.0121  lr: 9.00e-05  
[01/19 16:11:03] lb.utils.events INFO:  eta: 0:01:17  iteration: 380/1000  consumed samples: 6080  total_loss: 7.843  time: 0.1401(114.19)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:11:03] lb.utils.events INFO:  eta: 0:01:17  iteration: 381/1000  consumed samples: 6096  total_loss: 7.843  time: 0.1401(114.20)  data_time: 0.0114  lr: 9.00e-05  
[01/19 16:11:03] lb.utils.events INFO:  eta: 0:01:17  iteration: 382/1000  consumed samples: 6112  total_loss: 7.845  time: 0.1400(114.32)  data_time: 0.0114  lr: 9.00e-05  
[01/19 16:11:03] lb.utils.events INFO:  eta: 0:01:17  iteration: 383/1000  consumed samples: 6128  total_loss: 7.843  time: 0.1399(114.40)  data_time: 0.0116  lr: 9.00e-05  
[01/19 16:11:03] lb.utils.events INFO:  eta: 0:01:15  iteration: 384/1000  consumed samples: 6144  total_loss: 7.843  time: 0.1398(114.47)  data_time: 0.0112  lr: 9.00e-05  
[01/19 16:11:04] lb.utils.events INFO:  eta: 0:01:13  iteration: 385/1000  consumed samples: 6160  total_loss: 7.841  time: 0.1397(114.52)  data_time: 0.0112  lr: 9.00e-05  
[01/19 16:11:04] lb.utils.events INFO:  eta: 0:01:13  iteration: 386/1000  consumed samples: 6176  total_loss: 7.84  time: 0.1396(114.63)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:11:05] lb.utils.events INFO:  eta: 0:01:12  iteration: 387/1000  consumed samples: 6192  total_loss: 7.839  time: 0.1394(114.75)  data_time: 0.0114  lr: 9.00e-05  
[01/19 16:11:05] lb.utils.events INFO:  eta: 0:01:12  iteration: 388/1000  consumed samples: 6208  total_loss: 7.837  time: 0.1393(114.86)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:05] lb.utils.events INFO:  eta: 0:01:11  iteration: 389/1000  consumed samples: 6224  total_loss: 7.834  time: 0.1392(114.98)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:06] lb.utils.events INFO:  eta: 0:01:11  iteration: 390/1000  consumed samples: 6240  total_loss: 7.831  time: 0.1390(115.09)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:06] lb.utils.events INFO:  eta: 0:01:11  iteration: 391/1000  consumed samples: 6256  total_loss: 7.83  time: 0.1389(115.19)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:11:06] lb.utils.events INFO:  eta: 0:01:11  iteration: 392/1000  consumed samples: 6272  total_loss: 7.83  time: 0.1388(115.30)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:11:07] lb.utils.events INFO:  eta: 0:01:10  iteration: 393/1000  consumed samples: 6288  total_loss: 7.831  time: 0.1386(115.42)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:11:07] lb.utils.events INFO:  eta: 0:01:10  iteration: 394/1000  consumed samples: 6304  total_loss: 7.831  time: 0.1385(115.53)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:08] lb.utils.events INFO:  eta: 0:01:10  iteration: 395/1000  consumed samples: 6320  total_loss: 7.831  time: 0.1383(115.65)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:08] lb.utils.events INFO:  eta: 0:01:10  iteration: 396/1000  consumed samples: 6336  total_loss: 7.831  time: 0.1382(115.77)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:08] lb.utils.events INFO:  eta: 0:01:10  iteration: 397/1000  consumed samples: 6352  total_loss: 7.83  time: 0.1381(115.89)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:09] lb.utils.events INFO:  eta: 0:01:10  iteration: 398/1000  consumed samples: 6368  total_loss: 7.83  time: 0.1380(115.96)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:11:09] lb.utils.events INFO:  eta: 0:01:09  iteration: 399/1000  consumed samples: 6384  total_loss: 7.829  time: 0.1378(116.07)  data_time: 0.0053  lr: 9.00e-05  
[01/19 16:11:09] lb.utils.events INFO:  eta: 0:01:09  iteration: 400/1000  consumed samples: 6400  total_loss: 7.829  time: 0.1377(116.16)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:11:10] lb.utils.events INFO:  eta: 0:01:09  iteration: 401/1000  consumed samples: 6416  total_loss: 7.829  time: 0.1376(116.28)  data_time: 0.0053  lr: 9.00e-05  
[01/19 16:11:10] lb.utils.events INFO:  eta: 0:01:08  iteration: 402/1000  consumed samples: 6432  total_loss: 7.829  time: 0.1375(116.39)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:11:10] lb.utils.events INFO:  eta: 0:01:08  iteration: 403/1000  consumed samples: 6448  total_loss: 7.828  time: 0.1373(116.51)  data_time: 0.0053  lr: 9.00e-05  
[01/19 16:11:11] lb.utils.events INFO:  eta: 0:01:08  iteration: 404/1000  consumed samples: 6464  total_loss: 7.828  time: 0.1372(116.63)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:11:11] lb.utils.events INFO:  eta: 0:01:07  iteration: 405/1000  consumed samples: 6480  total_loss: 7.828  time: 0.1370(116.75)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:11:11] lb.utils.events INFO:  eta: 0:01:07  iteration: 406/1000  consumed samples: 6496  total_loss: 7.828  time: 0.1370(116.83)  data_time: 0.0060  lr: 9.00e-05  
[01/19 16:11:11] lb.utils.events INFO:  eta: 0:01:07  iteration: 407/1000  consumed samples: 6512  total_loss: 7.828  time: 0.1369(116.88)  data_time: 0.0057  lr: 9.00e-05  
[01/19 16:11:11] lb.utils.events INFO:  eta: 0:01:07  iteration: 408/1000  consumed samples: 6528  total_loss: 7.826  time: 0.1369(116.91)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:12] lb.utils.events INFO:  eta: 0:01:07  iteration: 409/1000  consumed samples: 6544  total_loss: 7.826  time: 0.1369(116.84)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:12] lb.utils.events INFO:  eta: 0:01:07  iteration: 410/1000  consumed samples: 6560  total_loss: 7.828  time: 0.1370(116.79)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:12] lb.utils.events INFO:  eta: 0:01:07  iteration: 411/1000  consumed samples: 6576  total_loss: 7.826  time: 0.1369(116.90)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:12] lb.utils.events INFO:  eta: 0:01:07  iteration: 412/1000  consumed samples: 6592  total_loss: 7.823  time: 0.1367(117.01)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:13] lb.utils.events INFO:  eta: 0:01:06  iteration: 413/1000  consumed samples: 6608  total_loss: 7.821  time: 0.1368(116.96)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:13] lb.utils.events INFO:  eta: 0:01:06  iteration: 414/1000  consumed samples: 6624  total_loss: 7.821  time: 0.1367(117.07)  data_time: 0.0057  lr: 9.00e-05  
[01/19 16:11:13] lb.utils.events INFO:  eta: 0:01:06  iteration: 415/1000  consumed samples: 6640  total_loss: 7.821  time: 0.1366(117.16)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:13] lb.utils.events INFO:  eta: 0:01:06  iteration: 416/1000  consumed samples: 6656  total_loss: 7.821  time: 0.1366(117.17)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 417/1000  consumed samples: 6672  total_loss: 7.82  time: 0.1365(117.21)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:11:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 418/1000  consumed samples: 6688  total_loss: 7.818  time: 0.1365(117.25)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 419/1000  consumed samples: 6704  total_loss: 7.817  time: 0.1366(117.17)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:14] lb.utils.events INFO:  eta: 0:01:06  iteration: 420/1000  consumed samples: 6720  total_loss: 7.815  time: 0.1364(117.27)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:15] lb.utils.events INFO:  eta: 0:01:06  iteration: 421/1000  consumed samples: 6736  total_loss: 7.815  time: 0.1363(117.38)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 422/1000  consumed samples: 6752  total_loss: 7.813  time: 0.1362(117.49)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 423/1000  consumed samples: 6768  total_loss: 7.813  time: 0.1361(117.56)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:15] lb.utils.events INFO:  eta: 0:01:05  iteration: 424/1000  consumed samples: 6784  total_loss: 7.812  time: 0.1361(117.52)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:16] lb.utils.events INFO:  eta: 0:01:05  iteration: 425/1000  consumed samples: 6800  total_loss: 7.812  time: 0.1361(117.60)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:16] lb.utils.events INFO:  eta: 0:01:05  iteration: 426/1000  consumed samples: 6816  total_loss: 7.813  time: 0.1359(117.71)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:16] lb.utils.events INFO:  eta: 0:01:05  iteration: 427/1000  consumed samples: 6832  total_loss: 7.812  time: 0.1359(117.77)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:16] lb.utils.events INFO:  eta: 0:01:05  iteration: 428/1000  consumed samples: 6848  total_loss: 7.811  time: 0.1359(117.71)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:17] lb.utils.events INFO:  eta: 0:01:04  iteration: 429/1000  consumed samples: 6864  total_loss: 7.811  time: 0.1358(117.81)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:17] lb.utils.events INFO:  eta: 0:01:04  iteration: 430/1000  consumed samples: 6880  total_loss: 7.811  time: 0.1357(117.92)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:17] lb.utils.events INFO:  eta: 0:01:04  iteration: 431/1000  consumed samples: 6896  total_loss: 7.809  time: 0.1357(117.92)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:17] lb.utils.events INFO:  eta: 0:01:04  iteration: 432/1000  consumed samples: 6912  total_loss: 7.809  time: 0.1357(117.93)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:17] lb.utils.events INFO:  eta: 0:01:04  iteration: 433/1000  consumed samples: 6928  total_loss: 7.808  time: 0.1357(117.90)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:18] lb.utils.events INFO:  eta: 0:01:04  iteration: 434/1000  consumed samples: 6944  total_loss: 7.807  time: 0.1356(118.01)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:18] lb.utils.events INFO:  eta: 0:01:04  iteration: 435/1000  consumed samples: 6960  total_loss: 7.807  time: 0.1356(117.98)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:18] lb.utils.events INFO:  eta: 0:01:04  iteration: 436/1000  consumed samples: 6976  total_loss: 7.807  time: 0.1356(117.96)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:18] lb.utils.events INFO:  eta: 0:01:04  iteration: 437/1000  consumed samples: 6992  total_loss: 7.803  time: 0.1355(118.07)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:19] lb.utils.events INFO:  eta: 0:01:04  iteration: 438/1000  consumed samples: 7008  total_loss: 7.799  time: 0.1354(118.17)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:19] lb.utils.events INFO:  eta: 0:01:03  iteration: 439/1000  consumed samples: 7024  total_loss: 7.798  time: 0.1354(118.17)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:19] lb.utils.events INFO:  eta: 0:01:03  iteration: 440/1000  consumed samples: 7040  total_loss: 7.796  time: 0.1353(118.28)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:19] lb.utils.events INFO:  eta: 0:01:03  iteration: 441/1000  consumed samples: 7056  total_loss: 7.796  time: 0.1353(118.27)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:20] lb.utils.events INFO:  eta: 0:01:03  iteration: 442/1000  consumed samples: 7072  total_loss: 7.796  time: 0.1352(118.38)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:11:20] lb.utils.events INFO:  eta: 0:01:03  iteration: 443/1000  consumed samples: 7088  total_loss: 7.795  time: 0.1351(118.47)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:11:20] lb.utils.events INFO:  eta: 0:01:02  iteration: 444/1000  consumed samples: 7104  total_loss: 7.793  time: 0.1349(118.58)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:11:20] lb.utils.events INFO:  eta: 0:01:02  iteration: 445/1000  consumed samples: 7120  total_loss: 7.791  time: 0.1348(118.68)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:11:21] lb.utils.events INFO:  eta: 0:01:01  iteration: 446/1000  consumed samples: 7136  total_loss: 7.79  time: 0.1347(118.79)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:21] lb.utils.events INFO:  eta: 0:01:01  iteration: 447/1000  consumed samples: 7152  total_loss: 7.79  time: 0.1346(118.89)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:21] lb.utils.events INFO:  eta: 0:01:01  iteration: 448/1000  consumed samples: 7168  total_loss: 7.79  time: 0.1345(118.98)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:21] lb.utils.events INFO:  eta: 0:01:01  iteration: 449/1000  consumed samples: 7184  total_loss: 7.79  time: 0.1344(119.08)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:22] lb.utils.events INFO:  eta: 0:01:00  iteration: 450/1000  consumed samples: 7200  total_loss: 7.79  time: 0.1343(119.18)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:22] lb.utils.events INFO:  eta: 0:01:00  iteration: 451/1000  consumed samples: 7216  total_loss: 7.79  time: 0.1341(119.28)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:22] lb.utils.events INFO:  eta: 0:01:00  iteration: 452/1000  consumed samples: 7232  total_loss: 7.789  time: 0.1340(119.38)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:23] lb.utils.events INFO:  eta: 0:01:00  iteration: 453/1000  consumed samples: 7248  total_loss: 7.788  time: 0.1339(119.47)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:23] lb.utils.events INFO:  eta: 0:01:00  iteration: 454/1000  consumed samples: 7264  total_loss: 7.787  time: 0.1338(119.57)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:11:23] lb.utils.events INFO:  eta: 0:01:00  iteration: 455/1000  consumed samples: 7280  total_loss: 7.786  time: 0.1337(119.66)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:24] lb.utils.events INFO:  eta: 0:00:59  iteration: 456/1000  consumed samples: 7296  total_loss: 7.783  time: 0.1336(119.76)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:24] lb.utils.events INFO:  eta: 0:00:59  iteration: 457/1000  consumed samples: 7312  total_loss: 7.78  time: 0.1335(119.82)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:24] lb.utils.events INFO:  eta: 0:00:59  iteration: 458/1000  consumed samples: 7328  total_loss: 7.78  time: 0.1334(119.92)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:24] lb.utils.events INFO:  eta: 0:00:59  iteration: 459/1000  consumed samples: 7344  total_loss: 7.779  time: 0.1333(120.00)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:25] lb.utils.events INFO:  eta: 0:00:58  iteration: 460/1000  consumed samples: 7360  total_loss: 7.778  time: 0.1332(120.10)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:25] lb.utils.events INFO:  eta: 0:00:58  iteration: 461/1000  consumed samples: 7376  total_loss: 7.779  time: 0.1331(120.20)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:25] lb.utils.events INFO:  eta: 0:00:58  iteration: 462/1000  consumed samples: 7392  total_loss: 7.779  time: 0.1330(120.30)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:26] lb.utils.events INFO:  eta: 0:00:58  iteration: 463/1000  consumed samples: 7408  total_loss: 7.779  time: 0.1329(120.40)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:26] lb.utils.events INFO:  eta: 0:00:57  iteration: 464/1000  consumed samples: 7424  total_loss: 7.778  time: 0.1329(120.39)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:26] lb.utils.events INFO:  eta: 0:00:57  iteration: 465/1000  consumed samples: 7440  total_loss: 7.778  time: 0.1329(120.43)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:26] lb.utils.events INFO:  eta: 0:00:58  iteration: 466/1000  consumed samples: 7456  total_loss: 7.778  time: 0.1329(120.42)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:27] lb.utils.events INFO:  eta: 0:00:57  iteration: 467/1000  consumed samples: 7472  total_loss: 7.778  time: 0.1328(120.52)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:27] lb.utils.events INFO:  eta: 0:00:57  iteration: 468/1000  consumed samples: 7488  total_loss: 7.778  time: 0.1328(120.52)  data_time: 0.0057  lr: 9.00e-05  
[01/19 16:11:27] lb.utils.events INFO:  eta: 0:00:57  iteration: 469/1000  consumed samples: 7504  total_loss: 7.778  time: 0.1326(120.62)  data_time: 0.0057  lr: 9.00e-05  
[01/19 16:11:27] lb.utils.events INFO:  eta: 0:00:57  iteration: 470/1000  consumed samples: 7520  total_loss: 7.778  time: 0.1325(120.72)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:11:28] lb.utils.events INFO:  eta: 0:00:57  iteration: 471/1000  consumed samples: 7536  total_loss: 7.777  time: 0.1324(120.80)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:11:28] lb.utils.events INFO:  eta: 0:00:57  iteration: 472/1000  consumed samples: 7552  total_loss: 7.774  time: 0.1324(120.88)  data_time: 0.0055  lr: 9.00e-05  
[01/19 16:11:28] lb.utils.events INFO:  eta: 0:00:56  iteration: 473/1000  consumed samples: 7568  total_loss: 7.774  time: 0.1323(120.91)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:11:28] lb.utils.events INFO:  eta: 0:00:56  iteration: 474/1000  consumed samples: 7584  total_loss: 7.773  time: 0.1322(121.00)  data_time: 0.0057  lr: 9.00e-05  
[01/19 16:11:29] lb.utils.events INFO:  eta: 0:00:56  iteration: 475/1000  consumed samples: 7600  total_loss: 7.772  time: 0.1321(121.10)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:11:29] lb.utils.events INFO:  eta: 0:00:56  iteration: 476/1000  consumed samples: 7616  total_loss: 7.772  time: 0.1320(121.19)  data_time: 0.0055  lr: 9.00e-05  
[01/19 16:11:29] lb.utils.events INFO:  eta: 0:00:56  iteration: 477/1000  consumed samples: 7632  total_loss: 7.772  time: 0.1319(121.29)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:11:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 478/1000  consumed samples: 7648  total_loss: 7.771  time: 0.1318(121.38)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:11:30] lb.utils.events INFO:  eta: 0:00:55  iteration: 479/1000  consumed samples: 7664  total_loss: 7.771  time: 0.1317(121.48)  data_time: 0.0053  lr: 9.00e-05  
[01/19 16:11:30] lb.utils.events INFO:  eta: 0:00:55  iteration: 480/1000  consumed samples: 7680  total_loss: 7.769  time: 0.1316(121.58)  data_time: 0.0052  lr: 9.00e-05  
[01/19 16:11:30] lb.utils.events INFO:  eta: 0:00:55  iteration: 481/1000  consumed samples: 7696  total_loss: 7.767  time: 0.1315(121.67)  data_time: 0.0052  lr: 9.00e-05  
[01/19 16:11:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 482/1000  consumed samples: 7712  total_loss: 7.767  time: 0.1314(121.75)  data_time: 0.0055  lr: 9.00e-05  
[01/19 16:11:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 483/1000  consumed samples: 7728  total_loss: 7.767  time: 0.1313(121.85)  data_time: 0.0053  lr: 9.00e-05  
[01/19 16:11:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 484/1000  consumed samples: 7744  total_loss: 7.766  time: 0.1312(121.94)  data_time: 0.0052  lr: 9.00e-05  
[01/19 16:11:31] lb.utils.events INFO:  eta: 0:00:54  iteration: 485/1000  consumed samples: 7760  total_loss: 7.763  time: 0.1312(121.99)  data_time: 0.0053  lr: 9.00e-05  
[01/19 16:11:31] lb.utils.events INFO:  eta: 0:00:54  iteration: 486/1000  consumed samples: 7776  total_loss: 7.761  time: 0.1311(122.08)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:11:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 487/1000  consumed samples: 7792  total_loss: 7.759  time: 0.1311(122.05)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:11:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 488/1000  consumed samples: 7808  total_loss: 7.759  time: 0.1310(122.14)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:11:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 489/1000  consumed samples: 7824  total_loss: 7.759  time: 0.1309(122.23)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:11:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 490/1000  consumed samples: 7840  total_loss: 7.759  time: 0.1308(122.32)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 491/1000  consumed samples: 7856  total_loss: 7.759  time: 0.1307(122.41)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 492/1000  consumed samples: 7872  total_loss: 7.758  time: 0.1307(122.46)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 493/1000  consumed samples: 7888  total_loss: 7.758  time: 0.1306(122.55)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:11:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 494/1000  consumed samples: 7904  total_loss: 7.756  time: 0.1306(122.50)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:34] lb.utils.events INFO:  eta: 0:00:53  iteration: 495/1000  consumed samples: 7920  total_loss: 7.755  time: 0.1305(122.59)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:34] lb.utils.events INFO:  eta: 0:00:53  iteration: 496/1000  consumed samples: 7936  total_loss: 7.754  time: 0.1305(122.56)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:34] lb.utils.events INFO:  eta: 0:00:53  iteration: 497/1000  consumed samples: 7952  total_loss: 7.754  time: 0.1306(122.55)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:34] lb.utils.events INFO:  eta: 0:00:53  iteration: 498/1000  consumed samples: 7968  total_loss: 7.753  time: 0.1305(122.62)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:35] lb.utils.events INFO:  eta: 0:00:53  iteration: 499/1000  consumed samples: 7984  total_loss: 7.753  time: 0.1304(122.71)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 500/1000  consumed samples: 8000  total_loss: 7.752  time: 0.1303(122.80)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:11:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 501/1000  consumed samples: 8016  total_loss: 7.751  time: 0.1302(122.89)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:11:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 502/1000  consumed samples: 8032  total_loss: 7.751  time: 0.1301(122.98)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:11:36] lb.utils.events INFO:  eta: 0:00:52  iteration: 503/1000  consumed samples: 8048  total_loss: 7.747  time: 0.1300(123.07)  data_time: 0.0035  lr: 9.00e-05  
[01/19 16:11:36] lb.utils.events INFO:  eta: 0:00:52  iteration: 504/1000  consumed samples: 8064  total_loss: 7.747  time: 0.1299(123.17)  data_time: 0.0035  lr: 9.00e-05  
[01/19 16:11:36] lb.utils.events INFO:  eta: 0:00:51  iteration: 505/1000  consumed samples: 8080  total_loss: 7.742  time: 0.1299(123.22)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:37] lb.utils.events INFO:  eta: 0:00:51  iteration: 506/1000  consumed samples: 8096  total_loss: 7.742  time: 0.1298(123.31)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:37] lb.utils.events INFO:  eta: 0:00:51  iteration: 507/1000  consumed samples: 8112  total_loss: 7.738  time: 0.1297(123.40)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:37] lb.utils.events INFO:  eta: 0:00:51  iteration: 508/1000  consumed samples: 8128  total_loss: 7.735  time: 0.1296(123.49)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:37] lb.utils.events INFO:  eta: 0:00:51  iteration: 509/1000  consumed samples: 8144  total_loss: 7.735  time: 0.1295(123.58)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:38] lb.utils.events INFO:  eta: 0:00:51  iteration: 510/1000  consumed samples: 8160  total_loss: 7.735  time: 0.1294(123.66)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:38] lb.utils.events INFO:  eta: 0:00:51  iteration: 511/1000  consumed samples: 8176  total_loss: 7.735  time: 0.1293(123.74)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:38] lb.utils.events INFO:  eta: 0:00:50  iteration: 512/1000  consumed samples: 8192  total_loss: 7.732  time: 0.1292(123.82)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:38] lb.utils.events INFO:  eta: 0:00:50  iteration: 513/1000  consumed samples: 8208  total_loss: 7.732  time: 0.1291(123.90)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:39] lb.utils.events INFO:  eta: 0:00:50  iteration: 514/1000  consumed samples: 8224  total_loss: 7.732  time: 0.1291(123.95)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:39] lb.utils.events INFO:  eta: 0:00:50  iteration: 515/1000  consumed samples: 8240  total_loss: 7.729  time: 0.1290(124.03)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:39] lb.utils.events INFO:  eta: 0:00:50  iteration: 516/1000  consumed samples: 8256  total_loss: 7.729  time: 0.1290(124.04)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:39] lb.utils.events INFO:  eta: 0:00:50  iteration: 517/1000  consumed samples: 8272  total_loss: 7.729  time: 0.1289(124.13)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:40] lb.utils.events INFO:  eta: 0:00:50  iteration: 518/1000  consumed samples: 8288  total_loss: 7.729  time: 0.1288(124.21)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:40] lb.utils.events INFO:  eta: 0:00:49  iteration: 519/1000  consumed samples: 8304  total_loss: 7.726  time: 0.1287(124.30)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:40] lb.utils.events INFO:  eta: 0:00:49  iteration: 520/1000  consumed samples: 8320  total_loss: 7.726  time: 0.1286(124.38)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:40] lb.utils.events INFO:  eta: 0:00:49  iteration: 521/1000  consumed samples: 8336  total_loss: 7.729  time: 0.1285(124.47)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:41] lb.utils.events INFO:  eta: 0:00:49  iteration: 522/1000  consumed samples: 8352  total_loss: 7.726  time: 0.1285(124.55)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:41] lb.utils.events INFO:  eta: 0:00:49  iteration: 523/1000  consumed samples: 8368  total_loss: 7.725  time: 0.1284(124.64)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:41] lb.utils.events INFO:  eta: 0:00:49  iteration: 524/1000  consumed samples: 8384  total_loss: 7.725  time: 0.1285(124.52)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:11:41] lb.utils.events INFO:  eta: 0:00:49  iteration: 525/1000  consumed samples: 8400  total_loss: 7.724  time: 0.1284(124.61)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:42] lb.utils.events INFO:  eta: 0:00:48  iteration: 526/1000  consumed samples: 8416  total_loss: 7.724  time: 0.1283(124.69)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:42] lb.utils.events INFO:  eta: 0:00:48  iteration: 527/1000  consumed samples: 8432  total_loss: 7.725  time: 0.1282(124.78)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:42] lb.utils.events INFO:  eta: 0:00:48  iteration: 528/1000  consumed samples: 8448  total_loss: 7.724  time: 0.1282(124.81)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:11:42] lb.utils.events INFO:  eta: 0:00:48  iteration: 529/1000  consumed samples: 8464  total_loss: 7.723  time: 0.1281(124.87)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:43] lb.utils.events INFO:  eta: 0:00:48  iteration: 530/1000  consumed samples: 8480  total_loss: 7.722  time: 0.1281(124.90)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:43] lb.utils.events INFO:  eta: 0:00:48  iteration: 531/1000  consumed samples: 8496  total_loss: 7.722  time: 0.1281(124.88)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:43] lb.utils.events INFO:  eta: 0:00:48  iteration: 532/1000  consumed samples: 8512  total_loss: 7.722  time: 0.1280(124.97)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:11:43] lb.utils.events INFO:  eta: 0:00:48  iteration: 533/1000  consumed samples: 8528  total_loss: 7.722  time: 0.1280(125.05)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:44] lb.utils.events INFO:  eta: 0:00:47  iteration: 534/1000  consumed samples: 8544  total_loss: 7.72  time: 0.1279(125.13)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:44] lb.utils.events INFO:  eta: 0:00:47  iteration: 535/1000  consumed samples: 8560  total_loss: 7.72  time: 0.1278(125.22)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:44] lb.utils.events INFO:  eta: 0:00:47  iteration: 536/1000  consumed samples: 8576  total_loss: 7.72  time: 0.1277(125.25)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:11:44] lb.utils.events INFO:  eta: 0:00:47  iteration: 537/1000  consumed samples: 8592  total_loss: 7.72  time: 0.1277(125.27)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:45] lb.utils.events INFO:  eta: 0:00:47  iteration: 538/1000  consumed samples: 8608  total_loss: 7.722  time: 0.1277(125.31)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:45] lb.utils.events INFO:  eta: 0:00:47  iteration: 539/1000  consumed samples: 8624  total_loss: 7.72  time: 0.1276(125.39)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:45] lb.utils.events INFO:  eta: 0:00:47  iteration: 540/1000  consumed samples: 8640  total_loss: 7.718  time: 0.1277(125.33)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:45] lb.utils.events INFO:  eta: 0:00:47  iteration: 541/1000  consumed samples: 8656  total_loss: 7.718  time: 0.1278(125.18)  data_time: 0.0101  lr: 9.00e-05  
[01/19 16:11:46] lb.utils.events INFO:  eta: 0:00:47  iteration: 542/1000  consumed samples: 8672  total_loss: 7.716  time: 0.1278(125.20)  data_time: 0.0102  lr: 9.00e-05  
[01/19 16:11:46] lb.utils.events INFO:  eta: 0:00:47  iteration: 543/1000  consumed samples: 8688  total_loss: 7.712  time: 0.1278(125.19)  data_time: 0.0101  lr: 9.00e-05  
[01/19 16:11:46] lb.utils.events INFO:  eta: 0:00:47  iteration: 544/1000  consumed samples: 8704  total_loss: 7.712  time: 0.1279(125.11)  data_time: 0.0101  lr: 9.00e-05  
[01/19 16:11:46] lb.utils.events INFO:  eta: 0:00:47  iteration: 545/1000  consumed samples: 8720  total_loss: 7.711  time: 0.1279(125.14)  data_time: 0.0114  lr: 9.00e-05  
[01/19 16:11:47] lb.utils.events INFO:  eta: 0:00:46  iteration: 546/1000  consumed samples: 8736  total_loss: 7.711  time: 0.1278(125.23)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:11:47] lb.utils.events INFO:  eta: 0:00:46  iteration: 547/1000  consumed samples: 8752  total_loss: 7.711  time: 0.1277(125.31)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:11:47] lb.utils.events INFO:  eta: 0:00:46  iteration: 548/1000  consumed samples: 8768  total_loss: 7.709  time: 0.1276(125.36)  data_time: 0.0112  lr: 9.00e-05  
[01/19 16:11:47] lb.utils.events INFO:  eta: 0:00:46  iteration: 549/1000  consumed samples: 8784  total_loss: 7.709  time: 0.1276(125.44)  data_time: 0.0112  lr: 9.00e-05  
[01/19 16:11:48] lb.utils.events INFO:  eta: 0:00:46  iteration: 550/1000  consumed samples: 8800  total_loss: 7.709  time: 0.1275(125.52)  data_time: 0.0112  lr: 9.00e-05  
[01/19 16:11:48] lb.utils.events INFO:  eta: 0:00:46  iteration: 551/1000  consumed samples: 8816  total_loss: 7.706  time: 0.1274(125.60)  data_time: 0.0113  lr: 9.00e-05  
[01/19 16:11:48] lb.utils.events INFO:  eta: 0:00:46  iteration: 552/1000  consumed samples: 8832  total_loss: 7.706  time: 0.1273(125.68)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:11:48] lb.utils.events INFO:  eta: 0:00:46  iteration: 553/1000  consumed samples: 8848  total_loss: 7.704  time: 0.1273(125.71)  data_time: 0.0110  lr: 9.00e-05  
[01/19 16:11:49] lb.utils.events INFO:  eta: 0:00:45  iteration: 554/1000  consumed samples: 8864  total_loss: 7.701  time: 0.1273(125.69)  data_time: 0.0110  lr: 9.00e-05  
[01/19 16:11:49] lb.utils.events INFO:  eta: 0:00:45  iteration: 555/1000  consumed samples: 8880  total_loss: 7.701  time: 0.1273(125.71)  data_time: 0.0110  lr: 9.00e-05  
[01/19 16:11:49] lb.utils.events INFO:  eta: 0:00:45  iteration: 556/1000  consumed samples: 8896  total_loss: 7.701  time: 0.1272(125.79)  data_time: 0.0110  lr: 9.00e-05  
[01/19 16:11:49] lb.utils.events INFO:  eta: 0:00:45  iteration: 557/1000  consumed samples: 8912  total_loss: 7.698  time: 0.1271(125.86)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:11:50] lb.utils.events INFO:  eta: 0:00:45  iteration: 558/1000  consumed samples: 8928  total_loss: 7.698  time: 0.1270(125.94)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:11:50] lb.utils.events INFO:  eta: 0:00:45  iteration: 559/1000  consumed samples: 8944  total_loss: 7.694  time: 0.1270(126.02)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:11:50] lb.utils.events INFO:  eta: 0:00:45  iteration: 560/1000  consumed samples: 8960  total_loss: 7.694  time: 0.1269(126.10)  data_time: 0.0111  lr: 9.00e-05  
[01/19 16:11:50] lb.utils.events INFO:  eta: 0:00:44  iteration: 561/1000  consumed samples: 8976  total_loss: 7.692  time: 0.1268(126.17)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:11:51] lb.utils.events INFO:  eta: 0:00:44  iteration: 562/1000  consumed samples: 8992  total_loss: 7.692  time: 0.1267(126.25)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:11:51] lb.utils.events INFO:  eta: 0:00:44  iteration: 563/1000  consumed samples: 9008  total_loss: 7.691  time: 0.1267(126.33)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:11:51] lb.utils.events INFO:  eta: 0:00:44  iteration: 564/1000  consumed samples: 9024  total_loss: 7.692  time: 0.1266(126.41)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:11:51] lb.utils.events INFO:  eta: 0:00:44  iteration: 565/1000  consumed samples: 9040  total_loss: 7.691  time: 0.1265(126.48)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:52] lb.utils.events INFO:  eta: 0:00:44  iteration: 566/1000  consumed samples: 9056  total_loss: 7.687  time: 0.1264(126.56)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:53] lb.utils.events INFO:  eta: 0:00:44  iteration: 567/1000  consumed samples: 9072  total_loss: 7.687  time: 0.1263(126.64)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:11:53] lb.utils.events INFO:  eta: 0:00:43  iteration: 568/1000  consumed samples: 9088  total_loss: 7.683  time: 0.1263(126.71)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:53] lb.utils.events INFO:  eta: 0:00:43  iteration: 569/1000  consumed samples: 9104  total_loss: 7.683  time: 0.1263(126.65)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:53] lb.utils.events INFO:  eta: 0:00:43  iteration: 570/1000  consumed samples: 9120  total_loss: 7.683  time: 0.1263(126.72)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:54] lb.utils.events INFO:  eta: 0:00:43  iteration: 571/1000  consumed samples: 9136  total_loss: 7.683  time: 0.1263(126.69)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:54] lb.utils.events INFO:  eta: 0:00:43  iteration: 572/1000  consumed samples: 9152  total_loss: 7.683  time: 0.1262(126.77)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:54] lb.utils.events INFO:  eta: 0:00:43  iteration: 573/1000  consumed samples: 9168  total_loss: 7.687  time: 0.1261(126.84)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:54] lb.utils.events INFO:  eta: 0:00:43  iteration: 574/1000  consumed samples: 9184  total_loss: 7.687  time: 0.1261(126.87)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:55] lb.utils.events INFO:  eta: 0:00:43  iteration: 575/1000  consumed samples: 9200  total_loss: 7.683  time: 0.1260(126.95)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:55] lb.utils.events INFO:  eta: 0:00:42  iteration: 576/1000  consumed samples: 9216  total_loss: 7.682  time: 0.1260(127.02)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:55] lb.utils.events INFO:  eta: 0:00:42  iteration: 577/1000  consumed samples: 9232  total_loss: 7.682  time: 0.1259(127.09)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:55] lb.utils.events INFO:  eta: 0:00:42  iteration: 578/1000  consumed samples: 9248  total_loss: 7.678  time: 0.1258(127.16)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:11:56] lb.utils.events INFO:  eta: 0:00:42  iteration: 579/1000  consumed samples: 9264  total_loss: 7.674  time: 0.1257(127.24)  data_time: 0.0046  lr: 9.00e-05  
[01/19 16:11:56] lb.utils.events INFO:  eta: 0:00:42  iteration: 580/1000  consumed samples: 9280  total_loss: 7.674  time: 0.1257(127.32)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:56] lb.utils.events INFO:  eta: 0:00:42  iteration: 581/1000  consumed samples: 9296  total_loss: 7.673  time: 0.1256(127.39)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:57] lb.utils.events INFO:  eta: 0:00:42  iteration: 582/1000  consumed samples: 9312  total_loss: 7.673  time: 0.1255(127.46)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:57] lb.utils.events INFO:  eta: 0:00:42  iteration: 583/1000  consumed samples: 9328  total_loss: 7.673  time: 0.1254(127.54)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:57] lb.utils.events INFO:  eta: 0:00:41  iteration: 584/1000  consumed samples: 9344  total_loss: 7.669  time: 0.1254(127.60)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:11:58] lb.utils.events INFO:  eta: 0:00:41  iteration: 585/1000  consumed samples: 9360  total_loss: 7.669  time: 0.1254(127.62)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:58] lb.utils.events INFO:  eta: 0:00:41  iteration: 586/1000  consumed samples: 9376  total_loss: 7.669  time: 0.1254(127.62)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:11:58] lb.utils.events INFO:  eta: 0:00:41  iteration: 587/1000  consumed samples: 9392  total_loss: 7.665  time: 0.1253(127.69)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:11:58] lb.utils.events INFO:  eta: 0:00:41  iteration: 588/1000  consumed samples: 9408  total_loss: 7.669  time: 0.1253(127.71)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:59] lb.utils.events INFO:  eta: 0:00:41  iteration: 589/1000  consumed samples: 9424  total_loss: 7.665  time: 0.1252(127.78)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:59] lb.utils.events INFO:  eta: 0:00:41  iteration: 590/1000  consumed samples: 9440  total_loss: 7.663  time: 0.1252(127.76)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:59] lb.utils.events INFO:  eta: 0:00:41  iteration: 591/1000  consumed samples: 9456  total_loss: 7.662  time: 0.1252(127.77)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:11:59] lb.utils.events INFO:  eta: 0:00:41  iteration: 592/1000  consumed samples: 9472  total_loss: 7.66  time: 0.1253(127.71)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:11:59] lb.utils.events INFO:  eta: 0:00:41  iteration: 593/1000  consumed samples: 9488  total_loss: 7.66  time: 0.1253(127.65)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:00] lb.utils.events INFO:  eta: 0:00:41  iteration: 594/1000  consumed samples: 9504  total_loss: 7.658  time: 0.1253(127.64)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:00] lb.utils.events INFO:  eta: 0:00:40  iteration: 595/1000  consumed samples: 9520  total_loss: 7.658  time: 0.1253(127.71)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:00] lb.utils.events INFO:  eta: 0:00:40  iteration: 596/1000  consumed samples: 9536  total_loss: 7.658  time: 0.1252(127.78)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:00] lb.utils.events INFO:  eta: 0:00:40  iteration: 597/1000  consumed samples: 9552  total_loss: 7.658  time: 0.1251(127.85)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:01] lb.utils.events INFO:  eta: 0:00:40  iteration: 598/1000  consumed samples: 9568  total_loss: 7.658  time: 0.1251(127.92)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:01] lb.utils.events INFO:  eta: 0:00:40  iteration: 599/1000  consumed samples: 9584  total_loss: 7.656  time: 0.1251(127.93)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:02] lb.utils.events INFO:  eta: 0:00:40  iteration: 600/1000  consumed samples: 9600  total_loss: 7.655  time: 0.1250(128.01)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:02] lb.utils.events INFO:  eta: 0:00:40  iteration: 601/1000  consumed samples: 9616  total_loss: 7.655  time: 0.1249(128.08)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:02] lb.utils.events INFO:  eta: 0:00:40  iteration: 602/1000  consumed samples: 9632  total_loss: 7.654  time: 0.1249(128.15)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:03] lb.utils.events INFO:  eta: 0:00:39  iteration: 603/1000  consumed samples: 9648  total_loss: 7.655  time: 0.1248(128.18)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:03] lb.utils.events INFO:  eta: 0:00:39  iteration: 604/1000  consumed samples: 9664  total_loss: 7.654  time: 0.1248(128.25)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:03] lb.utils.events INFO:  eta: 0:00:39  iteration: 605/1000  consumed samples: 9680  total_loss: 7.654  time: 0.1248(128.23)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:03] lb.utils.events INFO:  eta: 0:00:39  iteration: 606/1000  consumed samples: 9696  total_loss: 7.651  time: 0.1247(128.30)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:04] lb.utils.events INFO:  eta: 0:00:39  iteration: 607/1000  consumed samples: 9712  total_loss: 7.651  time: 0.1247(128.32)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:04] lb.utils.events INFO:  eta: 0:00:39  iteration: 608/1000  consumed samples: 9728  total_loss: 7.651  time: 0.1246(128.36)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:04] lb.utils.events INFO:  eta: 0:00:39  iteration: 609/1000  consumed samples: 9744  total_loss: 7.648  time: 0.1246(128.43)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:05] lb.utils.events INFO:  eta: 0:00:39  iteration: 610/1000  consumed samples: 9760  total_loss: 7.646  time: 0.1245(128.50)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:05] lb.utils.events INFO:  eta: 0:00:39  iteration: 611/1000  consumed samples: 9776  total_loss: 7.645  time: 0.1244(128.57)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:05] lb.utils.events INFO:  eta: 0:00:38  iteration: 612/1000  consumed samples: 9792  total_loss: 7.646  time: 0.1244(128.57)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:06] lb.utils.events INFO:  eta: 0:00:38  iteration: 613/1000  consumed samples: 9808  total_loss: 7.646  time: 0.1244(128.63)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:06] lb.utils.events INFO:  eta: 0:00:38  iteration: 614/1000  consumed samples: 9824  total_loss: 7.645  time: 0.1244(128.58)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:06] lb.utils.events INFO:  eta: 0:00:38  iteration: 615/1000  consumed samples: 9840  total_loss: 7.644  time: 0.1244(128.65)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:07] lb.utils.events INFO:  eta: 0:00:38  iteration: 616/1000  consumed samples: 9856  total_loss: 7.644  time: 0.1243(128.70)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:07] lb.utils.events INFO:  eta: 0:00:38  iteration: 617/1000  consumed samples: 9872  total_loss: 7.643  time: 0.1243(128.76)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:07] lb.utils.events INFO:  eta: 0:00:38  iteration: 618/1000  consumed samples: 9888  total_loss: 7.641  time: 0.1242(128.82)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:07] lb.utils.events INFO:  eta: 0:00:37  iteration: 619/1000  consumed samples: 9904  total_loss: 7.641  time: 0.1241(128.89)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:07] lb.utils.events INFO:  eta: 0:00:37  iteration: 620/1000  consumed samples: 9920  total_loss: 7.641  time: 0.1241(128.96)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:08] lb.utils.events INFO:  eta: 0:00:37  iteration: 621/1000  consumed samples: 9936  total_loss: 7.639  time: 0.1240(129.02)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:08] lb.utils.events INFO:  eta: 0:00:37  iteration: 622/1000  consumed samples: 9952  total_loss: 7.639  time: 0.1239(129.10)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:08] lb.utils.events INFO:  eta: 0:00:37  iteration: 623/1000  consumed samples: 9968  total_loss: 7.638  time: 0.1239(129.17)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:08] lb.utils.events INFO:  eta: 0:00:37  iteration: 624/1000  consumed samples: 9984  total_loss: 7.638  time: 0.1239(129.18)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:09] lb.utils.events INFO:  eta: 0:00:36  iteration: 625/1000  consumed samples: 10000  total_loss: 7.638  time: 0.1239(129.17)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:09] lb.utils.events INFO:  eta: 0:00:36  iteration: 626/1000  consumed samples: 10016  total_loss: 7.637  time: 0.1238(129.23)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:09] lb.utils.events INFO:  eta: 0:00:36  iteration: 627/1000  consumed samples: 10032  total_loss: 7.637  time: 0.1237(129.30)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:09] lb.utils.events INFO:  eta: 0:00:36  iteration: 628/1000  consumed samples: 10048  total_loss: 7.637  time: 0.1237(129.37)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:10] lb.utils.events INFO:  eta: 0:00:36  iteration: 629/1000  consumed samples: 10064  total_loss: 7.636  time: 0.1236(129.43)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:10] lb.utils.events INFO:  eta: 0:00:35  iteration: 630/1000  consumed samples: 10080  total_loss: 7.636  time: 0.1235(129.51)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:10] lb.utils.events INFO:  eta: 0:00:35  iteration: 631/1000  consumed samples: 10096  total_loss: 7.636  time: 0.1235(129.58)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:10] lb.utils.events INFO:  eta: 0:00:35  iteration: 632/1000  consumed samples: 10112  total_loss: 7.636  time: 0.1235(129.53)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:11] lb.utils.events INFO:  eta: 0:00:35  iteration: 633/1000  consumed samples: 10128  total_loss: 7.636  time: 0.1235(129.59)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:11] lb.utils.events INFO:  eta: 0:00:35  iteration: 634/1000  consumed samples: 10144  total_loss: 7.635  time: 0.1234(129.66)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:11] lb.utils.events INFO:  eta: 0:00:35  iteration: 635/1000  consumed samples: 10160  total_loss: 7.633  time: 0.1234(129.61)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:11] lb.utils.events INFO:  eta: 0:00:35  iteration: 636/1000  consumed samples: 10176  total_loss: 7.632  time: 0.1234(129.66)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:12] lb.utils.events INFO:  eta: 0:00:35  iteration: 637/1000  consumed samples: 10192  total_loss: 7.633  time: 0.1233(129.72)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:12] lb.utils.events INFO:  eta: 0:00:35  iteration: 638/1000  consumed samples: 10208  total_loss: 7.635  time: 0.1233(129.78)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:12] lb.utils.events INFO:  eta: 0:00:34  iteration: 639/1000  consumed samples: 10224  total_loss: 7.633  time: 0.1232(129.85)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:12] lb.utils.events INFO:  eta: 0:00:34  iteration: 640/1000  consumed samples: 10240  total_loss: 7.632  time: 0.1233(129.76)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:12] lb.utils.events INFO:  eta: 0:00:34  iteration: 641/1000  consumed samples: 10256  total_loss: 7.632  time: 0.1233(129.81)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:13] lb.utils.events INFO:  eta: 0:00:34  iteration: 642/1000  consumed samples: 10272  total_loss: 7.631  time: 0.1232(129.83)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:13] lb.utils.events INFO:  eta: 0:00:34  iteration: 643/1000  consumed samples: 10288  total_loss: 7.63  time: 0.1232(129.89)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:13] lb.utils.events INFO:  eta: 0:00:34  iteration: 644/1000  consumed samples: 10304  total_loss: 7.628  time: 0.1231(129.96)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:13] lb.utils.events INFO:  eta: 0:00:34  iteration: 645/1000  consumed samples: 10320  total_loss: 7.628  time: 0.1231(130.02)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:14] lb.utils.events INFO:  eta: 0:00:34  iteration: 646/1000  consumed samples: 10336  total_loss: 7.626  time: 0.1230(130.09)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:14] lb.utils.events INFO:  eta: 0:00:34  iteration: 647/1000  consumed samples: 10352  total_loss: 7.624  time: 0.1229(130.16)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:14] lb.utils.events INFO:  eta: 0:00:33  iteration: 648/1000  consumed samples: 10368  total_loss: 7.624  time: 0.1229(130.22)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:15] lb.utils.events INFO:  eta: 0:00:33  iteration: 649/1000  consumed samples: 10384  total_loss: 7.624  time: 0.1228(130.28)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:15] lb.utils.events INFO:  eta: 0:00:33  iteration: 650/1000  consumed samples: 10400  total_loss: 7.622  time: 0.1228(130.35)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:15] lb.utils.events INFO:  eta: 0:00:33  iteration: 651/1000  consumed samples: 10416  total_loss: 7.62  time: 0.1227(130.41)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:15] lb.utils.events INFO:  eta: 0:00:33  iteration: 652/1000  consumed samples: 10432  total_loss: 7.618  time: 0.1226(130.48)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:16] lb.utils.events INFO:  eta: 0:00:33  iteration: 653/1000  consumed samples: 10448  total_loss: 7.618  time: 0.1226(130.51)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:16] lb.utils.events INFO:  eta: 0:00:33  iteration: 654/1000  consumed samples: 10464  total_loss: 7.618  time: 0.1225(130.56)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:16] lb.utils.events INFO:  eta: 0:00:33  iteration: 655/1000  consumed samples: 10480  total_loss: 7.617  time: 0.1225(130.59)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:16] lb.utils.events INFO:  eta: 0:00:33  iteration: 656/1000  consumed samples: 10496  total_loss: 7.618  time: 0.1225(130.58)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:17] lb.utils.events INFO:  eta: 0:00:33  iteration: 657/1000  consumed samples: 10512  total_loss: 7.618  time: 0.1225(130.58)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:17] lb.utils.events INFO:  eta: 0:00:32  iteration: 658/1000  consumed samples: 10528  total_loss: 7.617  time: 0.1225(130.64)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:17] lb.utils.events INFO:  eta: 0:00:32  iteration: 659/1000  consumed samples: 10544  total_loss: 7.615  time: 0.1224(130.71)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:17] lb.utils.events INFO:  eta: 0:00:32  iteration: 660/1000  consumed samples: 10560  total_loss: 7.615  time: 0.1223(130.78)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:17] lb.utils.events INFO:  eta: 0:00:32  iteration: 661/1000  consumed samples: 10576  total_loss: 7.615  time: 0.1223(130.80)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:18] lb.utils.events INFO:  eta: 0:00:32  iteration: 662/1000  consumed samples: 10592  total_loss: 7.614  time: 0.1223(130.84)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:18] lb.utils.events INFO:  eta: 0:00:32  iteration: 663/1000  consumed samples: 10608  total_loss: 7.614  time: 0.1223(130.85)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:18] lb.utils.events INFO:  eta: 0:00:32  iteration: 664/1000  consumed samples: 10624  total_loss: 7.614  time: 0.1223(130.87)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:18] lb.utils.events INFO:  eta: 0:00:32  iteration: 665/1000  consumed samples: 10640  total_loss: 7.614  time: 0.1222(130.92)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:19] lb.utils.events INFO:  eta: 0:00:32  iteration: 666/1000  consumed samples: 10656  total_loss: 7.611  time: 0.1222(130.98)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:19] lb.utils.events INFO:  eta: 0:00:31  iteration: 667/1000  consumed samples: 10672  total_loss: 7.611  time: 0.1221(131.03)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:19] lb.utils.events INFO:  eta: 0:00:31  iteration: 668/1000  consumed samples: 10688  total_loss: 7.607  time: 0.1221(131.08)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:19] lb.utils.events INFO:  eta: 0:00:31  iteration: 669/1000  consumed samples: 10704  total_loss: 7.607  time: 0.1221(131.03)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:19] lb.utils.events INFO:  eta: 0:00:31  iteration: 670/1000  consumed samples: 10720  total_loss: 7.607  time: 0.1221(131.00)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:20] lb.utils.events INFO:  eta: 0:00:31  iteration: 671/1000  consumed samples: 10736  total_loss: 7.606  time: 0.1222(130.97)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:20] lb.utils.events INFO:  eta: 0:00:31  iteration: 672/1000  consumed samples: 10752  total_loss: 7.604  time: 0.1222(130.91)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:20] lb.utils.events INFO:  eta: 0:00:31  iteration: 673/1000  consumed samples: 10768  total_loss: 7.603  time: 0.1222(130.98)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:20] lb.utils.events INFO:  eta: 0:00:31  iteration: 674/1000  consumed samples: 10784  total_loss: 7.604  time: 0.1222(130.95)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:21] lb.utils.events INFO:  eta: 0:00:31  iteration: 675/1000  consumed samples: 10800  total_loss: 7.604  time: 0.1223(130.88)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:21] lb.utils.events INFO:  eta: 0:00:31  iteration: 676/1000  consumed samples: 10816  total_loss: 7.603  time: 0.1222(130.94)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:21] lb.utils.events INFO:  eta: 0:00:31  iteration: 677/1000  consumed samples: 10832  total_loss: 7.603  time: 0.1222(130.92)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:21] lb.utils.events INFO:  eta: 0:00:31  iteration: 678/1000  consumed samples: 10848  total_loss: 7.603  time: 0.1222(130.99)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:22] lb.utils.events INFO:  eta: 0:00:30  iteration: 679/1000  consumed samples: 10864  total_loss: 7.602  time: 0.1221(131.05)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:22] lb.utils.events INFO:  eta: 0:00:30  iteration: 680/1000  consumed samples: 10880  total_loss: 7.599  time: 0.1220(131.12)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:22] lb.utils.events INFO:  eta: 0:00:30  iteration: 681/1000  consumed samples: 10896  total_loss: 7.599  time: 0.1220(131.18)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:23] lb.utils.events INFO:  eta: 0:00:30  iteration: 682/1000  consumed samples: 10912  total_loss: 7.596  time: 0.1219(131.24)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:23] lb.utils.events INFO:  eta: 0:00:30  iteration: 683/1000  consumed samples: 10928  total_loss: 7.596  time: 0.1219(131.30)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:23] lb.utils.events INFO:  eta: 0:00:30  iteration: 684/1000  consumed samples: 10944  total_loss: 7.596  time: 0.1218(131.36)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:23] lb.utils.events INFO:  eta: 0:00:30  iteration: 685/1000  consumed samples: 10960  total_loss: 7.599  time: 0.1217(131.42)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:24] lb.utils.events INFO:  eta: 0:00:29  iteration: 686/1000  consumed samples: 10976  total_loss: 7.596  time: 0.1217(131.48)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:24] lb.utils.events INFO:  eta: 0:00:29  iteration: 687/1000  consumed samples: 10992  total_loss: 7.596  time: 0.1216(131.54)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:24] lb.utils.events INFO:  eta: 0:00:29  iteration: 688/1000  consumed samples: 11008  total_loss: 7.591  time: 0.1216(131.54)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:24] lb.utils.events INFO:  eta: 0:00:29  iteration: 689/1000  consumed samples: 11024  total_loss: 7.596  time: 0.1216(131.60)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:25] lb.utils.events INFO:  eta: 0:00:29  iteration: 690/1000  consumed samples: 11040  total_loss: 7.591  time: 0.1216(131.60)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:25] lb.utils.events INFO:  eta: 0:00:29  iteration: 691/1000  consumed samples: 11056  total_loss: 7.585  time: 0.1216(131.58)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:25] lb.utils.events INFO:  eta: 0:00:29  iteration: 692/1000  consumed samples: 11072  total_loss: 7.581  time: 0.1215(131.64)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:25] lb.utils.events INFO:  eta: 0:00:29  iteration: 693/1000  consumed samples: 11088  total_loss: 7.581  time: 0.1215(131.70)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:26] lb.utils.events INFO:  eta: 0:00:29  iteration: 694/1000  consumed samples: 11104  total_loss: 7.58  time: 0.1214(131.76)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:26] lb.utils.events INFO:  eta: 0:00:29  iteration: 695/1000  consumed samples: 11120  total_loss: 7.58  time: 0.1214(131.82)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:26] lb.utils.events INFO:  eta: 0:00:28  iteration: 696/1000  consumed samples: 11136  total_loss: 7.58  time: 0.1214(131.83)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:26] lb.utils.events INFO:  eta: 0:00:28  iteration: 697/1000  consumed samples: 11152  total_loss: 7.579  time: 0.1213(131.88)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:27] lb.utils.events INFO:  eta: 0:00:28  iteration: 698/1000  consumed samples: 11168  total_loss: 7.579  time: 0.1213(131.94)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:27] lb.utils.events INFO:  eta: 0:00:28  iteration: 699/1000  consumed samples: 11184  total_loss: 7.579  time: 0.1212(131.97)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:27] lb.utils.events INFO:  eta: 0:00:28  iteration: 700/1000  consumed samples: 11200  total_loss: 7.578  time: 0.1212(132.04)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:27] lb.utils.events INFO:  eta: 0:00:28  iteration: 701/1000  consumed samples: 11216  total_loss: 7.575  time: 0.1211(132.10)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:28] lb.utils.events INFO:  eta: 0:00:28  iteration: 702/1000  consumed samples: 11232  total_loss: 7.573  time: 0.1211(132.10)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:28] lb.utils.events INFO:  eta: 0:00:28  iteration: 703/1000  consumed samples: 11248  total_loss: 7.57  time: 0.1211(132.16)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:28] lb.utils.events INFO:  eta: 0:00:28  iteration: 704/1000  consumed samples: 11264  total_loss: 7.57  time: 0.1210(132.19)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:28] lb.utils.events INFO:  eta: 0:00:28  iteration: 705/1000  consumed samples: 11280  total_loss: 7.567  time: 0.1210(132.20)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:28] lb.utils.events INFO:  eta: 0:00:27  iteration: 706/1000  consumed samples: 11296  total_loss: 7.566  time: 0.1211(132.11)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:29] lb.utils.events INFO:  eta: 0:00:27  iteration: 707/1000  consumed samples: 11312  total_loss: 7.566  time: 0.1211(132.14)  data_time: 0.0036  lr: 9.00e-05  
[01/19 16:12:29] lb.utils.events INFO:  eta: 0:00:27  iteration: 708/1000  consumed samples: 11328  total_loss: 7.566  time: 0.1210(132.20)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:29] lb.utils.events INFO:  eta: 0:00:27  iteration: 709/1000  consumed samples: 11344  total_loss: 7.564  time: 0.1210(132.26)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:29] lb.utils.events INFO:  eta: 0:00:27  iteration: 710/1000  consumed samples: 11360  total_loss: 7.565  time: 0.1209(132.30)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 711/1000  consumed samples: 11376  total_loss: 7.565  time: 0.1209(132.29)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 712/1000  consumed samples: 11392  total_loss: 7.564  time: 0.1209(132.34)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:12:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 713/1000  consumed samples: 11408  total_loss: 7.564  time: 0.1208(132.40)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:30] lb.utils.events INFO:  eta: 0:00:27  iteration: 714/1000  consumed samples: 11424  total_loss: 7.564  time: 0.1208(132.45)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:31] lb.utils.events INFO:  eta: 0:00:27  iteration: 715/1000  consumed samples: 11440  total_loss: 7.564  time: 0.1207(132.51)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 716/1000  consumed samples: 11456  total_loss: 7.564  time: 0.1207(132.51)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 717/1000  consumed samples: 11472  total_loss: 7.564  time: 0.1207(132.52)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 718/1000  consumed samples: 11488  total_loss: 7.563  time: 0.1207(132.52)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:31] lb.utils.events INFO:  eta: 0:00:26  iteration: 719/1000  consumed samples: 11504  total_loss: 7.563  time: 0.1207(132.57)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:32] lb.utils.events INFO:  eta: 0:00:26  iteration: 720/1000  consumed samples: 11520  total_loss: 7.563  time: 0.1206(132.63)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:32] lb.utils.events INFO:  eta: 0:00:26  iteration: 721/1000  consumed samples: 11536  total_loss: 7.563  time: 0.1206(132.69)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:32] lb.utils.events INFO:  eta: 0:00:26  iteration: 722/1000  consumed samples: 11552  total_loss: 7.562  time: 0.1205(132.75)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:32] lb.utils.events INFO:  eta: 0:00:26  iteration: 723/1000  consumed samples: 11568  total_loss: 7.562  time: 0.1205(132.81)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:33] lb.utils.events INFO:  eta: 0:00:26  iteration: 724/1000  consumed samples: 11584  total_loss: 7.562  time: 0.1204(132.87)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:33] lb.utils.events INFO:  eta: 0:00:26  iteration: 725/1000  consumed samples: 11600  total_loss: 7.561  time: 0.1204(132.89)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:33] lb.utils.events INFO:  eta: 0:00:25  iteration: 726/1000  consumed samples: 11616  total_loss: 7.561  time: 0.1204(132.90)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:33] lb.utils.events INFO:  eta: 0:00:25  iteration: 727/1000  consumed samples: 11632  total_loss: 7.561  time: 0.1204(132.93)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:34] lb.utils.events INFO:  eta: 0:00:25  iteration: 728/1000  consumed samples: 11648  total_loss: 7.561  time: 0.1204(132.92)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:34] lb.utils.events INFO:  eta: 0:00:25  iteration: 729/1000  consumed samples: 11664  total_loss: 7.561  time: 0.1203(132.97)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:34] lb.utils.events INFO:  eta: 0:00:25  iteration: 730/1000  consumed samples: 11680  total_loss: 7.557  time: 0.1203(133.03)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:34] lb.utils.events INFO:  eta: 0:00:25  iteration: 731/1000  consumed samples: 11696  total_loss: 7.557  time: 0.1202(133.09)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:35] lb.utils.events INFO:  eta: 0:00:25  iteration: 732/1000  consumed samples: 11712  total_loss: 7.553  time: 0.1203(133.02)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:35] lb.utils.events INFO:  eta: 0:00:25  iteration: 733/1000  consumed samples: 11728  total_loss: 7.551  time: 0.1203(133.01)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:35] lb.utils.events INFO:  eta: 0:00:25  iteration: 734/1000  consumed samples: 11744  total_loss: 7.551  time: 0.1203(133.01)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:35] lb.utils.events INFO:  eta: 0:00:25  iteration: 735/1000  consumed samples: 11760  total_loss: 7.551  time: 0.1203(132.99)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:35] lb.utils.events INFO:  eta: 0:00:25  iteration: 736/1000  consumed samples: 11776  total_loss: 7.551  time: 0.1203(133.02)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:36] lb.utils.events INFO:  eta: 0:00:25  iteration: 737/1000  consumed samples: 11792  total_loss: 7.551  time: 0.1203(133.02)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:36] lb.utils.events INFO:  eta: 0:00:24  iteration: 738/1000  consumed samples: 11808  total_loss: 7.551  time: 0.1203(133.02)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:36] lb.utils.events INFO:  eta: 0:00:24  iteration: 739/1000  consumed samples: 11824  total_loss: 7.551  time: 0.1203(133.02)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:36] lb.utils.events INFO:  eta: 0:00:24  iteration: 740/1000  consumed samples: 11840  total_loss: 7.55  time: 0.1203(133.05)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:37] lb.utils.events INFO:  eta: 0:00:24  iteration: 741/1000  consumed samples: 11856  total_loss: 7.549  time: 0.1203(133.00)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:37] lb.utils.events INFO:  eta: 0:00:24  iteration: 742/1000  consumed samples: 11872  total_loss: 7.549  time: 0.1203(132.95)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:37] lb.utils.events INFO:  eta: 0:00:24  iteration: 743/1000  consumed samples: 11888  total_loss: 7.549  time: 0.1204(132.93)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:37] lb.utils.events INFO:  eta: 0:00:24  iteration: 744/1000  consumed samples: 11904  total_loss: 7.549  time: 0.1204(132.93)  data_time: 0.0038  lr: 9.00e-05  
[01/19 16:12:37] lb.utils.events INFO:  eta: 0:00:24  iteration: 745/1000  consumed samples: 11920  total_loss: 7.55  time: 0.1203(132.98)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:38] lb.utils.events INFO:  eta: 0:00:24  iteration: 746/1000  consumed samples: 11936  total_loss: 7.55  time: 0.1203(133.00)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:38] lb.utils.events INFO:  eta: 0:00:24  iteration: 747/1000  consumed samples: 11952  total_loss: 7.551  time: 0.1204(132.92)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:38] lb.utils.events INFO:  eta: 0:00:24  iteration: 748/1000  consumed samples: 11968  total_loss: 7.551  time: 0.1203(132.98)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:38] lb.utils.events INFO:  eta: 0:00:24  iteration: 749/1000  consumed samples: 11984  total_loss: 7.551  time: 0.1204(132.92)  data_time: 0.0039  lr: 9.00e-05  
[01/19 16:12:39] lb.utils.events INFO:  eta: 0:00:23  iteration: 750/1000  consumed samples: 12000  total_loss: 7.55  time: 0.1203(132.95)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:39] lb.utils.events INFO:  eta: 0:00:23  iteration: 751/1000  consumed samples: 12016  total_loss: 7.55  time: 0.1203(132.99)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:39] lb.utils.events INFO:  eta: 0:00:23  iteration: 752/1000  consumed samples: 12032  total_loss: 7.549  time: 0.1203(132.97)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:39] lb.utils.events INFO:  eta: 0:00:23  iteration: 753/1000  consumed samples: 12048  total_loss: 7.548  time: 0.1203(133.02)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:40] lb.utils.events INFO:  eta: 0:00:23  iteration: 754/1000  consumed samples: 12064  total_loss: 7.549  time: 0.1202(133.07)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:40] lb.utils.events INFO:  eta: 0:00:23  iteration: 755/1000  consumed samples: 12080  total_loss: 7.548  time: 0.1202(133.08)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:40] lb.utils.events INFO:  eta: 0:00:23  iteration: 756/1000  consumed samples: 12096  total_loss: 7.547  time: 0.1202(133.06)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:40] lb.utils.events INFO:  eta: 0:00:23  iteration: 757/1000  consumed samples: 12112  total_loss: 7.545  time: 0.1202(133.12)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:41] lb.utils.events INFO:  eta: 0:00:23  iteration: 758/1000  consumed samples: 12128  total_loss: 7.544  time: 0.1201(133.17)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:41] lb.utils.events INFO:  eta: 0:00:23  iteration: 759/1000  consumed samples: 12144  total_loss: 7.545  time: 0.1201(133.23)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:41] lb.utils.events INFO:  eta: 0:00:22  iteration: 760/1000  consumed samples: 12160  total_loss: 7.544  time: 0.1201(133.22)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:41] lb.utils.events INFO:  eta: 0:00:22  iteration: 761/1000  consumed samples: 12176  total_loss: 7.544  time: 0.1201(133.27)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:42] lb.utils.events INFO:  eta: 0:00:22  iteration: 762/1000  consumed samples: 12192  total_loss: 7.543  time: 0.1201(133.24)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:42] lb.utils.events INFO:  eta: 0:00:22  iteration: 763/1000  consumed samples: 12208  total_loss: 7.541  time: 0.1201(133.17)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:42] lb.utils.events INFO:  eta: 0:00:22  iteration: 764/1000  consumed samples: 12224  total_loss: 7.539  time: 0.1201(133.22)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:42] lb.utils.events INFO:  eta: 0:00:22  iteration: 765/1000  consumed samples: 12240  total_loss: 7.539  time: 0.1201(133.22)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:42] lb.utils.events INFO:  eta: 0:00:22  iteration: 766/1000  consumed samples: 12256  total_loss: 7.539  time: 0.1201(133.26)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:43] lb.utils.events INFO:  eta: 0:00:22  iteration: 767/1000  consumed samples: 12272  total_loss: 7.539  time: 0.1201(133.22)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:12:43] lb.utils.events INFO:  eta: 0:00:22  iteration: 768/1000  consumed samples: 12288  total_loss: 7.539  time: 0.1201(133.28)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:43] lb.utils.events INFO:  eta: 0:00:22  iteration: 769/1000  consumed samples: 12304  total_loss: 7.537  time: 0.1200(133.33)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:43] lb.utils.events INFO:  eta: 0:00:21  iteration: 770/1000  consumed samples: 12320  total_loss: 7.539  time: 0.1200(133.38)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:44] lb.utils.events INFO:  eta: 0:00:21  iteration: 771/1000  consumed samples: 12336  total_loss: 7.537  time: 0.1200(133.34)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:44] lb.utils.events INFO:  eta: 0:00:21  iteration: 772/1000  consumed samples: 12352  total_loss: 7.537  time: 0.1200(133.37)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:44] lb.utils.events INFO:  eta: 0:00:21  iteration: 773/1000  consumed samples: 12368  total_loss: 7.537  time: 0.1200(133.37)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:44] lb.utils.events INFO:  eta: 0:00:21  iteration: 774/1000  consumed samples: 12384  total_loss: 7.537  time: 0.1199(133.42)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:45] lb.utils.events INFO:  eta: 0:00:21  iteration: 775/1000  consumed samples: 12400  total_loss: 7.535  time: 0.1200(133.39)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:45] lb.utils.events INFO:  eta: 0:00:21  iteration: 776/1000  consumed samples: 12416  total_loss: 7.535  time: 0.1200(133.36)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:45] lb.utils.events INFO:  eta: 0:00:21  iteration: 777/1000  consumed samples: 12432  total_loss: 7.534  time: 0.1200(133.34)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:45] lb.utils.events INFO:  eta: 0:00:21  iteration: 778/1000  consumed samples: 12448  total_loss: 7.533  time: 0.1200(133.35)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:45] lb.utils.events INFO:  eta: 0:00:21  iteration: 779/1000  consumed samples: 12464  total_loss: 7.533  time: 0.1200(133.34)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:46] lb.utils.events INFO:  eta: 0:00:21  iteration: 780/1000  consumed samples: 12480  total_loss: 7.533  time: 0.1199(133.39)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:46] lb.utils.events INFO:  eta: 0:00:21  iteration: 781/1000  consumed samples: 12496  total_loss: 7.533  time: 0.1199(133.43)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:46] lb.utils.events INFO:  eta: 0:00:20  iteration: 782/1000  consumed samples: 12512  total_loss: 7.532  time: 0.1199(133.42)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:46] lb.utils.events INFO:  eta: 0:00:20  iteration: 783/1000  consumed samples: 12528  total_loss: 7.533  time: 0.1199(133.45)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:47] lb.utils.events INFO:  eta: 0:00:20  iteration: 784/1000  consumed samples: 12544  total_loss: 7.533  time: 0.1198(133.50)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:47] lb.utils.events INFO:  eta: 0:00:20  iteration: 785/1000  consumed samples: 12560  total_loss: 7.533  time: 0.1198(133.55)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:47] lb.utils.events INFO:  eta: 0:00:20  iteration: 786/1000  consumed samples: 12576  total_loss: 7.533  time: 0.1198(133.54)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:47] lb.utils.events INFO:  eta: 0:00:20  iteration: 787/1000  consumed samples: 12592  total_loss: 7.533  time: 0.1199(133.47)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:48] lb.utils.events INFO:  eta: 0:00:20  iteration: 788/1000  consumed samples: 12608  total_loss: 7.533  time: 0.1198(133.52)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:12:48] lb.utils.events INFO:  eta: 0:00:20  iteration: 789/1000  consumed samples: 12624  total_loss: 7.533  time: 0.1198(133.57)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:48] lb.utils.events INFO:  eta: 0:00:20  iteration: 790/1000  consumed samples: 12640  total_loss: 7.533  time: 0.1199(133.50)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:48] lb.utils.events INFO:  eta: 0:00:20  iteration: 791/1000  consumed samples: 12656  total_loss: 7.532  time: 0.1198(133.51)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:48] lb.utils.events INFO:  eta: 0:00:19  iteration: 792/1000  consumed samples: 12672  total_loss: 7.532  time: 0.1198(133.56)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:49] lb.utils.events INFO:  eta: 0:00:19  iteration: 793/1000  consumed samples: 12688  total_loss: 7.531  time: 0.1198(133.58)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:49] lb.utils.events INFO:  eta: 0:00:19  iteration: 794/1000  consumed samples: 12704  total_loss: 7.53  time: 0.1198(133.56)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:49] lb.utils.events INFO:  eta: 0:00:19  iteration: 795/1000  consumed samples: 12720  total_loss: 7.531  time: 0.1198(133.61)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:49] lb.utils.events INFO:  eta: 0:00:19  iteration: 796/1000  consumed samples: 12736  total_loss: 7.531  time: 0.1197(133.63)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:50] lb.utils.events INFO:  eta: 0:00:19  iteration: 797/1000  consumed samples: 12752  total_loss: 7.531  time: 0.1198(133.58)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:50] lb.utils.events INFO:  eta: 0:00:19  iteration: 798/1000  consumed samples: 12768  total_loss: 7.53  time: 0.1198(133.59)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:50] lb.utils.events INFO:  eta: 0:00:19  iteration: 799/1000  consumed samples: 12784  total_loss: 7.53  time: 0.1198(133.58)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:50] lb.utils.events INFO:  eta: 0:00:19  iteration: 800/1000  consumed samples: 12800  total_loss: 7.53  time: 0.1198(133.57)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:51] lb.utils.events INFO:  eta: 0:00:19  iteration: 801/1000  consumed samples: 12816  total_loss: 7.53  time: 0.1198(133.57)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:51] lb.utils.events INFO:  eta: 0:00:19  iteration: 802/1000  consumed samples: 12832  total_loss: 7.53  time: 0.1198(133.57)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:51] lb.utils.events INFO:  eta: 0:00:19  iteration: 803/1000  consumed samples: 12848  total_loss: 7.529  time: 0.1198(133.54)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:51] lb.utils.events INFO:  eta: 0:00:18  iteration: 804/1000  consumed samples: 12864  total_loss: 7.528  time: 0.1198(133.55)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:51] lb.utils.events INFO:  eta: 0:00:18  iteration: 805/1000  consumed samples: 12880  total_loss: 7.526  time: 0.1198(133.53)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:52] lb.utils.events INFO:  eta: 0:00:18  iteration: 806/1000  consumed samples: 12896  total_loss: 7.526  time: 0.1198(133.58)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:52] lb.utils.events INFO:  eta: 0:00:18  iteration: 807/1000  consumed samples: 12912  total_loss: 7.526  time: 0.1197(133.64)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:52] lb.utils.events INFO:  eta: 0:00:18  iteration: 808/1000  consumed samples: 12928  total_loss: 7.528  time: 0.1198(133.60)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:52] lb.utils.events INFO:  eta: 0:00:18  iteration: 809/1000  consumed samples: 12944  total_loss: 7.528  time: 0.1197(133.65)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 810/1000  consumed samples: 12960  total_loss: 7.526  time: 0.1197(133.64)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 811/1000  consumed samples: 12976  total_loss: 7.526  time: 0.1197(133.69)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 812/1000  consumed samples: 12992  total_loss: 7.525  time: 0.1197(133.71)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:53] lb.utils.events INFO:  eta: 0:00:18  iteration: 813/1000  consumed samples: 13008  total_loss: 7.525  time: 0.1196(133.76)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 814/1000  consumed samples: 13024  total_loss: 7.525  time: 0.1196(133.80)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 815/1000  consumed samples: 13040  total_loss: 7.525  time: 0.1196(133.78)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 816/1000  consumed samples: 13056  total_loss: 7.525  time: 0.1196(133.79)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:12:54] lb.utils.events INFO:  eta: 0:00:17  iteration: 817/1000  consumed samples: 13072  total_loss: 7.525  time: 0.1196(133.81)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:55] lb.utils.events INFO:  eta: 0:00:17  iteration: 818/1000  consumed samples: 13088  total_loss: 7.525  time: 0.1196(133.80)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:55] lb.utils.events INFO:  eta: 0:00:17  iteration: 819/1000  consumed samples: 13104  total_loss: 7.524  time: 0.1196(133.83)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:55] lb.utils.events INFO:  eta: 0:00:17  iteration: 820/1000  consumed samples: 13120  total_loss: 7.524  time: 0.1196(133.83)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:55] lb.utils.events INFO:  eta: 0:00:17  iteration: 821/1000  consumed samples: 13136  total_loss: 7.524  time: 0.1195(133.85)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:55] lb.utils.events INFO:  eta: 0:00:17  iteration: 822/1000  consumed samples: 13152  total_loss: 7.524  time: 0.1195(133.90)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:56] lb.utils.events INFO:  eta: 0:00:17  iteration: 823/1000  consumed samples: 13168  total_loss: 7.524  time: 0.1195(133.93)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:56] lb.utils.events INFO:  eta: 0:00:17  iteration: 824/1000  consumed samples: 13184  total_loss: 7.525  time: 0.1195(133.90)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:56] lb.utils.events INFO:  eta: 0:00:17  iteration: 825/1000  consumed samples: 13200  total_loss: 7.524  time: 0.1195(133.93)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:56] lb.utils.events INFO:  eta: 0:00:16  iteration: 826/1000  consumed samples: 13216  total_loss: 7.523  time: 0.1194(133.98)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:57] lb.utils.events INFO:  eta: 0:00:16  iteration: 827/1000  consumed samples: 13232  total_loss: 7.523  time: 0.1194(134.02)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:57] lb.utils.events INFO:  eta: 0:00:16  iteration: 828/1000  consumed samples: 13248  total_loss: 7.521  time: 0.1193(134.07)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:57] lb.utils.events INFO:  eta: 0:00:16  iteration: 829/1000  consumed samples: 13264  total_loss: 7.521  time: 0.1193(134.08)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:12:57] lb.utils.events INFO:  eta: 0:00:16  iteration: 830/1000  consumed samples: 13280  total_loss: 7.52  time: 0.1193(134.12)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:58] lb.utils.events INFO:  eta: 0:00:16  iteration: 831/1000  consumed samples: 13296  total_loss: 7.519  time: 0.1193(134.14)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:58] lb.utils.events INFO:  eta: 0:00:16  iteration: 832/1000  consumed samples: 13312  total_loss: 7.518  time: 0.1193(134.14)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:58] lb.utils.events INFO:  eta: 0:00:16  iteration: 833/1000  consumed samples: 13328  total_loss: 7.518  time: 0.1193(134.09)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:58] lb.utils.events INFO:  eta: 0:00:16  iteration: 834/1000  consumed samples: 13344  total_loss: 7.516  time: 0.1194(134.06)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:58] lb.utils.events INFO:  eta: 0:00:16  iteration: 835/1000  consumed samples: 13360  total_loss: 7.516  time: 0.1193(134.11)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:59] lb.utils.events INFO:  eta: 0:00:16  iteration: 836/1000  consumed samples: 13376  total_loss: 7.518  time: 0.1193(134.08)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:59] lb.utils.events INFO:  eta: 0:00:15  iteration: 837/1000  consumed samples: 13392  total_loss: 7.518  time: 0.1193(134.13)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:59] lb.utils.events INFO:  eta: 0:00:15  iteration: 838/1000  consumed samples: 13408  total_loss: 7.518  time: 0.1193(134.07)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:12:59] lb.utils.events INFO:  eta: 0:00:15  iteration: 839/1000  consumed samples: 13424  total_loss: 7.519  time: 0.1194(134.01)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:00] lb.utils.events INFO:  eta: 0:00:15  iteration: 840/1000  consumed samples: 13440  total_loss: 7.518  time: 0.1195(133.94)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:00] lb.utils.events INFO:  eta: 0:00:15  iteration: 841/1000  consumed samples: 13456  total_loss: 7.516  time: 0.1195(133.89)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:00] lb.utils.events INFO:  eta: 0:00:15  iteration: 842/1000  consumed samples: 13472  total_loss: 7.516  time: 0.1195(133.88)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:00] lb.utils.events INFO:  eta: 0:00:15  iteration: 843/1000  consumed samples: 13488  total_loss: 7.516  time: 0.1195(133.89)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:00] lb.utils.events INFO:  eta: 0:00:15  iteration: 844/1000  consumed samples: 13504  total_loss: 7.516  time: 0.1195(133.89)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:01] lb.utils.events INFO:  eta: 0:00:15  iteration: 845/1000  consumed samples: 13520  total_loss: 7.515  time: 0.1195(133.92)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:01] lb.utils.events INFO:  eta: 0:00:15  iteration: 846/1000  consumed samples: 13536  total_loss: 7.513  time: 0.1195(133.84)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:01] lb.utils.events INFO:  eta: 0:00:15  iteration: 847/1000  consumed samples: 13552  total_loss: 7.51  time: 0.1195(133.84)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:01] lb.utils.events INFO:  eta: 0:00:14  iteration: 848/1000  consumed samples: 13568  total_loss: 7.51  time: 0.1196(133.83)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:02] lb.utils.events INFO:  eta: 0:00:14  iteration: 849/1000  consumed samples: 13584  total_loss: 7.51  time: 0.1195(133.85)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:02] lb.utils.events INFO:  eta: 0:00:14  iteration: 850/1000  consumed samples: 13600  total_loss: 7.51  time: 0.1195(133.89)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:02] lb.utils.events INFO:  eta: 0:00:14  iteration: 851/1000  consumed samples: 13616  total_loss: 7.513  time: 0.1195(133.94)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:02] lb.utils.events INFO:  eta: 0:00:14  iteration: 852/1000  consumed samples: 13632  total_loss: 7.513  time: 0.1194(133.99)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:03] lb.utils.events INFO:  eta: 0:00:14  iteration: 853/1000  consumed samples: 13648  total_loss: 7.51  time: 0.1194(134.01)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:03] lb.utils.events INFO:  eta: 0:00:14  iteration: 854/1000  consumed samples: 13664  total_loss: 7.509  time: 0.1193(134.06)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:03] lb.utils.events INFO:  eta: 0:00:14  iteration: 855/1000  consumed samples: 13680  total_loss: 7.51  time: 0.1194(134.03)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:03] lb.utils.events INFO:  eta: 0:00:14  iteration: 856/1000  consumed samples: 13696  total_loss: 7.51  time: 0.1194(134.01)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:03] lb.utils.events INFO:  eta: 0:00:14  iteration: 857/1000  consumed samples: 13712  total_loss: 7.509  time: 0.1194(134.06)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:04] lb.utils.events INFO:  eta: 0:00:13  iteration: 858/1000  consumed samples: 13728  total_loss: 7.51  time: 0.1193(134.10)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:04] lb.utils.events INFO:  eta: 0:00:13  iteration: 859/1000  consumed samples: 13744  total_loss: 7.51  time: 0.1193(134.14)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:04] lb.utils.events INFO:  eta: 0:00:13  iteration: 860/1000  consumed samples: 13760  total_loss: 7.51  time: 0.1192(134.19)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:04] lb.utils.events INFO:  eta: 0:00:13  iteration: 861/1000  consumed samples: 13776  total_loss: 7.509  time: 0.1192(134.21)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:05] lb.utils.events INFO:  eta: 0:00:13  iteration: 862/1000  consumed samples: 13792  total_loss: 7.509  time: 0.1192(134.25)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:05] lb.utils.events INFO:  eta: 0:00:13  iteration: 863/1000  consumed samples: 13808  total_loss: 7.509  time: 0.1191(134.30)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:05] lb.utils.events INFO:  eta: 0:00:13  iteration: 864/1000  consumed samples: 13824  total_loss: 7.509  time: 0.1191(134.34)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:05] lb.utils.events INFO:  eta: 0:00:13  iteration: 865/1000  consumed samples: 13840  total_loss: 7.506  time: 0.1191(134.31)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:06] lb.utils.events INFO:  eta: 0:00:13  iteration: 866/1000  consumed samples: 13856  total_loss: 7.506  time: 0.1191(134.36)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:06] lb.utils.events INFO:  eta: 0:00:12  iteration: 867/1000  consumed samples: 13872  total_loss: 7.503  time: 0.1190(134.41)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:06] lb.utils.events INFO:  eta: 0:00:12  iteration: 868/1000  consumed samples: 13888  total_loss: 7.506  time: 0.1190(134.41)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:06] lb.utils.events INFO:  eta: 0:00:12  iteration: 869/1000  consumed samples: 13904  total_loss: 7.503  time: 0.1190(134.45)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:07] lb.utils.events INFO:  eta: 0:00:12  iteration: 870/1000  consumed samples: 13920  total_loss: 7.502  time: 0.1190(134.48)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:07] lb.utils.events INFO:  eta: 0:00:12  iteration: 871/1000  consumed samples: 13936  total_loss: 7.502  time: 0.1191(134.39)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:07] lb.utils.events INFO:  eta: 0:00:12  iteration: 872/1000  consumed samples: 13952  total_loss: 7.502  time: 0.1190(134.43)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:07] lb.utils.events INFO:  eta: 0:00:12  iteration: 873/1000  consumed samples: 13968  total_loss: 7.502  time: 0.1190(134.43)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:08] lb.utils.events INFO:  eta: 0:00:12  iteration: 874/1000  consumed samples: 13984  total_loss: 7.502  time: 0.1190(134.48)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:08] lb.utils.events INFO:  eta: 0:00:12  iteration: 875/1000  consumed samples: 14000  total_loss: 7.502  time: 0.1189(134.53)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:08] lb.utils.events INFO:  eta: 0:00:12  iteration: 876/1000  consumed samples: 14016  total_loss: 7.502  time: 0.1189(134.52)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:08] lb.utils.events INFO:  eta: 0:00:12  iteration: 877/1000  consumed samples: 14032  total_loss: 7.502  time: 0.1190(134.50)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:08] lb.utils.events INFO:  eta: 0:00:11  iteration: 878/1000  consumed samples: 14048  total_loss: 7.501  time: 0.1189(134.54)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:09] lb.utils.events INFO:  eta: 0:00:11  iteration: 879/1000  consumed samples: 14064  total_loss: 7.499  time: 0.1190(134.48)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:09] lb.utils.events INFO:  eta: 0:00:11  iteration: 880/1000  consumed samples: 14080  total_loss: 7.499  time: 0.1190(134.45)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:09] lb.utils.events INFO:  eta: 0:00:11  iteration: 881/1000  consumed samples: 14096  total_loss: 7.498  time: 0.1190(134.49)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:09] lb.utils.events INFO:  eta: 0:00:11  iteration: 882/1000  consumed samples: 14112  total_loss: 7.498  time: 0.1189(134.54)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:10] lb.utils.events INFO:  eta: 0:00:11  iteration: 883/1000  consumed samples: 14128  total_loss: 7.497  time: 0.1189(134.59)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:10] lb.utils.events INFO:  eta: 0:00:11  iteration: 884/1000  consumed samples: 14144  total_loss: 7.497  time: 0.1189(134.56)  data_time: 0.0040  lr: 9.00e-05  
[01/19 16:13:10] lb.utils.events INFO:  eta: 0:00:11  iteration: 885/1000  consumed samples: 14160  total_loss: 7.495  time: 0.1189(134.61)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:10] lb.utils.events INFO:  eta: 0:00:11  iteration: 886/1000  consumed samples: 14176  total_loss: 7.495  time: 0.1188(134.65)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:11] lb.utils.events INFO:  eta: 0:00:10  iteration: 887/1000  consumed samples: 14192  total_loss: 7.497  time: 0.1188(134.69)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:11] lb.utils.events INFO:  eta: 0:00:10  iteration: 888/1000  consumed samples: 14208  total_loss: 7.497  time: 0.1187(134.74)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:11] lb.utils.events INFO:  eta: 0:00:10  iteration: 889/1000  consumed samples: 14224  total_loss: 7.497  time: 0.1187(134.78)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:11] lb.utils.events INFO:  eta: 0:00:10  iteration: 890/1000  consumed samples: 14240  total_loss: 7.495  time: 0.1187(134.77)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:12] lb.utils.events INFO:  eta: 0:00:10  iteration: 891/1000  consumed samples: 14256  total_loss: 7.495  time: 0.1187(134.82)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:12] lb.utils.events INFO:  eta: 0:00:10  iteration: 892/1000  consumed samples: 14272  total_loss: 7.494  time: 0.1186(134.86)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:12] lb.utils.events INFO:  eta: 0:00:10  iteration: 893/1000  consumed samples: 14288  total_loss: 7.495  time: 0.1187(134.83)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:12] lb.utils.events INFO:  eta: 0:00:10  iteration: 894/1000  consumed samples: 14304  total_loss: 7.495  time: 0.1186(134.87)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:13] lb.utils.events INFO:  eta: 0:00:10  iteration: 895/1000  consumed samples: 14320  total_loss: 7.497  time: 0.1186(134.91)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:13] lb.utils.events INFO:  eta: 0:00:10  iteration: 896/1000  consumed samples: 14336  total_loss: 7.495  time: 0.1186(134.96)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:13] lb.utils.events INFO:  eta: 0:00:09  iteration: 897/1000  consumed samples: 14352  total_loss: 7.495  time: 0.1185(134.96)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:13] lb.utils.events INFO:  eta: 0:00:09  iteration: 898/1000  consumed samples: 14368  total_loss: 7.495  time: 0.1186(134.96)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 899/1000  consumed samples: 14384  total_loss: 7.495  time: 0.1185(135.01)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 900/1000  consumed samples: 14400  total_loss: 7.494  time: 0.1185(135.05)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 901/1000  consumed samples: 14416  total_loss: 7.494  time: 0.1184(135.10)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:14] lb.utils.events INFO:  eta: 0:00:09  iteration: 902/1000  consumed samples: 14432  total_loss: 7.491  time: 0.1185(135.07)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:15] lb.utils.events INFO:  eta: 0:00:09  iteration: 903/1000  consumed samples: 14448  total_loss: 7.491  time: 0.1185(134.98)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:15] lb.utils.events INFO:  eta: 0:00:09  iteration: 904/1000  consumed samples: 14464  total_loss: 7.494  time: 0.1185(135.00)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:15] lb.utils.events INFO:  eta: 0:00:09  iteration: 905/1000  consumed samples: 14480  total_loss: 7.494  time: 0.1185(135.04)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:16] lb.utils.events INFO:  eta: 0:00:09  iteration: 906/1000  consumed samples: 14496  total_loss: 7.495  time: 0.1184(135.09)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:16] lb.utils.events INFO:  eta: 0:00:08  iteration: 907/1000  consumed samples: 14512  total_loss: 7.495  time: 0.1184(135.10)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:16] lb.utils.events INFO:  eta: 0:00:08  iteration: 908/1000  consumed samples: 14528  total_loss: 7.494  time: 0.1184(135.10)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:16] lb.utils.events INFO:  eta: 0:00:08  iteration: 909/1000  consumed samples: 14544  total_loss: 7.494  time: 0.1185(135.07)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:16] lb.utils.events INFO:  eta: 0:00:08  iteration: 910/1000  consumed samples: 14560  total_loss: 7.491  time: 0.1184(135.12)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:17] lb.utils.events INFO:  eta: 0:00:08  iteration: 911/1000  consumed samples: 14576  total_loss: 7.494  time: 0.1184(135.16)  data_time: 0.0041  lr: 9.00e-05  
[01/19 16:13:17] lb.utils.events INFO:  eta: 0:00:08  iteration: 912/1000  consumed samples: 14592  total_loss: 7.494  time: 0.1183(135.20)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:17] lb.utils.events INFO:  eta: 0:00:08  iteration: 913/1000  consumed samples: 14608  total_loss: 7.494  time: 0.1183(135.24)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:17] lb.utils.events INFO:  eta: 0:00:08  iteration: 914/1000  consumed samples: 14624  total_loss: 7.491  time: 0.1183(135.24)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:18] lb.utils.events INFO:  eta: 0:00:08  iteration: 915/1000  consumed samples: 14640  total_loss: 7.486  time: 0.1183(135.29)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:18] lb.utils.events INFO:  eta: 0:00:08  iteration: 916/1000  consumed samples: 14656  total_loss: 7.48  time: 0.1182(135.33)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:18] lb.utils.events INFO:  eta: 0:00:07  iteration: 917/1000  consumed samples: 14672  total_loss: 7.486  time: 0.1182(135.38)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:18] lb.utils.events INFO:  eta: 0:00:07  iteration: 918/1000  consumed samples: 14688  total_loss: 7.486  time: 0.1182(135.41)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:19] lb.utils.events INFO:  eta: 0:00:07  iteration: 919/1000  consumed samples: 14704  total_loss: 7.491  time: 0.1181(135.46)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:19] lb.utils.events INFO:  eta: 0:00:07  iteration: 920/1000  consumed samples: 14720  total_loss: 7.486  time: 0.1181(135.47)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:19] lb.utils.events INFO:  eta: 0:00:07  iteration: 921/1000  consumed samples: 14736  total_loss: 7.48  time: 0.1181(135.51)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:19] lb.utils.events INFO:  eta: 0:00:07  iteration: 922/1000  consumed samples: 14752  total_loss: 7.48  time: 0.1180(135.56)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:20] lb.utils.events INFO:  eta: 0:00:07  iteration: 923/1000  consumed samples: 14768  total_loss: 7.478  time: 0.1180(135.55)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:20] lb.utils.events INFO:  eta: 0:00:07  iteration: 924/1000  consumed samples: 14784  total_loss: 7.478  time: 0.1180(135.57)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:20] lb.utils.events INFO:  eta: 0:00:07  iteration: 925/1000  consumed samples: 14800  total_loss: 7.478  time: 0.1180(135.55)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:20] lb.utils.events INFO:  eta: 0:00:07  iteration: 926/1000  consumed samples: 14816  total_loss: 7.477  time: 0.1180(135.57)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:21] lb.utils.events INFO:  eta: 0:00:06  iteration: 927/1000  consumed samples: 14832  total_loss: 7.475  time: 0.1180(135.62)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:21] lb.utils.events INFO:  eta: 0:00:06  iteration: 928/1000  consumed samples: 14848  total_loss: 7.477  time: 0.1179(135.66)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:22] lb.utils.events INFO:  eta: 0:00:06  iteration: 929/1000  consumed samples: 14864  total_loss: 7.477  time: 0.1179(135.70)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:22] lb.utils.events INFO:  eta: 0:00:06  iteration: 930/1000  consumed samples: 14880  total_loss: 7.475  time: 0.1179(135.74)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 931/1000  consumed samples: 14896  total_loss: 7.475  time: 0.1178(135.79)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:13:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 932/1000  consumed samples: 14912  total_loss: 7.472  time: 0.1178(135.83)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 933/1000  consumed samples: 14928  total_loss: 7.472  time: 0.1178(135.88)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:24] lb.utils.events INFO:  eta: 0:00:06  iteration: 934/1000  consumed samples: 14944  total_loss: 7.47  time: 0.1177(135.92)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:25] lb.utils.events INFO:  eta: 0:00:06  iteration: 935/1000  consumed samples: 14960  total_loss: 7.47  time: 0.1177(135.96)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:25] lb.utils.events INFO:  eta: 0:00:06  iteration: 936/1000  consumed samples: 14976  total_loss: 7.468  time: 0.1177(135.92)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:25] lb.utils.events INFO:  eta: 0:00:05  iteration: 937/1000  consumed samples: 14992  total_loss: 7.466  time: 0.1177(135.95)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:25] lb.utils.events INFO:  eta: 0:00:05  iteration: 938/1000  consumed samples: 15008  total_loss: 7.466  time: 0.1177(135.99)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:13:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 939/1000  consumed samples: 15024  total_loss: 7.465  time: 0.1177(135.97)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:13:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 940/1000  consumed samples: 15040  total_loss: 7.465  time: 0.1177(135.94)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:26] lb.utils.events INFO:  eta: 0:00:05  iteration: 941/1000  consumed samples: 15056  total_loss: 7.466  time: 0.1177(135.98)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:27] lb.utils.events INFO:  eta: 0:00:05  iteration: 942/1000  consumed samples: 15072  total_loss: 7.464  time: 0.1176(136.02)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:27] lb.utils.events INFO:  eta: 0:00:05  iteration: 943/1000  consumed samples: 15088  total_loss: 7.464  time: 0.1176(136.07)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:28] lb.utils.events INFO:  eta: 0:00:05  iteration: 944/1000  consumed samples: 15104  total_loss: 7.463  time: 0.1175(136.11)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:28] lb.utils.events INFO:  eta: 0:00:05  iteration: 945/1000  consumed samples: 15120  total_loss: 7.461  time: 0.1175(136.14)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:29] lb.utils.events INFO:  eta: 0:00:05  iteration: 946/1000  consumed samples: 15136  total_loss: 7.458  time: 0.1175(136.18)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:13:29] lb.utils.events INFO:  eta: 0:00:04  iteration: 947/1000  consumed samples: 15152  total_loss: 7.455  time: 0.1175(136.23)  data_time: 0.0045  lr: 9.00e-05  
[01/19 16:13:29] lb.utils.events INFO:  eta: 0:00:04  iteration: 948/1000  consumed samples: 15168  total_loss: 7.455  time: 0.1174(136.27)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:29] lb.utils.events INFO:  eta: 0:00:04  iteration: 949/1000  consumed samples: 15184  total_loss: 7.455  time: 0.1174(136.31)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:30] lb.utils.events INFO:  eta: 0:00:04  iteration: 950/1000  consumed samples: 15200  total_loss: 7.455  time: 0.1173(136.35)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:30] lb.utils.events INFO:  eta: 0:00:04  iteration: 951/1000  consumed samples: 15216  total_loss: 7.454  time: 0.1173(136.39)  data_time: 0.0042  lr: 9.00e-05  
[01/19 16:13:31] lb.utils.events INFO:  eta: 0:00:04  iteration: 952/1000  consumed samples: 15232  total_loss: 7.454  time: 0.1173(136.43)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:31] lb.utils.events INFO:  eta: 0:00:04  iteration: 953/1000  consumed samples: 15248  total_loss: 7.455  time: 0.1172(136.47)  data_time: 0.0043  lr: 9.00e-05  
[01/19 16:13:31] lb.utils.events INFO:  eta: 0:00:04  iteration: 954/1000  consumed samples: 15264  total_loss: 7.455  time: 0.1172(136.51)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:32] lb.utils.events INFO:  eta: 0:00:04  iteration: 955/1000  consumed samples: 15280  total_loss: 7.458  time: 0.1172(136.55)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:32] lb.utils.events INFO:  eta: 0:00:04  iteration: 956/1000  consumed samples: 15296  total_loss: 7.458  time: 0.1171(136.58)  data_time: 0.0044  lr: 9.00e-05  
[01/19 16:13:32] lb.utils.events INFO:  eta: 0:00:04  iteration: 957/1000  consumed samples: 15312  total_loss: 7.458  time: 0.1171(136.61)  data_time: 0.0047  lr: 9.00e-05  
[01/19 16:13:33] lb.utils.events INFO:  eta: 0:00:03  iteration: 958/1000  consumed samples: 15328  total_loss: 7.455  time: 0.1171(136.65)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:34] lb.utils.events INFO:  eta: 0:00:03  iteration: 959/1000  consumed samples: 15344  total_loss: 7.454  time: 0.1171(136.69)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:34] lb.utils.events INFO:  eta: 0:00:03  iteration: 960/1000  consumed samples: 15360  total_loss: 7.455  time: 0.1170(136.74)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:34] lb.utils.events INFO:  eta: 0:00:03  iteration: 961/1000  consumed samples: 15376  total_loss: 7.454  time: 0.1170(136.78)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:34] lb.utils.events INFO:  eta: 0:00:03  iteration: 962/1000  consumed samples: 15392  total_loss: 7.454  time: 0.1169(136.81)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:35] lb.utils.events INFO:  eta: 0:00:03  iteration: 963/1000  consumed samples: 15408  total_loss: 7.454  time: 0.1169(136.85)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:35] lb.utils.events INFO:  eta: 0:00:03  iteration: 964/1000  consumed samples: 15424  total_loss: 7.455  time: 0.1169(136.88)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:13:35] lb.utils.events INFO:  eta: 0:00:03  iteration: 965/1000  consumed samples: 15440  total_loss: 7.458  time: 0.1169(136.92)  data_time: 0.0048  lr: 9.00e-05  
[01/19 16:13:35] lb.utils.events INFO:  eta: 0:00:03  iteration: 966/1000  consumed samples: 15456  total_loss: 7.455  time: 0.1169(136.89)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:13:36] lb.utils.events INFO:  eta: 0:00:03  iteration: 967/1000  consumed samples: 15472  total_loss: 7.454  time: 0.1168(136.93)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:13:36] lb.utils.events INFO:  eta: 0:00:02  iteration: 968/1000  consumed samples: 15488  total_loss: 7.452  time: 0.1168(136.97)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:13:36] lb.utils.events INFO:  eta: 0:00:02  iteration: 969/1000  consumed samples: 15504  total_loss: 7.449  time: 0.1168(137.01)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:13:37] lb.utils.events INFO:  eta: 0:00:02  iteration: 970/1000  consumed samples: 15520  total_loss: 7.445  time: 0.1167(137.05)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:13:37] lb.utils.events INFO:  eta: 0:00:02  iteration: 971/1000  consumed samples: 15536  total_loss: 7.449  time: 0.1167(137.09)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:13:37] lb.utils.events INFO:  eta: 0:00:02  iteration: 972/1000  consumed samples: 15552  total_loss: 7.449  time: 0.1167(137.10)  data_time: 0.0051  lr: 9.00e-05  
[01/19 16:13:38] lb.utils.events INFO:  eta: 0:00:02  iteration: 973/1000  consumed samples: 15568  total_loss: 7.449  time: 0.1167(137.14)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:13:38] lb.utils.events INFO:  eta: 0:00:02  iteration: 974/1000  consumed samples: 15584  total_loss: 7.449  time: 0.1166(137.18)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:13:39] lb.utils.events INFO:  eta: 0:00:02  iteration: 975/1000  consumed samples: 15600  total_loss: 7.445  time: 0.1166(137.22)  data_time: 0.0050  lr: 9.00e-05  
[01/19 16:13:39] lb.utils.events INFO:  eta: 0:00:02  iteration: 976/1000  consumed samples: 15616  total_loss: 7.449  time: 0.1166(137.24)  data_time: 0.0049  lr: 9.00e-05  
[01/19 16:13:39] lb.utils.events INFO:  eta: 0:00:02  iteration: 977/1000  consumed samples: 15632  total_loss: 7.445  time: 0.1166(137.24)  data_time: 0.0063  lr: 9.00e-05  
[01/19 16:13:39] lb.utils.events INFO:  eta: 0:00:01  iteration: 978/1000  consumed samples: 15648  total_loss: 7.445  time: 0.1166(137.28)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:13:40] lb.utils.events INFO:  eta: 0:00:01  iteration: 979/1000  consumed samples: 15664  total_loss: 7.445  time: 0.1165(137.32)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:13:40] lb.utils.events INFO:  eta: 0:00:01  iteration: 980/1000  consumed samples: 15680  total_loss: 7.443  time: 0.1165(137.37)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:13:41] lb.utils.events INFO:  eta: 0:00:01  iteration: 981/1000  consumed samples: 15696  total_loss: 7.443  time: 0.1164(137.40)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:13:41] lb.utils.events INFO:  eta: 0:00:01  iteration: 982/1000  consumed samples: 15712  total_loss: 7.445  time: 0.1164(137.44)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:13:41] lb.utils.events INFO:  eta: 0:00:01  iteration: 983/1000  consumed samples: 15728  total_loss: 7.445  time: 0.1164(137.49)  data_time: 0.0061  lr: 9.00e-05  
[01/19 16:13:42] lb.utils.events INFO:  eta: 0:00:01  iteration: 984/1000  consumed samples: 15744  total_loss: 7.443  time: 0.1164(137.44)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:13:42] lb.utils.events INFO:  eta: 0:00:01  iteration: 985/1000  consumed samples: 15760  total_loss: 7.443  time: 0.1164(137.48)  data_time: 0.0060  lr: 9.00e-05  
[01/19 16:13:42] lb.utils.events INFO:  eta: 0:00:01  iteration: 986/1000  consumed samples: 15776  total_loss: 7.445  time: 0.1163(137.52)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:13:43] lb.utils.events INFO:  eta: 0:00:01  iteration: 987/1000  consumed samples: 15792  total_loss: 7.445  time: 0.1163(137.56)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:13:43] lb.utils.events INFO:  eta: 0:00:01  iteration: 988/1000  consumed samples: 15808  total_loss: 7.443  time: 0.1163(137.61)  data_time: 0.0058  lr: 9.00e-05  
[01/19 16:13:43] lb.utils.events INFO:  eta: 0:00:00  iteration: 989/1000  consumed samples: 15824  total_loss: 7.441  time: 0.1163(137.62)  data_time: 0.0059  lr: 9.00e-05  
[01/19 16:13:43] lb.utils.events INFO:  eta: 0:00:00  iteration: 990/1000  consumed samples: 15840  total_loss: 7.441  time: 0.1162(137.66)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:13:44] lb.utils.events INFO:  eta: 0:00:00  iteration: 991/1000  consumed samples: 15856  total_loss: 7.443  time: 0.1162(137.71)  data_time: 0.0055  lr: 9.00e-05  
[01/19 16:13:44] lb.utils.events INFO:  eta: 0:00:00  iteration: 992/1000  consumed samples: 15872  total_loss: 7.443  time: 0.1162(137.75)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:13:44] lb.utils.events INFO:  eta: 0:00:00  iteration: 993/1000  consumed samples: 15888  total_loss: 7.443  time: 0.1161(137.78)  data_time: 0.0056  lr: 9.00e-05  
[01/19 16:13:45] lb.utils.events INFO:  eta: 0:00:00  iteration: 994/1000  consumed samples: 15904  total_loss: 7.441  time: 0.1161(137.82)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:13:45] lb.utils.events INFO:  eta: 0:00:00  iteration: 995/1000  consumed samples: 15920  total_loss: 7.441  time: 0.1161(137.85)  data_time: 0.0054  lr: 9.00e-05  
[01/19 16:13:45] lb.utils.events INFO:  eta: 0:00:00  iteration: 996/1000  consumed samples: 15936  total_loss: 7.441  time: 0.1160(137.89)  data_time: 0.0055  lr: 9.00e-05  
[01/19 16:13:45] lb.utils.events INFO:  eta: 0:00:00  iteration: 997/1000  consumed samples: 15952  total_loss: 7.44  time: 0.1160(137.93)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:13:46] lb.utils.events INFO:  eta: 0:00:00  iteration: 998/1000  consumed samples: 15968  total_loss: 7.438  time: 0.1160(137.97)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:13:46] lb.utils.events INFO:  eta: 0:00:00  iteration: 999/1000  consumed samples: 15984  total_loss: 7.436  time: 0.1159(138.01)  data_time: 0.0037  lr: 9.00e-05  
[01/19 16:13:46] lb.utils.checkpoint INFO: Saving checkpoint to ./demo_output/test_config/model_final
[01/19 16:13:49] lb.trainer.hooks INFO: Overall training speed: 998 iterations in 0:01:55 (0.1159 s / it)
[01/19 16:13:49] lb.trainer.hooks INFO: Total training time: 0:04:40 (0:02:44 on hooks)
[01/20 10:37:37] libai INFO: Rank of current process: 0. World size: 1
[01/20 10:37:37] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=[], resume=False)
[01/20 10:37:37] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 10:37:37] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 10:37:37] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/20 10:37:37] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/20 10:37:37] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 10:37:40] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 10:37:40] libai INFO: Loadding megatron weight
[01/20 10:37:40] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/20 10:37:40] lb.data.build INFO: > building train, validation, and test datasets ...
[01/20 10:37:40] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/20 10:37:40] lb.data.build INFO:     train:      16000
[01/20 10:37:40] lb.data.build INFO:     validation: 160000
[01/20 10:37:40] lb.data.build INFO:     test:       160000
[01/20 10:37:40] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/20 10:37:40] lb.data.dataset_utils INFO:  > building dataset index ...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     reading sizes...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     reading pointers...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     reading document index...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/20 10:37:40] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/20 10:37:40] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.046299 seconds
[01/20 10:37:40] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/20 10:37:40] lb.data.dataset_utils INFO:     number of documents: 50000
[01/20 10:37:40] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/20 10:37:40] lb.data.dataset_utils INFO:  > dataset split:
[01/20 10:37:40] lb.data.dataset_utils INFO:     train:
[01/20 10:37:40] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/20 10:37:40] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/20 10:37:40] lb.data.dataset_utils INFO:     validation:
[01/20 10:37:40] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/20 10:37:40] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/20 10:37:40] lb.data.dataset_utils INFO:     test:
[01/20 10:37:40] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/20 10:37:40] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/20 10:37:41] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/20 10:37:41] lb.data.dataset_utils INFO:     loaded indexed file in 0.006 seconds
[01/20 10:37:41] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/20 10:37:41] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/20 10:37:41] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/20 10:37:41] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/20 10:37:41] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/20 10:37:41] lb.data.dataset_utils INFO:     loaded indexed file in 0.000 seconds
[01/20 10:37:41] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/20 10:37:41] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/20 10:37:41] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 10:37:46] lb.utils.events INFO:  iteration: 1/1000  consumed samples: 16  total_loss: 8.637  data_time: 0.0922  lr: 0.00e+00  
[01/20 10:37:47] lb.utils.events INFO:  eta: 0:03:42  iteration: 2/1000  consumed samples: 32  total_loss: 8.65  data_time: 0.0481  lr: 1.00e-05  
[01/20 10:37:47] lb.utils.events INFO:  eta: 0:03:28  iteration: 3/1000  consumed samples: 48  total_loss: 8.663  time: 0.2093(76.44)  data_time: 0.0331  lr: 2.00e-05  
[01/20 10:37:47] lb.utils.events INFO:  eta: 0:03:23  iteration: 4/1000  consumed samples: 64  total_loss: 8.65  time: 0.2044(78.28)  data_time: 0.0256  lr: 3.00e-05  
[01/20 10:37:47] lb.utils.events INFO:  eta: 0:03:23  iteration: 5/1000  consumed samples: 80  total_loss: 8.663  time: 0.2045(78.23)  data_time: 0.0234  lr: 4.00e-05  
[01/20 10:37:48] lb.utils.events INFO:  eta: 0:03:20  iteration: 6/1000  consumed samples: 96  total_loss: 8.675  time: 0.2032(78.74)  data_time: 0.0198  lr: 5.00e-05  
[01/20 10:37:48] lb.utils.events INFO:  eta: 0:03:23  iteration: 7/1000  consumed samples: 112  total_loss: 8.663  time: 0.2040(78.45)  data_time: 0.0173  lr: 6.00e-05  
[01/20 10:37:48] lb.utils.events INFO:  eta: 0:03:20  iteration: 8/1000  consumed samples: 128  total_loss: 8.65  time: 0.2033(78.69)  data_time: 0.0155  lr: 7.00e-05  
[01/20 10:37:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 9/1000  consumed samples: 144  total_loss: 8.637  time: 0.2037(78.56)  data_time: 0.0145  lr: 8.00e-05  
[01/20 10:37:48] lb.utils.events INFO:  eta: 0:03:22  iteration: 10/1000  consumed samples: 160  total_loss: 8.635  time: 0.2040(78.42)  data_time: 0.0133  lr: 9.00e-05  
[01/20 10:37:49] lb.utils.events INFO:  eta: 0:03:23  iteration: 11/1000  consumed samples: 176  total_loss: 8.633  time: 0.2043(78.30)  data_time: 0.0125  lr: 9.00e-05  
[01/20 10:37:49] lb.utils.events INFO:  eta: 0:03:22  iteration: 12/1000  consumed samples: 192  total_loss: 8.632  time: 0.2038(78.49)  data_time: 0.0117  lr: 9.00e-05  
[01/20 10:37:49] lb.utils.events INFO:  eta: 0:03:22  iteration: 13/1000  consumed samples: 208  total_loss: 8.631  time: 0.2042(78.36)  data_time: 0.0112  lr: 9.00e-05  
[01/20 10:37:49] lb.utils.events INFO:  eta: 0:03:22  iteration: 14/1000  consumed samples: 224  total_loss: 8.632  time: 0.2038(78.50)  data_time: 0.0107  lr: 9.00e-05  
[01/20 10:37:49] lb.utils.events INFO:  eta: 0:03:21  iteration: 15/1000  consumed samples: 240  total_loss: 8.631  time: 0.2034(78.64)  data_time: 0.0101  lr: 9.00e-05  
[01/20 10:37:50] lb.utils.events INFO:  eta: 0:03:21  iteration: 16/1000  consumed samples: 256  total_loss: 8.63  time: 0.2036(78.57)  data_time: 0.0099  lr: 9.00e-05  
[01/20 10:37:50] lb.utils.events INFO:  eta: 0:03:21  iteration: 17/1000  consumed samples: 272  total_loss: 8.629  time: 0.2038(78.50)  data_time: 0.0096  lr: 9.00e-05  
[01/20 10:37:50] lb.utils.events INFO:  eta: 0:03:21  iteration: 18/1000  consumed samples: 288  total_loss: 8.621  time: 0.2029(78.86)  data_time: 0.0092  lr: 9.00e-05  
[01/20 10:37:50] lb.utils.events INFO:  eta: 0:03:21  iteration: 19/1000  consumed samples: 304  total_loss: 8.613  time: 0.2031(78.76)  data_time: 0.0088  lr: 9.00e-05  
[01/20 10:37:50] lb.utils.events INFO:  eta: 0:03:20  iteration: 20/1000  consumed samples: 320  total_loss: 8.613  time: 0.2030(78.80)  data_time: 0.0085  lr: 9.00e-05  
[01/20 10:37:51] lb.utils.events INFO:  eta: 0:03:21  iteration: 21/1000  consumed samples: 336  total_loss: 8.613  time: 0.2032(78.74)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:37:51] lb.utils.events INFO:  eta: 0:03:20  iteration: 22/1000  consumed samples: 352  total_loss: 8.595  time: 0.2030(78.81)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:37:51] lb.utils.events INFO:  eta: 0:03:19  iteration: 23/1000  consumed samples: 368  total_loss: 8.578  time: 0.2028(78.89)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:37:51] lb.utils.events INFO:  eta: 0:03:17  iteration: 24/1000  consumed samples: 384  total_loss: 8.536  time: 0.2027(78.93)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:37:52] lb.utils.events INFO:  eta: 0:03:19  iteration: 25/1000  consumed samples: 400  total_loss: 8.494  time: 0.2029(78.86)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:37:52] lb.utils.events INFO:  eta: 0:03:19  iteration: 26/1000  consumed samples: 416  total_loss: 8.491  time: 0.2031(78.79)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:37:52] lb.utils.events INFO:  eta: 0:03:19  iteration: 27/1000  consumed samples: 432  total_loss: 8.489  time: 0.2032(78.74)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:37:52] lb.utils.events INFO:  eta: 0:03:19  iteration: 28/1000  consumed samples: 448  total_loss: 8.488  time: 0.2034(78.67)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:37:52] lb.utils.events INFO:  eta: 0:03:19  iteration: 29/1000  consumed samples: 464  total_loss: 8.487  time: 0.2035(78.63)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:37:53] lb.utils.events INFO:  eta: 0:03:19  iteration: 30/1000  consumed samples: 480  total_loss: 8.484  time: 0.2034(78.66)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:37:53] lb.utils.events INFO:  eta: 0:03:19  iteration: 31/1000  consumed samples: 496  total_loss: 8.48  time: 0.2035(78.62)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:37:53] lb.utils.events INFO:  eta: 0:03:19  iteration: 32/1000  consumed samples: 512  total_loss: 8.471  time: 0.2036(78.58)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:37:53] lb.utils.events INFO:  eta: 0:03:19  iteration: 33/1000  consumed samples: 528  total_loss: 8.463  time: 0.2037(78.54)  data_time: 0.0059  lr: 9.00e-05  
[01/20 10:37:53] lb.utils.events INFO:  eta: 0:03:19  iteration: 34/1000  consumed samples: 544  total_loss: 8.448  time: 0.2036(78.58)  data_time: 0.0059  lr: 9.00e-05  
[01/20 10:37:54] lb.utils.events INFO:  eta: 0:03:18  iteration: 35/1000  consumed samples: 560  total_loss: 8.432  time: 0.2037(78.53)  data_time: 0.0058  lr: 9.00e-05  
[01/20 10:37:54] lb.utils.events INFO:  eta: 0:03:18  iteration: 36/1000  consumed samples: 576  total_loss: 8.423  time: 0.2037(78.56)  data_time: 0.0056  lr: 9.00e-05  
[01/20 10:37:54] lb.utils.events INFO:  eta: 0:03:18  iteration: 37/1000  consumed samples: 592  total_loss: 8.413  time: 0.2038(78.52)  data_time: 0.0056  lr: 9.00e-05  
[01/20 10:37:54] lb.utils.events INFO:  eta: 0:03:18  iteration: 38/1000  consumed samples: 608  total_loss: 8.412  time: 0.2037(78.56)  data_time: 0.0057  lr: 9.00e-05  
[01/20 10:37:55] lb.utils.events INFO:  eta: 0:03:18  iteration: 39/1000  consumed samples: 624  total_loss: 8.41  time: 0.2038(78.52)  data_time: 0.0057  lr: 9.00e-05  
[01/20 10:37:55] lb.utils.events INFO:  eta: 0:03:17  iteration: 40/1000  consumed samples: 640  total_loss: 8.392  time: 0.2037(78.56)  data_time: 0.0058  lr: 9.00e-05  
[01/20 10:37:55] lb.utils.events INFO:  eta: 0:03:17  iteration: 41/1000  consumed samples: 656  total_loss: 8.374  time: 0.2038(78.52)  data_time: 0.0059  lr: 9.00e-05  
[01/20 10:37:55] lb.utils.events INFO:  eta: 0:03:17  iteration: 42/1000  consumed samples: 672  total_loss: 8.355  time: 0.2027(78.95)  data_time: 0.0058  lr: 9.00e-05  
[01/20 10:37:55] lb.utils.events INFO:  eta: 0:03:17  iteration: 43/1000  consumed samples: 688  total_loss: 8.335  time: 0.2028(78.90)  data_time: 0.0058  lr: 9.00e-05  
[01/20 10:37:56] lb.utils.events INFO:  eta: 0:03:17  iteration: 44/1000  consumed samples: 704  total_loss: 8.332  time: 0.2029(78.87)  data_time: 0.0058  lr: 9.00e-05  
[01/20 10:37:56] lb.utils.events INFO:  eta: 0:03:16  iteration: 45/1000  consumed samples: 720  total_loss: 8.328  time: 0.2028(78.88)  data_time: 0.0057  lr: 9.00e-05  
[01/20 10:37:56] lb.utils.events INFO:  eta: 0:03:16  iteration: 46/1000  consumed samples: 736  total_loss: 8.328  time: 0.2027(78.92)  data_time: 0.0057  lr: 9.00e-05  
[01/20 10:37:56] lb.utils.events INFO:  eta: 0:03:16  iteration: 47/1000  consumed samples: 752  total_loss: 8.327  time: 0.2029(78.87)  data_time: 0.0057  lr: 9.00e-05  
[01/20 10:37:56] lb.utils.events INFO:  eta: 0:03:16  iteration: 48/1000  consumed samples: 768  total_loss: 8.303  time: 0.2030(78.83)  data_time: 0.0057  lr: 9.00e-05  
[01/20 10:37:57] lb.utils.events INFO:  eta: 0:03:16  iteration: 49/1000  consumed samples: 784  total_loss: 8.278  time: 0.2026(78.98)  data_time: 0.0055  lr: 9.00e-05  
[01/20 10:37:57] lb.utils.events INFO:  eta: 0:03:15  iteration: 50/1000  consumed samples: 800  total_loss: 8.278  time: 0.2027(78.94)  data_time: 0.0055  lr: 9.00e-05  
[01/20 10:37:57] lb.utils.events INFO:  eta: 0:03:15  iteration: 51/1000  consumed samples: 816  total_loss: 8.277  time: 0.2027(78.95)  data_time: 0.0055  lr: 9.00e-05  
[01/20 10:37:57] lb.utils.events INFO:  eta: 0:03:15  iteration: 52/1000  consumed samples: 832  total_loss: 8.276  time: 0.2028(78.90)  data_time: 0.0055  lr: 9.00e-05  
[01/20 10:37:58] lb.utils.events INFO:  eta: 0:03:15  iteration: 53/1000  consumed samples: 848  total_loss: 8.274  time: 0.2029(78.86)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:37:58] lb.utils.events INFO:  eta: 0:03:15  iteration: 54/1000  consumed samples: 864  total_loss: 8.259  time: 0.2030(78.83)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:37:58] lb.utils.events INFO:  eta: 0:03:14  iteration: 55/1000  consumed samples: 880  total_loss: 8.244  time: 0.2029(78.84)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:37:58] lb.utils.events INFO:  eta: 0:03:14  iteration: 56/1000  consumed samples: 896  total_loss: 8.244  time: 0.2030(78.80)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:37:58] lb.utils.events INFO:  eta: 0:03:14  iteration: 57/1000  consumed samples: 912  total_loss: 8.243  time: 0.2031(78.77)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:37:59] lb.utils.events INFO:  eta: 0:03:14  iteration: 58/1000  consumed samples: 928  total_loss: 8.242  time: 0.2032(78.74)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:37:59] lb.utils.events INFO:  eta: 0:03:14  iteration: 59/1000  consumed samples: 944  total_loss: 8.24  time: 0.2032(78.75)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:37:59] lb.utils.events INFO:  eta: 0:03:13  iteration: 60/1000  consumed samples: 960  total_loss: 8.236  time: 0.2034(78.68)  data_time: 0.0106  lr: 9.00e-05  
[01/20 10:37:59] lb.utils.events INFO:  eta: 0:03:13  iteration: 61/1000  consumed samples: 976  total_loss: 8.231  time: 0.2033(78.69)  data_time: 0.0104  lr: 9.00e-05  
[01/20 10:37:59] lb.utils.events INFO:  eta: 0:03:13  iteration: 62/1000  consumed samples: 992  total_loss: 8.229  time: 0.2032(78.75)  data_time: 0.0105  lr: 9.00e-05  
[01/20 10:38:00] lb.utils.events INFO:  eta: 0:03:13  iteration: 63/1000  consumed samples: 1008  total_loss: 8.227  time: 0.2029(78.87)  data_time: 0.0105  lr: 9.00e-05  
[01/20 10:38:00] lb.utils.events INFO:  eta: 0:03:12  iteration: 64/1000  consumed samples: 1024  total_loss: 8.227  time: 0.2029(78.85)  data_time: 0.0106  lr: 9.00e-05  
[01/20 10:38:00] lb.utils.events INFO:  eta: 0:03:12  iteration: 65/1000  consumed samples: 1040  total_loss: 8.227  time: 0.2029(78.86)  data_time: 0.0106  lr: 9.00e-05  
[01/20 10:38:00] lb.utils.events INFO:  eta: 0:03:12  iteration: 66/1000  consumed samples: 1056  total_loss: 8.227  time: 0.2030(78.83)  data_time: 0.0106  lr: 9.00e-05  
[01/20 10:38:01] lb.utils.events INFO:  eta: 0:03:12  iteration: 67/1000  consumed samples: 1072  total_loss: 8.227  time: 0.2030(78.84)  data_time: 0.0105  lr: 9.00e-05  
[01/20 10:38:01] lb.utils.events INFO:  eta: 0:03:12  iteration: 68/1000  consumed samples: 1088  total_loss: 8.222  time: 0.2030(78.81)  data_time: 0.0105  lr: 9.00e-05  
[01/20 10:38:01] lb.utils.events INFO:  eta: 0:03:11  iteration: 69/1000  consumed samples: 1104  total_loss: 8.218  time: 0.2030(78.81)  data_time: 0.0104  lr: 9.00e-05  
[01/20 10:38:01] lb.utils.events INFO:  eta: 0:03:11  iteration: 70/1000  consumed samples: 1120  total_loss: 8.214  time: 0.2031(78.78)  data_time: 0.0104  lr: 9.00e-05  
[01/20 10:38:01] lb.utils.events INFO:  eta: 0:03:11  iteration: 71/1000  consumed samples: 1136  total_loss: 8.209  time: 0.2031(78.79)  data_time: 0.0104  lr: 9.00e-05  
[01/20 10:38:02] lb.utils.events INFO:  eta: 0:03:11  iteration: 72/1000  consumed samples: 1152  total_loss: 8.198  time: 0.2032(78.76)  data_time: 0.0104  lr: 9.00e-05  
[01/20 10:38:02] lb.utils.events INFO:  eta: 0:03:11  iteration: 73/1000  consumed samples: 1168  total_loss: 8.186  time: 0.2031(78.77)  data_time: 0.0102  lr: 9.00e-05  
[01/20 10:38:02] lb.utils.events INFO:  eta: 0:03:10  iteration: 74/1000  consumed samples: 1184  total_loss: 8.185  time: 0.2032(78.73)  data_time: 0.0101  lr: 9.00e-05  
[01/20 10:38:02] lb.utils.events INFO:  eta: 0:03:10  iteration: 75/1000  consumed samples: 1200  total_loss: 8.183  time: 0.2032(78.74)  data_time: 0.0101  lr: 9.00e-05  
[01/20 10:38:02] lb.utils.events INFO:  eta: 0:03:10  iteration: 76/1000  consumed samples: 1216  total_loss: 8.178  time: 0.2033(78.71)  data_time: 0.0102  lr: 9.00e-05  
[01/20 10:38:03] lb.utils.events INFO:  eta: 0:03:10  iteration: 77/1000  consumed samples: 1232  total_loss: 8.172  time: 0.2018(79.30)  data_time: 0.0100  lr: 9.00e-05  
[01/20 10:38:03] lb.utils.events INFO:  eta: 0:03:09  iteration: 78/1000  consumed samples: 1248  total_loss: 8.172  time: 0.2016(79.35)  data_time: 0.0101  lr: 9.00e-05  
[01/20 10:38:03] lb.utils.events INFO:  eta: 0:03:09  iteration: 79/1000  consumed samples: 1264  total_loss: 8.172  time: 0.2014(79.46)  data_time: 0.0102  lr: 9.00e-05  
[01/20 10:38:03] lb.utils.events INFO:  eta: 0:03:09  iteration: 80/1000  consumed samples: 1280  total_loss: 8.169  time: 0.2013(79.49)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:38:04] lb.utils.events INFO:  eta: 0:03:09  iteration: 81/1000  consumed samples: 1296  total_loss: 8.167  time: 0.2013(79.49)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:04] lb.utils.events INFO:  eta: 0:03:08  iteration: 82/1000  consumed samples: 1312  total_loss: 8.161  time: 0.2013(79.47)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:04] lb.utils.events INFO:  eta: 0:03:08  iteration: 83/1000  consumed samples: 1328  total_loss: 8.155  time: 0.2014(79.43)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:04] lb.utils.events INFO:  eta: 0:03:08  iteration: 84/1000  consumed samples: 1344  total_loss: 8.15  time: 0.2015(79.40)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:04] lb.utils.events INFO:  eta: 0:03:08  iteration: 85/1000  consumed samples: 1360  total_loss: 8.145  time: 0.2015(79.39)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:38:05] lb.utils.events INFO:  eta: 0:03:08  iteration: 86/1000  consumed samples: 1376  total_loss: 8.141  time: 0.2016(79.36)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:38:05] lb.utils.events INFO:  eta: 0:03:08  iteration: 87/1000  consumed samples: 1392  total_loss: 8.136  time: 0.2016(79.35)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:05] lb.utils.events INFO:  eta: 0:03:07  iteration: 88/1000  consumed samples: 1408  total_loss: 8.129  time: 0.2017(79.32)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:06] lb.utils.events INFO:  eta: 0:03:07  iteration: 89/1000  consumed samples: 1424  total_loss: 8.121  time: 0.2003(79.86)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:06] lb.utils.events INFO:  eta: 0:03:07  iteration: 90/1000  consumed samples: 1440  total_loss: 8.115  time: 0.2004(79.83)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:06] lb.utils.events INFO:  eta: 0:03:07  iteration: 91/1000  consumed samples: 1456  total_loss: 8.108  time: 0.2005(79.79)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:38:06] lb.utils.events INFO:  eta: 0:03:07  iteration: 92/1000  consumed samples: 1472  total_loss: 8.103  time: 0.2006(79.76)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:07] lb.utils.events INFO:  eta: 0:03:06  iteration: 93/1000  consumed samples: 1488  total_loss: 8.097  time: 0.2005(79.80)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:07] lb.utils.events INFO:  eta: 0:03:06  iteration: 94/1000  consumed samples: 1504  total_loss: 8.103  time: 0.2006(79.76)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:07] lb.utils.events INFO:  eta: 0:03:06  iteration: 95/1000  consumed samples: 1520  total_loss: 8.097  time: 0.2006(79.75)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:07] lb.utils.events INFO:  eta: 0:03:06  iteration: 96/1000  consumed samples: 1536  total_loss: 8.092  time: 0.2007(79.72)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:07] lb.utils.events INFO:  eta: 0:03:05  iteration: 97/1000  consumed samples: 1552  total_loss: 8.087  time: 0.2007(79.71)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:08] lb.utils.events INFO:  eta: 0:03:05  iteration: 98/1000  consumed samples: 1568  total_loss: 8.081  time: 0.2008(79.67)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:08] lb.utils.events INFO:  eta: 0:03:05  iteration: 99/1000  consumed samples: 1584  total_loss: 8.075  time: 0.2008(79.67)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:08] lb.utils.events INFO:  eta: 0:03:05  iteration: 100/1000  consumed samples: 1600  total_loss: 8.072  time: 0.2009(79.64)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:08] lb.utils.events INFO:  eta: 0:03:05  iteration: 101/1000  consumed samples: 1616  total_loss: 8.069  time: 0.2010(79.60)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:09] lb.utils.events INFO:  eta: 0:03:05  iteration: 102/1000  consumed samples: 1632  total_loss: 8.066  time: 0.2011(79.57)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:09] lb.utils.events INFO:  eta: 0:03:04  iteration: 103/1000  consumed samples: 1648  total_loss: 8.063  time: 0.2011(79.56)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:09] lb.utils.events INFO:  eta: 0:03:04  iteration: 104/1000  consumed samples: 1664  total_loss: 8.061  time: 0.2012(79.53)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:09] lb.utils.events INFO:  eta: 0:03:04  iteration: 105/1000  consumed samples: 1680  total_loss: 8.058  time: 0.2012(79.52)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:09] lb.utils.events INFO:  eta: 0:03:04  iteration: 106/1000  consumed samples: 1696  total_loss: 8.058  time: 0.2013(79.49)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:10] lb.utils.events INFO:  eta: 0:03:04  iteration: 107/1000  consumed samples: 1712  total_loss: 8.058  time: 0.2013(79.48)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:10] lb.utils.events INFO:  eta: 0:03:03  iteration: 108/1000  consumed samples: 1728  total_loss: 8.058  time: 0.2014(79.45)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:10] lb.utils.events INFO:  eta: 0:03:03  iteration: 109/1000  consumed samples: 1744  total_loss: 8.058  time: 0.2014(79.44)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:11] lb.utils.events INFO:  eta: 0:03:03  iteration: 110/1000  consumed samples: 1760  total_loss: 8.058  time: 0.2003(79.89)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:11] lb.utils.events INFO:  eta: 0:03:03  iteration: 111/1000  consumed samples: 1776  total_loss: 8.058  time: 0.1992(80.33)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:11] lb.utils.events INFO:  eta: 0:03:02  iteration: 112/1000  consumed samples: 1792  total_loss: 8.058  time: 0.1981(80.75)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:12] lb.utils.events INFO:  eta: 0:03:02  iteration: 113/1000  consumed samples: 1808  total_loss: 8.058  time: 0.1971(81.18)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:12] lb.utils.events INFO:  eta: 0:03:02  iteration: 114/1000  consumed samples: 1824  total_loss: 8.058  time: 0.1960(81.62)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:13] lb.utils.events INFO:  eta: 0:03:02  iteration: 115/1000  consumed samples: 1840  total_loss: 8.058  time: 0.1950(82.04)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:13] lb.utils.events INFO:  eta: 0:03:01  iteration: 116/1000  consumed samples: 1856  total_loss: 8.058  time: 0.1941(82.45)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:14] lb.utils.events INFO:  eta: 0:03:01  iteration: 117/1000  consumed samples: 1872  total_loss: 8.058  time: 0.1931(82.86)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:14] lb.utils.events INFO:  eta: 0:03:00  iteration: 118/1000  consumed samples: 1888  total_loss: 8.058  time: 0.1921(83.28)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:15] lb.utils.events INFO:  eta: 0:03:00  iteration: 119/1000  consumed samples: 1904  total_loss: 8.058  time: 0.1912(83.69)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:15] lb.utils.events INFO:  eta: 0:02:59  iteration: 120/1000  consumed samples: 1920  total_loss: 8.058  time: 0.1903(84.09)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:15] lb.utils.events INFO:  eta: 0:02:58  iteration: 121/1000  consumed samples: 1936  total_loss: 8.058  time: 0.1894(84.50)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:16] lb.utils.events INFO:  eta: 0:02:58  iteration: 122/1000  consumed samples: 1952  total_loss: 8.058  time: 0.1885(84.90)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:16] lb.utils.events INFO:  eta: 0:02:58  iteration: 123/1000  consumed samples: 1968  total_loss: 8.058  time: 0.1876(85.30)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:17] lb.utils.events INFO:  eta: 0:02:57  iteration: 124/1000  consumed samples: 1984  total_loss: 8.057  time: 0.1874(85.38)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:17] lb.utils.events INFO:  eta: 0:02:57  iteration: 125/1000  consumed samples: 2000  total_loss: 8.058  time: 0.1873(85.41)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:17] lb.utils.events INFO:  eta: 0:02:57  iteration: 126/1000  consumed samples: 2016  total_loss: 8.057  time: 0.1875(85.32)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:17] lb.utils.events INFO:  eta: 0:02:57  iteration: 127/1000  consumed samples: 2032  total_loss: 8.056  time: 0.1877(85.26)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:17] lb.utils.events INFO:  eta: 0:02:57  iteration: 128/1000  consumed samples: 2048  total_loss: 8.055  time: 0.1877(85.24)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:18] lb.utils.events INFO:  eta: 0:02:56  iteration: 129/1000  consumed samples: 2064  total_loss: 8.054  time: 0.1878(85.19)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:18] lb.utils.events INFO:  eta: 0:02:56  iteration: 130/1000  consumed samples: 2080  total_loss: 8.054  time: 0.1879(85.13)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:18] lb.utils.events INFO:  eta: 0:02:56  iteration: 131/1000  consumed samples: 2096  total_loss: 8.053  time: 0.1881(85.08)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:18] lb.utils.events INFO:  eta: 0:02:56  iteration: 132/1000  consumed samples: 2112  total_loss: 8.052  time: 0.1882(85.03)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:19] lb.utils.events INFO:  eta: 0:02:56  iteration: 133/1000  consumed samples: 2128  total_loss: 8.052  time: 0.1883(84.97)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:19] lb.utils.events INFO:  eta: 0:02:55  iteration: 134/1000  consumed samples: 2144  total_loss: 8.049  time: 0.1885(84.90)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:19] lb.utils.events INFO:  eta: 0:02:55  iteration: 135/1000  consumed samples: 2160  total_loss: 8.046  time: 0.1886(84.82)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:19] lb.utils.events INFO:  eta: 0:02:55  iteration: 136/1000  consumed samples: 2176  total_loss: 8.049  time: 0.1888(84.75)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:19] lb.utils.events INFO:  eta: 0:02:55  iteration: 137/1000  consumed samples: 2192  total_loss: 8.052  time: 0.1890(84.68)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:20] lb.utils.events INFO:  eta: 0:02:55  iteration: 138/1000  consumed samples: 2208  total_loss: 8.049  time: 0.1891(84.60)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:20] lb.utils.events INFO:  eta: 0:02:55  iteration: 139/1000  consumed samples: 2224  total_loss: 8.046  time: 0.1893(84.54)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:38:20] lb.utils.events INFO:  eta: 0:02:55  iteration: 140/1000  consumed samples: 2240  total_loss: 8.043  time: 0.1894(84.47)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:20] lb.utils.events INFO:  eta: 0:02:54  iteration: 141/1000  consumed samples: 2256  total_loss: 8.041  time: 0.1895(84.42)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:20] lb.utils.events INFO:  eta: 0:02:54  iteration: 142/1000  consumed samples: 2272  total_loss: 8.039  time: 0.1896(84.38)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:21] lb.utils.events INFO:  eta: 0:02:54  iteration: 143/1000  consumed samples: 2288  total_loss: 8.037  time: 0.1897(84.33)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:21] lb.utils.events INFO:  eta: 0:02:54  iteration: 144/1000  consumed samples: 2304  total_loss: 8.034  time: 0.1899(84.27)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:38:21] lb.utils.events INFO:  eta: 0:02:54  iteration: 145/1000  consumed samples: 2320  total_loss: 8.03  time: 0.1899(84.25)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:38:21] lb.utils.events INFO:  eta: 0:02:53  iteration: 146/1000  consumed samples: 2336  total_loss: 8.034  time: 0.1901(84.18)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:38:22] lb.utils.events INFO:  eta: 0:02:53  iteration: 147/1000  consumed samples: 2352  total_loss: 8.03  time: 0.1902(84.14)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:38:22] lb.utils.events INFO:  eta: 0:02:53  iteration: 148/1000  consumed samples: 2368  total_loss: 8.034  time: 0.1902(84.10)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:38:22] lb.utils.events INFO:  eta: 0:02:53  iteration: 149/1000  consumed samples: 2384  total_loss: 8.03  time: 0.1903(84.08)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:38:22] lb.utils.events INFO:  eta: 0:02:53  iteration: 150/1000  consumed samples: 2400  total_loss: 8.029  time: 0.1904(84.02)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:38:22] lb.utils.events INFO:  eta: 0:02:52  iteration: 151/1000  consumed samples: 2416  total_loss: 8.028  time: 0.1906(83.96)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:38:23] lb.utils.events INFO:  eta: 0:02:52  iteration: 152/1000  consumed samples: 2432  total_loss: 8.026  time: 0.1906(83.92)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:38:23] lb.utils.events INFO:  eta: 0:02:52  iteration: 153/1000  consumed samples: 2448  total_loss: 8.023  time: 0.1908(83.86)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:38:23] lb.utils.events INFO:  eta: 0:02:52  iteration: 154/1000  consumed samples: 2464  total_loss: 8.021  time: 0.1909(83.81)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:38:23] lb.utils.events INFO:  eta: 0:02:52  iteration: 155/1000  consumed samples: 2480  total_loss: 8.02  time: 0.1910(83.75)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:24] lb.utils.events INFO:  eta: 0:02:52  iteration: 156/1000  consumed samples: 2496  total_loss: 8.017  time: 0.1912(83.69)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:24] lb.utils.events INFO:  eta: 0:02:51  iteration: 157/1000  consumed samples: 2512  total_loss: 8.014  time: 0.1913(83.66)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:24] lb.utils.events INFO:  eta: 0:02:51  iteration: 158/1000  consumed samples: 2528  total_loss: 8.017  time: 0.1913(83.62)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:24] lb.utils.events INFO:  eta: 0:02:51  iteration: 159/1000  consumed samples: 2544  total_loss: 8.014  time: 0.1914(83.59)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:24] lb.utils.events INFO:  eta: 0:02:51  iteration: 160/1000  consumed samples: 2560  total_loss: 8.014  time: 0.1915(83.53)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:25] lb.utils.events INFO:  eta: 0:02:51  iteration: 161/1000  consumed samples: 2576  total_loss: 8.014  time: 0.1917(83.48)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:25] lb.utils.events INFO:  eta: 0:02:51  iteration: 162/1000  consumed samples: 2592  total_loss: 8.013  time: 0.1918(83.42)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:25] lb.utils.events INFO:  eta: 0:02:51  iteration: 163/1000  consumed samples: 2608  total_loss: 8.014  time: 0.1919(83.39)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:25] lb.utils.events INFO:  eta: 0:02:50  iteration: 164/1000  consumed samples: 2624  total_loss: 8.013  time: 0.1919(83.36)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:25] lb.utils.events INFO:  eta: 0:02:50  iteration: 165/1000  consumed samples: 2640  total_loss: 8.013  time: 0.1921(83.31)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:26] lb.utils.events INFO:  eta: 0:02:50  iteration: 166/1000  consumed samples: 2656  total_loss: 8.012  time: 0.1922(83.26)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:26] lb.utils.events INFO:  eta: 0:02:50  iteration: 167/1000  consumed samples: 2672  total_loss: 8.011  time: 0.1923(83.21)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:26] lb.utils.events INFO:  eta: 0:02:50  iteration: 168/1000  consumed samples: 2688  total_loss: 8.011  time: 0.1924(83.18)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:26] lb.utils.events INFO:  eta: 0:02:49  iteration: 169/1000  consumed samples: 2704  total_loss: 8.01  time: 0.1924(83.15)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:27] lb.utils.events INFO:  eta: 0:02:49  iteration: 170/1000  consumed samples: 2720  total_loss: 8.011  time: 0.1924(83.14)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:27] lb.utils.events INFO:  eta: 0:02:49  iteration: 171/1000  consumed samples: 2736  total_loss: 8.01  time: 0.1926(83.09)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:27] lb.utils.events INFO:  eta: 0:02:49  iteration: 172/1000  consumed samples: 2752  total_loss: 8.008  time: 0.1927(83.05)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:27] lb.utils.events INFO:  eta: 0:02:49  iteration: 173/1000  consumed samples: 2768  total_loss: 8.005  time: 0.1927(83.01)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:27] lb.utils.events INFO:  eta: 0:02:48  iteration: 174/1000  consumed samples: 2784  total_loss: 8.008  time: 0.1928(82.97)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:28] lb.utils.events INFO:  eta: 0:02:48  iteration: 175/1000  consumed samples: 2800  total_loss: 8.005  time: 0.1930(82.92)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:28] lb.utils.events INFO:  eta: 0:02:48  iteration: 176/1000  consumed samples: 2816  total_loss: 8.008  time: 0.1930(82.91)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:28] lb.utils.events INFO:  eta: 0:02:48  iteration: 177/1000  consumed samples: 2832  total_loss: 8.005  time: 0.1930(82.88)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:28] lb.utils.events INFO:  eta: 0:02:48  iteration: 178/1000  consumed samples: 2848  total_loss: 8.008  time: 0.1931(82.84)  data_time: 0.0047  lr: 9.00e-05  
[01/20 10:38:28] lb.utils.events INFO:  eta: 0:02:47  iteration: 179/1000  consumed samples: 2864  total_loss: 8.005  time: 0.1933(82.79)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:29] lb.utils.events INFO:  eta: 0:02:47  iteration: 180/1000  consumed samples: 2880  total_loss: 8.005  time: 0.1934(82.75)  data_time: 0.0045  lr: 9.00e-05  
[01/20 10:38:29] lb.utils.events INFO:  eta: 0:02:47  iteration: 181/1000  consumed samples: 2896  total_loss: 8.004  time: 0.1934(82.72)  data_time: 0.0045  lr: 9.00e-05  
[01/20 10:38:29] lb.utils.events INFO:  eta: 0:02:47  iteration: 182/1000  consumed samples: 2912  total_loss: 8.004  time: 0.1935(82.68)  data_time: 0.0045  lr: 9.00e-05  
[01/20 10:38:29] lb.utils.events INFO:  eta: 0:02:47  iteration: 183/1000  consumed samples: 2928  total_loss: 8.003  time: 0.1936(82.65)  data_time: 0.0045  lr: 9.00e-05  
[01/20 10:38:30] lb.utils.events INFO:  eta: 0:02:46  iteration: 184/1000  consumed samples: 2944  total_loss: 8  time: 0.1936(82.65)  data_time: 0.0044  lr: 9.00e-05  
[01/20 10:38:30] lb.utils.events INFO:  eta: 0:02:46  iteration: 185/1000  consumed samples: 2960  total_loss: 7.997  time: 0.1937(82.60)  data_time: 0.0045  lr: 9.00e-05  
[01/20 10:38:30] lb.utils.events INFO:  eta: 0:02:46  iteration: 186/1000  consumed samples: 2976  total_loss: 8  time: 0.1938(82.56)  data_time: 0.0045  lr: 9.00e-05  
[01/20 10:38:30] lb.utils.events INFO:  eta: 0:02:46  iteration: 187/1000  consumed samples: 2992  total_loss: 8.003  time: 0.1939(82.52)  data_time: 0.0046  lr: 9.00e-05  
[01/20 10:38:30] lb.utils.events INFO:  eta: 0:02:46  iteration: 188/1000  consumed samples: 3008  total_loss: 8.004  time: 0.1939(82.50)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:31] lb.utils.events INFO:  eta: 0:02:46  iteration: 189/1000  consumed samples: 3024  total_loss: 8.003  time: 0.1940(82.46)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:31] lb.utils.events INFO:  eta: 0:02:45  iteration: 190/1000  consumed samples: 3040  total_loss: 8  time: 0.1941(82.44)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:31] lb.utils.events INFO:  eta: 0:02:45  iteration: 191/1000  consumed samples: 3056  total_loss: 7.997  time: 0.1942(82.39)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:31] lb.utils.events INFO:  eta: 0:02:45  iteration: 192/1000  consumed samples: 3072  total_loss: 7.997  time: 0.1943(82.35)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:32] lb.utils.events INFO:  eta: 0:02:45  iteration: 193/1000  consumed samples: 3088  total_loss: 7.997  time: 0.1944(82.31)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:32] lb.utils.events INFO:  eta: 0:02:45  iteration: 194/1000  consumed samples: 3104  total_loss: 7.997  time: 0.1945(82.27)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:32] lb.utils.events INFO:  eta: 0:02:45  iteration: 195/1000  consumed samples: 3120  total_loss: 7.997  time: 0.1946(82.24)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:32] lb.utils.events INFO:  eta: 0:02:45  iteration: 196/1000  consumed samples: 3136  total_loss: 7.996  time: 0.1946(82.21)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:32] lb.utils.events INFO:  eta: 0:02:44  iteration: 197/1000  consumed samples: 3152  total_loss: 7.996  time: 0.1947(82.19)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:33] lb.utils.events INFO:  eta: 0:02:44  iteration: 198/1000  consumed samples: 3168  total_loss: 7.996  time: 0.1945(82.25)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:33] lb.utils.events INFO:  eta: 0:02:44  iteration: 199/1000  consumed samples: 3184  total_loss: 7.996  time: 0.1946(82.23)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:33] lb.utils.events INFO:  eta: 0:02:44  iteration: 200/1000  consumed samples: 3200  total_loss: 7.996  time: 0.1947(82.20)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:33] lb.utils.events INFO:  eta: 0:02:44  iteration: 201/1000  consumed samples: 3216  total_loss: 7.996  time: 0.1947(82.17)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:34] lb.utils.events INFO:  eta: 0:02:43  iteration: 202/1000  consumed samples: 3232  total_loss: 7.994  time: 0.1948(82.15)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:34] lb.utils.events INFO:  eta: 0:02:43  iteration: 203/1000  consumed samples: 3248  total_loss: 7.994  time: 0.1948(82.13)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:34] lb.utils.events INFO:  eta: 0:02:43  iteration: 204/1000  consumed samples: 3264  total_loss: 7.991  time: 0.1949(82.11)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:34] lb.utils.events INFO:  eta: 0:02:43  iteration: 205/1000  consumed samples: 3280  total_loss: 7.99  time: 0.1949(82.08)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:34] lb.utils.events INFO:  eta: 0:02:42  iteration: 206/1000  consumed samples: 3296  total_loss: 7.99  time: 0.1950(82.06)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:35] lb.utils.events INFO:  eta: 0:02:42  iteration: 207/1000  consumed samples: 3312  total_loss: 7.988  time: 0.1950(82.04)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:35] lb.utils.events INFO:  eta: 0:02:42  iteration: 208/1000  consumed samples: 3328  total_loss: 7.988  time: 0.1951(82.02)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:35] lb.utils.events INFO:  eta: 0:02:42  iteration: 209/1000  consumed samples: 3344  total_loss: 7.987  time: 0.1951(82.00)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:35] lb.utils.events INFO:  eta: 0:02:42  iteration: 210/1000  consumed samples: 3360  total_loss: 7.986  time: 0.1952(81.96)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:35] lb.utils.events INFO:  eta: 0:02:42  iteration: 211/1000  consumed samples: 3376  total_loss: 7.983  time: 0.1953(81.94)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:36] lb.utils.events INFO:  eta: 0:02:41  iteration: 212/1000  consumed samples: 3392  total_loss: 7.98  time: 0.1954(81.90)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:36] lb.utils.events INFO:  eta: 0:02:41  iteration: 213/1000  consumed samples: 3408  total_loss: 7.979  time: 0.1954(81.90)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:36] lb.utils.events INFO:  eta: 0:02:41  iteration: 214/1000  consumed samples: 3424  total_loss: 7.979  time: 0.1954(81.88)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:36] lb.utils.events INFO:  eta: 0:02:41  iteration: 215/1000  consumed samples: 3440  total_loss: 7.976  time: 0.1955(81.84)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:37] lb.utils.events INFO:  eta: 0:02:41  iteration: 216/1000  consumed samples: 3456  total_loss: 7.974  time: 0.1955(81.82)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:37] lb.utils.events INFO:  eta: 0:02:40  iteration: 217/1000  consumed samples: 3472  total_loss: 7.974  time: 0.1956(81.79)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:37] lb.utils.events INFO:  eta: 0:02:40  iteration: 218/1000  consumed samples: 3488  total_loss: 7.973  time: 0.1957(81.77)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:37] lb.utils.events INFO:  eta: 0:02:40  iteration: 219/1000  consumed samples: 3504  total_loss: 7.97  time: 0.1957(81.74)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:37] lb.utils.events INFO:  eta: 0:02:40  iteration: 220/1000  consumed samples: 3520  total_loss: 7.969  time: 0.1958(81.72)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:38] lb.utils.events INFO:  eta: 0:02:40  iteration: 221/1000  consumed samples: 3536  total_loss: 7.969  time: 0.1959(81.69)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:38] lb.utils.events INFO:  eta: 0:02:40  iteration: 222/1000  consumed samples: 3552  total_loss: 7.969  time: 0.1959(81.67)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:38:38] lb.utils.events INFO:  eta: 0:02:39  iteration: 223/1000  consumed samples: 3568  total_loss: 7.968  time: 0.1960(81.64)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:38] lb.utils.events INFO:  eta: 0:02:39  iteration: 224/1000  consumed samples: 3584  total_loss: 7.966  time: 0.1960(81.62)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:39] lb.utils.events INFO:  eta: 0:02:39  iteration: 225/1000  consumed samples: 3600  total_loss: 7.966  time: 0.1961(81.58)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:38:39] lb.utils.events INFO:  eta: 0:02:39  iteration: 226/1000  consumed samples: 3616  total_loss: 7.965  time: 0.1962(81.56)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:39] lb.utils.events INFO:  eta: 0:02:39  iteration: 227/1000  consumed samples: 3632  total_loss: 7.962  time: 0.1962(81.53)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:39] lb.utils.events INFO:  eta: 0:02:38  iteration: 228/1000  consumed samples: 3648  total_loss: 7.961  time: 0.1963(81.51)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:39] lb.utils.events INFO:  eta: 0:02:38  iteration: 229/1000  consumed samples: 3664  total_loss: 7.961  time: 0.1963(81.50)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:40] lb.utils.events INFO:  eta: 0:02:38  iteration: 230/1000  consumed samples: 3680  total_loss: 7.96  time: 0.1964(81.48)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:40] lb.utils.events INFO:  eta: 0:02:38  iteration: 231/1000  consumed samples: 3696  total_loss: 7.959  time: 0.1964(81.45)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:40] lb.utils.events INFO:  eta: 0:02:38  iteration: 232/1000  consumed samples: 3712  total_loss: 7.958  time: 0.1965(81.43)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:38:40] lb.utils.events INFO:  eta: 0:02:37  iteration: 233/1000  consumed samples: 3728  total_loss: 7.958  time: 0.1966(81.40)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:41] lb.utils.events INFO:  eta: 0:02:37  iteration: 234/1000  consumed samples: 3744  total_loss: 7.958  time: 0.1966(81.38)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:41] lb.utils.events INFO:  eta: 0:02:37  iteration: 235/1000  consumed samples: 3760  total_loss: 7.956  time: 0.1967(81.35)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:41] lb.utils.events INFO:  eta: 0:02:37  iteration: 236/1000  consumed samples: 3776  total_loss: 7.955  time: 0.1967(81.33)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:41] lb.utils.events INFO:  eta: 0:02:37  iteration: 237/1000  consumed samples: 3792  total_loss: 7.955  time: 0.1968(81.32)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:41] lb.utils.events INFO:  eta: 0:02:36  iteration: 238/1000  consumed samples: 3808  total_loss: 7.955  time: 0.1968(81.29)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:42] lb.utils.events INFO:  eta: 0:02:36  iteration: 239/1000  consumed samples: 3824  total_loss: 7.955  time: 0.1969(81.27)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:42] lb.utils.events INFO:  eta: 0:02:36  iteration: 240/1000  consumed samples: 3840  total_loss: 7.955  time: 0.1969(81.24)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:42] lb.utils.events INFO:  eta: 0:02:36  iteration: 241/1000  consumed samples: 3856  total_loss: 7.954  time: 0.1970(81.23)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:42] lb.utils.events INFO:  eta: 0:02:36  iteration: 242/1000  consumed samples: 3872  total_loss: 7.952  time: 0.1970(81.20)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:42] lb.utils.events INFO:  eta: 0:02:35  iteration: 243/1000  consumed samples: 3888  total_loss: 7.952  time: 0.1971(81.18)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:43] lb.utils.events INFO:  eta: 0:02:35  iteration: 244/1000  consumed samples: 3904  total_loss: 7.95  time: 0.1972(81.16)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:43] lb.utils.events INFO:  eta: 0:02:35  iteration: 245/1000  consumed samples: 3920  total_loss: 7.952  time: 0.1972(81.14)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:43] lb.utils.events INFO:  eta: 0:02:35  iteration: 246/1000  consumed samples: 3936  total_loss: 7.952  time: 0.1973(81.11)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:43] lb.utils.events INFO:  eta: 0:02:35  iteration: 247/1000  consumed samples: 3952  total_loss: 7.954  time: 0.1973(81.09)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:44] lb.utils.events INFO:  eta: 0:02:34  iteration: 248/1000  consumed samples: 3968  total_loss: 7.952  time: 0.1974(81.06)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:44] lb.utils.events INFO:  eta: 0:02:34  iteration: 249/1000  consumed samples: 3984  total_loss: 7.95  time: 0.1974(81.05)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:44] lb.utils.events INFO:  eta: 0:02:34  iteration: 250/1000  consumed samples: 4000  total_loss: 7.949  time: 0.1975(81.02)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:44] lb.utils.events INFO:  eta: 0:02:34  iteration: 251/1000  consumed samples: 4016  total_loss: 7.948  time: 0.1975(81.01)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:44] lb.utils.events INFO:  eta: 0:02:34  iteration: 252/1000  consumed samples: 4032  total_loss: 7.948  time: 0.1976(80.98)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:45] lb.utils.events INFO:  eta: 0:02:33  iteration: 253/1000  consumed samples: 4048  total_loss: 7.945  time: 0.1976(80.96)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:45] lb.utils.events INFO:  eta: 0:02:33  iteration: 254/1000  consumed samples: 4064  total_loss: 7.943  time: 0.1977(80.93)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:45] lb.utils.events INFO:  eta: 0:02:33  iteration: 255/1000  consumed samples: 4080  total_loss: 7.942  time: 0.1977(80.92)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:45] lb.utils.events INFO:  eta: 0:02:33  iteration: 256/1000  consumed samples: 4096  total_loss: 7.942  time: 0.1978(80.89)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:46] lb.utils.events INFO:  eta: 0:02:33  iteration: 257/1000  consumed samples: 4112  total_loss: 7.942  time: 0.1978(80.87)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:46] lb.utils.events INFO:  eta: 0:02:33  iteration: 258/1000  consumed samples: 4128  total_loss: 7.942  time: 0.1979(80.84)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:46] lb.utils.events INFO:  eta: 0:02:32  iteration: 259/1000  consumed samples: 4144  total_loss: 7.942  time: 0.1980(80.82)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:46] lb.utils.events INFO:  eta: 0:02:32  iteration: 260/1000  consumed samples: 4160  total_loss: 7.942  time: 0.1980(80.81)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:46] lb.utils.events INFO:  eta: 0:02:32  iteration: 261/1000  consumed samples: 4176  total_loss: 7.942  time: 0.1980(80.79)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:47] lb.utils.events INFO:  eta: 0:02:32  iteration: 262/1000  consumed samples: 4192  total_loss: 7.941  time: 0.1981(80.78)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:47] lb.utils.events INFO:  eta: 0:02:31  iteration: 263/1000  consumed samples: 4208  total_loss: 7.941  time: 0.1981(80.76)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:47] lb.utils.events INFO:  eta: 0:02:31  iteration: 264/1000  consumed samples: 4224  total_loss: 7.938  time: 0.1982(80.74)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:47] lb.utils.events INFO:  eta: 0:02:31  iteration: 265/1000  consumed samples: 4240  total_loss: 7.933  time: 0.1982(80.72)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:48] lb.utils.events INFO:  eta: 0:02:31  iteration: 266/1000  consumed samples: 4256  total_loss: 7.933  time: 0.1983(80.69)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:48] lb.utils.events INFO:  eta: 0:02:31  iteration: 267/1000  consumed samples: 4272  total_loss: 7.933  time: 0.1983(80.67)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:48] lb.utils.events INFO:  eta: 0:02:31  iteration: 268/1000  consumed samples: 4288  total_loss: 7.933  time: 0.1984(80.64)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:48] lb.utils.events INFO:  eta: 0:02:30  iteration: 269/1000  consumed samples: 4304  total_loss: 7.93  time: 0.1984(80.63)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:48] lb.utils.events INFO:  eta: 0:02:30  iteration: 270/1000  consumed samples: 4320  total_loss: 7.929  time: 0.1985(80.60)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:49] lb.utils.events INFO:  eta: 0:02:30  iteration: 271/1000  consumed samples: 4336  total_loss: 7.929  time: 0.1985(80.59)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:49] lb.utils.events INFO:  eta: 0:02:30  iteration: 272/1000  consumed samples: 4352  total_loss: 7.927  time: 0.1986(80.56)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:49] lb.utils.events INFO:  eta: 0:02:30  iteration: 273/1000  consumed samples: 4368  total_loss: 7.924  time: 0.1986(80.55)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:49] lb.utils.events INFO:  eta: 0:02:29  iteration: 274/1000  consumed samples: 4384  total_loss: 7.923  time: 0.1987(80.52)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:50] lb.utils.events INFO:  eta: 0:02:29  iteration: 275/1000  consumed samples: 4400  total_loss: 7.923  time: 0.1987(80.51)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:50] lb.utils.events INFO:  eta: 0:02:29  iteration: 276/1000  consumed samples: 4416  total_loss: 7.923  time: 0.1988(80.48)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:50] lb.utils.events INFO:  eta: 0:02:29  iteration: 277/1000  consumed samples: 4432  total_loss: 7.923  time: 0.1988(80.47)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:50] lb.utils.events INFO:  eta: 0:02:29  iteration: 278/1000  consumed samples: 4448  total_loss: 7.922  time: 0.1989(80.45)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:50] lb.utils.events INFO:  eta: 0:02:28  iteration: 279/1000  consumed samples: 4464  total_loss: 7.921  time: 0.1989(80.44)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:51] lb.utils.events INFO:  eta: 0:02:28  iteration: 280/1000  consumed samples: 4480  total_loss: 7.921  time: 0.1990(80.41)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:51] lb.utils.events INFO:  eta: 0:02:28  iteration: 281/1000  consumed samples: 4496  total_loss: 7.921  time: 0.1990(80.40)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:51] lb.utils.events INFO:  eta: 0:02:28  iteration: 282/1000  consumed samples: 4512  total_loss: 7.921  time: 0.1991(80.38)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:51] lb.utils.events INFO:  eta: 0:02:28  iteration: 283/1000  consumed samples: 4528  total_loss: 7.921  time: 0.1991(80.37)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:52] lb.utils.events INFO:  eta: 0:02:27  iteration: 284/1000  consumed samples: 4544  total_loss: 7.921  time: 0.1991(80.35)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:52] lb.utils.events INFO:  eta: 0:02:27  iteration: 285/1000  consumed samples: 4560  total_loss: 7.921  time: 0.1992(80.34)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:52] lb.utils.events INFO:  eta: 0:02:27  iteration: 286/1000  consumed samples: 4576  total_loss: 7.92  time: 0.1992(80.31)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:52] lb.utils.events INFO:  eta: 0:02:27  iteration: 287/1000  consumed samples: 4592  total_loss: 7.92  time: 0.1992(80.30)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:52] lb.utils.events INFO:  eta: 0:02:27  iteration: 288/1000  consumed samples: 4608  total_loss: 7.919  time: 0.1993(80.28)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:53] lb.utils.events INFO:  eta: 0:02:26  iteration: 289/1000  consumed samples: 4624  total_loss: 7.918  time: 0.1993(80.27)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:53] lb.utils.events INFO:  eta: 0:02:26  iteration: 290/1000  consumed samples: 4640  total_loss: 7.919  time: 0.1994(80.25)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:53] lb.utils.events INFO:  eta: 0:02:26  iteration: 291/1000  consumed samples: 4656  total_loss: 7.918  time: 0.1994(80.24)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:53] lb.utils.events INFO:  eta: 0:02:26  iteration: 292/1000  consumed samples: 4672  total_loss: 7.917  time: 0.1995(80.22)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:38:53] lb.utils.events INFO:  eta: 0:02:26  iteration: 293/1000  consumed samples: 4688  total_loss: 7.915  time: 0.1995(80.21)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:54] lb.utils.events INFO:  eta: 0:02:25  iteration: 294/1000  consumed samples: 4704  total_loss: 7.914  time: 0.1995(80.19)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:54] lb.utils.events INFO:  eta: 0:02:25  iteration: 295/1000  consumed samples: 4720  total_loss: 7.914  time: 0.1996(80.18)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:54] lb.utils.events INFO:  eta: 0:02:25  iteration: 296/1000  consumed samples: 4736  total_loss: 7.912  time: 0.1996(80.16)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:38:54] lb.utils.events INFO:  eta: 0:02:25  iteration: 297/1000  consumed samples: 4752  total_loss: 7.911  time: 0.1996(80.15)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:55] lb.utils.events INFO:  eta: 0:02:25  iteration: 298/1000  consumed samples: 4768  total_loss: 7.911  time: 0.1997(80.12)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:55] lb.utils.events INFO:  eta: 0:02:24  iteration: 299/1000  consumed samples: 4784  total_loss: 7.91  time: 0.1997(80.11)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:55] lb.utils.events INFO:  eta: 0:02:24  iteration: 300/1000  consumed samples: 4800  total_loss: 7.91  time: 0.1998(80.09)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:55] lb.utils.events INFO:  eta: 0:02:24  iteration: 301/1000  consumed samples: 4816  total_loss: 7.91  time: 0.1998(80.09)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:55] lb.utils.events INFO:  eta: 0:02:24  iteration: 302/1000  consumed samples: 4832  total_loss: 7.909  time: 0.1998(80.07)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:56] lb.utils.events INFO:  eta: 0:02:24  iteration: 303/1000  consumed samples: 4848  total_loss: 7.908  time: 0.1999(80.05)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:56] lb.utils.events INFO:  eta: 0:02:23  iteration: 304/1000  consumed samples: 4864  total_loss: 7.907  time: 0.1999(80.03)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:56] lb.utils.events INFO:  eta: 0:02:23  iteration: 305/1000  consumed samples: 4880  total_loss: 7.908  time: 0.2000(80.02)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:56] lb.utils.events INFO:  eta: 0:02:23  iteration: 306/1000  consumed samples: 4896  total_loss: 7.908  time: 0.2000(80.00)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:57] lb.utils.events INFO:  eta: 0:02:23  iteration: 307/1000  consumed samples: 4912  total_loss: 7.908  time: 0.2000(79.99)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:57] lb.utils.events INFO:  eta: 0:02:23  iteration: 308/1000  consumed samples: 4928  total_loss: 7.908  time: 0.2001(79.97)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:57] lb.utils.events INFO:  eta: 0:02:22  iteration: 309/1000  consumed samples: 4944  total_loss: 7.908  time: 0.2001(79.96)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:57] lb.utils.events INFO:  eta: 0:02:22  iteration: 310/1000  consumed samples: 4960  total_loss: 7.908  time: 0.2002(79.94)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:57] lb.utils.events INFO:  eta: 0:02:22  iteration: 311/1000  consumed samples: 4976  total_loss: 7.907  time: 0.2002(79.92)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:58] lb.utils.events INFO:  eta: 0:02:22  iteration: 312/1000  consumed samples: 4992  total_loss: 7.905  time: 0.2002(79.90)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:38:58] lb.utils.events INFO:  eta: 0:02:22  iteration: 313/1000  consumed samples: 5008  total_loss: 7.907  time: 0.2003(79.89)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:58] lb.utils.events INFO:  eta: 0:02:21  iteration: 314/1000  consumed samples: 5024  total_loss: 7.905  time: 0.2003(79.87)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:58] lb.utils.events INFO:  eta: 0:02:21  iteration: 315/1000  consumed samples: 5040  total_loss: 7.901  time: 0.2004(79.86)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:59] lb.utils.events INFO:  eta: 0:02:21  iteration: 316/1000  consumed samples: 5056  total_loss: 7.899  time: 0.2004(79.84)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:59] lb.utils.events INFO:  eta: 0:02:21  iteration: 317/1000  consumed samples: 5072  total_loss: 7.898  time: 0.2004(79.83)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:59] lb.utils.events INFO:  eta: 0:02:21  iteration: 318/1000  consumed samples: 5088  total_loss: 7.898  time: 0.2005(79.81)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:59] lb.utils.events INFO:  eta: 0:02:21  iteration: 319/1000  consumed samples: 5104  total_loss: 7.898  time: 0.2005(79.80)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:38:59] lb.utils.events INFO:  eta: 0:02:20  iteration: 320/1000  consumed samples: 5120  total_loss: 7.897  time: 0.2006(79.78)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:00] lb.utils.events INFO:  eta: 0:02:20  iteration: 321/1000  consumed samples: 5136  total_loss: 7.896  time: 0.2005(79.81)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:00] lb.utils.events INFO:  eta: 0:02:20  iteration: 322/1000  consumed samples: 5152  total_loss: 7.896  time: 0.2005(79.79)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:00] lb.utils.events INFO:  eta: 0:02:20  iteration: 323/1000  consumed samples: 5168  total_loss: 7.895  time: 0.2006(79.78)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:00] lb.utils.events INFO:  eta: 0:02:20  iteration: 324/1000  consumed samples: 5184  total_loss: 7.895  time: 0.2006(79.76)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:01] lb.utils.events INFO:  eta: 0:02:19  iteration: 325/1000  consumed samples: 5200  total_loss: 7.893  time: 0.2006(79.75)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:01] lb.utils.events INFO:  eta: 0:02:19  iteration: 326/1000  consumed samples: 5216  total_loss: 7.891  time: 0.2007(79.73)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:01] lb.utils.events INFO:  eta: 0:02:19  iteration: 327/1000  consumed samples: 5232  total_loss: 7.888  time: 0.2007(79.73)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:01] lb.utils.events INFO:  eta: 0:02:19  iteration: 328/1000  consumed samples: 5248  total_loss: 7.888  time: 0.2007(79.71)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:01] lb.utils.events INFO:  eta: 0:02:19  iteration: 329/1000  consumed samples: 5264  total_loss: 7.888  time: 0.2008(79.70)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:02] lb.utils.events INFO:  eta: 0:02:18  iteration: 330/1000  consumed samples: 5280  total_loss: 7.884  time: 0.2008(79.69)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:02] lb.utils.events INFO:  eta: 0:02:18  iteration: 331/1000  consumed samples: 5296  total_loss: 7.881  time: 0.2008(79.68)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:02] lb.utils.events INFO:  eta: 0:02:18  iteration: 332/1000  consumed samples: 5312  total_loss: 7.878  time: 0.2009(79.66)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:02] lb.utils.events INFO:  eta: 0:02:18  iteration: 333/1000  consumed samples: 5328  total_loss: 7.878  time: 0.2009(79.65)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:03] lb.utils.events INFO:  eta: 0:02:18  iteration: 334/1000  consumed samples: 5344  total_loss: 7.878  time: 0.2009(79.64)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:03] lb.utils.events INFO:  eta: 0:02:17  iteration: 335/1000  consumed samples: 5360  total_loss: 7.875  time: 0.2009(79.64)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:03] lb.utils.events INFO:  eta: 0:02:17  iteration: 336/1000  consumed samples: 5376  total_loss: 7.873  time: 0.2009(79.63)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:03] lb.utils.events INFO:  eta: 0:02:17  iteration: 337/1000  consumed samples: 5392  total_loss: 7.873  time: 0.2010(79.62)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:03] lb.utils.events INFO:  eta: 0:02:17  iteration: 338/1000  consumed samples: 5408  total_loss: 7.873  time: 0.2010(79.60)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:04] lb.utils.events INFO:  eta: 0:02:17  iteration: 339/1000  consumed samples: 5424  total_loss: 7.873  time: 0.2010(79.61)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:04] lb.utils.events INFO:  eta: 0:02:16  iteration: 340/1000  consumed samples: 5440  total_loss: 7.873  time: 0.2010(79.59)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:04] lb.utils.events INFO:  eta: 0:02:16  iteration: 341/1000  consumed samples: 5456  total_loss: 7.873  time: 0.2011(79.58)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:04] lb.utils.events INFO:  eta: 0:02:16  iteration: 342/1000  consumed samples: 5472  total_loss: 7.873  time: 0.2011(79.56)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:05] lb.utils.events INFO:  eta: 0:02:16  iteration: 343/1000  consumed samples: 5488  total_loss: 7.873  time: 0.2011(79.55)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:05] lb.utils.events INFO:  eta: 0:02:16  iteration: 344/1000  consumed samples: 5504  total_loss: 7.872  time: 0.2012(79.54)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:05] lb.utils.events INFO:  eta: 0:02:15  iteration: 345/1000  consumed samples: 5520  total_loss: 7.872  time: 0.2012(79.53)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:05] lb.utils.events INFO:  eta: 0:02:15  iteration: 346/1000  consumed samples: 5536  total_loss: 7.871  time: 0.2012(79.51)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:05] lb.utils.events INFO:  eta: 0:02:15  iteration: 347/1000  consumed samples: 5552  total_loss: 7.869  time: 0.2013(79.50)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:06] lb.utils.events INFO:  eta: 0:02:15  iteration: 348/1000  consumed samples: 5568  total_loss: 7.868  time: 0.2013(79.48)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:06] lb.utils.events INFO:  eta: 0:02:15  iteration: 349/1000  consumed samples: 5584  total_loss: 7.868  time: 0.2013(79.48)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:06] lb.utils.events INFO:  eta: 0:02:14  iteration: 350/1000  consumed samples: 5600  total_loss: 7.868  time: 0.2014(79.46)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:06] lb.utils.events INFO:  eta: 0:02:14  iteration: 351/1000  consumed samples: 5616  total_loss: 7.866  time: 0.2014(79.45)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:07] lb.utils.events INFO:  eta: 0:02:14  iteration: 352/1000  consumed samples: 5632  total_loss: 7.866  time: 0.2014(79.43)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:07] lb.utils.events INFO:  eta: 0:02:14  iteration: 353/1000  consumed samples: 5648  total_loss: 7.868  time: 0.2014(79.43)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:07] lb.utils.events INFO:  eta: 0:02:14  iteration: 354/1000  consumed samples: 5664  total_loss: 7.868  time: 0.2015(79.42)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:07] lb.utils.events INFO:  eta: 0:02:13  iteration: 355/1000  consumed samples: 5680  total_loss: 7.866  time: 0.2015(79.41)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:07] lb.utils.events INFO:  eta: 0:02:13  iteration: 356/1000  consumed samples: 5696  total_loss: 7.864  time: 0.2015(79.39)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:08] lb.utils.events INFO:  eta: 0:02:13  iteration: 357/1000  consumed samples: 5712  total_loss: 7.866  time: 0.2015(79.39)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:08] lb.utils.events INFO:  eta: 0:02:13  iteration: 358/1000  consumed samples: 5728  total_loss: 7.864  time: 0.2016(79.37)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:08] lb.utils.events INFO:  eta: 0:02:13  iteration: 359/1000  consumed samples: 5744  total_loss: 7.864  time: 0.2016(79.36)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:08] lb.utils.events INFO:  eta: 0:02:12  iteration: 360/1000  consumed samples: 5760  total_loss: 7.863  time: 0.2016(79.35)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:08] lb.utils.events INFO:  eta: 0:02:12  iteration: 361/1000  consumed samples: 5776  total_loss: 7.862  time: 0.2017(79.34)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:09] lb.utils.events INFO:  eta: 0:02:12  iteration: 362/1000  consumed samples: 5792  total_loss: 7.862  time: 0.2017(79.33)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:09] lb.utils.events INFO:  eta: 0:02:12  iteration: 363/1000  consumed samples: 5808  total_loss: 7.861  time: 0.2017(79.32)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:09] lb.utils.events INFO:  eta: 0:02:12  iteration: 364/1000  consumed samples: 5824  total_loss: 7.861  time: 0.2018(79.30)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:09] lb.utils.events INFO:  eta: 0:02:11  iteration: 365/1000  consumed samples: 5840  total_loss: 7.861  time: 0.2018(79.30)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:10] lb.utils.events INFO:  eta: 0:02:11  iteration: 366/1000  consumed samples: 5856  total_loss: 7.858  time: 0.2018(79.28)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:10] lb.utils.events INFO:  eta: 0:02:11  iteration: 367/1000  consumed samples: 5872  total_loss: 7.852  time: 0.2018(79.27)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:10] lb.utils.events INFO:  eta: 0:02:11  iteration: 368/1000  consumed samples: 5888  total_loss: 7.848  time: 0.2019(79.26)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:10] lb.utils.events INFO:  eta: 0:02:11  iteration: 369/1000  consumed samples: 5904  total_loss: 7.847  time: 0.2019(79.26)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:10] lb.utils.events INFO:  eta: 0:02:10  iteration: 370/1000  consumed samples: 5920  total_loss: 7.847  time: 0.2019(79.25)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:11] lb.utils.events INFO:  eta: 0:02:10  iteration: 371/1000  consumed samples: 5936  total_loss: 7.846  time: 0.2019(79.23)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:11] lb.utils.events INFO:  eta: 0:02:10  iteration: 372/1000  consumed samples: 5952  total_loss: 7.845  time: 0.2019(79.24)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:11] lb.utils.events INFO:  eta: 0:02:10  iteration: 373/1000  consumed samples: 5968  total_loss: 7.843  time: 0.2019(79.23)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:11] lb.utils.events INFO:  eta: 0:02:10  iteration: 374/1000  consumed samples: 5984  total_loss: 7.842  time: 0.2020(79.22)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:12] lb.utils.events INFO:  eta: 0:02:09  iteration: 375/1000  consumed samples: 6000  total_loss: 7.843  time: 0.2020(79.21)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:12] lb.utils.events INFO:  eta: 0:02:09  iteration: 376/1000  consumed samples: 6016  total_loss: 7.842  time: 0.2020(79.20)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:12] lb.utils.events INFO:  eta: 0:02:09  iteration: 377/1000  consumed samples: 6032  total_loss: 7.843  time: 0.2020(79.19)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:12] lb.utils.events INFO:  eta: 0:02:09  iteration: 378/1000  consumed samples: 6048  total_loss: 7.842  time: 0.2020(79.19)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:12] lb.utils.events INFO:  eta: 0:02:08  iteration: 379/1000  consumed samples: 6064  total_loss: 7.843  time: 0.2021(79.18)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:13] lb.utils.events INFO:  eta: 0:02:08  iteration: 380/1000  consumed samples: 6080  total_loss: 7.843  time: 0.2021(79.17)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:13] lb.utils.events INFO:  eta: 0:02:08  iteration: 381/1000  consumed samples: 6096  total_loss: 7.843  time: 0.2021(79.16)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:13] lb.utils.events INFO:  eta: 0:02:08  iteration: 382/1000  consumed samples: 6112  total_loss: 7.845  time: 0.2021(79.16)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:13] lb.utils.events INFO:  eta: 0:02:08  iteration: 383/1000  consumed samples: 6128  total_loss: 7.843  time: 0.2021(79.15)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:14] lb.utils.events INFO:  eta: 0:02:07  iteration: 384/1000  consumed samples: 6144  total_loss: 7.843  time: 0.2022(79.13)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:14] lb.utils.events INFO:  eta: 0:02:07  iteration: 385/1000  consumed samples: 6160  total_loss: 7.841  time: 0.2022(79.13)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:14] lb.utils.events INFO:  eta: 0:02:07  iteration: 386/1000  consumed samples: 6176  total_loss: 7.84  time: 0.2022(79.11)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:14] lb.utils.events INFO:  eta: 0:02:07  iteration: 387/1000  consumed samples: 6192  total_loss: 7.838  time: 0.2023(79.11)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:14] lb.utils.events INFO:  eta: 0:02:07  iteration: 388/1000  consumed samples: 6208  total_loss: 7.837  time: 0.2023(79.09)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:15] lb.utils.events INFO:  eta: 0:02:06  iteration: 389/1000  consumed samples: 6224  total_loss: 7.834  time: 0.2023(79.09)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:15] lb.utils.events INFO:  eta: 0:02:06  iteration: 390/1000  consumed samples: 6240  total_loss: 7.831  time: 0.2023(79.07)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:15] lb.utils.events INFO:  eta: 0:02:06  iteration: 391/1000  consumed samples: 6256  total_loss: 7.83  time: 0.2024(79.06)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:15] lb.utils.events INFO:  eta: 0:02:06  iteration: 392/1000  consumed samples: 6272  total_loss: 7.83  time: 0.2024(79.06)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:16] lb.utils.events INFO:  eta: 0:02:06  iteration: 393/1000  consumed samples: 6288  total_loss: 7.831  time: 0.2024(79.05)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:16] lb.utils.events INFO:  eta: 0:02:05  iteration: 394/1000  consumed samples: 6304  total_loss: 7.831  time: 0.2024(79.05)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:16] lb.utils.events INFO:  eta: 0:02:05  iteration: 395/1000  consumed samples: 6320  total_loss: 7.831  time: 0.2024(79.05)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:16] lb.utils.events INFO:  eta: 0:02:05  iteration: 396/1000  consumed samples: 6336  total_loss: 7.831  time: 0.2024(79.03)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:16] lb.utils.events INFO:  eta: 0:02:05  iteration: 397/1000  consumed samples: 6352  total_loss: 7.83  time: 0.2024(79.04)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:17] lb.utils.events INFO:  eta: 0:02:05  iteration: 398/1000  consumed samples: 6368  total_loss: 7.83  time: 0.2025(79.02)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:17] lb.utils.events INFO:  eta: 0:02:04  iteration: 399/1000  consumed samples: 6384  total_loss: 7.829  time: 0.2025(79.02)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:17] lb.utils.events INFO:  eta: 0:02:04  iteration: 400/1000  consumed samples: 6400  total_loss: 7.829  time: 0.2025(79.00)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:17] lb.utils.events INFO:  eta: 0:02:04  iteration: 401/1000  consumed samples: 6416  total_loss: 7.829  time: 0.2025(79.00)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:18] lb.utils.events INFO:  eta: 0:02:04  iteration: 402/1000  consumed samples: 6432  total_loss: 7.829  time: 0.2025(79.00)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:18] lb.utils.events INFO:  eta: 0:02:04  iteration: 403/1000  consumed samples: 6448  total_loss: 7.828  time: 0.2026(78.99)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:18] lb.utils.events INFO:  eta: 0:02:03  iteration: 404/1000  consumed samples: 6464  total_loss: 7.828  time: 0.2026(78.98)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:18] lb.utils.events INFO:  eta: 0:02:03  iteration: 405/1000  consumed samples: 6480  total_loss: 7.828  time: 0.2026(78.97)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:18] lb.utils.events INFO:  eta: 0:02:03  iteration: 406/1000  consumed samples: 6496  total_loss: 7.828  time: 0.2026(78.96)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:19] lb.utils.events INFO:  eta: 0:02:03  iteration: 407/1000  consumed samples: 6512  total_loss: 7.828  time: 0.2026(78.96)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:19] lb.utils.events INFO:  eta: 0:02:02  iteration: 408/1000  consumed samples: 6528  total_loss: 7.826  time: 0.2027(78.95)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:19] lb.utils.events INFO:  eta: 0:02:02  iteration: 409/1000  consumed samples: 6544  total_loss: 7.826  time: 0.2027(78.94)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:19] lb.utils.events INFO:  eta: 0:02:02  iteration: 410/1000  consumed samples: 6560  total_loss: 7.828  time: 0.2027(78.93)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:20] lb.utils.events INFO:  eta: 0:02:02  iteration: 411/1000  consumed samples: 6576  total_loss: 7.826  time: 0.2027(78.92)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:20] lb.utils.events INFO:  eta: 0:02:02  iteration: 412/1000  consumed samples: 6592  total_loss: 7.823  time: 0.2027(78.93)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:20] lb.utils.events INFO:  eta: 0:02:01  iteration: 413/1000  consumed samples: 6608  total_loss: 7.821  time: 0.2027(78.92)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:20] lb.utils.events INFO:  eta: 0:02:01  iteration: 414/1000  consumed samples: 6624  total_loss: 7.821  time: 0.2028(78.91)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:20] lb.utils.events INFO:  eta: 0:02:01  iteration: 415/1000  consumed samples: 6640  total_loss: 7.821  time: 0.2028(78.91)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:21] lb.utils.events INFO:  eta: 0:02:01  iteration: 416/1000  consumed samples: 6656  total_loss: 7.821  time: 0.2028(78.90)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:21] lb.utils.events INFO:  eta: 0:02:01  iteration: 417/1000  consumed samples: 6672  total_loss: 7.82  time: 0.2028(78.88)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:21] lb.utils.events INFO:  eta: 0:02:00  iteration: 418/1000  consumed samples: 6688  total_loss: 7.818  time: 0.2029(78.87)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:21] lb.utils.events INFO:  eta: 0:02:00  iteration: 419/1000  consumed samples: 6704  total_loss: 7.817  time: 0.2029(78.87)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:22] lb.utils.events INFO:  eta: 0:02:00  iteration: 420/1000  consumed samples: 6720  total_loss: 7.815  time: 0.2029(78.86)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:22] lb.utils.events INFO:  eta: 0:02:00  iteration: 421/1000  consumed samples: 6736  total_loss: 7.815  time: 0.2029(78.86)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:22] lb.utils.events INFO:  eta: 0:02:00  iteration: 422/1000  consumed samples: 6752  total_loss: 7.813  time: 0.2029(78.85)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:22] lb.utils.events INFO:  eta: 0:01:59  iteration: 423/1000  consumed samples: 6768  total_loss: 7.813  time: 0.2029(78.84)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:22] lb.utils.events INFO:  eta: 0:01:59  iteration: 424/1000  consumed samples: 6784  total_loss: 7.812  time: 0.2030(78.83)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:23] lb.utils.events INFO:  eta: 0:01:59  iteration: 425/1000  consumed samples: 6800  total_loss: 7.812  time: 0.2030(78.82)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:23] lb.utils.events INFO:  eta: 0:01:59  iteration: 426/1000  consumed samples: 6816  total_loss: 7.813  time: 0.2030(78.81)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:23] lb.utils.events INFO:  eta: 0:01:59  iteration: 427/1000  consumed samples: 6832  total_loss: 7.812  time: 0.2030(78.81)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:23] lb.utils.events INFO:  eta: 0:01:58  iteration: 428/1000  consumed samples: 6848  total_loss: 7.811  time: 0.2031(78.80)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:24] lb.utils.events INFO:  eta: 0:01:58  iteration: 429/1000  consumed samples: 6864  total_loss: 7.811  time: 0.2031(78.79)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:24] lb.utils.events INFO:  eta: 0:01:58  iteration: 430/1000  consumed samples: 6880  total_loss: 7.811  time: 0.2031(78.78)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:24] lb.utils.events INFO:  eta: 0:01:58  iteration: 431/1000  consumed samples: 6896  total_loss: 7.809  time: 0.2031(78.77)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:24] lb.utils.events INFO:  eta: 0:01:58  iteration: 432/1000  consumed samples: 6912  total_loss: 7.809  time: 0.2031(78.76)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:24] lb.utils.events INFO:  eta: 0:01:57  iteration: 433/1000  consumed samples: 6928  total_loss: 7.808  time: 0.2032(78.76)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:25] lb.utils.events INFO:  eta: 0:01:57  iteration: 434/1000  consumed samples: 6944  total_loss: 7.807  time: 0.2032(78.75)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:25] lb.utils.events INFO:  eta: 0:01:57  iteration: 435/1000  consumed samples: 6960  total_loss: 7.807  time: 0.2032(78.74)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:25] lb.utils.events INFO:  eta: 0:01:57  iteration: 436/1000  consumed samples: 6976  total_loss: 7.807  time: 0.2032(78.74)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:25] lb.utils.events INFO:  eta: 0:01:57  iteration: 437/1000  consumed samples: 6992  total_loss: 7.803  time: 0.2032(78.73)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:25] lb.utils.events INFO:  eta: 0:01:56  iteration: 438/1000  consumed samples: 7008  total_loss: 7.799  time: 0.2032(78.73)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:26] lb.utils.events INFO:  eta: 0:01:56  iteration: 439/1000  consumed samples: 7024  total_loss: 7.798  time: 0.2032(78.72)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:26] lb.utils.events INFO:  eta: 0:01:56  iteration: 440/1000  consumed samples: 7040  total_loss: 7.796  time: 0.2033(78.71)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:26] lb.utils.events INFO:  eta: 0:01:56  iteration: 441/1000  consumed samples: 7056  total_loss: 7.796  time: 0.2033(78.70)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:26] lb.utils.events INFO:  eta: 0:01:56  iteration: 442/1000  consumed samples: 7072  total_loss: 7.796  time: 0.2033(78.70)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:27] lb.utils.events INFO:  eta: 0:01:55  iteration: 443/1000  consumed samples: 7088  total_loss: 7.795  time: 0.2033(78.69)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:27] lb.utils.events INFO:  eta: 0:01:55  iteration: 444/1000  consumed samples: 7104  total_loss: 7.793  time: 0.2034(78.68)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:27] lb.utils.events INFO:  eta: 0:01:55  iteration: 445/1000  consumed samples: 7120  total_loss: 7.791  time: 0.2034(78.68)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:27] lb.utils.events INFO:  eta: 0:01:55  iteration: 446/1000  consumed samples: 7136  total_loss: 7.79  time: 0.2034(78.67)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:27] lb.utils.events INFO:  eta: 0:01:54  iteration: 447/1000  consumed samples: 7152  total_loss: 7.79  time: 0.2034(78.66)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:28] lb.utils.events INFO:  eta: 0:01:54  iteration: 448/1000  consumed samples: 7168  total_loss: 7.79  time: 0.2034(78.65)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:28] lb.utils.events INFO:  eta: 0:01:54  iteration: 449/1000  consumed samples: 7184  total_loss: 7.79  time: 0.2034(78.65)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:28] lb.utils.events INFO:  eta: 0:01:54  iteration: 450/1000  consumed samples: 7200  total_loss: 7.79  time: 0.2035(78.64)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:28] lb.utils.events INFO:  eta: 0:01:54  iteration: 451/1000  consumed samples: 7216  total_loss: 7.79  time: 0.2035(78.63)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:29] lb.utils.events INFO:  eta: 0:01:53  iteration: 452/1000  consumed samples: 7232  total_loss: 7.789  time: 0.2035(78.62)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:29] lb.utils.events INFO:  eta: 0:01:53  iteration: 453/1000  consumed samples: 7248  total_loss: 7.788  time: 0.2035(78.61)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:29] lb.utils.events INFO:  eta: 0:01:53  iteration: 454/1000  consumed samples: 7264  total_loss: 7.787  time: 0.2036(78.60)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:29] lb.utils.events INFO:  eta: 0:01:53  iteration: 455/1000  consumed samples: 7280  total_loss: 7.786  time: 0.2036(78.59)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:29] lb.utils.events INFO:  eta: 0:01:53  iteration: 456/1000  consumed samples: 7296  total_loss: 7.783  time: 0.2036(78.58)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:30] lb.utils.events INFO:  eta: 0:01:52  iteration: 457/1000  consumed samples: 7312  total_loss: 7.78  time: 0.2036(78.57)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:30] lb.utils.events INFO:  eta: 0:01:52  iteration: 458/1000  consumed samples: 7328  total_loss: 7.78  time: 0.2037(78.57)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:30] lb.utils.events INFO:  eta: 0:01:52  iteration: 459/1000  consumed samples: 7344  total_loss: 7.779  time: 0.2037(78.56)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:30] lb.utils.events INFO:  eta: 0:01:52  iteration: 460/1000  consumed samples: 7360  total_loss: 7.778  time: 0.2037(78.54)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:31] lb.utils.events INFO:  eta: 0:01:52  iteration: 461/1000  consumed samples: 7376  total_loss: 7.779  time: 0.2037(78.54)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:31] lb.utils.events INFO:  eta: 0:01:51  iteration: 462/1000  consumed samples: 7392  total_loss: 7.779  time: 0.2037(78.54)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:31] lb.utils.events INFO:  eta: 0:01:51  iteration: 463/1000  consumed samples: 7408  total_loss: 7.779  time: 0.2038(78.53)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:39:31] lb.utils.events INFO:  eta: 0:01:51  iteration: 464/1000  consumed samples: 7424  total_loss: 7.778  time: 0.2038(78.52)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:39:31] lb.utils.events INFO:  eta: 0:01:51  iteration: 465/1000  consumed samples: 7440  total_loss: 7.778  time: 0.2038(78.51)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:32] lb.utils.events INFO:  eta: 0:01:51  iteration: 466/1000  consumed samples: 7456  total_loss: 7.778  time: 0.2038(78.50)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:32] lb.utils.events INFO:  eta: 0:01:50  iteration: 467/1000  consumed samples: 7472  total_loss: 7.778  time: 0.2038(78.49)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:32] lb.utils.events INFO:  eta: 0:01:50  iteration: 468/1000  consumed samples: 7488  total_loss: 7.778  time: 0.2039(78.48)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:32] lb.utils.events INFO:  eta: 0:01:50  iteration: 469/1000  consumed samples: 7504  total_loss: 7.778  time: 0.2039(78.48)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:33] lb.utils.events INFO:  eta: 0:01:50  iteration: 470/1000  consumed samples: 7520  total_loss: 7.778  time: 0.2039(78.48)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:33] lb.utils.events INFO:  eta: 0:01:50  iteration: 471/1000  consumed samples: 7536  total_loss: 7.777  time: 0.2039(78.49)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:33] lb.utils.events INFO:  eta: 0:01:49  iteration: 472/1000  consumed samples: 7552  total_loss: 7.774  time: 0.2039(78.48)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:33] lb.utils.events INFO:  eta: 0:01:49  iteration: 473/1000  consumed samples: 7568  total_loss: 7.774  time: 0.2039(78.47)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:33] lb.utils.events INFO:  eta: 0:01:49  iteration: 474/1000  consumed samples: 7584  total_loss: 7.773  time: 0.2039(78.46)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:34] lb.utils.events INFO:  eta: 0:01:49  iteration: 475/1000  consumed samples: 7600  total_loss: 7.772  time: 0.2039(78.47)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:34] lb.utils.events INFO:  eta: 0:01:49  iteration: 476/1000  consumed samples: 7616  total_loss: 7.772  time: 0.2039(78.46)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:34] lb.utils.events INFO:  eta: 0:01:48  iteration: 477/1000  consumed samples: 7632  total_loss: 7.772  time: 0.2039(78.46)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:34] lb.utils.events INFO:  eta: 0:01:48  iteration: 478/1000  consumed samples: 7648  total_loss: 7.771  time: 0.2040(78.45)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:35] lb.utils.events INFO:  eta: 0:01:48  iteration: 479/1000  consumed samples: 7664  total_loss: 7.771  time: 0.2040(78.45)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:35] lb.utils.events INFO:  eta: 0:01:48  iteration: 480/1000  consumed samples: 7680  total_loss: 7.769  time: 0.2040(78.44)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:35] lb.utils.events INFO:  eta: 0:01:48  iteration: 481/1000  consumed samples: 7696  total_loss: 7.767  time: 0.2040(78.44)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:35] lb.utils.events INFO:  eta: 0:01:47  iteration: 482/1000  consumed samples: 7712  total_loss: 7.767  time: 0.2040(78.43)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:35] lb.utils.events INFO:  eta: 0:01:47  iteration: 483/1000  consumed samples: 7728  total_loss: 7.767  time: 0.2040(78.42)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:36] lb.utils.events INFO:  eta: 0:01:47  iteration: 484/1000  consumed samples: 7744  total_loss: 7.766  time: 0.2040(78.42)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:36] lb.utils.events INFO:  eta: 0:01:47  iteration: 485/1000  consumed samples: 7760  total_loss: 7.763  time: 0.2040(78.41)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:36] lb.utils.events INFO:  eta: 0:01:46  iteration: 486/1000  consumed samples: 7776  total_loss: 7.761  time: 0.2041(78.41)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:36] lb.utils.events INFO:  eta: 0:01:46  iteration: 487/1000  consumed samples: 7792  total_loss: 7.759  time: 0.2041(78.41)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:37] lb.utils.events INFO:  eta: 0:01:46  iteration: 488/1000  consumed samples: 7808  total_loss: 7.759  time: 0.2041(78.40)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:37] lb.utils.events INFO:  eta: 0:01:46  iteration: 489/1000  consumed samples: 7824  total_loss: 7.759  time: 0.2041(78.39)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:37] lb.utils.events INFO:  eta: 0:01:46  iteration: 490/1000  consumed samples: 7840  total_loss: 7.759  time: 0.2041(78.39)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:37] lb.utils.events INFO:  eta: 0:01:45  iteration: 491/1000  consumed samples: 7856  total_loss: 7.759  time: 0.2041(78.38)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:37] lb.utils.events INFO:  eta: 0:01:45  iteration: 492/1000  consumed samples: 7872  total_loss: 7.758  time: 0.2042(78.37)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:38] lb.utils.events INFO:  eta: 0:01:45  iteration: 493/1000  consumed samples: 7888  total_loss: 7.757  time: 0.2042(78.37)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:38] lb.utils.events INFO:  eta: 0:01:45  iteration: 494/1000  consumed samples: 7904  total_loss: 7.756  time: 0.2042(78.36)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:38] lb.utils.events INFO:  eta: 0:01:45  iteration: 495/1000  consumed samples: 7920  total_loss: 7.755  time: 0.2042(78.35)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:38] lb.utils.events INFO:  eta: 0:01:44  iteration: 496/1000  consumed samples: 7936  total_loss: 7.754  time: 0.2042(78.34)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:39] lb.utils.events INFO:  eta: 0:01:44  iteration: 497/1000  consumed samples: 7952  total_loss: 7.754  time: 0.2042(78.34)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:39] lb.utils.events INFO:  eta: 0:01:44  iteration: 498/1000  consumed samples: 7968  total_loss: 7.753  time: 0.2043(78.33)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:39] lb.utils.events INFO:  eta: 0:01:44  iteration: 499/1000  consumed samples: 7984  total_loss: 7.753  time: 0.2043(78.33)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:39] lb.utils.events INFO:  eta: 0:01:44  iteration: 500/1000  consumed samples: 8000  total_loss: 7.753  time: 0.2043(78.32)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:39] lb.utils.events INFO:  eta: 0:01:43  iteration: 501/1000  consumed samples: 8016  total_loss: 7.751  time: 0.2043(78.31)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:40] lb.utils.events INFO:  eta: 0:01:43  iteration: 502/1000  consumed samples: 8032  total_loss: 7.751  time: 0.2043(78.30)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:40] lb.utils.events INFO:  eta: 0:01:43  iteration: 503/1000  consumed samples: 8048  total_loss: 7.747  time: 0.2043(78.30)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:40] lb.utils.events INFO:  eta: 0:01:43  iteration: 504/1000  consumed samples: 8064  total_loss: 7.747  time: 0.2044(78.29)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:40] lb.utils.events INFO:  eta: 0:01:43  iteration: 505/1000  consumed samples: 8080  total_loss: 7.742  time: 0.2044(78.29)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:41] lb.utils.events INFO:  eta: 0:01:42  iteration: 506/1000  consumed samples: 8096  total_loss: 7.742  time: 0.2044(78.28)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:41] lb.utils.events INFO:  eta: 0:01:42  iteration: 507/1000  consumed samples: 8112  total_loss: 7.738  time: 0.2044(78.27)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:41] lb.utils.events INFO:  eta: 0:01:42  iteration: 508/1000  consumed samples: 8128  total_loss: 7.735  time: 0.2044(78.27)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:41] lb.utils.events INFO:  eta: 0:01:42  iteration: 509/1000  consumed samples: 8144  total_loss: 7.735  time: 0.2044(78.27)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:41] lb.utils.events INFO:  eta: 0:01:42  iteration: 510/1000  consumed samples: 8160  total_loss: 7.735  time: 0.2044(78.27)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 511/1000  consumed samples: 8176  total_loss: 7.735  time: 0.2045(78.26)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 512/1000  consumed samples: 8192  total_loss: 7.732  time: 0.2045(78.25)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 513/1000  consumed samples: 8208  total_loss: 7.732  time: 0.2045(78.25)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:42] lb.utils.events INFO:  eta: 0:01:41  iteration: 514/1000  consumed samples: 8224  total_loss: 7.732  time: 0.2045(78.24)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:43] lb.utils.events INFO:  eta: 0:01:41  iteration: 515/1000  consumed samples: 8240  total_loss: 7.729  time: 0.2045(78.23)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 516/1000  consumed samples: 8256  total_loss: 7.729  time: 0.2045(78.23)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 517/1000  consumed samples: 8272  total_loss: 7.729  time: 0.2045(78.22)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 518/1000  consumed samples: 8288  total_loss: 7.729  time: 0.2046(78.22)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:43] lb.utils.events INFO:  eta: 0:01:40  iteration: 519/1000  consumed samples: 8304  total_loss: 7.726  time: 0.2046(78.21)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:44] lb.utils.events INFO:  eta: 0:01:39  iteration: 520/1000  consumed samples: 8320  total_loss: 7.726  time: 0.2046(78.21)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:44] lb.utils.events INFO:  eta: 0:01:39  iteration: 521/1000  consumed samples: 8336  total_loss: 7.729  time: 0.2046(78.20)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:44] lb.utils.events INFO:  eta: 0:01:39  iteration: 522/1000  consumed samples: 8352  total_loss: 7.726  time: 0.2046(78.20)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:44] lb.utils.events INFO:  eta: 0:01:39  iteration: 523/1000  consumed samples: 8368  total_loss: 7.725  time: 0.2046(78.19)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:44] lb.utils.events INFO:  eta: 0:01:39  iteration: 524/1000  consumed samples: 8384  total_loss: 7.724  time: 0.2046(78.19)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 525/1000  consumed samples: 8400  total_loss: 7.724  time: 0.2047(78.18)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 526/1000  consumed samples: 8416  total_loss: 7.724  time: 0.2047(78.18)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 527/1000  consumed samples: 8432  total_loss: 7.724  time: 0.2047(78.17)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:45] lb.utils.events INFO:  eta: 0:01:38  iteration: 528/1000  consumed samples: 8448  total_loss: 7.724  time: 0.2047(78.16)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:46] lb.utils.events INFO:  eta: 0:01:38  iteration: 529/1000  consumed samples: 8464  total_loss: 7.723  time: 0.2047(78.16)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 530/1000  consumed samples: 8480  total_loss: 7.722  time: 0.2047(78.15)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 531/1000  consumed samples: 8496  total_loss: 7.722  time: 0.2047(78.15)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 532/1000  consumed samples: 8512  total_loss: 7.722  time: 0.2048(78.14)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:46] lb.utils.events INFO:  eta: 0:01:37  iteration: 533/1000  consumed samples: 8528  total_loss: 7.722  time: 0.2048(78.13)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:47] lb.utils.events INFO:  eta: 0:01:37  iteration: 534/1000  consumed samples: 8544  total_loss: 7.72  time: 0.2048(78.13)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:47] lb.utils.events INFO:  eta: 0:01:36  iteration: 535/1000  consumed samples: 8560  total_loss: 7.72  time: 0.2048(78.12)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:47] lb.utils.events INFO:  eta: 0:01:36  iteration: 536/1000  consumed samples: 8576  total_loss: 7.72  time: 0.2048(78.12)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:47] lb.utils.events INFO:  eta: 0:01:36  iteration: 537/1000  consumed samples: 8592  total_loss: 7.72  time: 0.2048(78.11)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:48] lb.utils.events INFO:  eta: 0:01:36  iteration: 538/1000  consumed samples: 8608  total_loss: 7.722  time: 0.2048(78.11)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:48] lb.utils.events INFO:  eta: 0:01:36  iteration: 539/1000  consumed samples: 8624  total_loss: 7.72  time: 0.2049(78.10)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 540/1000  consumed samples: 8640  total_loss: 7.718  time: 0.2049(78.10)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 541/1000  consumed samples: 8656  total_loss: 7.718  time: 0.2049(78.09)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:48] lb.utils.events INFO:  eta: 0:01:35  iteration: 542/1000  consumed samples: 8672  total_loss: 7.716  time: 0.2049(78.08)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:49] lb.utils.events INFO:  eta: 0:01:35  iteration: 543/1000  consumed samples: 8688  total_loss: 7.712  time: 0.2049(78.07)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:49] lb.utils.events INFO:  eta: 0:01:35  iteration: 544/1000  consumed samples: 8704  total_loss: 7.712  time: 0.2049(78.07)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 545/1000  consumed samples: 8720  total_loss: 7.711  time: 0.2050(78.06)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:39:49] lb.utils.events INFO:  eta: 0:01:34  iteration: 546/1000  consumed samples: 8736  total_loss: 7.71  time: 0.2050(78.06)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:50] lb.utils.events INFO:  eta: 0:01:34  iteration: 547/1000  consumed samples: 8752  total_loss: 7.71  time: 0.2050(78.05)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:39:50] lb.utils.events INFO:  eta: 0:01:34  iteration: 548/1000  consumed samples: 8768  total_loss: 7.709  time: 0.2050(78.05)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:50] lb.utils.events INFO:  eta: 0:01:34  iteration: 549/1000  consumed samples: 8784  total_loss: 7.709  time: 0.2050(78.04)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:50] lb.utils.events INFO:  eta: 0:01:33  iteration: 550/1000  consumed samples: 8800  total_loss: 7.709  time: 0.2050(78.04)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:39:50] lb.utils.events INFO:  eta: 0:01:33  iteration: 551/1000  consumed samples: 8816  total_loss: 7.706  time: 0.2050(78.03)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:51] lb.utils.events INFO:  eta: 0:01:33  iteration: 552/1000  consumed samples: 8832  total_loss: 7.706  time: 0.2050(78.03)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:51] lb.utils.events INFO:  eta: 0:01:33  iteration: 553/1000  consumed samples: 8848  total_loss: 7.704  time: 0.2051(78.02)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 554/1000  consumed samples: 8864  total_loss: 7.701  time: 0.2051(78.02)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:51] lb.utils.events INFO:  eta: 0:01:32  iteration: 555/1000  consumed samples: 8880  total_loss: 7.701  time: 0.2051(78.01)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:52] lb.utils.events INFO:  eta: 0:01:32  iteration: 556/1000  consumed samples: 8896  total_loss: 7.701  time: 0.2051(78.01)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:52] lb.utils.events INFO:  eta: 0:01:32  iteration: 557/1000  consumed samples: 8912  total_loss: 7.698  time: 0.2051(78.00)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:39:52] lb.utils.events INFO:  eta: 0:01:32  iteration: 558/1000  consumed samples: 8928  total_loss: 7.698  time: 0.2051(78.00)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 559/1000  consumed samples: 8944  total_loss: 7.694  time: 0.2051(77.99)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:39:52] lb.utils.events INFO:  eta: 0:01:31  iteration: 560/1000  consumed samples: 8960  total_loss: 7.694  time: 0.2052(77.99)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:53] lb.utils.events INFO:  eta: 0:01:31  iteration: 561/1000  consumed samples: 8976  total_loss: 7.692  time: 0.2052(77.98)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:53] lb.utils.events INFO:  eta: 0:01:31  iteration: 562/1000  consumed samples: 8992  total_loss: 7.692  time: 0.2052(77.98)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:53] lb.utils.events INFO:  eta: 0:01:31  iteration: 563/1000  consumed samples: 9008  total_loss: 7.691  time: 0.2052(77.97)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:53] lb.utils.events INFO:  eta: 0:01:30  iteration: 564/1000  consumed samples: 9024  total_loss: 7.692  time: 0.2052(77.97)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:54] lb.utils.events INFO:  eta: 0:01:30  iteration: 565/1000  consumed samples: 9040  total_loss: 7.691  time: 0.2052(77.96)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:39:54] lb.utils.events INFO:  eta: 0:01:30  iteration: 566/1000  consumed samples: 9056  total_loss: 7.687  time: 0.2052(77.96)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:54] lb.utils.events INFO:  eta: 0:01:30  iteration: 567/1000  consumed samples: 9072  total_loss: 7.687  time: 0.2053(77.95)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:39:54] lb.utils.events INFO:  eta: 0:01:30  iteration: 568/1000  consumed samples: 9088  total_loss: 7.683  time: 0.2053(77.95)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:54] lb.utils.events INFO:  eta: 0:01:29  iteration: 569/1000  consumed samples: 9104  total_loss: 7.683  time: 0.2053(77.94)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:55] lb.utils.events INFO:  eta: 0:01:29  iteration: 570/1000  consumed samples: 9120  total_loss: 7.683  time: 0.2053(77.94)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:39:55] lb.utils.events INFO:  eta: 0:01:29  iteration: 571/1000  consumed samples: 9136  total_loss: 7.683  time: 0.2053(77.93)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:39:55] lb.utils.events INFO:  eta: 0:01:29  iteration: 572/1000  consumed samples: 9152  total_loss: 7.683  time: 0.2053(77.93)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:39:55] lb.utils.events INFO:  eta: 0:01:29  iteration: 573/1000  consumed samples: 9168  total_loss: 7.687  time: 0.2053(77.92)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:39:56] lb.utils.events INFO:  eta: 0:01:28  iteration: 574/1000  consumed samples: 9184  total_loss: 7.687  time: 0.2053(77.92)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:39:56] lb.utils.events INFO:  eta: 0:01:28  iteration: 575/1000  consumed samples: 9200  total_loss: 7.683  time: 0.2054(77.91)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:39:56] lb.utils.events INFO:  eta: 0:01:28  iteration: 576/1000  consumed samples: 9216  total_loss: 7.682  time: 0.2054(77.91)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:39:56] lb.utils.events INFO:  eta: 0:01:28  iteration: 577/1000  consumed samples: 9232  total_loss: 7.682  time: 0.2054(77.90)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:39:56] lb.utils.events INFO:  eta: 0:01:28  iteration: 578/1000  consumed samples: 9248  total_loss: 7.678  time: 0.2054(77.90)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:39:57] lb.utils.events INFO:  eta: 0:01:27  iteration: 579/1000  consumed samples: 9264  total_loss: 7.674  time: 0.2054(77.90)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:39:57] lb.utils.events INFO:  eta: 0:01:27  iteration: 580/1000  consumed samples: 9280  total_loss: 7.674  time: 0.2054(77.89)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:39:57] lb.utils.events INFO:  eta: 0:01:27  iteration: 581/1000  consumed samples: 9296  total_loss: 7.673  time: 0.2054(77.89)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:39:57] lb.utils.events INFO:  eta: 0:01:27  iteration: 582/1000  consumed samples: 9312  total_loss: 7.673  time: 0.2054(77.89)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:39:58] lb.utils.events INFO:  eta: 0:01:27  iteration: 583/1000  consumed samples: 9328  total_loss: 7.673  time: 0.2054(77.89)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:39:58] lb.utils.events INFO:  eta: 0:01:26  iteration: 584/1000  consumed samples: 9344  total_loss: 7.669  time: 0.2054(77.88)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:39:58] lb.utils.events INFO:  eta: 0:01:26  iteration: 585/1000  consumed samples: 9360  total_loss: 7.669  time: 0.2054(77.88)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:39:58] lb.utils.events INFO:  eta: 0:01:26  iteration: 586/1000  consumed samples: 9376  total_loss: 7.669  time: 0.2054(77.88)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:39:58] lb.utils.events INFO:  eta: 0:01:26  iteration: 587/1000  consumed samples: 9392  total_loss: 7.665  time: 0.2055(77.87)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:39:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 588/1000  consumed samples: 9408  total_loss: 7.669  time: 0.2055(77.87)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:39:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 589/1000  consumed samples: 9424  total_loss: 7.665  time: 0.2055(77.86)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:39:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 590/1000  consumed samples: 9440  total_loss: 7.663  time: 0.2055(77.86)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:39:59] lb.utils.events INFO:  eta: 0:01:25  iteration: 591/1000  consumed samples: 9456  total_loss: 7.662  time: 0.2055(77.86)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:00] lb.utils.events INFO:  eta: 0:01:25  iteration: 592/1000  consumed samples: 9472  total_loss: 7.66  time: 0.2055(77.85)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 593/1000  consumed samples: 9488  total_loss: 7.66  time: 0.2055(77.85)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 594/1000  consumed samples: 9504  total_loss: 7.658  time: 0.2055(77.85)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 595/1000  consumed samples: 9520  total_loss: 7.658  time: 0.2056(77.84)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:00] lb.utils.events INFO:  eta: 0:01:24  iteration: 596/1000  consumed samples: 9536  total_loss: 7.658  time: 0.2056(77.84)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:01] lb.utils.events INFO:  eta: 0:01:24  iteration: 597/1000  consumed samples: 9552  total_loss: 7.658  time: 0.2056(77.83)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:01] lb.utils.events INFO:  eta: 0:01:23  iteration: 598/1000  consumed samples: 9568  total_loss: 7.658  time: 0.2056(77.82)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:01] lb.utils.events INFO:  eta: 0:01:23  iteration: 599/1000  consumed samples: 9584  total_loss: 7.656  time: 0.2056(77.82)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:01] lb.utils.events INFO:  eta: 0:01:23  iteration: 600/1000  consumed samples: 9600  total_loss: 7.655  time: 0.2056(77.82)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:02] lb.utils.events INFO:  eta: 0:01:23  iteration: 601/1000  consumed samples: 9616  total_loss: 7.655  time: 0.2056(77.81)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:02] lb.utils.events INFO:  eta: 0:01:23  iteration: 602/1000  consumed samples: 9632  total_loss: 7.654  time: 0.2056(77.81)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 603/1000  consumed samples: 9648  total_loss: 7.655  time: 0.2056(77.80)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 604/1000  consumed samples: 9664  total_loss: 7.654  time: 0.2057(77.80)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:02] lb.utils.events INFO:  eta: 0:01:22  iteration: 605/1000  consumed samples: 9680  total_loss: 7.654  time: 0.2057(77.79)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:03] lb.utils.events INFO:  eta: 0:01:22  iteration: 606/1000  consumed samples: 9696  total_loss: 7.652  time: 0.2057(77.79)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 607/1000  consumed samples: 9712  total_loss: 7.652  time: 0.2057(77.79)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 608/1000  consumed samples: 9728  total_loss: 7.652  time: 0.2057(77.78)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:03] lb.utils.events INFO:  eta: 0:01:21  iteration: 609/1000  consumed samples: 9744  total_loss: 7.648  time: 0.2057(77.78)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:04] lb.utils.events INFO:  eta: 0:01:21  iteration: 610/1000  consumed samples: 9760  total_loss: 7.646  time: 0.2057(77.78)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:04] lb.utils.events INFO:  eta: 0:01:21  iteration: 611/1000  consumed samples: 9776  total_loss: 7.645  time: 0.2057(77.77)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 612/1000  consumed samples: 9792  total_loss: 7.646  time: 0.2057(77.77)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 613/1000  consumed samples: 9808  total_loss: 7.646  time: 0.2057(77.77)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:04] lb.utils.events INFO:  eta: 0:01:20  iteration: 614/1000  consumed samples: 9824  total_loss: 7.645  time: 0.2058(77.76)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:05] lb.utils.events INFO:  eta: 0:01:20  iteration: 615/1000  consumed samples: 9840  total_loss: 7.644  time: 0.2058(77.76)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:05] lb.utils.events INFO:  eta: 0:01:20  iteration: 616/1000  consumed samples: 9856  total_loss: 7.644  time: 0.2058(77.76)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 617/1000  consumed samples: 9872  total_loss: 7.643  time: 0.2058(77.76)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:05] lb.utils.events INFO:  eta: 0:01:19  iteration: 618/1000  consumed samples: 9888  total_loss: 7.641  time: 0.2058(77.75)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:06] lb.utils.events INFO:  eta: 0:01:19  iteration: 619/1000  consumed samples: 9904  total_loss: 7.641  time: 0.2058(77.75)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:06] lb.utils.events INFO:  eta: 0:01:19  iteration: 620/1000  consumed samples: 9920  total_loss: 7.641  time: 0.2058(77.75)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:06] lb.utils.events INFO:  eta: 0:01:19  iteration: 621/1000  consumed samples: 9936  total_loss: 7.639  time: 0.2058(77.74)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:06] lb.utils.events INFO:  eta: 0:01:18  iteration: 622/1000  consumed samples: 9952  total_loss: 7.639  time: 0.2058(77.74)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:06] lb.utils.events INFO:  eta: 0:01:18  iteration: 623/1000  consumed samples: 9968  total_loss: 7.638  time: 0.2058(77.74)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:07] lb.utils.events INFO:  eta: 0:01:18  iteration: 624/1000  consumed samples: 9984  total_loss: 7.638  time: 0.2058(77.73)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:07] lb.utils.events INFO:  eta: 0:01:18  iteration: 625/1000  consumed samples: 10000  total_loss: 7.638  time: 0.2058(77.73)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:07] lb.utils.events INFO:  eta: 0:01:18  iteration: 626/1000  consumed samples: 10016  total_loss: 7.637  time: 0.2059(77.72)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:07] lb.utils.events INFO:  eta: 0:01:17  iteration: 627/1000  consumed samples: 10032  total_loss: 7.637  time: 0.2059(77.72)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:08] lb.utils.events INFO:  eta: 0:01:17  iteration: 628/1000  consumed samples: 10048  total_loss: 7.637  time: 0.2059(77.71)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:08] lb.utils.events INFO:  eta: 0:01:17  iteration: 629/1000  consumed samples: 10064  total_loss: 7.636  time: 0.2059(77.71)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:08] lb.utils.events INFO:  eta: 0:01:17  iteration: 630/1000  consumed samples: 10080  total_loss: 7.636  time: 0.2059(77.71)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:08] lb.utils.events INFO:  eta: 0:01:16  iteration: 631/1000  consumed samples: 10096  total_loss: 7.636  time: 0.2059(77.70)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:08] lb.utils.events INFO:  eta: 0:01:16  iteration: 632/1000  consumed samples: 10112  total_loss: 7.636  time: 0.2059(77.70)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:09] lb.utils.events INFO:  eta: 0:01:16  iteration: 633/1000  consumed samples: 10128  total_loss: 7.636  time: 0.2059(77.70)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:09] lb.utils.events INFO:  eta: 0:01:16  iteration: 634/1000  consumed samples: 10144  total_loss: 7.634  time: 0.2059(77.69)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:09] lb.utils.events INFO:  eta: 0:01:16  iteration: 635/1000  consumed samples: 10160  total_loss: 7.633  time: 0.2060(77.69)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:09] lb.utils.events INFO:  eta: 0:01:15  iteration: 636/1000  consumed samples: 10176  total_loss: 7.632  time: 0.2060(77.68)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:09] lb.utils.events INFO:  eta: 0:01:15  iteration: 637/1000  consumed samples: 10192  total_loss: 7.633  time: 0.2060(77.68)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:10] lb.utils.events INFO:  eta: 0:01:15  iteration: 638/1000  consumed samples: 10208  total_loss: 7.634  time: 0.2060(77.67)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:10] lb.utils.events INFO:  eta: 0:01:15  iteration: 639/1000  consumed samples: 10224  total_loss: 7.633  time: 0.2060(77.67)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:10] lb.utils.events INFO:  eta: 0:01:15  iteration: 640/1000  consumed samples: 10240  total_loss: 7.632  time: 0.2060(77.67)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:10] lb.utils.events INFO:  eta: 0:01:14  iteration: 641/1000  consumed samples: 10256  total_loss: 7.632  time: 0.2060(77.66)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 642/1000  consumed samples: 10272  total_loss: 7.631  time: 0.2060(77.66)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 643/1000  consumed samples: 10288  total_loss: 7.63  time: 0.2060(77.65)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 644/1000  consumed samples: 10304  total_loss: 7.628  time: 0.2061(77.65)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:11] lb.utils.events INFO:  eta: 0:01:14  iteration: 645/1000  consumed samples: 10320  total_loss: 7.628  time: 0.2060(77.66)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:11] lb.utils.events INFO:  eta: 0:01:13  iteration: 646/1000  consumed samples: 10336  total_loss: 7.626  time: 0.2059(77.71)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 647/1000  consumed samples: 10352  total_loss: 7.624  time: 0.2058(77.76)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 648/1000  consumed samples: 10368  total_loss: 7.624  time: 0.2056(77.82)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 649/1000  consumed samples: 10384  total_loss: 7.624  time: 0.2054(77.88)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:12] lb.utils.events INFO:  eta: 0:01:13  iteration: 650/1000  consumed samples: 10400  total_loss: 7.622  time: 0.2053(77.94)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 651/1000  consumed samples: 10416  total_loss: 7.62  time: 0.2053(77.94)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 652/1000  consumed samples: 10432  total_loss: 7.618  time: 0.2053(77.93)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 653/1000  consumed samples: 10448  total_loss: 7.618  time: 0.2053(77.93)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:13] lb.utils.events INFO:  eta: 0:01:12  iteration: 654/1000  consumed samples: 10464  total_loss: 7.618  time: 0.2053(77.93)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:13] lb.utils.events INFO:  eta: 0:01:11  iteration: 655/1000  consumed samples: 10480  total_loss: 7.617  time: 0.2053(77.92)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 656/1000  consumed samples: 10496  total_loss: 7.618  time: 0.2053(77.92)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 657/1000  consumed samples: 10512  total_loss: 7.618  time: 0.2054(77.91)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 658/1000  consumed samples: 10528  total_loss: 7.617  time: 0.2054(77.91)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:14] lb.utils.events INFO:  eta: 0:01:11  iteration: 659/1000  consumed samples: 10544  total_loss: 7.615  time: 0.2054(77.91)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 660/1000  consumed samples: 10560  total_loss: 7.615  time: 0.2054(77.90)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 661/1000  consumed samples: 10576  total_loss: 7.615  time: 0.2054(77.90)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 662/1000  consumed samples: 10592  total_loss: 7.614  time: 0.2054(77.89)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 663/1000  consumed samples: 10608  total_loss: 7.614  time: 0.2054(77.89)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:15] lb.utils.events INFO:  eta: 0:01:10  iteration: 664/1000  consumed samples: 10624  total_loss: 7.614  time: 0.2054(77.88)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 665/1000  consumed samples: 10640  total_loss: 7.614  time: 0.2054(77.88)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 666/1000  consumed samples: 10656  total_loss: 7.611  time: 0.2055(77.87)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 667/1000  consumed samples: 10672  total_loss: 7.611  time: 0.2055(77.87)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:16] lb.utils.events INFO:  eta: 0:01:09  iteration: 668/1000  consumed samples: 10688  total_loss: 7.607  time: 0.2055(77.86)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:17] lb.utils.events INFO:  eta: 0:01:09  iteration: 669/1000  consumed samples: 10704  total_loss: 7.607  time: 0.2055(77.85)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 670/1000  consumed samples: 10720  total_loss: 7.607  time: 0.2055(77.85)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 671/1000  consumed samples: 10736  total_loss: 7.606  time: 0.2055(77.84)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 672/1000  consumed samples: 10752  total_loss: 7.604  time: 0.2056(77.84)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:17] lb.utils.events INFO:  eta: 0:01:08  iteration: 673/1000  consumed samples: 10768  total_loss: 7.603  time: 0.2056(77.83)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:18] lb.utils.events INFO:  eta: 0:01:08  iteration: 674/1000  consumed samples: 10784  total_loss: 7.604  time: 0.2056(77.84)  data_time: 0.0069  lr: 9.00e-05  
[01/20 10:40:18] lb.utils.events INFO:  eta: 0:01:07  iteration: 675/1000  consumed samples: 10800  total_loss: 7.604  time: 0.2056(77.83)  data_time: 0.0070  lr: 9.00e-05  
[01/20 10:40:18] lb.utils.events INFO:  eta: 0:01:07  iteration: 676/1000  consumed samples: 10816  total_loss: 7.603  time: 0.2056(77.83)  data_time: 0.0070  lr: 9.00e-05  
[01/20 10:40:18] lb.utils.events INFO:  eta: 0:01:07  iteration: 677/1000  consumed samples: 10832  total_loss: 7.603  time: 0.2056(77.83)  data_time: 0.0069  lr: 9.00e-05  
[01/20 10:40:19] lb.utils.events INFO:  eta: 0:01:07  iteration: 678/1000  consumed samples: 10848  total_loss: 7.603  time: 0.2056(77.82)  data_time: 0.0069  lr: 9.00e-05  
[01/20 10:40:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 679/1000  consumed samples: 10864  total_loss: 7.602  time: 0.2056(77.82)  data_time: 0.0069  lr: 9.00e-05  
[01/20 10:40:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 680/1000  consumed samples: 10880  total_loss: 7.599  time: 0.2056(77.82)  data_time: 0.0070  lr: 9.00e-05  
[01/20 10:40:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 681/1000  consumed samples: 10896  total_loss: 7.599  time: 0.2056(77.81)  data_time: 0.0070  lr: 9.00e-05  
[01/20 10:40:19] lb.utils.events INFO:  eta: 0:01:06  iteration: 682/1000  consumed samples: 10912  total_loss: 7.596  time: 0.2056(77.81)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:20] lb.utils.events INFO:  eta: 0:01:06  iteration: 683/1000  consumed samples: 10928  total_loss: 7.596  time: 0.2056(77.80)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:20] lb.utils.events INFO:  eta: 0:01:05  iteration: 684/1000  consumed samples: 10944  total_loss: 7.596  time: 0.2057(77.80)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:20] lb.utils.events INFO:  eta: 0:01:05  iteration: 685/1000  consumed samples: 10960  total_loss: 7.599  time: 0.2057(77.80)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:20] lb.utils.events INFO:  eta: 0:01:05  iteration: 686/1000  consumed samples: 10976  total_loss: 7.596  time: 0.2057(77.79)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:21] lb.utils.events INFO:  eta: 0:01:05  iteration: 687/1000  consumed samples: 10992  total_loss: 7.596  time: 0.2057(77.78)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:21] lb.utils.events INFO:  eta: 0:01:05  iteration: 688/1000  consumed samples: 11008  total_loss: 7.591  time: 0.2057(77.78)  data_time: 0.0071  lr: 9.00e-05  
[01/20 10:40:21] lb.utils.events INFO:  eta: 0:01:04  iteration: 689/1000  consumed samples: 11024  total_loss: 7.596  time: 0.2057(77.78)  data_time: 0.0072  lr: 9.00e-05  
[01/20 10:40:21] lb.utils.events INFO:  eta: 0:01:04  iteration: 690/1000  consumed samples: 11040  total_loss: 7.591  time: 0.2057(77.77)  data_time: 0.0072  lr: 9.00e-05  
[01/20 10:40:21] lb.utils.events INFO:  eta: 0:01:04  iteration: 691/1000  consumed samples: 11056  total_loss: 7.584  time: 0.2057(77.77)  data_time: 0.0072  lr: 9.00e-05  
[01/20 10:40:22] lb.utils.events INFO:  eta: 0:01:04  iteration: 692/1000  consumed samples: 11072  total_loss: 7.581  time: 0.2058(77.76)  data_time: 0.0073  lr: 9.00e-05  
[01/20 10:40:22] lb.utils.events INFO:  eta: 0:01:04  iteration: 693/1000  consumed samples: 11088  total_loss: 7.581  time: 0.2058(77.76)  data_time: 0.0073  lr: 9.00e-05  
[01/20 10:40:22] lb.utils.events INFO:  eta: 0:01:03  iteration: 694/1000  consumed samples: 11104  total_loss: 7.58  time: 0.2057(77.76)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:22] lb.utils.events INFO:  eta: 0:01:03  iteration: 695/1000  consumed samples: 11120  total_loss: 7.58  time: 0.2057(77.76)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:23] lb.utils.events INFO:  eta: 0:01:03  iteration: 696/1000  consumed samples: 11136  total_loss: 7.58  time: 0.2058(77.76)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:23] lb.utils.events INFO:  eta: 0:01:03  iteration: 697/1000  consumed samples: 11152  total_loss: 7.579  time: 0.2058(77.76)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:23] lb.utils.events INFO:  eta: 0:01:03  iteration: 698/1000  consumed samples: 11168  total_loss: 7.579  time: 0.2058(77.76)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:40:23] lb.utils.events INFO:  eta: 0:01:02  iteration: 699/1000  consumed samples: 11184  total_loss: 7.579  time: 0.2058(77.75)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:40:23] lb.utils.events INFO:  eta: 0:01:02  iteration: 700/1000  consumed samples: 11200  total_loss: 7.577  time: 0.2058(77.75)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:40:24] lb.utils.events INFO:  eta: 0:01:02  iteration: 701/1000  consumed samples: 11216  total_loss: 7.575  time: 0.2058(77.74)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:24] lb.utils.events INFO:  eta: 0:01:02  iteration: 702/1000  consumed samples: 11232  total_loss: 7.573  time: 0.2058(77.74)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:24] lb.utils.events INFO:  eta: 0:01:01  iteration: 703/1000  consumed samples: 11248  total_loss: 7.57  time: 0.2058(77.73)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:24] lb.utils.events INFO:  eta: 0:01:01  iteration: 704/1000  consumed samples: 11264  total_loss: 7.57  time: 0.2058(77.73)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 705/1000  consumed samples: 11280  total_loss: 7.567  time: 0.2059(77.72)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:40:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 706/1000  consumed samples: 11296  total_loss: 7.566  time: 0.2059(77.72)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:25] lb.utils.events INFO:  eta: 0:01:01  iteration: 707/1000  consumed samples: 11312  total_loss: 7.566  time: 0.2059(77.72)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:25] lb.utils.events INFO:  eta: 0:01:00  iteration: 708/1000  consumed samples: 11328  total_loss: 7.566  time: 0.2059(77.71)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:25] lb.utils.events INFO:  eta: 0:01:00  iteration: 709/1000  consumed samples: 11344  total_loss: 7.564  time: 0.2059(77.71)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 710/1000  consumed samples: 11360  total_loss: 7.565  time: 0.2059(77.70)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 711/1000  consumed samples: 11376  total_loss: 7.565  time: 0.2059(77.70)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:26] lb.utils.events INFO:  eta: 0:01:00  iteration: 712/1000  consumed samples: 11392  total_loss: 7.564  time: 0.2059(77.70)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:40:26] lb.utils.events INFO:  eta: 0:00:59  iteration: 713/1000  consumed samples: 11408  total_loss: 7.564  time: 0.2059(77.69)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 714/1000  consumed samples: 11424  total_loss: 7.564  time: 0.2059(77.69)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 715/1000  consumed samples: 11440  total_loss: 7.564  time: 0.2060(77.68)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 716/1000  consumed samples: 11456  total_loss: 7.564  time: 0.2060(77.68)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:27] lb.utils.events INFO:  eta: 0:00:59  iteration: 717/1000  consumed samples: 11472  total_loss: 7.564  time: 0.2060(77.68)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:27] lb.utils.events INFO:  eta: 0:00:58  iteration: 718/1000  consumed samples: 11488  total_loss: 7.563  time: 0.2060(77.68)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 719/1000  consumed samples: 11504  total_loss: 7.563  time: 0.2060(77.67)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 720/1000  consumed samples: 11520  total_loss: 7.563  time: 0.2060(77.67)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:28] lb.utils.events INFO:  eta: 0:00:58  iteration: 721/1000  consumed samples: 11536  total_loss: 7.563  time: 0.2060(77.68)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:28] lb.utils.events INFO:  eta: 0:00:57  iteration: 722/1000  consumed samples: 11552  total_loss: 7.562  time: 0.2060(77.68)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 723/1000  consumed samples: 11568  total_loss: 7.562  time: 0.2060(77.67)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 724/1000  consumed samples: 11584  total_loss: 7.562  time: 0.2060(77.67)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 725/1000  consumed samples: 11600  total_loss: 7.561  time: 0.2060(77.66)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:29] lb.utils.events INFO:  eta: 0:00:57  iteration: 726/1000  consumed samples: 11616  total_loss: 7.561  time: 0.2060(77.66)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:29] lb.utils.events INFO:  eta: 0:00:56  iteration: 727/1000  consumed samples: 11632  total_loss: 7.561  time: 0.2060(77.66)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 728/1000  consumed samples: 11648  total_loss: 7.561  time: 0.2060(77.66)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 729/1000  consumed samples: 11664  total_loss: 7.561  time: 0.2060(77.67)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 730/1000  consumed samples: 11680  total_loss: 7.557  time: 0.2060(77.67)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:30] lb.utils.events INFO:  eta: 0:00:56  iteration: 731/1000  consumed samples: 11696  total_loss: 7.557  time: 0.2060(77.66)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 732/1000  consumed samples: 11712  total_loss: 7.553  time: 0.2060(77.66)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 733/1000  consumed samples: 11728  total_loss: 7.551  time: 0.2060(77.66)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 734/1000  consumed samples: 11744  total_loss: 7.551  time: 0.2060(77.65)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 735/1000  consumed samples: 11760  total_loss: 7.551  time: 0.2061(77.65)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:31] lb.utils.events INFO:  eta: 0:00:55  iteration: 736/1000  consumed samples: 11776  total_loss: 7.551  time: 0.2061(77.64)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 737/1000  consumed samples: 11792  total_loss: 7.551  time: 0.2061(77.64)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 738/1000  consumed samples: 11808  total_loss: 7.551  time: 0.2061(77.64)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 739/1000  consumed samples: 11824  total_loss: 7.551  time: 0.2061(77.64)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:32] lb.utils.events INFO:  eta: 0:00:54  iteration: 740/1000  consumed samples: 11840  total_loss: 7.55  time: 0.2061(77.63)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:33] lb.utils.events INFO:  eta: 0:00:54  iteration: 741/1000  consumed samples: 11856  total_loss: 7.549  time: 0.2061(77.63)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 742/1000  consumed samples: 11872  total_loss: 7.549  time: 0.2061(77.62)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 743/1000  consumed samples: 11888  total_loss: 7.549  time: 0.2061(77.62)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 744/1000  consumed samples: 11904  total_loss: 7.549  time: 0.2061(77.62)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:33] lb.utils.events INFO:  eta: 0:00:53  iteration: 745/1000  consumed samples: 11920  total_loss: 7.55  time: 0.2062(77.61)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:34] lb.utils.events INFO:  eta: 0:00:52  iteration: 746/1000  consumed samples: 11936  total_loss: 7.55  time: 0.2061(77.62)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:34] lb.utils.events INFO:  eta: 0:00:52  iteration: 747/1000  consumed samples: 11952  total_loss: 7.551  time: 0.2062(77.61)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:34] lb.utils.events INFO:  eta: 0:00:52  iteration: 748/1000  consumed samples: 11968  total_loss: 7.551  time: 0.2062(77.61)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:34] lb.utils.events INFO:  eta: 0:00:52  iteration: 749/1000  consumed samples: 11984  total_loss: 7.551  time: 0.2062(77.61)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:35] lb.utils.events INFO:  eta: 0:00:52  iteration: 750/1000  consumed samples: 12000  total_loss: 7.55  time: 0.2062(77.60)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:35] lb.utils.events INFO:  eta: 0:00:51  iteration: 751/1000  consumed samples: 12016  total_loss: 7.55  time: 0.2062(77.60)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:35] lb.utils.events INFO:  eta: 0:00:51  iteration: 752/1000  consumed samples: 12032  total_loss: 7.549  time: 0.2062(77.60)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:35] lb.utils.events INFO:  eta: 0:00:51  iteration: 753/1000  consumed samples: 12048  total_loss: 7.548  time: 0.2062(77.59)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:35] lb.utils.events INFO:  eta: 0:00:51  iteration: 754/1000  consumed samples: 12064  total_loss: 7.549  time: 0.2062(77.59)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:36] lb.utils.events INFO:  eta: 0:00:51  iteration: 755/1000  consumed samples: 12080  total_loss: 7.548  time: 0.2062(77.58)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:36] lb.utils.events INFO:  eta: 0:00:50  iteration: 756/1000  consumed samples: 12096  total_loss: 7.547  time: 0.2062(77.58)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:36] lb.utils.events INFO:  eta: 0:00:50  iteration: 757/1000  consumed samples: 12112  total_loss: 7.546  time: 0.2062(77.58)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:36] lb.utils.events INFO:  eta: 0:00:50  iteration: 758/1000  consumed samples: 12128  total_loss: 7.545  time: 0.2062(77.58)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:36] lb.utils.events INFO:  eta: 0:00:50  iteration: 759/1000  consumed samples: 12144  total_loss: 7.546  time: 0.2062(77.58)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:37] lb.utils.events INFO:  eta: 0:00:50  iteration: 760/1000  consumed samples: 12160  total_loss: 7.545  time: 0.2063(77.57)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:37] lb.utils.events INFO:  eta: 0:00:49  iteration: 761/1000  consumed samples: 12176  total_loss: 7.545  time: 0.2063(77.57)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:37] lb.utils.events INFO:  eta: 0:00:49  iteration: 762/1000  consumed samples: 12192  total_loss: 7.543  time: 0.2063(77.57)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:37] lb.utils.events INFO:  eta: 0:00:49  iteration: 763/1000  consumed samples: 12208  total_loss: 7.541  time: 0.2063(77.56)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:38] lb.utils.events INFO:  eta: 0:00:49  iteration: 764/1000  consumed samples: 12224  total_loss: 7.539  time: 0.2063(77.56)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:38] lb.utils.events INFO:  eta: 0:00:49  iteration: 765/1000  consumed samples: 12240  total_loss: 7.539  time: 0.2063(77.56)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:38] lb.utils.events INFO:  eta: 0:00:48  iteration: 766/1000  consumed samples: 12256  total_loss: 7.539  time: 0.2063(77.55)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:38] lb.utils.events INFO:  eta: 0:00:48  iteration: 767/1000  consumed samples: 12272  total_loss: 7.539  time: 0.2063(77.55)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:38] lb.utils.events INFO:  eta: 0:00:48  iteration: 768/1000  consumed samples: 12288  total_loss: 7.539  time: 0.2063(77.55)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:39] lb.utils.events INFO:  eta: 0:00:48  iteration: 769/1000  consumed samples: 12304  total_loss: 7.537  time: 0.2063(77.55)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:40:39] lb.utils.events INFO:  eta: 0:00:47  iteration: 770/1000  consumed samples: 12320  total_loss: 7.539  time: 0.2063(77.54)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:40:39] lb.utils.events INFO:  eta: 0:00:47  iteration: 771/1000  consumed samples: 12336  total_loss: 7.537  time: 0.2064(77.54)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:40:39] lb.utils.events INFO:  eta: 0:00:47  iteration: 772/1000  consumed samples: 12352  total_loss: 7.537  time: 0.2064(77.53)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:40:40] lb.utils.events INFO:  eta: 0:00:47  iteration: 773/1000  consumed samples: 12368  total_loss: 7.537  time: 0.2064(77.53)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:40:40] lb.utils.events INFO:  eta: 0:00:47  iteration: 774/1000  consumed samples: 12384  total_loss: 7.537  time: 0.2064(77.53)  data_time: 0.0027  lr: 9.00e-05  
[01/20 10:40:40] lb.utils.events INFO:  eta: 0:00:46  iteration: 775/1000  consumed samples: 12400  total_loss: 7.535  time: 0.2064(77.53)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:40] lb.utils.events INFO:  eta: 0:00:46  iteration: 776/1000  consumed samples: 12416  total_loss: 7.535  time: 0.2064(77.52)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:40] lb.utils.events INFO:  eta: 0:00:46  iteration: 777/1000  consumed samples: 12432  total_loss: 7.534  time: 0.2064(77.52)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:41] lb.utils.events INFO:  eta: 0:00:46  iteration: 778/1000  consumed samples: 12448  total_loss: 7.533  time: 0.2064(77.52)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:41] lb.utils.events INFO:  eta: 0:00:46  iteration: 779/1000  consumed samples: 12464  total_loss: 7.533  time: 0.2064(77.52)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:41] lb.utils.events INFO:  eta: 0:00:45  iteration: 780/1000  consumed samples: 12480  total_loss: 7.533  time: 0.2064(77.51)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:41] lb.utils.events INFO:  eta: 0:00:45  iteration: 781/1000  consumed samples: 12496  total_loss: 7.533  time: 0.2064(77.51)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:42] lb.utils.events INFO:  eta: 0:00:45  iteration: 782/1000  consumed samples: 12512  total_loss: 7.532  time: 0.2064(77.50)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:42] lb.utils.events INFO:  eta: 0:00:45  iteration: 783/1000  consumed samples: 12528  total_loss: 7.533  time: 0.2064(77.50)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:42] lb.utils.events INFO:  eta: 0:00:45  iteration: 784/1000  consumed samples: 12544  total_loss: 7.533  time: 0.2064(77.50)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:42] lb.utils.events INFO:  eta: 0:00:44  iteration: 785/1000  consumed samples: 12560  total_loss: 7.533  time: 0.2065(77.50)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:42] lb.utils.events INFO:  eta: 0:00:44  iteration: 786/1000  consumed samples: 12576  total_loss: 7.533  time: 0.2065(77.50)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:43] lb.utils.events INFO:  eta: 0:00:44  iteration: 787/1000  consumed samples: 12592  total_loss: 7.533  time: 0.2065(77.49)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:43] lb.utils.events INFO:  eta: 0:00:44  iteration: 788/1000  consumed samples: 12608  total_loss: 7.533  time: 0.2065(77.49)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:43] lb.utils.events INFO:  eta: 0:00:43  iteration: 789/1000  consumed samples: 12624  total_loss: 7.533  time: 0.2065(77.49)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:43] lb.utils.events INFO:  eta: 0:00:43  iteration: 790/1000  consumed samples: 12640  total_loss: 7.533  time: 0.2065(77.49)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:44] lb.utils.events INFO:  eta: 0:00:43  iteration: 791/1000  consumed samples: 12656  total_loss: 7.532  time: 0.2065(77.48)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:44] lb.utils.events INFO:  eta: 0:00:43  iteration: 792/1000  consumed samples: 12672  total_loss: 7.532  time: 0.2065(77.48)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:44] lb.utils.events INFO:  eta: 0:00:43  iteration: 793/1000  consumed samples: 12688  total_loss: 7.531  time: 0.2065(77.48)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:44] lb.utils.events INFO:  eta: 0:00:42  iteration: 794/1000  consumed samples: 12704  total_loss: 7.53  time: 0.2065(77.48)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:44] lb.utils.events INFO:  eta: 0:00:42  iteration: 795/1000  consumed samples: 12720  total_loss: 7.531  time: 0.2065(77.47)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:45] lb.utils.events INFO:  eta: 0:00:42  iteration: 796/1000  consumed samples: 12736  total_loss: 7.531  time: 0.2065(77.47)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:45] lb.utils.events INFO:  eta: 0:00:42  iteration: 797/1000  consumed samples: 12752  total_loss: 7.531  time: 0.2065(77.47)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:45] lb.utils.events INFO:  eta: 0:00:42  iteration: 798/1000  consumed samples: 12768  total_loss: 7.53  time: 0.2065(77.47)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:45] lb.utils.events INFO:  eta: 0:00:41  iteration: 799/1000  consumed samples: 12784  total_loss: 7.53  time: 0.2066(77.46)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:46] lb.utils.events INFO:  eta: 0:00:41  iteration: 800/1000  consumed samples: 12800  total_loss: 7.53  time: 0.2066(77.46)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:46] lb.utils.events INFO:  eta: 0:00:41  iteration: 801/1000  consumed samples: 12816  total_loss: 7.53  time: 0.2066(77.45)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:46] lb.utils.events INFO:  eta: 0:00:41  iteration: 802/1000  consumed samples: 12832  total_loss: 7.53  time: 0.2066(77.45)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:46] lb.utils.events INFO:  eta: 0:00:41  iteration: 803/1000  consumed samples: 12848  total_loss: 7.529  time: 0.2066(77.46)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:46] lb.utils.events INFO:  eta: 0:00:40  iteration: 804/1000  consumed samples: 12864  total_loss: 7.528  time: 0.2066(77.45)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:47] lb.utils.events INFO:  eta: 0:00:40  iteration: 805/1000  consumed samples: 12880  total_loss: 7.526  time: 0.2066(77.45)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:47] lb.utils.events INFO:  eta: 0:00:40  iteration: 806/1000  consumed samples: 12896  total_loss: 7.526  time: 0.2066(77.45)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:47] lb.utils.events INFO:  eta: 0:00:40  iteration: 807/1000  consumed samples: 12912  total_loss: 7.526  time: 0.2066(77.44)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:47] lb.utils.events INFO:  eta: 0:00:40  iteration: 808/1000  consumed samples: 12928  total_loss: 7.528  time: 0.2066(77.44)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:48] lb.utils.events INFO:  eta: 0:00:39  iteration: 809/1000  consumed samples: 12944  total_loss: 7.528  time: 0.2066(77.44)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:48] lb.utils.events INFO:  eta: 0:00:39  iteration: 810/1000  consumed samples: 12960  total_loss: 7.526  time: 0.2066(77.44)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:48] lb.utils.events INFO:  eta: 0:00:39  iteration: 811/1000  consumed samples: 12976  total_loss: 7.526  time: 0.2066(77.43)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:40:48] lb.utils.events INFO:  eta: 0:00:39  iteration: 812/1000  consumed samples: 12992  total_loss: 7.525  time: 0.2066(77.43)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:48] lb.utils.events INFO:  eta: 0:00:38  iteration: 813/1000  consumed samples: 13008  total_loss: 7.525  time: 0.2067(77.43)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:40:49] lb.utils.events INFO:  eta: 0:00:38  iteration: 814/1000  consumed samples: 13024  total_loss: 7.525  time: 0.2067(77.42)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:40:49] lb.utils.events INFO:  eta: 0:00:38  iteration: 815/1000  consumed samples: 13040  total_loss: 7.525  time: 0.2067(77.42)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:49] lb.utils.events INFO:  eta: 0:00:38  iteration: 816/1000  consumed samples: 13056  total_loss: 7.525  time: 0.2067(77.42)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:40:49] lb.utils.events INFO:  eta: 0:00:38  iteration: 817/1000  consumed samples: 13072  total_loss: 7.525  time: 0.2067(77.41)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:50] lb.utils.events INFO:  eta: 0:00:37  iteration: 818/1000  consumed samples: 13088  total_loss: 7.525  time: 0.2067(77.41)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:50] lb.utils.events INFO:  eta: 0:00:37  iteration: 819/1000  consumed samples: 13104  total_loss: 7.524  time: 0.2067(77.41)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:50] lb.utils.events INFO:  eta: 0:00:37  iteration: 820/1000  consumed samples: 13120  total_loss: 7.524  time: 0.2067(77.41)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:50] lb.utils.events INFO:  eta: 0:00:37  iteration: 821/1000  consumed samples: 13136  total_loss: 7.524  time: 0.2067(77.40)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:50] lb.utils.events INFO:  eta: 0:00:37  iteration: 822/1000  consumed samples: 13152  total_loss: 7.524  time: 0.2067(77.40)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:51] lb.utils.events INFO:  eta: 0:00:36  iteration: 823/1000  consumed samples: 13168  total_loss: 7.524  time: 0.2067(77.40)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:51] lb.utils.events INFO:  eta: 0:00:36  iteration: 824/1000  consumed samples: 13184  total_loss: 7.525  time: 0.2067(77.40)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:51] lb.utils.events INFO:  eta: 0:00:36  iteration: 825/1000  consumed samples: 13200  total_loss: 7.524  time: 0.2067(77.40)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:51] lb.utils.events INFO:  eta: 0:00:36  iteration: 826/1000  consumed samples: 13216  total_loss: 7.523  time: 0.2067(77.40)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:52] lb.utils.events INFO:  eta: 0:00:36  iteration: 827/1000  consumed samples: 13232  total_loss: 7.523  time: 0.2067(77.39)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:52] lb.utils.events INFO:  eta: 0:00:35  iteration: 828/1000  consumed samples: 13248  total_loss: 7.521  time: 0.2067(77.39)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:52] lb.utils.events INFO:  eta: 0:00:35  iteration: 829/1000  consumed samples: 13264  total_loss: 7.521  time: 0.2068(77.39)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:52] lb.utils.events INFO:  eta: 0:00:35  iteration: 830/1000  consumed samples: 13280  total_loss: 7.52  time: 0.2068(77.38)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:52] lb.utils.events INFO:  eta: 0:00:35  iteration: 831/1000  consumed samples: 13296  total_loss: 7.519  time: 0.2068(77.38)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:53] lb.utils.events INFO:  eta: 0:00:34  iteration: 832/1000  consumed samples: 13312  total_loss: 7.518  time: 0.2068(77.38)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:53] lb.utils.events INFO:  eta: 0:00:34  iteration: 833/1000  consumed samples: 13328  total_loss: 7.518  time: 0.2068(77.38)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:53] lb.utils.events INFO:  eta: 0:00:34  iteration: 834/1000  consumed samples: 13344  total_loss: 7.516  time: 0.2068(77.37)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:53] lb.utils.events INFO:  eta: 0:00:34  iteration: 835/1000  consumed samples: 13360  total_loss: 7.516  time: 0.2068(77.37)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:54] lb.utils.events INFO:  eta: 0:00:34  iteration: 836/1000  consumed samples: 13376  total_loss: 7.518  time: 0.2068(77.37)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:54] lb.utils.events INFO:  eta: 0:00:33  iteration: 837/1000  consumed samples: 13392  total_loss: 7.518  time: 0.2068(77.36)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:54] lb.utils.events INFO:  eta: 0:00:33  iteration: 838/1000  consumed samples: 13408  total_loss: 7.518  time: 0.2068(77.36)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:54] lb.utils.events INFO:  eta: 0:00:33  iteration: 839/1000  consumed samples: 13424  total_loss: 7.519  time: 0.2068(77.36)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:54] lb.utils.events INFO:  eta: 0:00:33  iteration: 840/1000  consumed samples: 13440  total_loss: 7.518  time: 0.2068(77.35)  data_time: 0.0028  lr: 9.00e-05  
[01/20 10:40:55] lb.utils.events INFO:  eta: 0:00:33  iteration: 841/1000  consumed samples: 13456  total_loss: 7.516  time: 0.2068(77.35)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:55] lb.utils.events INFO:  eta: 0:00:32  iteration: 842/1000  consumed samples: 13472  total_loss: 7.516  time: 0.2069(77.35)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:55] lb.utils.events INFO:  eta: 0:00:32  iteration: 843/1000  consumed samples: 13488  total_loss: 7.516  time: 0.2069(77.35)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:55] lb.utils.events INFO:  eta: 0:00:32  iteration: 844/1000  consumed samples: 13504  total_loss: 7.516  time: 0.2069(77.34)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:56] lb.utils.events INFO:  eta: 0:00:32  iteration: 845/1000  consumed samples: 13520  total_loss: 7.515  time: 0.2069(77.34)  data_time: 0.0029  lr: 9.00e-05  
[01/20 10:40:56] lb.utils.events INFO:  eta: 0:00:32  iteration: 846/1000  consumed samples: 13536  total_loss: 7.513  time: 0.2069(77.34)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:56] lb.utils.events INFO:  eta: 0:00:31  iteration: 847/1000  consumed samples: 13552  total_loss: 7.51  time: 0.2069(77.34)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:56] lb.utils.events INFO:  eta: 0:00:31  iteration: 848/1000  consumed samples: 13568  total_loss: 7.51  time: 0.2069(77.33)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:56] lb.utils.events INFO:  eta: 0:00:31  iteration: 849/1000  consumed samples: 13584  total_loss: 7.51  time: 0.2069(77.33)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:57] lb.utils.events INFO:  eta: 0:00:31  iteration: 850/1000  consumed samples: 13600  total_loss: 7.51  time: 0.2069(77.33)  data_time: 0.0030  lr: 9.00e-05  
[01/20 10:40:57] lb.utils.events INFO:  eta: 0:00:31  iteration: 851/1000  consumed samples: 13616  total_loss: 7.513  time: 0.2069(77.33)  data_time: 0.0031  lr: 9.00e-05  
[01/20 10:40:57] lb.utils.events INFO:  eta: 0:00:30  iteration: 852/1000  consumed samples: 13632  total_loss: 7.513  time: 0.2069(77.32)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:57] lb.utils.events INFO:  eta: 0:00:30  iteration: 853/1000  consumed samples: 13648  total_loss: 7.51  time: 0.2069(77.32)  data_time: 0.0032  lr: 9.00e-05  
[01/20 10:40:58] lb.utils.events INFO:  eta: 0:00:30  iteration: 854/1000  consumed samples: 13664  total_loss: 7.509  time: 0.2069(77.32)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:40:58] lb.utils.events INFO:  eta: 0:00:30  iteration: 855/1000  consumed samples: 13680  total_loss: 7.51  time: 0.2069(77.32)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:58] lb.utils.events INFO:  eta: 0:00:29  iteration: 856/1000  consumed samples: 13696  total_loss: 7.51  time: 0.2070(77.31)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:58] lb.utils.events INFO:  eta: 0:00:29  iteration: 857/1000  consumed samples: 13712  total_loss: 7.509  time: 0.2070(77.31)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:40:58] lb.utils.events INFO:  eta: 0:00:29  iteration: 858/1000  consumed samples: 13728  total_loss: 7.51  time: 0.2070(77.31)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:59] lb.utils.events INFO:  eta: 0:00:29  iteration: 859/1000  consumed samples: 13744  total_loss: 7.51  time: 0.2070(77.30)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:59] lb.utils.events INFO:  eta: 0:00:29  iteration: 860/1000  consumed samples: 13760  total_loss: 7.51  time: 0.2070(77.30)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:59] lb.utils.events INFO:  eta: 0:00:28  iteration: 861/1000  consumed samples: 13776  total_loss: 7.509  time: 0.2070(77.30)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:40:59] lb.utils.events INFO:  eta: 0:00:28  iteration: 862/1000  consumed samples: 13792  total_loss: 7.509  time: 0.2070(77.30)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:41:00] lb.utils.events INFO:  eta: 0:00:28  iteration: 863/1000  consumed samples: 13808  total_loss: 7.509  time: 0.2070(77.29)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:41:00] lb.utils.events INFO:  eta: 0:00:28  iteration: 864/1000  consumed samples: 13824  total_loss: 7.509  time: 0.2070(77.29)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:41:00] lb.utils.events INFO:  eta: 0:00:28  iteration: 865/1000  consumed samples: 13840  total_loss: 7.506  time: 0.2070(77.29)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:00] lb.utils.events INFO:  eta: 0:00:27  iteration: 866/1000  consumed samples: 13856  total_loss: 7.506  time: 0.2070(77.29)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:00] lb.utils.events INFO:  eta: 0:00:27  iteration: 867/1000  consumed samples: 13872  total_loss: 7.503  time: 0.2070(77.29)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:01] lb.utils.events INFO:  eta: 0:00:27  iteration: 868/1000  consumed samples: 13888  total_loss: 7.506  time: 0.2070(77.29)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:01] lb.utils.events INFO:  eta: 0:00:27  iteration: 869/1000  consumed samples: 13904  total_loss: 7.503  time: 0.2070(77.28)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:01] lb.utils.events INFO:  eta: 0:00:27  iteration: 870/1000  consumed samples: 13920  total_loss: 7.502  time: 0.2070(77.28)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:01] lb.utils.events INFO:  eta: 0:00:26  iteration: 871/1000  consumed samples: 13936  total_loss: 7.502  time: 0.2070(77.28)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:02] lb.utils.events INFO:  eta: 0:00:26  iteration: 872/1000  consumed samples: 13952  total_loss: 7.502  time: 0.2070(77.28)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:02] lb.utils.events INFO:  eta: 0:00:26  iteration: 873/1000  consumed samples: 13968  total_loss: 7.502  time: 0.2070(77.28)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:41:02] lb.utils.events INFO:  eta: 0:00:26  iteration: 874/1000  consumed samples: 13984  total_loss: 7.502  time: 0.2070(77.28)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:41:02] lb.utils.events INFO:  eta: 0:00:25  iteration: 875/1000  consumed samples: 14000  total_loss: 7.502  time: 0.2070(77.28)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:41:02] lb.utils.events INFO:  eta: 0:00:25  iteration: 876/1000  consumed samples: 14016  total_loss: 7.502  time: 0.2071(77.28)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:03] lb.utils.events INFO:  eta: 0:00:25  iteration: 877/1000  consumed samples: 14032  total_loss: 7.502  time: 0.2071(77.28)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:03] lb.utils.events INFO:  eta: 0:00:25  iteration: 878/1000  consumed samples: 14048  total_loss: 7.501  time: 0.2071(77.27)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:03] lb.utils.events INFO:  eta: 0:00:25  iteration: 879/1000  consumed samples: 14064  total_loss: 7.499  time: 0.2071(77.27)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:03] lb.utils.events INFO:  eta: 0:00:24  iteration: 880/1000  consumed samples: 14080  total_loss: 7.499  time: 0.2071(77.27)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:04] lb.utils.events INFO:  eta: 0:00:24  iteration: 881/1000  consumed samples: 14096  total_loss: 7.498  time: 0.2071(77.26)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:04] lb.utils.events INFO:  eta: 0:00:24  iteration: 882/1000  consumed samples: 14112  total_loss: 7.498  time: 0.2071(77.26)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:04] lb.utils.events INFO:  eta: 0:00:24  iteration: 883/1000  consumed samples: 14128  total_loss: 7.497  time: 0.2070(77.30)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:04] lb.utils.events INFO:  eta: 0:00:24  iteration: 884/1000  consumed samples: 14144  total_loss: 7.497  time: 0.2068(77.35)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:04] lb.utils.events INFO:  eta: 0:00:23  iteration: 885/1000  consumed samples: 14160  total_loss: 7.495  time: 0.2067(77.40)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:05] lb.utils.events INFO:  eta: 0:00:23  iteration: 886/1000  consumed samples: 14176  total_loss: 7.495  time: 0.2066(77.45)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:05] lb.utils.events INFO:  eta: 0:00:23  iteration: 887/1000  consumed samples: 14192  total_loss: 7.497  time: 0.2064(77.51)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:05] lb.utils.events INFO:  eta: 0:00:23  iteration: 888/1000  consumed samples: 14208  total_loss: 7.497  time: 0.2063(77.56)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:05] lb.utils.events INFO:  eta: 0:00:23  iteration: 889/1000  consumed samples: 14224  total_loss: 7.497  time: 0.2063(77.56)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:06] lb.utils.events INFO:  eta: 0:00:22  iteration: 890/1000  consumed samples: 14240  total_loss: 7.495  time: 0.2062(77.59)  data_time: 0.0033  lr: 9.00e-05  
[01/20 10:41:06] lb.utils.events INFO:  eta: 0:00:22  iteration: 891/1000  consumed samples: 14256  total_loss: 7.495  time: 0.2061(77.62)  data_time: 0.0034  lr: 9.00e-05  
[01/20 10:41:06] lb.utils.events INFO:  eta: 0:00:22  iteration: 892/1000  consumed samples: 14272  total_loss: 7.494  time: 0.2060(77.68)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:06] lb.utils.events INFO:  eta: 0:00:22  iteration: 893/1000  consumed samples: 14288  total_loss: 7.495  time: 0.2059(77.70)  data_time: 0.0035  lr: 9.00e-05  
[01/20 10:41:06] lb.utils.events INFO:  eta: 0:00:22  iteration: 894/1000  consumed samples: 14304  total_loss: 7.495  time: 0.2058(77.74)  data_time: 0.0036  lr: 9.00e-05  
[01/20 10:41:07] lb.utils.events INFO:  eta: 0:00:21  iteration: 895/1000  consumed samples: 14320  total_loss: 7.497  time: 0.2057(77.79)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:41:07] lb.utils.events INFO:  eta: 0:00:21  iteration: 896/1000  consumed samples: 14336  total_loss: 7.495  time: 0.2057(77.80)  data_time: 0.0037  lr: 9.00e-05  
[01/20 10:41:07] lb.utils.events INFO:  eta: 0:00:21  iteration: 897/1000  consumed samples: 14352  total_loss: 7.495  time: 0.2055(77.84)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:07] lb.utils.events INFO:  eta: 0:00:21  iteration: 898/1000  consumed samples: 14368  total_loss: 7.495  time: 0.2054(77.90)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:08] lb.utils.events INFO:  eta: 0:00:20  iteration: 899/1000  consumed samples: 14384  total_loss: 7.495  time: 0.2053(77.94)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:08] lb.utils.events INFO:  eta: 0:00:20  iteration: 900/1000  consumed samples: 14400  total_loss: 7.494  time: 0.2052(77.96)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:08] lb.utils.events INFO:  eta: 0:00:20  iteration: 901/1000  consumed samples: 14416  total_loss: 7.494  time: 0.2051(78.01)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:08] lb.utils.events INFO:  eta: 0:00:20  iteration: 902/1000  consumed samples: 14432  total_loss: 7.491  time: 0.2050(78.06)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:08] lb.utils.events INFO:  eta: 0:00:20  iteration: 903/1000  consumed samples: 14448  total_loss: 7.491  time: 0.2048(78.11)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:09] lb.utils.events INFO:  eta: 0:00:19  iteration: 904/1000  consumed samples: 14464  total_loss: 7.494  time: 0.2047(78.16)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:09] lb.utils.events INFO:  eta: 0:00:19  iteration: 905/1000  consumed samples: 14480  total_loss: 7.494  time: 0.2047(78.18)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:09] lb.utils.events INFO:  eta: 0:00:19  iteration: 906/1000  consumed samples: 14496  total_loss: 7.495  time: 0.2045(78.23)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:09] lb.utils.events INFO:  eta: 0:00:19  iteration: 907/1000  consumed samples: 14512  total_loss: 7.495  time: 0.2044(78.27)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:10] lb.utils.events INFO:  eta: 0:00:19  iteration: 908/1000  consumed samples: 14528  total_loss: 7.494  time: 0.2044(78.27)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:10] lb.utils.events INFO:  eta: 0:00:18  iteration: 909/1000  consumed samples: 14544  total_loss: 7.494  time: 0.2044(78.29)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:10] lb.utils.events INFO:  eta: 0:00:18  iteration: 910/1000  consumed samples: 14560  total_loss: 7.491  time: 0.2043(78.32)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:10] lb.utils.events INFO:  eta: 0:00:18  iteration: 911/1000  consumed samples: 14576  total_loss: 7.494  time: 0.2042(78.37)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:10] lb.utils.events INFO:  eta: 0:00:18  iteration: 912/1000  consumed samples: 14592  total_loss: 7.494  time: 0.2040(78.42)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:11] lb.utils.events INFO:  eta: 0:00:18  iteration: 913/1000  consumed samples: 14608  total_loss: 7.494  time: 0.2040(78.43)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:11] lb.utils.events INFO:  eta: 0:00:17  iteration: 914/1000  consumed samples: 14624  total_loss: 7.491  time: 0.2040(78.43)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:11] lb.utils.events INFO:  eta: 0:00:17  iteration: 915/1000  consumed samples: 14640  total_loss: 7.486  time: 0.2040(78.45)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:11] lb.utils.events INFO:  eta: 0:00:17  iteration: 916/1000  consumed samples: 14656  total_loss: 7.48  time: 0.2039(78.48)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:12] lb.utils.events INFO:  eta: 0:00:17  iteration: 917/1000  consumed samples: 14672  total_loss: 7.486  time: 0.2037(78.53)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:12] lb.utils.events INFO:  eta: 0:00:16  iteration: 918/1000  consumed samples: 14688  total_loss: 7.486  time: 0.2036(78.57)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:12] lb.utils.events INFO:  eta: 0:00:16  iteration: 919/1000  consumed samples: 14704  total_loss: 7.491  time: 0.2035(78.62)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:12] lb.utils.events INFO:  eta: 0:00:16  iteration: 920/1000  consumed samples: 14720  total_loss: 7.486  time: 0.2034(78.66)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:13] lb.utils.events INFO:  eta: 0:00:16  iteration: 921/1000  consumed samples: 14736  total_loss: 7.48  time: 0.2033(78.70)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:13] lb.utils.events INFO:  eta: 0:00:16  iteration: 922/1000  consumed samples: 14752  total_loss: 7.48  time: 0.2033(78.71)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:13] lb.utils.events INFO:  eta: 0:00:15  iteration: 923/1000  consumed samples: 14768  total_loss: 7.478  time: 0.2032(78.76)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:13] lb.utils.events INFO:  eta: 0:00:15  iteration: 924/1000  consumed samples: 14784  total_loss: 7.478  time: 0.2030(78.81)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:13] lb.utils.events INFO:  eta: 0:00:15  iteration: 925/1000  consumed samples: 14800  total_loss: 7.478  time: 0.2029(78.86)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:14] lb.utils.events INFO:  eta: 0:00:15  iteration: 926/1000  consumed samples: 14816  total_loss: 7.477  time: 0.2028(78.91)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:14] lb.utils.events INFO:  eta: 0:00:15  iteration: 927/1000  consumed samples: 14832  total_loss: 7.475  time: 0.2026(78.96)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:14] lb.utils.events INFO:  eta: 0:00:14  iteration: 928/1000  consumed samples: 14848  total_loss: 7.477  time: 0.2026(78.97)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:14] lb.utils.events INFO:  eta: 0:00:14  iteration: 929/1000  consumed samples: 14864  total_loss: 7.477  time: 0.2026(78.97)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:15] lb.utils.events INFO:  eta: 0:00:14  iteration: 930/1000  consumed samples: 14880  total_loss: 7.475  time: 0.2026(78.98)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:15] lb.utils.events INFO:  eta: 0:00:14  iteration: 931/1000  consumed samples: 14896  total_loss: 7.475  time: 0.2026(78.99)  data_time: 0.0038  lr: 9.00e-05  
[01/20 10:41:15] lb.utils.events INFO:  eta: 0:00:14  iteration: 932/1000  consumed samples: 14912  total_loss: 7.472  time: 0.2024(79.04)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:15] lb.utils.events INFO:  eta: 0:00:13  iteration: 933/1000  consumed samples: 14928  total_loss: 7.472  time: 0.2023(79.09)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:15] lb.utils.events INFO:  eta: 0:00:13  iteration: 934/1000  consumed samples: 14944  total_loss: 7.47  time: 0.2022(79.12)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:16] lb.utils.events INFO:  eta: 0:00:13  iteration: 935/1000  consumed samples: 14960  total_loss: 7.47  time: 0.2021(79.15)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:16] lb.utils.events INFO:  eta: 0:00:13  iteration: 936/1000  consumed samples: 14976  total_loss: 7.468  time: 0.2020(79.20)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:16] lb.utils.events INFO:  eta: 0:00:12  iteration: 937/1000  consumed samples: 14992  total_loss: 7.466  time: 0.2020(79.20)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:16] lb.utils.events INFO:  eta: 0:00:12  iteration: 938/1000  consumed samples: 15008  total_loss: 7.466  time: 0.2019(79.23)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:17] lb.utils.events INFO:  eta: 0:00:12  iteration: 939/1000  consumed samples: 15024  total_loss: 7.465  time: 0.2018(79.28)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:17] lb.utils.events INFO:  eta: 0:00:12  iteration: 940/1000  consumed samples: 15040  total_loss: 7.465  time: 0.2017(79.31)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:17] lb.utils.events INFO:  eta: 0:00:12  iteration: 941/1000  consumed samples: 15056  total_loss: 7.466  time: 0.2016(79.36)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:17] lb.utils.events INFO:  eta: 0:00:11  iteration: 942/1000  consumed samples: 15072  total_loss: 7.464  time: 0.2015(79.40)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:17] lb.utils.events INFO:  eta: 0:00:11  iteration: 943/1000  consumed samples: 15088  total_loss: 7.464  time: 0.2014(79.45)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:18] lb.utils.events INFO:  eta: 0:00:11  iteration: 944/1000  consumed samples: 15104  total_loss: 7.463  time: 0.2013(79.50)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:18] lb.utils.events INFO:  eta: 0:00:11  iteration: 945/1000  consumed samples: 15120  total_loss: 7.461  time: 0.2011(79.55)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:18] lb.utils.events INFO:  eta: 0:00:11  iteration: 946/1000  consumed samples: 15136  total_loss: 7.458  time: 0.2010(79.60)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:18] lb.utils.events INFO:  eta: 0:00:10  iteration: 947/1000  consumed samples: 15152  total_loss: 7.455  time: 0.2009(79.65)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:19] lb.utils.events INFO:  eta: 0:00:10  iteration: 948/1000  consumed samples: 15168  total_loss: 7.455  time: 0.2008(79.69)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:19] lb.utils.events INFO:  eta: 0:00:10  iteration: 949/1000  consumed samples: 15184  total_loss: 7.455  time: 0.2008(79.69)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:19] lb.utils.events INFO:  eta: 0:00:10  iteration: 950/1000  consumed samples: 15200  total_loss: 7.455  time: 0.2007(79.71)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:19] lb.utils.events INFO:  eta: 0:00:10  iteration: 951/1000  consumed samples: 15216  total_loss: 7.454  time: 0.2007(79.72)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:19] lb.utils.events INFO:  eta: 0:00:09  iteration: 952/1000  consumed samples: 15232  total_loss: 7.454  time: 0.2007(79.72)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:20] lb.utils.events INFO:  eta: 0:00:09  iteration: 953/1000  consumed samples: 15248  total_loss: 7.455  time: 0.2007(79.73)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:20] lb.utils.events INFO:  eta: 0:00:09  iteration: 954/1000  consumed samples: 15264  total_loss: 7.455  time: 0.2006(79.78)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:20] lb.utils.events INFO:  eta: 0:00:09  iteration: 955/1000  consumed samples: 15280  total_loss: 7.458  time: 0.2005(79.81)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:20] lb.utils.events INFO:  eta: 0:00:09  iteration: 956/1000  consumed samples: 15296  total_loss: 7.458  time: 0.2004(79.86)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:21] lb.utils.events INFO:  eta: 0:00:08  iteration: 957/1000  consumed samples: 15312  total_loss: 7.458  time: 0.2004(79.86)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:21] lb.utils.events INFO:  eta: 0:00:08  iteration: 958/1000  consumed samples: 15328  total_loss: 7.455  time: 0.2002(79.91)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:21] lb.utils.events INFO:  eta: 0:00:08  iteration: 959/1000  consumed samples: 15344  total_loss: 7.454  time: 0.2002(79.93)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:21] lb.utils.events INFO:  eta: 0:00:08  iteration: 960/1000  consumed samples: 15360  total_loss: 7.455  time: 0.2000(79.98)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:22] lb.utils.events INFO:  eta: 0:00:07  iteration: 961/1000  consumed samples: 15376  total_loss: 7.454  time: 0.1999(80.03)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:22] lb.utils.events INFO:  eta: 0:00:07  iteration: 962/1000  consumed samples: 15392  total_loss: 7.454  time: 0.1998(80.07)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:22] lb.utils.events INFO:  eta: 0:00:07  iteration: 963/1000  consumed samples: 15408  total_loss: 7.454  time: 0.1997(80.11)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:22] lb.utils.events INFO:  eta: 0:00:07  iteration: 964/1000  consumed samples: 15424  total_loss: 7.455  time: 0.1996(80.16)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:23] lb.utils.events INFO:  eta: 0:00:07  iteration: 965/1000  consumed samples: 15440  total_loss: 7.458  time: 0.1995(80.21)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 966/1000  consumed samples: 15456  total_loss: 7.455  time: 0.1994(80.25)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 967/1000  consumed samples: 15472  total_loss: 7.454  time: 0.1993(80.29)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 968/1000  consumed samples: 15488  total_loss: 7.452  time: 0.1992(80.32)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:23] lb.utils.events INFO:  eta: 0:00:06  iteration: 969/1000  consumed samples: 15504  total_loss: 7.449  time: 0.1991(80.37)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:24] lb.utils.events INFO:  eta: 0:00:06  iteration: 970/1000  consumed samples: 15520  total_loss: 7.445  time: 0.1990(80.41)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:24] lb.utils.events INFO:  eta: 0:00:05  iteration: 971/1000  consumed samples: 15536  total_loss: 7.449  time: 0.1990(80.42)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:24] lb.utils.events INFO:  eta: 0:00:05  iteration: 972/1000  consumed samples: 15552  total_loss: 7.449  time: 0.1989(80.45)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:24] lb.utils.events INFO:  eta: 0:00:05  iteration: 973/1000  consumed samples: 15568  total_loss: 7.449  time: 0.1989(80.44)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:25] lb.utils.events INFO:  eta: 0:00:05  iteration: 974/1000  consumed samples: 15584  total_loss: 7.449  time: 0.1989(80.46)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:25] lb.utils.events INFO:  eta: 0:00:05  iteration: 975/1000  consumed samples: 15600  total_loss: 7.445  time: 0.1988(80.47)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:25] lb.utils.events INFO:  eta: 0:00:04  iteration: 976/1000  consumed samples: 15616  total_loss: 7.449  time: 0.1987(80.52)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:25] lb.utils.events INFO:  eta: 0:00:04  iteration: 977/1000  consumed samples: 15632  total_loss: 7.445  time: 0.1986(80.56)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:25] lb.utils.events INFO:  eta: 0:00:04  iteration: 978/1000  consumed samples: 15648  total_loss: 7.445  time: 0.1986(80.58)  data_time: 0.0039  lr: 9.00e-05  
[01/20 10:41:26] lb.utils.events INFO:  eta: 0:00:04  iteration: 979/1000  consumed samples: 15664  total_loss: 7.445  time: 0.1985(80.62)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:26] lb.utils.events INFO:  eta: 0:00:03  iteration: 980/1000  consumed samples: 15680  total_loss: 7.443  time: 0.1984(80.66)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:26] lb.utils.events INFO:  eta: 0:00:03  iteration: 981/1000  consumed samples: 15696  total_loss: 7.443  time: 0.1983(80.67)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:26] lb.utils.events INFO:  eta: 0:00:03  iteration: 982/1000  consumed samples: 15712  total_loss: 7.445  time: 0.1982(80.71)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:27] lb.utils.events INFO:  eta: 0:00:03  iteration: 983/1000  consumed samples: 15728  total_loss: 7.445  time: 0.1982(80.72)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:27] lb.utils.events INFO:  eta: 0:00:03  iteration: 984/1000  consumed samples: 15744  total_loss: 7.443  time: 0.1981(80.76)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:27] lb.utils.events INFO:  eta: 0:00:02  iteration: 985/1000  consumed samples: 15760  total_loss: 7.443  time: 0.1981(80.78)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:27] lb.utils.events INFO:  eta: 0:00:02  iteration: 986/1000  consumed samples: 15776  total_loss: 7.445  time: 0.1980(80.82)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:27] lb.utils.events INFO:  eta: 0:00:02  iteration: 987/1000  consumed samples: 15792  total_loss: 7.445  time: 0.1978(80.87)  data_time: 0.0040  lr: 9.00e-05  
[01/20 10:41:28] lb.utils.events INFO:  eta: 0:00:02  iteration: 988/1000  consumed samples: 15808  total_loss: 7.443  time: 0.1977(80.92)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:28] lb.utils.events INFO:  eta: 0:00:02  iteration: 989/1000  consumed samples: 15824  total_loss: 7.441  time: 0.1977(80.93)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:28] lb.utils.events INFO:  eta: 0:00:01  iteration: 990/1000  consumed samples: 15840  total_loss: 7.441  time: 0.1976(80.97)  data_time: 0.0041  lr: 9.00e-05  
[01/20 10:41:28] lb.utils.events INFO:  eta: 0:00:01  iteration: 991/1000  consumed samples: 15856  total_loss: 7.443  time: 0.1975(81.02)  data_time: 0.0042  lr: 9.00e-05  
[01/20 10:41:29] lb.utils.events INFO:  eta: 0:00:01  iteration: 992/1000  consumed samples: 15872  total_loss: 7.443  time: 0.1974(81.07)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:29] lb.utils.events INFO:  eta: 0:00:01  iteration: 993/1000  consumed samples: 15888  total_loss: 7.443  time: 0.1973(81.08)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:29] lb.utils.events INFO:  eta: 0:00:01  iteration: 994/1000  consumed samples: 15904  total_loss: 7.441  time: 0.1972(81.13)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:29] lb.utils.events INFO:  eta: 0:00:00  iteration: 995/1000  consumed samples: 15920  total_loss: 7.441  time: 0.1971(81.17)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:30] lb.utils.events INFO:  eta: 0:00:00  iteration: 996/1000  consumed samples: 15936  total_loss: 7.441  time: 0.1970(81.21)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:30] lb.utils.events INFO:  eta: 0:00:00  iteration: 997/1000  consumed samples: 15952  total_loss: 7.44  time: 0.1970(81.23)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:30] lb.utils.events INFO:  eta: 0:00:00  iteration: 998/1000  consumed samples: 15968  total_loss: 7.438  time: 0.1969(81.26)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:30] lb.utils.events INFO:  eta: 0:00:00  iteration: 999/1000  consumed samples: 15984  total_loss: 7.436  time: 0.1968(81.31)  data_time: 0.0043  lr: 9.00e-05  
[01/20 10:41:30] lb.utils.checkpoint INFO: Saving checkpoint to ./demo_output/test_config/model_final
[01/20 10:41:31] lb.trainer.hooks INFO: Overall training speed: 998 iterations in 0:03:16 (0.1968 s / it)
[01/20 10:41:31] lb.trainer.hooks INFO: Total training time: 0:03:44 (0:00:28 on hooks)
[01/20 10:51:19] libai INFO: Rank of current process: 0. World size: 1
[01/20 10:51:19] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/20 10:51:19] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 10:51:19] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 10:51:19] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/20 10:51:19] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/20 10:51:19] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 10:51:22] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 10:51:22] libai INFO: Loadding megatron weight
[01/20 10:51:22] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/20 10:51:23] lb.data.build INFO: > building train, validation, and test datasets ...
[01/20 10:51:23] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/20 10:51:23] lb.data.build INFO:     train:      16000
[01/20 10:51:23] lb.data.build INFO:     validation: 160000
[01/20 10:51:23] lb.data.build INFO:     test:       160000
[01/20 10:51:23] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/20 10:51:23] lb.data.dataset_utils INFO:  > building dataset index ...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     reading sizes...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     reading pointers...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     reading document index...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/20 10:51:23] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/20 10:51:23] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.049219 seconds
[01/20 10:51:23] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/20 10:51:23] lb.data.dataset_utils INFO:     number of documents: 50000
[01/20 10:51:23] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/20 10:51:23] lb.data.dataset_utils INFO:  > dataset split:
[01/20 10:51:23] lb.data.dataset_utils INFO:     train:
[01/20 10:51:23] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/20 10:51:23] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/20 10:51:23] lb.data.dataset_utils INFO:     validation:
[01/20 10:51:23] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/20 10:51:23] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/20 10:51:23] lb.data.dataset_utils INFO:     test:
[01/20 10:51:23] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/20 10:51:23] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/20 10:51:23] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/20 10:51:23] lb.data.dataset_utils INFO:     loaded indexed file in 0.020 seconds
[01/20 10:51:23] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/20 10:51:23] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/20 10:51:23] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/20 10:51:23] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/20 10:51:23] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/20 10:51:23] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/20 10:51:23] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/20 10:51:23] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/20 10:51:24] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 10:56:28] libai INFO: Rank of current process: 0. World size: 1
[01/20 10:56:28] libai INFO: Command line arguments: Namespace(config_file='configs/compare_loss.py', eval_only=False, opts=['train.log_period=1', 'graph.enabled=False'], resume=False)
[01/20 10:56:28] libai INFO: Contents of args.config_file=configs/compare_loss.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mnlp_data[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mscheduler[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m

[38;5;242m# Set all dropout to 0.[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mattention_probs_dropout_prob[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0[39m

[38;5;242m# Set matched model arguments[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_iter[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1000[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mlog_period[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1[39m

[38;5;15moptim[39m[38;5;197m.[39m[38;5;15mlr[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m0.0001[39m

[38;5;242m# Set a constant lr scheduler after warmup[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15m_target_[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mWarmupMultiStepLR[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mwarmup_iters[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m10[39m
[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmilestones[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15m[[39m[38;5;141m1000000[39m[38;5;15m][39m
[38;5;81mdel[39m[38;5;15m [39m[38;5;15mscheduler[39m[38;5;197m.[39m[38;5;15mmax_iters[39m

[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mseq_length[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mdataset_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mstandard_bert[39m[38;5;186m"[39m
[38;5;15mdata[39m[38;5;197m.[39m[38;5;15mtokenizer_type[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;186m"[39m[38;5;186mBertCNWWMTokenizer[39m[38;5;186m"[39m

[38;5;242m# fmt: off[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 10:56:28] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 10:56:28] lb.tokenizer.tokenizer INFO: > building BertCNWWMTokenizer tokenizer ...
[01/20 10:56:28] lb.tokenizer.tokenizer INFO:  > padded vocab (size: 21130) with 118 dummy tokens (new size: 21248)
[01/20 10:56:28] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 10:56:31] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.0, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.0, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.0
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 10:56:31] libai INFO: Loadding megatron weight
[01/20 10:56:31] lb.utils.load_megatron_weight INFO: Loading megatron weight
[01/20 10:56:32] lb.data.build INFO: > building train, validation, and test datasets ...
[01/20 10:56:32] lb.data.build INFO:  > datasets target sizes (minimum size):
[01/20 10:56:32] lb.data.build INFO:     train:      16000
[01/20 10:56:32] lb.data.build INFO:     validation: 160000
[01/20 10:56:32] lb.data.build INFO:     test:       160000
[01/20 10:56:32] lb.data.dataset_utils INFO: > building train, validation, and test datasets 
[01/20 10:56:32] lb.data.dataset_utils INFO:  > building dataset index ...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     warming up index mmap file...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     reading sizes...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     reading pointers...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     reading document index...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     warming up data mmap file...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     creating numpy buffer of mmap...
[01/20 10:56:32] lb.data.indexed_dataset INFO:     creating memory view of numpy buffer...
[01/20 10:56:32] lb.data.dataset_utils INFO:  > finished creating indexed dataset in 0.047706 seconds
[01/20 10:56:32] lb.data.dataset_utils INFO:  > indexed dataset stats:
[01/20 10:56:32] lb.data.dataset_utils INFO:     number of documents: 50000
[01/20 10:56:32] lb.data.dataset_utils INFO:     number of sentences: 1249934
[01/20 10:56:32] lb.data.dataset_utils INFO:  > dataset split:
[01/20 10:56:32] lb.data.dataset_utils INFO:     train:
[01/20 10:56:32] lb.data.dataset_utils INFO:      document indices in [0, 47450) total of 47450 documents
[01/20 10:56:32] lb.data.dataset_utils INFO:      sentence indices in [0, 1188464) total of 1188464 sentences
[01/20 10:56:32] lb.data.dataset_utils INFO:     validation:
[01/20 10:56:32] lb.data.dataset_utils INFO:      document indices in [47450, 49950) total of 2500 documents
[01/20 10:56:32] lb.data.dataset_utils INFO:      sentence indices in [1188464, 1248643) total of 60179 sentences
[01/20 10:56:32] lb.data.dataset_utils INFO:     test:
[01/20 10:56:32] lb.data.dataset_utils INFO:      document indices in [49950, 50000) total of 50 documents
[01/20 10:56:32] lb.data.dataset_utils INFO:      sentence indices in [1248643, 1249934) total of 1291 sentences
[01/20 10:56:32] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_train_indexmap_16000mns_509msl_0.10ssp_1234s.npy
[01/20 10:56:32] lb.data.dataset_utils INFO:     loaded indexed file in 0.024 seconds
[01/20 10:56:32] lb.data.dataset_utils INFO:     total number of samples: 113036
[01/20 10:56:32] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_valid_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/20 10:56:32] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/20 10:56:32] lb.data.dataset_utils INFO:     total number of samples: 164791
[01/20 10:56:32] lb.data.dataset_utils INFO:  > loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_test_indexmap_160000mns_509msl_0.10ssp_1234s.npy
[01/20 10:56:32] lb.data.dataset_utils INFO:     loaded indexed file in 0.001 seconds
[01/20 10:56:32] lb.data.dataset_utils INFO:     total number of samples: 160043
[01/20 10:56:32] lb.data.dataset_utils INFO: > finished creating standard_bert datasets ...
[01/20 10:56:33] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 11:24:50] libai INFO: Rank of current process: 0. World size: 1
[01/20 11:24:50] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 11:24:50] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# fmt: off[39m
[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 11:24:50] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 11:24:50] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 11:24:55] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 11:27:05] libai INFO: Rank of current process: 0. World size: 1
[01/20 11:27:05] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 11:27:05] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# fmt: off[39m
[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 11:27:05] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 11:27:05] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 11:27:10] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 14:10:42] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:10:42] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:10:42] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# fmt: off[39m
[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 14:10:42] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:10:42] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:10:48] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 14:24:28] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:24:28] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:24:28] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# fmt: off[39m
[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 14:24:28] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:24:28] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:24:34] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 14:24:34] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 14:24:34] lb.config.instantiate ERROR: Error when instantiating libai.data.bert_dataset.BertDataset!
[01/20 14:26:27] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:26:27] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:26:27] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# fmt: off[39m
[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 14:26:27] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:26:27] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:26:33] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 14:26:33] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 14:29:09] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:29:09] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:29:09] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenizer[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# fmt: off[39m
[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m
[38;5;242m# fmt: on[39m

[01/20 14:29:09] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:29:10] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:46:14] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:46:14] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:46:14] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenizer[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 14:46:14] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:46:15] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:47:02] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:47:02] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:47:03] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdata[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenizer[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 14:47:03] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:47:03] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:47:03] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 14:47:09] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 14:47:09] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 14:56:46] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:56:46] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:56:46] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 14:56:46] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:56:47] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:58:08] libai INFO: Rank of current process: 0. World size: 1
[01/20 14:58:08] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 14:58:08] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 14:58:08] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 14:58:08] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 14:58:17] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 14:58:34] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=30522, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:03:59] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:03:59] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:03:59] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:03:59] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:03:59] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:04:03] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:04:17] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:04:35] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:04:35] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:04:36] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:04:36] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:04:36] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:04:36] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:04:41] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:04:41] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:09:24] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:09:24] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:09:24] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:09:24] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:09:24] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:09:25] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:09:30] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:09:30] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:09:30] lb.config.instantiate ERROR: Error when instantiating libai.data.bert_dataset.BertDataset!
[01/20 15:15:09] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:15:09] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:15:10] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:15:10] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:15:10] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:15:10] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:15:15] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:15:15] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.142362 seconds
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:15:15] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: WARNING: could not find index map file /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy, building the indices on rank 0 ...
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: building samples index mapping for /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence ...
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: done building samples index maping
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: saved the index mapping in /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: elapsed time to build and save samples mapping (seconds): 0.048312
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.002 seconds
[01/20 15:15:15] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:15:15] lb.config.instantiate ERROR: Error when instantiating libai.data.build.build_nlp_train_val_test_loader!
[01/20 15:32:35] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:32:35] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:32:35] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:32:35] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:32:36] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:32:36] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:32:41] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:32:41] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.116868 seconds
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:32:41] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:32:41] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:32:41] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.022 seconds
[01/20 15:32:41] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:33:01] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:33:01] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:33:01] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:33:01] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:33:01] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:33:01] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:33:06] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:33:06] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.102182 seconds
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:33:06] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:33:06] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:33:06] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.020 seconds
[01/20 15:33:06] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:34:55] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:34:55] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.log_period=1'], resume=False)
[01/20 15:34:56] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:34:56] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:34:56] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:34:56] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:35:01] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:35:01] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.132624 seconds
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:35:01] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:35:01] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:35:01] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.020 seconds
[01/20 15:35:01] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:35:01] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:35:10] lb.utils.events INFO:  iteration: 1/10000  consumed samples: 32  total_loss: 11  data_time: 1.2932  lr: 0.00e+00  
[01/20 15:35:11] lb.utils.events INFO:  eta: 3:51:14  iteration: 2/10000  consumed samples: 64  total_loss: 10.86  data_time: 0.6531  lr: 1.00e-06  
[01/20 15:35:13] lb.utils.events INFO:  eta: 3:48:18  iteration: 3/10000  consumed samples: 96  total_loss: 10.82  time: 1.3704(23.35)  data_time: 0.4368  lr: 2.00e-06  
[01/20 15:35:14] lb.utils.events INFO:  eta: 3:48:16  iteration: 4/10000  consumed samples: 128  total_loss: 10.81  time: 1.3703(23.35)  data_time: 0.3296  lr: 3.00e-06  
[01/20 15:35:15] lb.utils.events INFO:  eta: 3:48:16  iteration: 5/10000  consumed samples: 160  total_loss: 10.8  time: 1.3729(23.31)  data_time: 0.2652  lr: 4.00e-06  
[01/20 15:35:17] lb.utils.events INFO:  eta: 3:48:49  iteration: 6/10000  consumed samples: 192  total_loss: 10.77  time: 1.3740(23.29)  data_time: 0.2226  lr: 5.00e-06  
[01/20 15:35:18] lb.utils.events INFO:  eta: 3:49:23  iteration: 7/10000  consumed samples: 224  total_loss: 10.74  time: 1.3753(23.27)  data_time: 0.1913  lr: 6.00e-06  
[01/20 15:35:20] lb.utils.events INFO:  eta: 3:49:25  iteration: 8/10000  consumed samples: 256  total_loss: 10.73  time: 1.3773(23.23)  data_time: 0.1686  lr: 7.00e-06  
[01/20 15:35:21] lb.utils.events INFO:  eta: 3:49:26  iteration: 9/10000  consumed samples: 288  total_loss: 10.71  time: 1.3788(23.21)  data_time: 0.1510  lr: 8.00e-06  
[01/20 15:35:22] lb.utils.events INFO:  eta: 3:49:37  iteration: 10/10000  consumed samples: 320  total_loss: 10.67  time: 1.3796(23.20)  data_time: 0.1370  lr: 9.00e-06  
[01/20 15:35:24] lb.utils.events INFO:  eta: 3:49:48  iteration: 11/10000  consumed samples: 352  total_loss: 10.63  time: 1.3805(23.18)  data_time: 0.1249  lr: 1.00e-05  
[01/20 15:35:25] lb.utils.events INFO:  eta: 3:50:09  iteration: 12/10000  consumed samples: 384  total_loss: 10.62  time: 1.3821(23.15)  data_time: 0.1150  lr: 1.10e-05  
[01/20 15:35:27] lb.utils.events INFO:  eta: 3:50:30  iteration: 13/10000  consumed samples: 416  total_loss: 10.6  time: 1.3825(23.15)  data_time: 0.1068  lr: 1.20e-05  
[01/20 15:35:28] lb.utils.events INFO:  eta: 3:50:39  iteration: 14/10000  consumed samples: 448  total_loss: 10.56  time: 1.3831(23.14)  data_time: 0.1002  lr: 1.30e-05  
[01/20 15:35:29] lb.utils.events INFO:  eta: 3:50:47  iteration: 15/10000  consumed samples: 480  total_loss: 10.53  time: 1.3846(23.11)  data_time: 0.0939  lr: 1.40e-05  
[01/20 15:38:19] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:38:19] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 15:38:19] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:38:19] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:38:19] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:38:19] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:38:24] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:38:24] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.095764 seconds
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:38:24] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:38:24] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:38:24] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.005 seconds
[01/20 15:38:24] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:38:24] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:38:59] libai INFO: Rank of current process: 0. World size: 1
[01/20 15:38:59] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 15:38:59] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:38:59] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:38:59] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:38:59] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:39:05] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:39:05] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.105533 seconds
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:39:05] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:39:05] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:39:05] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.007 seconds
[01/20 15:39:05] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:39:05] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:39:40] lb.utils.events INFO:  eta: 4:00:21  iteration: 20/10000  consumed samples: 640  total_loss: 10.3  time: 1.4456(22.14)  data_time: 0.0413  lr: 1.90e-05  
[01/20 15:42:39] libai INFO: Rank of current process: 0. World size: 4
[01/20 15:42:39] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 15:42:39] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:42:39] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:42:39] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:42:39] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:42:45] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:42:45] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.102963 seconds
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:42:45] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:42:45] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:42:45] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.007 seconds
[01/20 15:42:45] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:42:46] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:43:29] lb.utils.events INFO:  eta: 4:09:03  iteration: 20/10000  consumed samples: 2560  total_loss: 10.22  time: 1.4970(85.50)  data_time: 0.0498  lr: 1.90e-05  
[01/20 15:43:59] lb.utils.events INFO:  eta: 4:12:11  iteration: 40/10000  consumed samples: 5120  total_loss: 9.945  time: 1.5181(84.31)  data_time: 0.0089  lr: 3.90e-05  
[01/20 15:45:37] libai INFO: Rank of current process: 0. World size: 4
[01/20 15:45:37] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 15:45:37] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:45:37] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:45:37] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:45:37] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:45:44] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:45:44] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.110968 seconds
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:45:44] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:45:44] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:45:44] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.007 seconds
[01/20 15:45:44] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:45:44] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:45:47] lb.trainer.trainer ERROR: Exception during training:
Traceback (most recent call last):
  File "/workspace/libai/libai/trainer/trainer.py", line 136, in train
    self.run_step()
  File "/workspace/libai/libai/trainer/default.py", line 350, in run_step
    self._trainer.run_step(self.get_batch)
  File "/workspace/libai/libai/trainer/trainer.py", line 310, in run_step
    losses = self.graph(*data)
  File "/home/dev/.local/lib/python3.6/site-packages/oneflow/nn/graph/graph.py", line 258, in __call__
    self._compile(*args)
  File "/home/dev/.local/lib/python3.6/site-packages/oneflow/nn/graph/graph.py", line 497, in _compile
    eager_outputs = self._build_graph(*args)
  File "/home/dev/.local/lib/python3.6/site-packages/oneflow/nn/graph/graph.py", line 618, in _build_graph
    oneflow._oneflow_internal.CurJobBuildAndInferCtx_Complete()
IndexError: vector::_M_range_check: __n (which is 1) >= this->size() (which is 1)
[01/20 15:45:47] lb.trainer.hooks INFO: Total training time: 0:00:02 (0:00:00 on hooks)
[01/20 15:49:56] libai INFO: Rank of current process: 0. World size: 4
[01/20 15:49:56] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 15:49:57] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:49:57] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:49:57] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:49:57] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:50:03] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:50:03] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.110487 seconds
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:50:03] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:50:03] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:50:03] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.006 seconds
[01/20 15:50:03] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:50:04] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:52:47] libai INFO: Rank of current process: 0. World size: 4
[01/20 15:52:47] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 15:52:47] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:52:47] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:52:47] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:52:47] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 15:52:54] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:52:54] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.127017 seconds
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:52:54] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:52:54] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:52:54] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.007 seconds
[01/20 15:52:54] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:52:54] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:56:03] lb.utils.events INFO:  eta: 2:02:11  iteration: 20/10000  consumed samples: 2560  total_loss: 10.78  time: 0.7396(173.07)  data_time: 0.0750  lr: 1.90e-05  
[01/20 15:57:45] libai INFO: Rank of current process: 0. World size: 4
[01/20 15:57:45] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=4'], resume=False)
[01/20 15:57:45] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 15:57:45] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 15:57:45] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 15:57:45] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 376 dummy tokens (new size: 21504)
[01/20 15:57:50] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21504, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 15:57:50] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.113508 seconds
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 15:57:50] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 15:57:50] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 15:57:50] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.006 seconds
[01/20 15:57:50] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 15:57:50] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 15:59:51] lb.utils.events INFO:  eta: 0:36:50  iteration: 20/10000  consumed samples: 320  total_loss: 10.73  time: 0.2365(67.66)  data_time: 0.0583  lr: 1.90e-05  
[01/20 15:59:55] lb.utils.events INFO:  eta: 0:36:40  iteration: 40/10000  consumed samples: 640  total_loss: 10.67  time: 0.2304(69.44)  data_time: 0.0294  lr: 3.90e-05  
[01/20 16:00:00] lb.utils.events INFO:  eta: 0:35:43  iteration: 60/10000  consumed samples: 960  total_loss: 10.45  time: 0.2250(71.10)  data_time: 0.0286  lr: 5.90e-05  
[01/20 16:00:04] lb.utils.events INFO:  eta: 0:35:27  iteration: 80/10000  consumed samples: 1280  total_loss: 10.31  time: 0.2208(72.45)  data_time: 0.0262  lr: 7.90e-05  
[01/20 16:00:08] lb.utils.events INFO:  eta: 0:35:19  iteration: 100/10000  consumed samples: 1600  total_loss: 10.14  time: 0.2205(72.57)  data_time: 0.0236  lr: 9.90e-05  
[01/20 16:00:13] lb.utils.events INFO:  eta: 0:35:12  iteration: 120/10000  consumed samples: 1920  total_loss: 9.898  time: 0.2205(72.56)  data_time: 0.0237  lr: 9.79e-05  
[01/20 16:00:17] lb.utils.events INFO:  eta: 0:35:20  iteration: 140/10000  consumed samples: 2240  total_loss: 9.676  time: 0.2210(72.40)  data_time: 0.0253  lr: 9.67e-05  
[01/20 16:00:22] lb.utils.events INFO:  eta: 0:35:08  iteration: 160/10000  consumed samples: 2560  total_loss: 9.394  time: 0.2198(72.80)  data_time: 0.0238  lr: 9.52e-05  
[01/20 16:00:26] lb.utils.events INFO:  eta: 0:35:09  iteration: 180/10000  consumed samples: 2880  total_loss: 9.117  time: 0.2203(72.62)  data_time: 0.0240  lr: 9.36e-05  
[01/20 16:00:31] lb.utils.events INFO:  eta: 0:35:11  iteration: 200/10000  consumed samples: 3200  total_loss: 8.827  time: 0.2223(71.99)  data_time: 0.0246  lr: 9.18e-05  
[01/20 16:00:36] lb.utils.events INFO:  eta: 0:35:07  iteration: 220/10000  consumed samples: 3520  total_loss: 8.346  time: 0.2231(71.71)  data_time: 0.0228  lr: 8.99e-05  
[01/20 16:00:40] lb.utils.events INFO:  eta: 0:35:03  iteration: 240/10000  consumed samples: 3840  total_loss: 8.182  time: 0.2239(71.47)  data_time: 0.0246  lr: 8.78e-05  
[01/20 16:00:45] lb.utils.events INFO:  eta: 0:35:02  iteration: 260/10000  consumed samples: 4160  total_loss: 8.111  time: 0.2242(71.38)  data_time: 0.0236  lr: 8.56e-05  
[01/20 16:00:49] lb.utils.events INFO:  eta: 0:34:53  iteration: 280/10000  consumed samples: 4480  total_loss: 8.055  time: 0.2233(71.66)  data_time: 0.0230  lr: 8.32e-05  
[01/20 16:00:53] lb.utils.events INFO:  eta: 0:34:48  iteration: 300/10000  consumed samples: 4800  total_loss: 8.032  time: 0.2229(71.77)  data_time: 0.0229  lr: 8.07e-05  
[01/20 16:00:58] lb.utils.events INFO:  eta: 0:34:43  iteration: 320/10000  consumed samples: 5120  total_loss: 7.998  time: 0.2227(71.83)  data_time: 0.0235  lr: 7.80e-05  
[01/20 16:01:11] libai INFO: Rank of current process: 0. World size: 4
[01/20 16:01:11] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 16:01:11] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:01:11] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:01:11] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:01:11] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:01:16] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:01:16] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.122158 seconds
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:01:16] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:01:16] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:01:16] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.007 seconds
[01/20 16:01:16] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:01:16] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 16:04:36] libai INFO: Rank of current process: 0. World size: 4
[01/20 16:04:36] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 16:04:36] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:04:36] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:04:36] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:04:36] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:04:40] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:04:40] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:04:40] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:04:40] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:04:40] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:04:40] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:04:40] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:04:40] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:04:41] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:04:41] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:04:41] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.107256 seconds
[01/20 16:04:41] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:04:41] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:04:41] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:04:41] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:04:41] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.005 seconds
[01/20 16:04:41] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:04:41] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 16:08:02] libai INFO: Rank of current process: 0. World size: 4
[01/20 16:08:02] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 16:08:02] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:08:02] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:08:02] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:08:02] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:08:06] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:08:06] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.101259 seconds
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:08:06] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:08:06] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:08:06] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.006 seconds
[01/20 16:08:06] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:08:07] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 16:11:50] libai INFO: Rank of current process: 0. World size: 4
[01/20 16:11:50] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 16:11:50] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:11:50] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:11:50] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:11:50] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:11:55] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=row)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:11:55] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.103814 seconds
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:11:55] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:11:55] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:11:55] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.007 seconds
[01/20 16:11:55] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:11:55] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 16:19:42] libai INFO: Rank of current process: 0. World size: 4
[01/20 16:19:42] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 16:19:42] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mFalse[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:19:42] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:19:42] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:19:42] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:19:47] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=data)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:19:47] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.105975 seconds
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:19:47] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:19:47] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:19:47] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.006 seconds
[01/20 16:19:47] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:19:48] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 16:25:08] lb.utils.events INFO:  eta: 0:44:54  iteration: 20/10000  consumed samples: 640  total_loss: 10.55  time: 0.2714(117.89)  data_time: 0.0531  lr: 1.90e-05  
[01/20 16:25:13] lb.utils.events INFO:  eta: 0:43:18  iteration: 40/10000  consumed samples: 1280  total_loss: 10.36  time: 0.2636(121.39)  data_time: 0.0233  lr: 3.90e-05  
[01/20 16:25:18] lb.utils.events INFO:  eta: 0:42:59  iteration: 60/10000  consumed samples: 1920  total_loss: 10.23  time: 0.2625(121.91)  data_time: 0.0249  lr: 5.90e-05  
[01/20 16:25:24] lb.utils.events INFO:  eta: 0:42:54  iteration: 80/10000  consumed samples: 2560  total_loss: 10.03  time: 0.2627(121.82)  data_time: 0.0242  lr: 7.90e-05  
[01/20 16:25:51] libai INFO: Rank of current process: 0. World size: 4
[01/20 16:25:51] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=['train.dist.tensor_parallel_size=2'], resume=False)
[01/20 16:25:51] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m5[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m384[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mintermediate_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m1536[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mmax_position_embeddings[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m512[39m

[38;5;242m# model.cfg.num_attention_heads = 16[39m
[38;5;242m# model.cfg.hidden_size = 768[39m
[38;5;242m# model.cfg.hidden_layers = 5[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mtrain_micro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;81mTrue[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:25:51] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:25:51] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:25:51] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:25:56] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=384)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=384)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=384)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=384, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=384, out_features=1152, bias=True, parallel=col)
          (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=384, out_features=1536, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=1536, out_features=384, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=384, out_features=384, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=384, out_features=2, bias=True, parallel=data)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:25:56] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.130611 seconds
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:25:56] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:25:56] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:25:56] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.006 seconds
[01/20 16:25:56] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:25:56] lb.trainer.trainer INFO: Starting training from iteration 0
[01/20 16:40:58] libai INFO: Rank of current process: 0. World size: 1
[01/20 16:40:58] libai INFO: Command line arguments: Namespace(config_file='configs/bert_large_pretrain.py', eval_only=False, opts=[], resume=False)
[01/20 16:40:58] libai INFO: Contents of args.config_file=configs/bert_large_pretrain.py:
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mconfig[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mLazyCall[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m.[39m[38;5;15mbert[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mpretrain_model[39m[38;5;15m [39m[38;5;81mas[39m[38;5;15m [39m[38;5;15mmodel[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mtrain[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mtrain[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15moptim[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15moptim[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mscheduler[39m
[38;5;197mfrom[39m[38;5;15m [39m[38;5;15m.[39m[38;5;15mcommon[39m[38;5;15m.[39m[38;5;15mdata[39m[38;5;15m.[39m[38;5;15mbert_dataset[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mdataloader[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mtokenization[39m

[38;5;197mfrom[39m[38;5;15m [39m[38;5;15mlibai[39m[38;5;15m.[39m[38;5;15mmodels[39m[38;5;15m [39m[38;5;197mimport[39m[38;5;15m [39m[38;5;15mBertForPretrainingGraph[39m

[38;5;242m# Bert-large model config[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mnum_attention_heads[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m768[39m
[38;5;15mmodel[39m[38;5;197m.[39m[38;5;15mcfg[39m[38;5;197m.[39m[38;5;15mhidden_layers[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m8[39m

[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mmicro_batch_size[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;141m16[39m

[38;5;242m# Set fp16 ON[39m
[38;5;242m# train.amp.enabled = True[39m

[38;5;242m# LazyCall[39m
[38;5;15mgraph[39m[38;5;15m [39m[38;5;197m=[39m[38;5;15m [39m[38;5;15mdict[39m[38;5;15m([39m
[38;5;15m    [39m[38;5;242m# options for graph or eager mode[39m
[38;5;15m    [39m[38;5;15menabled[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15mdebug[39m[38;5;197m=[39m[38;5;197m-[39m[38;5;141m1[39m[38;5;15m,[39m[38;5;15m  [39m[38;5;242m# debug mode for graph[39m
[38;5;15m    [39m[38;5;15mtrain_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m
[38;5;15m        [39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m
[38;5;15m        [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mTrue[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m    [39m[38;5;15meval_graph[39m[38;5;197m=[39m[38;5;15mLazyCall[39m[38;5;15m([39m[38;5;15mBertForPretrainingGraph[39m[38;5;15m)[39m[38;5;15m([39m[38;5;15mfp16[39m[38;5;197m=[39m[38;5;15mtrain[39m[38;5;197m.[39m[38;5;15mamp[39m[38;5;197m.[39m[38;5;15menabled[39m[38;5;15m,[39m[38;5;15m [39m[38;5;15mis_train[39m[38;5;197m=[39m[38;5;81mFalse[39m[38;5;15m)[39m[38;5;15m,[39m
[38;5;15m)[39m

[01/20 16:40:58] lb.utils.distributed WARNING: Please set `train.dist.pipeline_num_layers` if you want to train with pipeline parallelism, otherwise just ignore it.
[01/20 16:40:58] libai INFO: Full config saved to ./demo_output/test_config/config.yaml
[01/20 16:40:58] lb.tokenizer.build INFO:  > padded vocab (size: 21128) with 120 dummy tokens (new size: 21248)
[01/20 16:41:04] lb.trainer.default INFO: Model:
BertForPreTraining(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (vocab_embeddings): VocabEmbedding(num_embeddings=21248, embedding_dim=768)
      (position_embeddings): Embedding(num_embeddings=512, embedding_dim=768)
      (tokentype_embeddings): Embedding(num_embeddings=2, embedding_dim=768)
      (embedding_dropout): Dropout(p=0.1, inplace=False)
    )
    (extended_attn_mask): BertExtendedAttnMask()
    (encoders): ModuleList(
      (0): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (1): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (2): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (3): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (4): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (5): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (6): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
      (7): TransformerLayer(
        (input_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (self_attention): MultiheadAttention(
          hidden_size=768, num_heads=16, is_cross_attention=False
          (dropout): Dropout(p=0.1, inplace=False)
          (query_key_value): Linear1D(in_features=768, out_features=2304, bias=True, parallel=col)
          (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=row)
        )
        (post_attention_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): MLP(
          bias_gelu_fusion=True, bias_dropout_fusion=True, dropout=0.1
          (dense_h_to_4h): Linear1D(in_features=768, out_features=4096, bias=True, parallel=col)
          (dense_4h_to_h): Linear1D(in_features=4096, out_features=768, bias=True, parallel=row)
        )
      )
    )
    (final_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (pooler): BertPooler(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): Tanh()
    )
  )
  (cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (dense): Linear1D(in_features=768, out_features=768, bias=True, parallel=col)
      (activation_func): GELU()
      (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    )
    (seq_relationship): Linear1D(in_features=768, out_features=2, bias=True, parallel=data)
  )
  (lm_logits): LMLogits()
  (loss_func): BertLoss(
    (lm_loss): ParallelCrossEntropyLoss()
  )
)
[01/20 16:41:04] lb.trainer.default INFO: Prepare training, validating, testing set
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: building dataset index ...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: warming up index mmap file...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: reading sizes...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: reading pointers...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: reading document index...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: warming up data mmap file...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: creating numpy buffer of mmap...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: creating memory view of numpy buffer...
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: inished creating indexed dataset in 0.125442 seconds
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: indexed dataset stats:
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: number of documents: 50000
[01/20 16:41:04] lb.data.data_utils.indexed_dataset INFO: number of sentences: 1249934
[01/20 16:41:04] lb.data.data_utils.reindexed_dataset INFO: loading indexed mapping from /workspace/idea_model/idea_bert/output_data/loss_compara_content_sentence_509msl_0.1ssp_sample_mapping.npy
[01/20 16:41:04] lb.data.data_utils.reindexed_dataset INFO: loaded indexed file in 0.006 seconds
[01/20 16:41:04] lb.data.data_utils.reindexed_dataset INFO: total number of samples: 119067
[01/20 16:41:04] lb.trainer.trainer INFO: Starting training from iteration 0
